
DATABASE
SYSTEM CONCEPTS
SIXTH EDITION
Abraham Silberschatz
Yale University
Henry F. Korth
Lehigh University
S. Sudarshan
Indian Institute of Technology, Bombay
TM
silberschatz6e_fm_i-ii.indd Page i  12/3/09  2:51:50 PM user silberschatz6e_fm_i-ii.indd Page i  12/3/09  2:51:50 PM user /Users/user/Desktop/Temp Work/00November_2009/24:11:09/VYN/silberschatz /Users/user/Desktop/Temp Work/00November_2009/24:11:09/VYN/silberschatz
DATABASE SYSTEM CONCEPTS, SIXTH EDITION
Published by McGraw-Hill, a business unit of The McGraw-Hill Companies, Inc., 1221 Avenue 
of the Americas, New York, NY 10020. Copyright © 2011 by The McGraw-Hill Companies, Inc. 
All rights reserved.  Previous editions ©  2006, 2002, and 1999. No part of this publication may 
be reproduced or distributed in any form or by any means, or stored in a database or retrieval 
system, without the prior written consent of The McGraw-Hill Companies, Inc., including, but 
not limited to, in any network or other electronic storage or transmission, or broadcast for dis-
tance learning.
Some ancillaries, including electronic and print components, may not be available to customers 
outside the United States.
This book is printed on acid-free paper. 
1 2 3 4 5 6 7 8 9 0 DOC/DOC 1 0 9 8 7 6 5 4 3 2 1 0
ISBN 978-0-07-352332-3
MHID 0-07-352332-1
Global Publisher: Raghothaman Srinivasan
Director of Development: Kristine Tibbetts 
Senior Marketing Manager: Curt Reynolds 
Project Manager: Melissa M. Leick
Senior Production Supervisor: Laura Fuller
Design Coordinator: Brenda A. Rolwes
Cover Designer: Studio Montage, St. Louis, Missouri
(USE) Cover Image: © Brand X Pictures/PunchStock 
Compositor: Aptara
®
, Inc.
Typeface: 10/12  Palatino
Printer: R. R. Donnelley
All credits appearing on page or at the end of the book are considered to be an extension of the 
copyright page.
Library of Congress Cataloging-in-Publication Data
Silberschatz, Abraham.
 Database system concepts / Abraham Silberschatz. — 6th ed.
  p. cm.
 ISBN 978-0-07-352332-3 (alk. paper)
1. Database management.  I. Title. 
 QA76.9.D3S5637 2011
 005.74—dc22
 2009039039
The Internet addresses listed in the text were accurate at the time of publication. The inclusion of 
a Web site does not indicate an endorsement by the authors of McGraw-Hill, and McGraw-Hill 
does not guarantee the accuracy of the information presented at these sites.
www.mhhe.com
TM
silberschatz6e_fm_i-ii.indd Page ii  12/3/09  2:51:51 PM user silberschatz6e_fm_i-ii.indd Page ii  12/3/09  2:51:51 PM user /Users/user/Desktop/Temp Work/00November_2009/24:11:09/VYN/silberschatz /Users/user/Desktop/Temp Work/00November_2009/24:11:09/VYN/silberschatz
In memoryof my father Joseph Silberschatz
mymother VeraSilberschatz
and mygrandparents Stepha and Aaron Rosenblum
Avi Silberschatz
Tomy wife, Joan
mychildren, Abigail and Joseph
and myparents,Henryand Frances
Hank Korth
Tomy wife,Sita
mychildren, Madhur and Advaith
and mymother, Indira
S.Sudarshan
This page intentionally left blank 
Contents
Chapter1 Introduction
1.1 Database-SystemApplications 1
1.2 PurposeofDatabaseSystems 3
1.3 ViewofData 6
1.4 DatabaseLanguages 9
1.5 RelationalDatabases 12
1.6 DatabaseDesign 15
1.7 DataStorageandQuerying 20
1.8 TransactionManagement 22
1.9 DatabaseArchitecture 23
1.10 DataMining andInformation
Retrieval 25
1.11 SpecialtyDatabases 26
1.12 DatabaseUsersandAdministrators 27
1.13 HistoryofDatabaseSystems 29
1.14 Summary 31
Exercises 33
Bibliographical Notes 35
PARTONE RELATIONALDATABASES
Chapter2 IntroductiontotheRelationalModel
2.1 StructureofRelationalDatabases 39
2.2 DatabaseSchema 42
2.3 Keys 45
2.4 SchemaDiagrams 46
2.5 RelationalQueryLanguages 47
2.6 RelationalOperations 48
2.7 Summary 52
Exercises 53
Bibliographical Notes 55
Chapter3 IntroductiontoSQL
3.1 Overview oftheSQL Query
Language 57
3.2 SQLDataDe?nition 58
3.3 Basic Structure ofSQL Queries 63
3.4 AdditionalBasic Operations 74
3.5 SetOperations 79
3.6 NullValues 83
3.7 AggregateFunctions 84
3.8 NestedSubqueries 90
3.9 Modi?cationoftheDatabase 98
3.10 Summary 104
Exercises 105
Bibliographical Notes 112
v
vi Contents
Chapter4 IntermediateSQL
4.1 JoinExpressions 113
4.2 Views 120
4.3 Transactions 127
4.4 IntegrityConstraints 128
4.5 SQLDataTypesandSchemas 136
4.6 Authorization 143
4.7 Summary 150
Exercises 152
Bibliographical Notes 156
Chapter5 AdvancedSQL
5.1 Accessing SQL From aProgramming
Language 157
5.2 FunctionsandProcedures 173
5.3 Triggers 180
5.4 Recursive Queries** 187
5.5 AdvancedAggregation Features** 192
5.6 OLAP** 197
5.7 Summary 209
Exercises 211
Bibliographical Notes 216
Chapter6 FormalRelationalQueryLanguages
6.1 TheRelationalAlgebra 217
6.2 TheTupleRelationalCalculus 239
6.3 TheDomainRelationalCalculus 245
6.4 Summary 248
Exercises 249
Bibliographical Notes 254
PART TWO DATABASEDESIGN
Chapter7 DatabaseDesignandtheE-RModel
7.1 Overview oftheDesign Process 259
7.2 TheEntity-Relationship Model 262
7.3 Constraints 269
7.4 RemovingRedundantAttributesin
EntitySets 272
7.5 Entity-RelationshipDiagrams 274
7.6 ReductiontoRelationalSchemas 283
7.7 Entity-RelationshipDesign Issues 290
7.8 ExtendedE-RFeatures 295
7.9 AlternativeNotationsforModeling
Data 304
7.10 OtherAspectsofDatabaseDesign 310
7.11 Summary 313
Exercises 315
Bibliographical Notes 321
Contents vii
Chapter8 RelationalDatabaseDesign
8.1 FeaturesofGoodRelational
Designs 323
8.2 AtomicDomainsandFirst Normal
Form 327
8.3 DecompositionUsing Functional
Dependencies 329
8.4 Functional-DependencyTheory 338
8.5 Algorithms forDecomposition 348
8.6 DecompositionUsingMultivalued
Dependencies 355
8.7 MoreNormalForms 360
8.8 Database-DesignProcess 361
8.9 ModelingTemporalData 364
8.10 Summary 367
Exercises 368
Bibliographical Notes 374
Chapter9 ApplicationDesignandDevelopment
9.1 Application ProgramsandUser
Interfaces 375
9.2 WebFundamentals 377
9.3 ServletsandJSP 383
9.4 Application Architectures 391
9.5 RapidApplicationDevelopment 396
9.6 ApplicationPerformance 400
9.7 ApplicationSecurity 402
9.8 Encryption andItsApplications 411
9.9 Summary 417
Exercises 419
Bibliographical Notes 426
PART THREE DATASTORAGEANDQUERYING
Chapter10 StorageandFileStructure
10.1 Overview ofPhysical Storage
Media 429
10.2 MagneticDiskandFlash Storage 432
10.3 RAID 441
10.4 TertiaryStorage 449
10.5 File Organization 451
10.6 OrganizationofRecordsinFiles 457
10.7 Data-DictionaryStorage 462
10.8 DatabaseBuffer 464
10.9 Summary 468
Exercises 470
Bibliographical Notes 473
Chapter11 IndexingandHashing
11.1 Basic Concepts 475
11.2 Ordered Indices 476
11.3 B
+
-TreeIndex Files 485
11.4 B
+
-TreeExtensions 500
11.5 Multiple-Key Access 506
11.6 StaticHashing 509
11.7 DynamicHashing 515
11.8 ComparisonofOrdered Indexingand
Hashing 523
11.9 BitmapIndices 524
11.10 IndexDe?nition inSQL 528
11.11 Summary 529
Exercises 532
Bibliographical Notes 536
viii Contents
Chapter12 QueryProcessing
12.1 Overview 537
12.2 MeasuresofQueryCost 540
12.3 SelectionOperation 541
12.4 Sorting 546
12.5 JoinOperation 549
12.6 OtherOperations 563
12.7 EvaluationofExpressions 567
12.8 Summary 572
Exercises 574
BibliographicalNotes 577
Chapter13 QueryOptimization
13.1 Overview 579
13.2 TransformationofRelational
Expressions 582
13.3 EstimatingStatisticsofExpression
Results 590
13.4 ChoiceofEvaluationPlans 598
13.5 MaterializedViews** 607
13.6 AdvancedTopicsinQuery
Optimization** 612
13.7 Summary 615
Exercises 617
BibliographicalNotes 622
PART FOUR TRANSACTIONMANAGEMENT
Chapter14 Transactions
14.1 TransactionConcept 627
14.2 ASimpleTransactionModel 629
14.3 StorageStructure 632
14.4 TransactionAtomicityand
Durability 633
14.5 TransactionIsolation 635
14.6 Serializability 641
14.7 TransactionIsolationand
Atomicity 646
14.8 TransactionIsolationLevels 648
14.9 ImplementationofIsolationLevels 650
14.10 TransactionsasSQLStatements 653
14.11 Summary 655
Exercises 657
BibliographicalNotes 660
Chapter15 ConcurrencyControl
15.1 Lock-BasedProtocols 661
15.2 DeadlockHandling 674
15.3 MultipleGranularity 679
15.4 Timestamp-BasedProtocols 682
15.5 Validation-BasedProtocols 686
15.6 MultiversionSchemes 689
15.7 SnapshotIsolation 692
15.8 InsertOperations,DeleteOperations,
andPredicateReads 697
15.9 WeakLevelsofConsistencyin
Practice 701
15.10 ConcurrencyinIndexStructures** 704
15.11 Summary 708
Exercises 712
BibliographicalNotes 718
Contents ix
Chapter16 RecoverySystem
16.1 Failure Classi?cation 721
16.2 Storage 722
16.3 RecoveryandAtomicity 726
16.4 RecoveryAlgorithm 735
16.5 BufferManagement 738
16.6 Failure withLossofNonvolatile
Storage 743
16.7 Early LockReleaseandLogical Undo
Operations 744
16.8 ARIES** 750
16.9 RemoteBackup Systems 756
16.10 Summary 759
Exercises 762
Bibliographical Notes 766
PART FIVE SYSTEMARCHITECTURE
Chapter17 Database-SystemArchitectures
17.1 CentralizedandClient–Server
Architectures 769
17.2 ServerSystemArchitectures 772
17.3 ParallelSystems 777
17.4 DistributedSystems 784
17.5 NetworkTypes 788
17.6 Summary 791
Exercises 793
Bibliographical Notes 794
Chapter18 ParallelDatabases
18.1 Introduction 797
18.2 I/O Parallelism 798
18.3 InterqueryParallelism 802
18.4 IntraqueryParallelism 803
18.5 IntraoperationParallelism 804
18.6 InteroperationParallelism 813
18.7 QueryOptimization 814
18.8 DesignofParallelSystems 815
18.9 Parallelism onMulticore
Processors 817
18.10 Summary 819
Exercises 821
Bibliographical Notes 824
Chapter19 DistributedDatabases
19.1 HomogeneousandHeterogeneous
Databases 825
19.2 DistributedDataStorage 826
19.3 DistributedTransactions 830
19.4 CommitProtocols 832
19.5 ConcurrencyControlin Distributed
Databases 839
19.6 Availability 847
19.7 DistributedQueryProcessing 854
19.8 HeterogeneousDistributed
Databases 857
19.9 Cloud-BasedDatabases 861
19.10 DirectorySystems 870
19.11 Summary 875
Exercises 879
Bibliographical Notes 883
x Contents
PART SIX DATAWAREHOUSING,DATA
MINING,ANDINFORMATION RETRIEVAL
Chapter20 DataWarehousingandMining
20.1 Decision-Support Systems 887
20.2 DataWarehousing 889
20.3 DataMining 893
20.4 Classi?cation 894
20.5 Association Rules 904
20.6 OtherTypesofAssociations 906
20.7 Clustering 907
20.8 OtherFormsofDataMining 908
20.9 Summary 909
Exercises 911
Bibliographical Notes 914
Chapter21 InformationRetrieval
21.1 Overview 915
21.2 RelevanceRankingUsingTerms 917
21.3 RelevanceUsing Hyperlinks 920
21.4 Synonyms, Homonyms,and
Ontologies 925
21.5 Indexing ofDocuments 927
21.6 MeasuringRetrievalEffectiveness 929
21.7 CrawlingandIndexingtheWeb 930
21.8 InformationRetrieval:BeyondRanking
ofPages 931
21.9 DirectoriesandCategories 935
21.10 Summary 937
Exercises 939
Bibliographical Notes 941
PARTSEVEN SPECIALTYDATABASES
Chapter22 Object-BasedDatabases
22.1 Overview 945
22.2 ComplexDataTypes 946
22.3 StructuredTypesandInheritancein
SQL 949
22.4 TableInheritance 954
22.5 ArrayandMultiset Typesin SQL 956
22.6 Object-IdentityandReferenceTypesin
SQL 961
22.7 ImplementingO-RFeatures 963
22.8 PersistentProgramming
Languages 964
22.9 Object-RelationalMapping 973
22.10 Object-Orientedversus
Object-Relational 973
22.11 Summary 975
Exercises 976
Bibliographical Notes 980
Chapter23 XML
23.1 Motivation 981
23.2 Structure ofXMLData 986
23.3 XMLDocumentSchema 990
23.4 Querying andTransformation 998
23.5 ApplicationProgramInterfacesto
XML 1008
23.6 StorageofXMLData 1009
23.7 XMLApplications 1016
23.8 Summary 1019
Exercises 1021
Bibliographical Notes 1024
Contents xi
PARTEIGHT ADVANCEDTOPICS
Chapter24 AdvancedApplicationDevelopment
24.1 PerformanceTuning 1029
24.2 PerformanceBenchmarks 1045
24.3 OtherIssues in Application
Development 1048
24.4 Standardization 1051
24.5 Summary 1056
Exercises 1057
Bibliographical Notes 1059
Chapter25 SpatialandTemporalDataandMobility
25.1 Motivation 1061
25.2 TimeinDatabases 1062
25.3 SpatialandGeographicData 1064
25.4 Multimedia Databases 1076
25.5 MobilityandPersonalDatabases 1079
25.6 Summary 1085
Exercises 1087
Bibliographical Notes 1089
Chapter26 AdvancedTransactionProcessing
26.1 Transaction-ProcessingMonitors 1091
26.2 TransactionalWork?ows 1096
26.3 E-Commerce 1102
26.4 Main-MemoryDatabases 1105
26.5 Real-TimeTransactionSystems 1108
26.6 Long-DurationTransactions 1109
26.7 Summary 1115
Exercises 1117
Bibliographical Notes 1119
PARTNINE CASESTUDIES
Chapter27 PostgreSQL
27.1 Introduction 1123
27.2 UserInterfaces 1124
27.3 SQL VariationsandExtensions 1126
27.4 TransactionManagementin
PostgreSQL 1137
27.5 StorageandIndexing 1146
27.6 QueryProcessing and
Optimization 1151
27.7 SystemArchitecture 1154
Bibliographical Notes 1155
Chapter28 Oracle
28.1 DatabaseDesignandQuerying
Tools 1157
28.2 SQL VariationsandExtensions 1158
28.3 StorageandIndexing 1162
28.4 Query Processing and
Optimization 1172
28.5 Concurrency Controland
Recovery 1180
28.6 SystemArchitecture 1183
28.7 Replication,Distribution,andExternal
Data 1188
28.8 DatabaseAdministration Tools 1189
28.9 DataMining 1191
Bibliographical Notes 1191
xii Contents
Chapter29 IBMDB2UniversalDatabase
29.1 Overview 1193
29.2 Database-DesignTools 1194
29.3 SQL VariationsandExtensions 1195
29.4 StorageandIndexing 1200
29.5 Multidimensional Clustering 1203
29.6 Query Processing and
Optimization 1207
29.7 MaterializedQueryTables 1212
29.8 AutonomicFeaturesinDB2 1214
29.9 ToolsandUtilities 1215
29.10 ConcurrencyControland
Recovery 1217
29.11 SystemArchitecture 1219
29.12 Replication,Distribution,andExternal
Data 1220
29.13 Business Intelligence Features 1221
Bibliographical Notes 1222
Chapter30 MicrosoftSQLServer
30.1 Management,Design,andQuerying
Tools 1223
30.2 SQL VariationsandExtensions 1228
30.3 StorageandIndexing 1233
30.4 Query Processing and
Optimization 1236
30.5 ConcurrencyandRecovery 1241
30.6 System Architecture 1246
30.7 DataAccess 1248
30.8 DistributedHeterogeneousQuery
Processing 1250
30.9 Replication 1251
30.10 ServerProgrammingin .NET 1253
30.11 XMLSupport 1258
30.12 SQLServer Service Broker 1261
30.13 Business Intelligence 1263
Bibliographical Notes 1267
PART TEN APPENDICES
AppendixA DetailedUniversitySchema
A.1 Full Schema 1271
A.2 DDL 1272
A.3 SampleData 1276
AppendixB AdvancedRelationalDesign(contentsonline)
B.1 Multivalued Dependencies B1
B.3 Domain-KeyNormalForm B8
B.4 Summary B10
Exercises B10
Bibliographical Notes B12
AppendixC OtherRelationalQueryLanguages(contentsonline)
C.1 Query-by-Example C1
C.2 Microsoft Access C9
C.3 Datalog C11
C.4 Summary C25
Exercises C26
Bibliographical Notes C30
Contents xiii
AppendixD NetworkModel(contentsonline)
D.1 Basic Concepts D1
D.2 Data-StructureDiagrams D2
D.3 TheDBTGCODASYLModel D7
D.4 DBTGData-RetrievalFacility D13
D.5 DBTGUpdateFacility D20
D.6 DBTGSet-Processing Facility D22
D.7 MappingofNetworkstoFiles D27
D.8 Summary D31
Exercises D32
Bibliographical Notes D35
AppendixE HierarchicalModel(contentsonline)
E.1 Basic Concepts E1
E.2 Tree-StructureDiagrams E2
E.3 Data-RetrievalFacility E13
E.4 UpdateFacility E17
E.5 VirtualRecords E20
E.6 MappingofHierarchies toFiles E22
E.7 TheIMS DatabaseSystem E24
E.8 Summary E25
Exercises E26
Bibliographical Notes E29
Bibliography 1283
Index 1315
This page intentionally left blank 
Preface
Databasemanagementhasevolvedfromaspecializedcomputerapplicationtoa
centralcomponentofamoderncomputingenvironment,and,asaresult,knowl-
edge about database systems has become an essential part of an education in
computer science. In this text, we present the fundamental concepts of database
management. These concepts include aspects of database design, database lan-
guages,and database-systemimplementation.
This text is intended for a ?rst course in databases at the junior or senior
undergraduate, or ?rst-year graduate, level. In addition to basic material for
a ?rst course, the text contains advanced material that can be used for course
supplements,oras introductory materialfor anadvancedcourse.
We assume only a familiarity with basic data structures, computer organi-
zation, and a high-level programming language such as Java, C, or Pascal. We
present concepts as intuitive descriptions, many of which are based on our run-
ningexampleofauniversity.Importanttheoreticalresultsarecovered,butformal
proofs are omitted. In place of proofs, ?gures and examples are used to suggest
why a result is true. Formal descriptions and proofs of theoretical results may
be found in researchpapersand advanced textsthat are referencedin the biblio-
graphical notes.
The fundamental concepts and algorithms covered in the book are often
based on those used in existing commercial or experimental database systems.
Our aim is to present these concepts and algorithms in a general setting that is
nottiedtooneparticulardatabasesystem.Detailsofparticulardatabasesystems
arediscussedinPart 9, “CaseStudies.”
In this, the sixth edition of Database System Concepts,wehaveretainedthe
overallstyleof the prior editionswhile evolvingthe content and organization to
re?ectthechangesthatareoccurringinthewaydatabasesaredesigned,managed,
and used. We have also taken into account trends in the teaching of database
concepts and madeadaptationsto facilitatethesetrendswhereappropriate.
xv
xvi Preface
Organization
The textis organizedinnine major parts,plus ?veappendices.
• Overview (Chapter 1). Chapter 1 provides a general overview of the nature
and purposeofdatabase systems.Weexplainhow theconcept ofa database
system has developed, what the common features of database systems are,
what a database system does for the user, and how a database system in-
terfaces with operating systems. We also introduce an example database
application: a university organization consisting of multiple departments,
instructors, students, and courses. This application is used as a running ex-
ample throughout the book. This chapter is motivational, historical, and ex-
planatory innature.
• Part 1: Relational Databases (Chapters 2 through 6). Chapter 2 introduces
the relational model of data, covering basic concepts such as the structure
ofrelationaldatabases,databaseschemas,keys,schemadiagrams,relational
querylanguages,andrelationaloperations.Chapters3,4,and5focusonthe
mostin?uentialoftheuser-orientedrelationallanguages:SQL.Chapter6cov-
ersthe formalrelationalquerylanguages:relationalalgebra,tuplerelational
calculus, and domainrelationalcalculus.
Thechaptersinthispartdescribedatamanipulation:queries,updates,in-
sertions,anddeletions,assumingaschemadesignhasbeenprovided.Schema
designissuesaredeferredtoPart 2.
• Part 2: Database Design (Chapters 7 through 9). Chapter 7 provides an
overview of the database-design process, with major emphasis on database
designusingtheentity-relationshipdatamodel.Theentity-relationshipdata
modelprovidesahigh-levelviewoftheissuesindatabasedesign,andofthe
problems that we encounter in capturing the semantics of realistic applica-
tions within the constraints of a data model. UML class-diagram notation is
alsocoveredinthischapter.
Chapter 8 introduces the theory of relational database design. The the-
oryoffunctionaldependenciesandnormalizationiscovered,withemphasis
on the motivation and intuitive understanding of each normal form. This
chapter begins with an overview of relational design and relies on an intu-
itive understanding of logical implication of functional dependencies. This
allows the concept of normalization to be introduced prior to full coverage
of functional-dependency theory, which is presentedlater in the chapter. In-
structorsmaychoosetouseonlythisinitialcoverageinSections8.1through
8.3withoutlossofcontinuity.Instructorscoveringtheentirechapterwillben-
e?tfromstudentshavingagoodunderstandingofnormalizationconceptsto
motivatesomeofthechallengingconceptsoffunctional-dependencytheory.
Chapter9coversapplicationdesignanddevelopment.Thischapterempha-
sizestheconstructionofdatabaseapplicationswithWeb-basedinterfaces.In
addition,the chaptercoversapplicationsecurity.
Preface xvii
• Part 3: Data Storage and Querying (Chapters 10 through 13). Chapter 10
deals with storage devices, ?les, and data-storage structures. A variety of
data-accesstechniquesarepresentedinChapter11,includingB
+
-treeindices
and hashing. Chapters 12 and 13 address query-evaluation algorithms and
queryoptimization.Thesechaptersprovideanunderstandingoftheinternals
of thestorageand retrievalcomponents of a database.
• Part 4: Transaction Management (Chapters 14 through 16). Chapter 14 fo-
cuses on the fundamentals of a transaction-processing system: atomicity,
consistency,isolation,anddurability.Itprovidesanoverviewofthemethods
usedtoensuretheseproperties,including locking andsnapshot isolation.
Chapter15focusesonconcurrencycontrolandpresentsseveraltechniques
for ensuring serializability, including locking, timestamping, and optimistic
(validation) techniques. The chapter also covers deadlock issues. Alterna-
tives to serializability are covered, most notably the widely-used snapshot
isolation,which isdiscussedindetail.
Chapter 16 covers the primary techniques for ensuring correct transac-
tionexecutiondespitesystemcrashesandstoragefailures.Thesetechniques
include logs, checkpoints, and database dumps. The widely-used ARIES al-
gorithmis presented.
• Part 5: System Architecture (Chapters 17 through 19). Chapter 17 covers
computer-system architecture, and describes the in?uence of the underly-
ing computer system on the database system. We discuss centralized sys-
tems,client–serversystems,andparallelanddistributedarchitecturesinthis
chapter.
Chapter 18, on parallel databases, explores a variety of parallelization
techniques,includingI/Oparallelism,interqueryandintraqueryparallelism,
andinteroperationandintraoperationparallelism.Thechapteralsodescribes
parallel-systemdesign.
Chapter 19 covers distributed database systems, revisiting the issues
of database design, transaction management, and query evaluation and op-
timization, in the context of distributed databases. The chapter also cov-
ers issues of system availability during failures, heterogeneous distributed
databases,cloud-baseddatabases,anddistributeddirectorysystems.
• Part6:DataWarehousing,DataMining,andInformationRetrieval(Chap-
ters 20 and 21). Chapter 20 introduces the concepts of data warehousing
and data mining. Chapter 21 describes information-retrieval techniques for
querying textual data, including hyperlink-based techniques used in Web
searchengines.
Part 6 uses the modeling and language concepts from Parts 1 and 2, but
does not depend on Parts 3, 4, or 5. It can therefore be incorporated easily
into acourse that focuses on SQL andon databasedesign.
xviii Preface
• Part7: SpecialtyDatabases (Chapters 22 and 23). Chapter 22 covers object-
based databases. The chapter describes the object-relational data model,
whichextendstherelationaldatamodeltosupportcomplexdatatypes,type
inheritance, and object identity. The chapter also describes database access
fromobject-orientedprogramminglanguages.
Chapter23coverstheXMLstandardfordatarepresentation,whichisseeing
increasinguseintheexchangeandstorageofcomplexdata.Thechapteralso
describesquerylanguages for XML.
• Part 8: Advanced Topics (Chapters 24 through 26). Chapter 24 covers ad-
vanced issues in application development, including performance tuning,
performancebenchmarks,database-applicationtesting,andstandardization.
Chapter25coversspatialandgeographicdata,temporaldata,multimedia
data,and issuesinthe managementof mobileand personaldatabases.
Finally, Chapter 26 deals with advanced transaction processing. Top-
ics covered in the chapter include transaction-processing monitors, transac-
tional work?ows, electronic commerce, high-performance transaction sys-
tems,real-timetransaction systems,and long-durationtransactions.
• Part 9: Case Studies (Chapters 27 through 30). In this part, we present case
studiesoffouroftheleadingdatabasesystems,PostgreSQL,Oracle,IBM DB2,
and Microsoft SQL Server. These chapters outline unique features of each of
thesesystems,anddescribetheirinternalstructure.Theyprovideawealthof
interestinginformationabouttherespectiveproducts,andhelpyouseehow
the various implementation techniques described in earlier parts are used
in real systems. They also cover several interesting practical aspects in the
designofrealsystems.
• Appendices.Weprovide?veappendicesthatcovermaterialthatisofhistor-
ical nature or is advanced; these appendices are available only online on the
Web site of the book (http://www.db-book.com). An exception is Appendix A,
which presents details of our university schema including the full schema,
DDL,and allthetables.This appendixappearsintheactual text.
Appendix B describes other relational query languages, including QBE
Microsoft Access,andDatalog.
AppendixC describes advanced relational database design, including the
theoryofmultivalueddependencies,joindependencies,and theproject-join
anddomain-keynormalforms.Thisappendixisforthebene?tofindividuals
who wish to study the theory of relational database design in more detail,
and instructors who wish to do so in their courses. This appendix, too, is
availableonly online,onthe Websiteof thebook.
Although most new database applications use either the relational model
or the object-relational model,the network and hierarchical data models are
stillinuseinsomelegacyapplications.Forthebene?tofreaderswhowishto
learnaboutthesedatamodels,weprovideappendicesdescribingthenetwork
and hierarchical datamodels,inAppendicesDand Erespectively.
Preface xix
TheSixthEdition
Theproductionofthissixtheditionhasbeenguidedbythemanycommentsand
suggestionswereceivedconcerningtheearliereditions,byourownobservations
whileteachingatYaleUniversity,LehighUniversity,and IITBombay,andbyour
analysis of thedirectionsinwhich database technology isevolving.
Wehave replacedtheearlierrunning exampleof bank enterprisewith auni-
versityexample.Thisexamplehasanimmediateintuitiveconnectiontostudents
thatassistsnotonlyinrememberingtheexample,but,moreimportantly,ingain-
ingdeeperinsight intothevariousdesigndecisionsthat needtobemade.
Wehavereorganizedthebooksoastocollectallofour SQLcoveragetogether
andplaceitearlyinthebook.Chapters3,4,and5presentcompleteSQLcoverage.
Chapter 3 presents the basics of the language, with more advanced features in
Chapter 4. In Chapter 5, we present JDBC along with other means of accessing
SQL fromageneral-purposeprogramminglanguage.Wepresenttriggersandre-
cursion, and then conclude with coverage of online analytic processing (OLAP).
Introductory courses may choose to cover only certain sections of Chapter 5 or
defersectionsuntilafterthecoverageofdatabasedesignwithoutlossofcontinu-
ity.
Beyond these two major changes, we revised the material in each chapter,
bringing the older material up-to-date, adding discussions on recent develop-
mentsindatabasetechnology,andimprovingdescriptionsoftopicsthatstudents
found dif?cult to understand. We have also added new exercises and updated
references.Thelistof speci?cchanges includesthefollowing:
• EarliercoverageofSQL.ManyinstructorsuseSQLasakeycomponentofterm
projects (see our Web site, www.db-book.com, for sample projects). In order to
give students ample time for the projects, particularly for universities and
collegesonthequartersystem,itisessentialtoteach SQLasearlyaspossible.
Withthis inmind,we haveundertakenseveralchanges inorganization:
?
A new chapter on the relational model (Chapter 2) precedes SQL,laying
the conceptual foundation, without getting lost in details of relational
algebra.
?
Chapters3,4,and5providedetailedcoverageof SQL.Thesechaptersalso
discuss variants supported by different database systems, to minimize
problemsthatstudentsfacewhentheyexecutequeriesonactualdatabase
systems. These chapters cover all aspects of SQL, including queries, data
de?nition, constraint speci?cation, OLAP,andtheuseofSQL from within
a varietyof languages,including Java/JDBC.
?
Formal languages (Chapter 6) have beenpostponed to after SQL,andcan
be omitted without affecting the sequencing of other chapters. Only our
discussionofqueryoptimizationinChapter13dependsontherelational
algebra coverageof Chapter6.
xx Preface
• New database schema. We adopted a new schema, which is based on uni-
versity data, as a running example throughout the book. This schema is
moreintuitiveandmotivatingforstudentsthantheearlierbankschema,and
illustratesmorecomplexdesigntrade-offsinthe database-designchapters.
• More support for a hands-on student experience. To facilitate following
our running example, we list the database schema and the sample relation
instances for our university database together in Appendix A as well as
wheretheyareusedinthevariousregularchapters.Inaddition,weprovide,
onourWebsitehttp://www.db-book.com,SQLdata-de?nitionstatementsforthe
entire example, along with SQL statements to create our example relation
instances. This encourages students to run example queries directly on a
databasesystemand toexperimentwithmodifyingthose queries.
• Revised coverage of E-R model. The E-R diagram notation in Chapter 7 has
beenmodi?edtomakeitmorecompatiblewithUML.Thechapteralsomakes
good use of the new university database schema to illustrate more complex
designtrade-offs.
• Revised coverage ofrelational design. Chapter 8 now has a more readable
style, providing an intuitive understanding of functional dependencies and
normalization, before covering functional dependency theory; the theory is
motivatedmuchbetteras aresult.
• Expandedmaterialonapplicationdevelopmentandsecurity.Chapter9has
new material on application development, mirroring rapid changes in the
?eld. In particular, coverage of security has been expanded, considering its
criticality in today’s interconnected world, with an emphasis on practical
issuesoverabstract concepts.
• Revised and updated coverage of data storage, indexing and query op-
timization. Chapter 10 has been updated with new technology, including
expandedcoverageof?ashmemory.
Coverage of B
+
-trees in Chapter 11 has been revised to re?ect practical
implementations, including coverage of bulk loading, and the presentation
has been improved. The B
+
-tree examples in Chapter 11 have now been
revisedwith n = 4, to avoid the special case of empty nodes that arises with
the(unrealistic)valueofn = 3.
Chapter13hasnewmaterialonadvancedquery-optimizationtechniques.
• Revisedcoverageoftransactionmanagement.Chapter14providesfullcov-
erageof thebasics for anintroductorycourse,withadvanceddetailsfollow-
inginChapters15and16.Chapter14hasbeenexpandedtocoverthepractical
issues in transaction management faced by database users and database-
application developers. The chapter also includes an expanded overview of
topicscoveredinChapters15and16,ensuringthatevenifChapters15and16
areomitted,studentshaveabasicknowledgeoftheconceptsofconcurrency
control and recovery.
Preface xxi
Chapters 14 and 15 now include detailed coverage of snapshot isolation,
which is widely supported and used today, including coverage of potential
hazards whenusing it.
Chapter 16 now has a simpli?ed description of basic log-based recovery
leadingup to coverageof the ARIES algorithm.
• Revised and expanded coverage of distributed databases. We now cover
cloud data storage, which is gaining signi?cant interest for business appli-
cations. Cloud storage offers enterprises opportunities for improved cost-
management and increased storage scalability, particularly for Web-based
applications. We examine those advantages along with the potential draw-
backs and risks.
Multidatabases,whichwereearlierintheadvancedtransactionprocessing
chapter,arenow coveredearlieraspart ofthe distributeddatabasechapter.
• PostponedcoverageofobjectdatabasesandXML.Althoughobject-oriented
languages and XML are widely used outside of databases, their use in data-
bases is still limited, making them appropriate for more advanced courses,
or as supplementary material for an introductory course. These topics have
thereforebeenmovedtolaterinthebook, inChapters22and23.
• QBE, Microsoft Access, and Datalog in an online appendix. These topics,
whichwereearlierpartofachapteron “otherrelationallanguages,”arenow
coveredinonlineAppendixC.
Alltopicsnotlistedaboveareupdatedfromthe?fthedition,thoughtheiroverall
organizationis relativelyunchanged.
ReviewMaterialandExercises
Eachchapterhasalistofreviewterms,inadditiontoasummary,whichcanhelp
readersreviewkeytopicscoveredinthechapter.
Theexercisesaredividedintotwosets:practiceexercises andexercises.The
solutions for the practice exercises are publicly available on the Web site of the
book. Students are encouraged to solve the practice exercises on their own, and
later use the solutions on the Web site to check their own solutions. Solutions
to the other exercises are available only to instructors (see “Instructor’s Note,”
below,for informationon howto getthesolutions).
Many chapters have a tools section at the end of the chapter that provides
information on software tools related to the topic of the chapter; some of these
tools can be used for laboratory exercises. SQL DDL and sample data for the
universitydatabase and other relations usedin the exercisesareavailable on the
Websiteof the book, andcan beusedfor laboratory exercises.
xxii Preface
Instructor’sNote
The book contains both basic and advanced material, which might not be cov-
ered in a single semester. We have marked several sections as advanced, using
the symbol “**”. These sections may be omitted if so desired, without a loss of
continuity.Exercisesthataredif?cult(andcanbeomitted)arealsomarkedusing
the symbol “**”.
Itispossibletodesigncoursesbyusingvarioussubsetsofthechapters.Some
of the chapters can also be covered in an order different from their order in the
book. Weoutline someof thepossibilitieshere:
• Chapter5(Advanced SQL)canbeskippedordeferredtolaterwithoutlossof
continuity. We expect most courses will cover at least Section 5.1.1 early, as
JDBCislikelytobe a usefultoolinstudentprojects.
• Chapter6(FormalRelationalQueryLanguages)canbecoveredimmediately
afterChapter2,aheadofSQL.Alternatively,thischaptermaybeomittedfrom
anintroductory course.
We recommend covering Section 6.1 (relational algebra) if the course also
covers query processing. However, Sections 6.2 and 6.3 can be omitted if
studentswillnot beusing relationalcalculus as partof the course.
• Chapter 7 (E-R Model) can be covered ahead of Chapters 3, 4 and 5 if you so
desire,since Chapter7doesnot haveany dependencyon SQL.
• Chapter13(QueryOptimization)canbeomittedfromanintroductorycourse
without affecting coverageof any other chapter.
• Both our coverage of transaction processing (Chapters 14 through 16) and
our coverage of system architecture (Chapters 17 through 19) consist of an
overview chapter (Chapters 14 and 17, respectively), followed by chapters
with details. You might choose to use Chapters 14 and 17, while omitting
Chapters 15, 16, 18 and 19, if you defer these latter chapters to an advanced
course.
• Chapters 20 and 21, covering data warehousing, data mining, and informa-
tionretrieval,canbeusedasself-studymaterialoromittedfromanintroduc-
torycourse.
• Chapters 22 (Object-Based Databases), and 23 (XML) can be omitted from an
introductorycourse.
• Chapters24through26,coveringadvancedapplicationdevelopment,spatial,
temporalandmobiledata,andadvancedtransactionprocessing,aresuitable
for anadvancedcourseor for self-studyby students.
• Thecase-studyChapters27through30aresuitableforself-studybystudents.
Alternatively,theycanbeusedasanillustrationofconceptswhentheearlier
chapters arepresentedinclass.
Modelcoursesyllabi,basedonthetext,canbefoundontheWebsiteofthebook.
Preface xxiii
WebSiteandTeachingSupplements
A Web site for the book is available at the URL: http://www.db-book.com.TheWeb
sitecontains:
• Slidescovering allthechapters of thebook.
• Answerstothepracticeexercises.
• The?veappendices.
• Anup-to-dateerratalist.
• Laboratory material, including SQL DDL and sample data for the university
schema and other relations used in exercises,and instructions for setting up
and usingvariousdatabase systemsand tools.
Thefollowing additionalmaterialisavailableonly tofaculty:
• Aninstructor manual containing solutions to allexercisesinthe book.
• Aquestionbank containing extraexercises.
For more information about how to get a copy of the instructor manual and the
questionbank,pleasesendelectronicmailtocustomer.service@mcgraw-hill.com.
In the United States, you may call 800-338-3987. The McGraw-Hill Web site for
this book is http://www.mhhe.com/silberschatz.
ContactingUs
We have endeavored to eliminate typos, bugs, and the like from the text. But, as
in new releases of software, bugs almost surely remain; an up-to-date errata list
isaccessiblefromthebook’sWebsite.Wewouldappreciateitifyouwouldnotify
usofanyerrorsoromissionsinthebookthatarenotonthecurrentlistoferrata.
We would be glad to receive suggestions on improvements to the book. We
also welcome any contributions to the book Web site that could be of use to
other readers, such as programming exercises, project suggestions, online labs
and tutorials,and teaching tips.
Email should be addressed to db-book-authors@cs.yale.edu. Any other corre-
spondence should be sent toAvi Silberschatz,Department ofComputer Science,
Yale University, 51 Prospect Street, P.O. Box 208285, New Haven, CT 06520-8285
USA.
Acknowledgments
Manypeoplehavehelpeduswiththissixthedition,aswellaswiththeprevious
?veeditionsfromwhich itis derived.
xxiv Preface
SixthEdition
  Anastassia Ailamaki, Sailesh Krishnamurthy, Spiros Papadimitriou, and
Bianca Schroeder (Carnegie Mellon University) for writing Chapter 27 de-
scribingthe PostgreSQLdatabasesystem.
  Hakan Jakobsson (Oracle), for writing Chapter 28 on the Oracle database
system.
  Sriram Padmanabhan (IBM), for writing Chapter 29 describing the IBM DB2
databasesystem.
  SameetAgarwal,Jos´ eA.Blakeley,ThierryD’Hers,GeraldHinson,DirkMy-
ers, Vaqar Pirzada, Bill Ramos, Balaji Rathakrishnan, Michael Rys, Florian
Waas, and Michael Zwilling (all of Microsoft) for writing Chapter 30 de-
scribing the Microsoft SQL Server database system, and in particular Jos´ e
Blakeley for coordinating and editing the chapter; C´ esar Galindo-Legaria,
Goetz Graefe, Kalen Delaney, and Thomas Casey (all of Microsoft) for their
contributionstothepreviouseditionoftheMicrosoft SQLServerchapter.
  Daniel Abadi for reviewing the table of contents of the ?fth edition and
helpingwiththeneworganization.
  Steve Dolins, University of Florida; Rolando Fernanez, George Washington
University;FrantisekFranek,McMasterUniversity;LatifurKhan,University
ofTexas-Dallas;SanjayMadria,UniversityofMissouri-Rolla;ArisOuksel,
University of Illinois; and Richard Snodgrass, University of Waterloo; who
served as reviewers of the book and whose comments helped us greatly in
formulatingthissixthedition.
  JudiPaigeforherhelpingenerating?guresandpresentationslides.
  MarkWogahnformakingsurethatthesoftwaretoproducethebook,includ-
ingLaTeXmacrosandfonts,workedproperly.
  N.L.Sardaforfeedbackthathelpedusimproveseveralchapters,inparticular
Chapter11;VikramPudiformotivatingustoreplacetheearlierbankschema;
andShetalShahforfeedbackonseveralchapters.
  Students at Yale, Lehigh, and IIT Bombay, for their comments on the ?fth
edition,aswellasonpreprintsofthesixthedition.
PreviousEditions
  Chen Li and Sharad Mehrotra for providing material on JDBC and security
forthe?fthedition.
  MarilynTurnamianandNandprasadJoshiprovidedsecretarialassistancefor
the?fthedition,andMarilynalsopreparedanearlydraftofthecoverdesign
forthe?fthedition.
Preface xxv
  Lyn Dupr´ ecopyeditedthethirdeditionandSaraStrandtmaneditedthetext
ofthethirdedition.
  NileshDalvi,SumitSanghai,GauravBhalotia,ArvindHulgeriK.V.Ragha-
van, Prateek Kapadia, Sara Strandtman, Greg Speegle, and Dawn Bezviner
helpedtopreparetheinstructor’smanualforearliereditions.
  Theideaofusingshipsaspartofthecoverconceptwasoriginallysuggested
tousbyBruceStephan.
  The following people pointed out errors in the ?fth edition: Alex Coman,
Ravindra Guravannavar, Arvind Hulgeri, Rohit Kulshreshtha, Sang-Won
Lee,JoeH.C.Lu,AlexN.Napitupulu,H.K.Park,JianPei,FernandoSaenz
Perez,DonniePinkston,YmaPinto,RajarshiRakshit,SandeepSatpal,Amon
Seagull, Barry Soroka, Praveen Ranjan Srivastava, Hans Svensson, Moritz
Wiese,andEyobDeleleYirdaw.
  Thefollowingpeopleofferedsuggestionsandcommentsforthe?fthandear-
liereditionsofthebook.R.B.Abhyankar, HaniAbu-Salem,JamelR.Alsab-
bagh, Raj Ashar, Don Batory, Phil Bernhard, Christian Breimann, Gavin M.
Bierman,JanekBogucki,HaranBoral,PaulBourgeois,PhilBohannon,Robert
Brazile,YuriBreitbart,RamziBualuan,MichaelCarey,SoumenChakrabarti,
Tom Chappell, Zhengxin Chen, Y. C. Chin, Jan Chomicki, Laurens Damen,
PrasannaDhandapani,QinDing,ValentinDinu,J.Edwards,ChristosFalout-
sos, Homma Farian, Alan Fekete, Frantisek Franek, Shashi Gadia, Hector
Garcia-Molina, Goetz Graefe, Jim Gray, Le Gruenwald, Eitan M. Gurari,
WilliamHankley,BruceHillyer,RonHitchens,ChadHogg,ArvindHulgeri,
YannisIoannidis,ZhengJiaping,RandyM.Kaplan,GrahamJ.L.Kemp,Rami
Khouri,Hyoung-JooKim,WonKim,HenryKorth(fatherofHenryF.),Carol
Kroll, Hae Choon Lee, Sang-Won Lee, Irwin Levinstein, Mark Llewellyn,
Gary Lindstrom, Ling Liu, Dave Maier, Keith Marzullo, Marty Maskarinec,
Fletcher Mattox, Sharad Mehrotra, Jim Melton, Alberto Mendelzon, Ami
Motro, Bhagirath Narahari, Yiu-Kai Dennis Ng, Thanh-Duy Nguyen, Anil
Nigam,CyrilOrji,MeralOzsoyoglu,D.B.Phatak, JuanAltmayerPizzorno,
BrucePorter,SunilPrabhakar,JimPeterson,K.V.Raghavan,NahidRahman,
RajarshiRakshit,KrithiRamamritham,MikeReiter,GregRiccardi,Odinaldo
Rodriguez, Mark Roth, Marek Rusinkiewicz, Michael Rys, Sunita Sarawagi,
N. L. Sarda, Patrick Schmid, Nikhil Sethi, S. Seshadri, Stewart Shen, Shashi
Shekhar, Amit Sheth, Max Smolens, Nandit Soparkar, Greg Speegle, Jeff
Storey, Dilys Thomas, Prem Thomas, Tim Wahls, Anita Whitehall, Christo-
pherWilson,MarianneWinslett,WeiningZhang,andLiuZhenming.
BookProduction
The publisher was Raghu Srinivasan. The developmental editor was Melinda
D.Bilecki.TheprojectmanagerwasMelissaLeick.Themarketingmanagerwas
xxvi Preface
CurtReynolds.TheproductionsupervisorwasLauraFuller.Thebook designer
wasBrendaRolwes.ThecoverdesignerwasStudioMontage,St.Louis,Missouri.
The copyeditor was George Watson. The proofreaderwas KevinCampbell.The
freelance indexer was Tobiah Waldron. The Aptara team consisted of Raman
AroraandSudeshnaNandy
PersonalNotes
Sudarshan would like to acknowledge his wife, Sita, for her love and support,
andchildrenMadhurandAdvaithfortheirloveand joiedevivre.Hankwould
liketoacknowledgehiswife,Joan,andhischildren,AbbyandJoe,fortheirlove
andunderstanding.AviwouldliketoacknowledgeValerieforherlove,patience,
andsupportduringtherevisionofthisbook.
A.S.
H.F.K.
S.S.
CHAPTER
1
Introduction
Adatabase-managementsystem (DBMS) is a collection of interrelated data and
a set of programs to access those data. The collection of data, usually referred to
asthedatabase,contains informationrelevanttoanenterprise.Theprimarygoal
of a DBMS is to provide a way to store and retrieve database information that is
both convenient and ef?cient.
Database systems are designed to manage large bodies of information. Man-
agement of data involves both de?ning structures for storage of information
and providing mechanisms for the manipulation of information. In addition, the
databasesystemmustensurethesafetyoftheinformationstored,despitesystem
crashesorattemptsatunauthorizedaccess.Ifdataaretobesharedamongseveral
users, the system must avoid possible anomalous results.
Because information is so important in most organizations, computer scien-
tistshave developeda largebody ofconcepts and techniquesformanaging data.
These concepts and techniques form the focus of this book. This chapter brie?y
introduces the principles of database systems.
1.1 Database-System Applications
Databases are widely used. Here are some representative applications:
• Enterprise Information
?
Sales: For customer, product, and purchase information.
?
Accounting: For payments, receipts, account balances, assets and other
accounting information.
?
Human resources:Forinformationaboutemployees,salaries,payrolltaxes,
and bene?ts, and for generation of paychecks.
?
Manufacturing:Formanagementofthesupplychainandfortrackingpro-
ductionofitemsinfactories,inventoriesofitemsinwarehousesandstores,
and orders for items.
1
2 Chapter 1 Introduction
?
Online retailers: For sales data noted above plus online order tracking,
generation of recommendation lists, and maintenance of online product
evaluations.
• Banking and Finance
?
Banking:Forcustomerinformation,accounts,loans,andbankingtransac-
tions.
?
Credit card transactions: For purchases on credit cards and generation of
monthly statements.
?
Finance: For storing information about holdings, sales, and purchases of
?nancial instruments such as stocks and bonds; also for storing real-time
marketdatatoenableonlinetradingbycustomersandautomatedtrading
by the ?rm.
• Universities: For student information, course registrations, and grades (in
addition to standard enterprise information such as human resources and
accounting).
• Airlines:Forreservationsandscheduleinformation.Airlineswereamongthe
?rst touse databases ina geographically distributedmanner.
• Telecommunication: For keeping records of calls made, generating monthly
bills,maintainingbalancesonprepaidcallingcards,andstoringinformation
about the communication networks.
As the list illustrates, databases form an essential part of every enterprise today,
storing not only types of information that are common to most enterprises, but
also information that is speci?c to the category of the enterprise.
Over the course of the last four decades of the twentieth century, use of
databasesgrewinallenterprises.Intheearlydays,veryfewpeopleinteracteddi-
rectly with database systems, although without realizing it, they interacted with
databases indirectly—through printed reports such as credit card statements, or
through agents such as bank tellers and airline reservation agents. Then auto-
mated teller machines came along and let users interact directly with databases.
Phone interfaces to computers (interactive voice-response systems) also allowed
users to deal directly with databases—a caller could dial a number, and press
phone keys to enter information or to select alternative options, to ?nd ?ight
arrival/departuretimes,forexample,or toregisterfor coursesin a university.
TheInternetrevolutionofthelate1990ssharplyincreaseddirectuseraccessto
databases. Organizations converted many of their phone interfaces to databases
into Web interfaces, and made a variety of services and information available
online. For instance, when you access an online bookstore and browse a book or
musiccollection,youareaccessing datastoredinadatabase.When youenteran
orderonline,yourorderisstoredinadatabase.WhenyouaccessabankWebsite
and retrieve your bank balance and transaction information, the information is
retrievedfromthebank’sdatabasesystem.WhenyouaccessaWebsite,informa-
1.2 Purpose ofDatabase Systems 3
tion about you may be retrievedfrom a database to select which advertisements
you should see. Furthermore, data about your Web accesses may be stored in a
database.
Thus, although user interfaces hide details of access to a database, and most
people are not evenaware they are dealing with a database, accessing databases
forms an essentialpart of almost everyone’slife today.
The importance of database systems can be judged in another way—today,
database system vendors like Oracle are among the largest software companies
intheworld,anddatabasesystemsformanimportantpartoftheproductlineof
Microsoft and IBM.
1.2 Purpose of Database Systems
Database systems arose in response to early methods of computerized manage-
ment of commercial data. As an example of such methods, typical of the 1960s,
consider part of a university organization that, among other data, keeps infor-
mation about all instructors, students, departments, and course offerings. One
way to keep the information on a computer is to store it in operating system
?les. To allow users to manipulate the information, the system has a number of
application programs that manipulate the ?les, including programs to:
• Addnew students,instructors, and courses
• Registerstudentsfor coursesand generateclass rosters
• Assigngradestostudents,computegradepointaverages(GPA),andgenerate
transcripts
Systemprogrammers wrote these application programs to meet the needs of the
university.
New application programs are added to the system as the need arises. For
example,suppose that a universitydecidesto create anew major (say, computer
science).Asaresult,theuniversitycreatesanewdepartmentandcreatesnewper-
manent?les(oraddsinformationtoexisting?les)torecordinformationaboutall
theinstructorsinthedepartment,studentsinthatmajor,courseofferings,degree
requirements, etc. The university may have to write new application programs
todealwith rulesspeci?ctothenewmajor. Newapplicationprogramsmay also
have to be written to handle new rules in the university. Thus, as time goes by,
the systemacquires more ?les and more applicationprograms.
This typical ?le-processing system is supported by a conventional operat-
ing system. The system stores permanent records in various ?les, and it needs
differentapplicationprogramstoextractrecordsfrom,andaddrecordsto,theap-
propriate ?les. Before database management systems (DBMSs) were introduced,
organizations usually storedinformation in such systems.
Keepingorganizationalinformationina?le-processingsystemhasanumber
of major disadvantages:
4 Chapter 1 Introduction
• Data redundancy and inconsistency. Since different programmers create
the ?les and application programs over a long period, the various ?les are
likelytohavedifferentstructuresandtheprogramsmaybewritteninseveral
programminglanguages.Moreover,thesameinformationmaybeduplicated
in several places (?les). For example, if a student has a double major (say,
music and mathematics) the address and telephone number of that student
may appear ina ?le that consists ofstudent records of students inthe Music
department and in a ?le that consists of student records of students in the
Mathematicsdepartment.Thisredundancyleadstohigherstorageandaccess
cost.Inaddition,itmayleadtodatainconsistency;thatis,thevariouscopies
ofthesamedatamaynolongeragree.Forexample,achangedstudentaddress
may be re?ected in the Music department records but not elsewhere in the
system.
• Dif?cultyinaccessingdata.Supposethatone oftheuniversityclerksneeds
to?ndoutthenamesofallstudentswholivewithinaparticularpostal-code
area. The clerk asks the data-processing department to generate such a list.
Because the designers of the original system did not anticipate this request,
there is no application program on hand to meet it. There is, however, an
application program to generate the list of all students. The university clerk
has now two choices: either obtain the list of all students and extract the
needed information manually or ask a programmer to write the necessary
applicationprogram.Bothalternativesareobviouslyunsatisfactory.Suppose
that such a program is written, and that, several days later, the same clerk
needs to trim that list to include only those studentswho have takenat least
60 credit hours. As expected, a program to generate such a list does not
exist. Again, the clerk has the preceding two options, neither of which is
satisfactory.
The point here is that conventional ?le-processing environments do not
allowneededdatatoberetrievedinaconvenientandef?cientmanner.More
responsivedata-retrievalsystemsare requiredfor general use.
• Data isolation. Because data are scattered in various ?les, and ?les may
be in different formats, writing new application programs to retrieve the
appropriatedatais dif?cult.
• Integrity problems. The data values stored in the database must satisfy cer-
tain types of consistency constraints. Suppose the university maintains an
account for each department, and records the balance amount in each ac-
count.Supposealsothattheuniversityrequiresthattheaccountbalanceofa
department may neverfall below zero. Developersenforce these constraints
in the system by adding appropriate code in the various application pro-
grams. However, when new constraints are added, it is dif?cult to change
theprogramstoenforcethem.Theproblemiscompoundedwhenconstraints
involveseveraldataitemsfromdifferent?les.
• Atomicity problems. A computer system, like any other device, is subject
to failure. In many applications, it is crucial that, if a failure occurs, the data
1.2 Purpose ofDatabase Systems 5
be restored to the consistent state that existed prior to the failure. Consider
a program to transfer $500 from the account balance of department A to
the account balance of department B. If a system failure occurs during the
execution of the program, it is possible that the $500 was removed from the
balanceofdepartmentAbutwasnotcreditedtothebalanceofdepartmentB,
resulting in an inconsistent database state. Clearly, it is essential to database
consistency that either both the credit and debit occur, or that neither occur.
That is, the funds transfer must be atomic—it must happen in its entirety or
not at all. It is dif?cult to ensure atomicity in a conventional ?le-processing
system.
• Concurrent-accessanomalies.Forthesakeofoverallperformanceofthesys-
tem and faster response, many systems allow multiple users to update the
data simultaneously. Indeed, today, the largest Internet retailers may have
millions of accesses per day to their data by shoppers. In such an environ-
ment, interaction of concurrent updates is possible and may result in incon-
sistent data. Consider department A, with an account balance of $10,000. If
two department clerks debit the account balance (by say $500 and $100, re-
spectively)ofdepartment Aatalmostexactlythesametime,theresultofthe
concurrent executions may leave the budget in an incorrect (or inconsistent)
state.Supposethattheprogramsexecutingonbehalfofeachwithdrawalread
theoldbalance,reducethatvaluebytheamountbeingwithdrawn,andwrite
theresultback.Ifthetwoprogramsrunconcurrently,theymaybothreadthe
value $10,000, and write back $9500 and $9900, respectively. Depending on
which one writes the value last, the account balance of department A may
containeither$9500or$9900,ratherthanthecorrectvalueof$9400.Toguard
against this possibility,the system must maintain some form of supervision.
Butsupervisionisdif?culttoprovidebecausedatamaybeaccessedbymany
differentapplicationprogramsthat have not beencoordinated previously.
As another example, suppose a registrationprogram maintains a count of
students registered for a course, in order to enforce limits on the number of
studentsregistered.Whenastudentregisters,theprogramreadsthecurrent
count for the courses, veri?es that the count is not already at the limit, adds
one to the count, and stores the count back in the database. Suppose two
students register concurrently, with the count at (say) 39. The two program
executions may both read the value 39, and both would then write back 40,
leading to an incorrect increase of only 1, even though two students suc-
cessfully registered for the course and the count should be 41. Furthermore,
supposethecourseregistrationlimitwas40;intheabovecase bothstudents
would be able to register,leading to aviolationof the limitof 40 students.
• Security problems. Not every user of the database system should be able
to access all the data. For example, in a university, payroll personnel need
to see only that part of the database that has ?nancial information. They do
not need access to information about academic records. But, since applica-
tion programs are added to the ?le-processing system in an ad hoc manner,
enforcing such security constraints is dif?cult.
6 Chapter 1 Introduction
Thesedif?culties,amongothers,promptedthedevelopmentofdatabasesys-
tems. In what follows, we shall see the concepts and algorithms that enable
database systems to solve the problems with ?le-processing systems. In most of
this book, we use a university organization as a running example of a typical
data-processing application.
1.3 View of Data
A database system is a collection of interrelated data and a set of programs that
allow users to access and modify these data. A major purpose of a database
system is to provide users with an abstract view of the data. That is, the system
hides certain detailsofhow the data are storedand maintained.
1.3.1 Data Abstraction
Forthesystemtobeusable,itmustretrievedataef?ciently.Theneedforef?ciency
hasleddesignerstousecomplexdatastructurestorepresentdatainthedatabase.
Sincemanydatabase-systemusersarenotcomputertrained,developershidethe
complexity from users through several levels of abstraction, to simplify users’
interactions with the system:
• Physical level. The lowest level of abstraction describes how the data are ac-
tuallystored.The physical leveldescribescomplexlow-leveldatastructures
in detail.
• Logical level. The next-higher level of abstraction describes what data are
stored in the database, and what relationships exist among those data. The
logicallevelthusdescribesthe entiredatabase intermsofasmallnumberof
relatively simple structures. Although implementation of the simple struc-
tures at the logical level may involve complex physical-level structures, the
user of the logical level does not need to be aware of this complexity. This
is referredto asphysical dataindependence. Database administrators, who
must decide what information to keep in the database, use the logical level
of abstraction.
• View level. The highest level of abstraction describes only part of the entire
database. Even though the logical level uses simpler structures, complexity
remains because of the variety of information stored in a large database.
Many users ofthe databasesystemdonot needallthis information; instead,
theyneedtoaccess only apart ofthe database. The viewlevelofabstraction
existsto simplifytheir interaction with the system. The system may provide
many views for the same database.
Figure 1.1shows the relationship among the threelevelsof abstraction.
An analogy to the concept of data types in programming languages may
clarifythedistinctionamonglevelsofabstraction.Manyhigh-levelprogramming
1.3 View ofData 7
view 1 view 2
logical
level
physical
level
view n
…
view level
Figure 1.1 The three levels of data abstraction.
languagessupportthenotionofastructuredtype.Forexample,wemaydescribe
a recordas follows:
1
type instructor =record
ID :char (5);
name :char (20);
dept name :char (20);
salary :numeric(8,2);
end;
This code de?nes a new record type called instructor with four ?elds. Each ?eld
has a name and a type associated with it. A university organization may have
severalsuch record types,including
• department,with?eldsdept name, building,andbudget
• course,with?eldscourse id, title, dept name,andcredits
• student,with?eldsID, name, dept name,andtot cred
At the physical level, an instructor, department,orstudent record can be de-
scribed as a block of consecutive storage locations. The compiler hides this level
of detail from programmers. Similarly, the database system hides many of the
lowest-level storage details from database programmers. Database administra-
tors, on the other hand, may be aware of certaindetailsof the physical organiza-
tion of the data.
1
Theactualtypedeclarationdependsonthelanguagebeingused.CandC++usestructdeclarations.Javadoesnothave
such a declaration, but a simple class can be de?ned to the same effect.
8 Chapter 1 Introduction
At the logical level, each such record is described by a type de?nition, as
in the previous code segment, and the interrelationship of these record types is
de?ned as well. Programmers using a programming language work at this level
of abstraction. Similarly, database administrators usually work at this level of
abstraction.
Finally, at the view level, computer users see a set of application programs
thathidedetailsofthedatatypes.Attheviewlevel,severalviewsofthedatabase
are de?ned, and a database user sees some or all of these views. In addition
to hiding details of the logical level of the database, the views also provide a
securitymechanismtopreventusersfromaccessingcertainpartsofthedatabase.
For example, clerks in the university registrar of?ce can see only that part of the
database that has information about students; they cannot access information
about salaries of instructors.
1.3.2 Instances and Schemas
Databaseschangeovertimeasinformationisinsertedanddeleted.Thecollection
ofinformationstoredinthedatabaseataparticularmomentiscalledaninstance
ofthedatabase.Theoveralldesignofthedatabaseiscalledthedatabaseschema.
Schemas are changed infrequently,if at all.
Theconceptofdatabaseschemasandinstancescanbeunderstoodbyanalogy
toaprogramwritteninaprogramminglanguage.Adatabaseschemacorresponds
tothevariabledeclarations(alongwithassociatedtypede?nitions)inaprogram.
Eachvariablehasaparticularvalueatagiveninstant.Thevaluesofthevariables
in a program at a point in timecorrespond to an instance of a database schema.
Database systems have several schemas, partitioned according to the levels
ofabstraction.Thephysicalschemadescribesthedatabasedesignatthephysical
level,while thelogicalschema describesthe database designat the logicallevel.
A database may also have several schemas at the view level, sometimes called
subschemas, that describedifferentviewsof the database.
Ofthese,thelogicalschemaisbyfarthemostimportant,intermsofitseffect
onapplicationprograms,sinceprogrammersconstructapplicationsbyusingthe
logicalschema.Thephysicalschemaishiddenbeneaththelogicalschema,andcan
usually be changed easily without affecting application programs. Application
programs are said to exhibit physical data independence if they do not depend
on the physical schema, and thus need not be rewritten if the physical schema
changes.
We study languages for describing schemas after introducing the notion of
datamodelsin the next section.
1.3.3 Data Models
Underlyingthestructureofadatabaseisthedatamodel:acollectionofconceptual
tools for describing data, data relationships, data semantics, and consistency
constraints. A data model providesa way to describe the designof a database at
the physical, logical, and viewlevels.
1.4 Database Languages 9
There are a number of different data models that we shall cover in the text.
The data modelscan be classi?ed into four differentcategories:
• Relational Model. The relational model uses a collection of tables to repre-
sent both data and the relationships among those data. Each table has mul-
tiple columns, and each column has a unique name. Tables are also known
as relations. The relational model is an example of a record-based model.
Record-based models are so named because the database is structured in
?xed-format records of several types. Each table contains records of a par-
ticular type. Each record type de?nes a ?xed number of ?elds, or attributes.
The columns of the table correspond to the attributes ofthe record type.The
relational data model is the most widelyused data model, and a vast major-
ityofcurrentdatabasesystemsarebasedontherelationalmodel.Chapters2
through 8cover the relational modelin detail.
• Entity-Relationship Model. The entity-relationship (E-R) data model uses a
collectionofbasicobjects,calledentities,andrelationshipsamongtheseobjects.
An entity is a “thing” or “object” in the real world that is distinguishable
from other objects. The entity-relationship modelis widelyused in database
design,and Chapter7exploresit indetail.
• Object-BasedDataModel.Object-orientedprogramming(especiallyinJava,
C++, or C#) has become the dominant software-development methodology.
This led to the development of an object-oriented data model that can be
seen as extending the E-R model with notions of encapsulation, methods
(functions), and object identity. The object-relational data model combines
features of the object-oriented data model and relational data model. Chap-
ter 22 examinesthe object-relational data model.
• Semistructured Data Model. The semistructured data model permits the
speci?cationofdatawhereindividualdataitemsofthesametypemayhave
different sets of attributes. This is in contrast to the data models mentioned
earlier, where every data item of a particular type must have the same set
of attributes. The Extensible Markup Language (XML) is widely used to
representsemistructureddata.Chapter23 coversit.
Historically, the network data model and the hierarchical data model pre-
cededtherelationaldatamodel.Thesemodelsweretiedcloselytotheunderlying
implementation,and complicated the taskof modelingdata. Asa result theyare
used little now, except in old database code that is still in service in some places.
Theyare outlined online in AppendicesD and E for interestedreaders.
1.4 Database Languages
A database system provides a data-de?nition language to specify the database
schema and a data-manipulation language to express database queries and up-
10 Chapter 1 Introduction
dates. In practice, the data-de?nition and data-manipulation languages are not
two separate languages; instead they simply form parts of a single database lan-
guage, such as the widelyused SQL language.
1.4.1 Data-Manipulation Language
A data-manipulation language (DML) is a language that enables users to access
or manipulate data as organized by the appropriate data model. The types of
access are:
• Retrievalof information storedin the database
• Insertionof new information into the database
• Deletionof information from the database
• Modi?cation of information stored inthe database
There are basically two types:
• Procedural DMLs require a user to specify what data are needed and how to
get those data.
• Declarative DMLs(alsoreferredtoasnonproceduralDMLs)requireauserto
specify what dataare needed without specifying how to get those data.
Declarative DMLs are usually easier to learn and use than are procedural
DMLs. However, since a user does not have to specify how to get the data, the
database system has to ?gure out an ef?cient means of accessing data.
Aqueryisastatementrequestingtheretrievalofinformation.Theportionof
a DML that involves information retrieval is called a query language.Although
technically incorrect, it is common practice to use the terms query language and
data-manipulation language synonymously.
There are a number of database query languages in use, either commercially
orexperimentally.Westudythemostwidelyusedquerylanguage, SQL,inChap-
ters 3, 4, and 5. We also study some other query languages in Chapter 6.
The levels of abstraction that we discussed in Section 1.3 apply not only
to de?ning or structuring data, but also to manipulating data. At the physical
level, we must de?ne algorithms that allow ef?cient access to data. At higher
levels of abstraction, we emphasize ease of use. The goal is to allow humans
to interact ef?ciently with the system. The query processor component of the
database system (which we study in Chapters 12 and 13) translates DML queries
into sequences of actions at the physical levelof the database system.
1.4.2 Data-De?nition Language
We specify a database schema by a set of de?nitions expressed by a special
languagecalledadata-de?nitionlanguage(DDL).TheDDLisalsousedtospecify
additional propertiesof the data.
1.4 Database Languages 11
We specify the storage structure and access methods used by the database
system by a set of statements in a special type of DDL called a data storage and
de?nition language. These statements de?ne the implementation details of the
database schemas, which are usually hiddenfrom the users.
The data values stored in the database must satisfy certain consistency con-
straints. For example, suppose the university requires that the account balance
of a department must never be negative. The DDL provides facilities to specify
such constraints. The database system checks these constraints every time the
database is updated. In general, a constraint can be an arbitrary predicate per-
tainingtothedatabase.However,arbitrarypredicatesmaybecostlytotest.Thus,
databasesystemsimplementintegrityconstraintsthatcanbetestedwithminimal
overhead:
• Domain Constraints. A domain of possible values must be associated with
everyattribute(forexample,integertypes,charactertypes,date/timetypes).
Declaringanattributetobeofaparticulardomainactsasaconstraintonthe
values that it can take. Domain constraints are the most elementary form of
integrity constraint. They are tested easily by the system whenever a new
dataitemis enteredintothe database.
• Referential Integrity. There are cases where we wish to ensure that a value
thatappearsinonerelationfor agivensetofattributesalsoappearsinacer-
tain set of attributes in another relation (referential integrity). For example,
the department listed for each course must be one that actually exists. More
precisely,the dept namevalue ina course record must appear in the dept name
attributeofsomerecordofthedepartment relation. Database modi?cations
can causeviolationsofreferentialintegrity.Whenareferential-integritycon-
straintisviolated,thenormalprocedureistorejecttheactionthatcausedthe
violation.
• Assertions. An assertion is any condition that the database must always
satisfy. Domain constraints and referential-integrity constraints are special
forms of assertions. However, there are many constraints that we cannot
express by using only these special forms. For example, “Every department
musthaveatleast?vecoursesofferedeverysemester”mustbeexpressedas
an assertion. When an assertion is created, the system tests it for validity. If
theassertionisvalid,thenanyfuturemodi?cationtothedatabaseisallowed
only ifit does not cause that assertionto be violated.
• Authorization. We may want to differentiate among the users as far as the
typeofaccesstheyarepermittedonvariousdatavaluesinthedatabase.These
differentiations are expressed in terms of authorization, the most common
being: read authorization, which allows reading, but not modi?cation, of
data;insertauthorization,whichallowsinsertionofnewdata,butnotmod-
i?cation of existing data; update authorization, which allows modi?cation,
but not deletion,of data;and deleteauthorization,which allows deletionof
data. We may assign the user all, none, or a combination of these types of
authorization.
12 Chapter 1 Introduction
The DDL, just like any other programming language, gets as input some
instructions (statements) and generates some output. The output of the DDL is
placedinthedatadictionary,whichcontainsmetadata—thatis,dataaboutdata.
The data dictionary is considered to be a special type of table that can only be
accessed and updated by the database system itself (not a regular user). The
database system consults the data dictionary before reading or modifying actual
data.
1.5 Relational Databases
A relational database is based on the relational model and uses a collection of
tables to represent both data and the relationships among those data. It also in-
cludes a DML and DDL. In Chapter 2 we present a gentle introduction to the
fundamentals of the relational model. Most commercial relational database sys-
tems employ the SQL language, which we cover in great detail in Chapters 3, 4,
and 5. In Chapter 6 we discuss other in?uential languages.
1.5.1 Tables
Eachtablehas multiplecolumns andeachcolumnhasauniquename.Figure1.2
presents a sample relational database comprising two tables: one shows details
of university instructors and the other shows details of the various university
departments.
The ?rst table, the instructor table, shows, for example, that an instructor
namedEinsteinwith ID22222 isamemberofthePhysics departmentandhasan
annual salary of $95,000. The second table, department, shows, for example, that
the Biology department is located in the Watson building and has a budget of
$90,000. Of course, a real-world university would have many more departments
and instructors. We use small tables in the text to illustrate concepts. A larger
examplefor the same schema isavailable online.
The relational model is an example of a record-based model. Record-based
models are so named because the database is structured in ?xed-format records
ofseveraltypes.Eachtablecontainsrecordsofaparticulartype.Eachrecordtype
de?nesa?xednumberof?elds,orattributes.Thecolumnsofthetablecorrespond
to the attributes of the record type.
It is not hard to see how tables may be stored in ?les. For instance, a special
character (such as a comma) may be used to delimit the different attributes of a
record,and another special character (such as a new-line character) may be used
to delimit records. The relational model hides such low-level implementation
detailsfrom database developersand users.
We also note that it is possible to create schemas in the relational model that
have problems such as unnecessarily duplicated information. For example, sup-
posewestorethedepartmentbudgetasanattributeofthe instructorrecord.Then,
whenever the value of a particular budget (say that one for the Physics depart-
ment) changes, that change must to be re?ected in the records of all instructors
1.5 Relational Databases 13
ID name dept name salary
22222 Einstein Physics 95000
12121 Wu Finance 90000
32343 ElSaid History 60000
45565 Katz Comp.Sci. 75000
98345 Kim Elec.Eng. 80000
76766 Crick Biology 72000
10101 Srinivasan Comp.Sci. 65000
58583 Cali?eri History 62000
83821 Brandt Comp.Sci. 92000
15151 Mozart Music 40000
33456 Gold Physics 87000
76543 Singh Finance 80000
(a) The instructor table
dept name building budget
Comp. Sci. Taylor 100000
Biology Watson 90000
Elec.Eng. Taylor 85000
Music Packard 80000
Finance Painter 120000
History Painter 50000
Physics Watson 70000
(b) The department table
Figure 1.2 A sample relational database.
associated with the Physics department. In Chapter 8, we shall study how to
distinguishgood schema designs from bad schema designs.
1.5.2 Data-Manipulation Language
The SQL query language is nonprocedural. A query takes as input several tables
(possibly only one) and always returns a single table. Here is an example of an
SQL querythat ?nds the names of all instructors in the History department:
select instructor.name
from instructor
where instructor.dept name = ’History’;
Thequeryspeci?esthatthoserowsfromthetableinstructorwherethedept nameis
Historymustberetrieved,andthenameattributeoftheserowsmustbedisplayed.
Morespeci?cally,theresultofexecutingthisqueryisatablewithasinglecolumn
14 Chapter 1 Introduction
labeled name,and asetofrows, eachofwhich contains thenameof aninstructor
whosedept name,isHistory.IfthequeryisrunonthetableinFigure1.2,theresult
will consist of two rows, one with the name El Said and the other with the name
Cali?eri.
Queriesmayinvolveinformationfrommorethanonetable.Forinstance,the
following query ?nds the instructor ID and department name of all instructors
associated with a departmentwith budget of greaterthan $95,000.
select instructor.ID, department.dept name
from instructor, department
where instructor.dept name= department.dept nameand
department.budget> 95000;
If the above query were run on the tables in Figure 1.2, the system would ?nd
that there are two departments with budget of greater than $95,000—Computer
Science and Finance; there are ?ve instructors in these departments. Thus, the
result will consist of a table with two columns (ID, dept name) and ?ve rows:
(12121, Finance), (45565, Computer Science), (10101, Computer Science), (83821,
Computer Science),and (76543, Finance).
1.5.3 Data-De?nition Language
SQL provides a rich DDL that allows one to de?ne tables, integrity constraints,
assertions, etc.
For instance, the following SQL DDL statementde?nesthe departmenttable:
createtable department
(dept name char (20),
building char (15),
budget numeric (12,2));
Execution of the above DDL statement creates the department table with three
columns: dept name, building,andbudget, each of which has a speci?c data type
associatedwithit.WediscussdatatypesinmoredetailinChapter3.Inaddition,
the DDL statement updates the data dictionary, which contains metadata (see
Section1.4.2). The schema of atable is an exampleof metadata.
1.5.4 Database Access from Application Programs
SQL is not as powerful as a universal Turing machine; that is, there are some
computations that are possible using a general-purpose programming language
but are not possible using SQL. SQL also does not support actions such as input
from users, output to displays, or communication over the network. Such com-
putationsandactionsmustbewrittenina hostlanguage,suchasC,C++,orJava,
with embedded SQL queries that access the data in the database. Application
programsareprogramsthatareusedtointeractwiththedatabaseinthisfashion.
1.6 Database Design 15
Examplesin a universitysystemare programs that allow students to register for
courses,generateclassrosters,calculatestudentGPA,generatepayrollchecks,etc.
To access the database, DML statements need to be executed from the host
language. There are two ways to do this:
• By providing an application program interface (set of procedures) that can
be used to send DML and DDL statements to the database and retrieve the
results.
The Open Database Connectivity (ODBC) standard for use with the C
language is a commonly used application program interface standard. The
JavaDatabaseConnectivity(JDBC)standardprovidescorrespondingfeatures
tothe Javalanguage.
• By extending the host language syntax to embed DML calls within the host
language program. Usually, a special character prefaces DML calls, and a
preprocessor, called the DML precompiler, converts the DML statements to
normal procedurecalls in the host language.
1.6 Database Design
Databasesystemsaredesignedtomanagelargebodiesofinformation.Theselarge
bodies of information do not exist in isolation. They are part of the operation of
some enterprise whose end product may be information from the database or
may be some device or service for which the database plays only a supporting
role.
Database design mainly involves the design of the database schema. The
design of a complete database application environment that meets the needs of
the enterprise being modeled requires attention to a broader set of issues. In
this text, we focus initially on the writing of database queries and the design of
database schemas. Chapter 9discusses the overall process of applicationdesign.
1.6.1 Design Process
Ahigh-leveldatamodelprovidesthedatabasedesignerwithaconceptualframe-
work in which to specify the data requirements of the database users, and how
the database will be structured to ful?ll these requirements. The initial phase of
database design, then, is to characterize fully the data needs of the prospective
databaseusers.Thedatabasedesignerneedstointeractextensivelywithdomain
expertsanduserstocarryoutthistask.Theoutcomeofthisphaseisaspeci?cation
of userrequirements.
Next, the designer chooses a data model, and by applying the concepts of
thechosendatamodel,translatestheserequirementsintoaconceptualschemaof
the database. The schema developedat thisconceptual-design phase providesa
detailedoverviewof the enterprise.The designerreviewsthe schema tocon?rm
that all data requirements are indeed satis?ed and are not in con?ict with one
another. The designer can also examine the design to remove any redundant
16 Chapter 1 Introduction
features. The focus at this point is on describing the data and their relationships,
rather than on specifying physical storage details.
In terms of the relational model, the conceptual-design process involves de-
cisions on what attributes we want to capture in the database and how to group
theseattributestoformthevarioustables.The “what”partisbasicallyabusiness
decision,andweshallnotdiscussitfurtherinthistext.The“how”partismainlya
computer-scienceproblem.Thereareprincipallytwowaystotackletheproblem.
The ?rst one is to use the entity-relationship model (Section 1.6.3); the other is
to employ a set of algorithms (collectively known as normalization) that takes as
input the set of all attributesand generatesa set oftables (Section1.6.4).
A fully developed conceptual schema indicates the functional requirements
oftheenterprise.Inaspeci?cationoffunctionalrequirements,usersdescribethe
kindsofoperations(ortransactions)thatwillbeperformedonthedata.Example
operations include modifying or updating data, searching for and retrieving
speci?c data, and deleting data. At this stage of conceptual design, the designer
can reviewthe schema to ensure itmeetsfunctional requirements.
Theprocessofmovingfromanabstractdatamodeltotheimplementationof
thedatabaseproceedsintwo?naldesignphases.Inthelogical-designphase,the
designer maps the high-level conceptual schema onto the implementation data
model of the database system that will be used. The designer uses the resulting
system-speci?c database schema in the subsequent physical-design phase,in
which the physical features of the database are speci?ed. These features include
theformof?leorganizationandtheinternalstoragestructures;theyarediscussed
in Chapter 10.
1.6.2 Database Design for a University Organization
To illustrate the design process, let us examine how a database for a university
could be designed. The initial speci?cation of user requirements may be based
on interviews with the database users, and on the designer’s own analysis of
theorganization. Thedescriptionthatarisesfromthisdesignphaseservesasthe
basis for specifying the conceptual structure of the database. Here are the major
characteristics of the university.
• The universityis organized into departments. Each departmentis identi?ed
by a unique name (dept name), is located in a particular building, and has a
budget.
• Eachdepartmenthasalistofcoursesitoffers.Eachcoursehasassociatedwith
it a course id, title, dept name,andcredits, and may also have have associated
prerequisites.
• Instructors are identi?ed by their unique ID. Each instructor has name, asso-
ciated department(dept name), and salary.
• Studentsareidenti?edbytheirunique ID.Eachstudenthasaname,anassoci-
atedmajordepartment(dept name),andtot cred(totalcredithoursthestudent
earned thus far).
1.6 Database Design 17
  The university maintains a list of classrooms, specifying the name of the
building, room number,androom capacity.
  Theuniversitymaintainsalistofallclasses(sections)taught.Eachsectionis
identi?ed by a course id, sec id, year,andsemester, and has associated with it
a semester, year, building, room number,andtime slot id(thetimeslotwhenthe
classmeets).
  The department has a list of teaching assignments specifying, for each in-
structor,thesectionstheinstructoristeaching.
  The university has a list of all student course registrations, specifying, for
each student, the courses and the associated sections that the student has
taken(registeredfor).
Arealuniversitydatabasewouldbemuchmorecomplexthanthepreceding
design.Howeverweusethissimpli?edmodeltohelpyouunderstandconceptual
ideaswithoutgettinglostindetailsofacomplexdesign.
1.6.3 The Entity-Relationship Model
The entity-relationship (E-R) data model uses a collection of basic objects, called
entities,andrelationships among these objects. An entity is a “thing” or “object”
in the real world that is distinguishable from other objects. For example, each
personisanentity,andbankaccountscanbeconsideredasentities.
Entities are described in a database by a set of attributes. For example, the
attributesdept name,building,andbudgetmaydescribeoneparticulardepartment
in a university, and they form attributes of the department entity set. Similarly,
attributes ID, name,andsalarymaydescribean instructorentity.
2
The extraattribute IDisusedtoidentifyaninstructor uniquely(since it may
be possible to have two instructors with the same name and the same salary).
A unique instructor identi?er must be assigned to each instructor. In the United
States,many organizations use the social-security number of a person (a unique
number the U.S. government assigns to every person in the United States) as a
uniqueidenti?er.
Arelationshipisanassociationamongseveralentities.Forexample,amember
relationship associates an instructor with her department. The set of all entities
of the same type and the set of all relationships of the same type are termed an
entity setand relationship set,respectively.
The overalllogical structure (schema) of adatabase can be expressedgraph-
ically by an entity-relationship (E-R)diagram . There are several ways in which to
draw these diagrams. One of the most popular is to use the Uni?ed Modeling
Language (UML).Inthenotationweuse,whichisbasedon UML,anE-Rdiagram
isrepresentedasfollows:
2
Theastutereaderwillnoticethatwedroppedtheattribute dept namefromthesetofattributesdescribingthe instructor
entityset;thisisnotanerror.InChapter7weshallprovideadetailedexplanationofwhythisisthecase.
18 Chapter 1 Introduction
instructor
ID
name
salary
department
dept_name
building
budget
member
Figure 1.3 A sample E-R diagram.
• Entity sets are represented by a rectangular box with the entity set name in
the header and the attributeslistedbelow it.
• Relationship sets are represented by a diamond connecting a pair of related
entity sets.The name of the relationshipis placedinside the diamond.
Asanillustration,considerpartofauniversitydatabaseconsistingofinstruc-
tors and the departments with which they are associated. Figure 1.3 shows the
corresponding E-R diagram. The E-R diagram indicates that there are two entity
sets, instructor and department, with attributes as outlined earlier. The diagram
also shows a relationship memberbetween instructor and department.
In addition to entities and relationships, the E-R model represents certain
constraints to which the contents of a database must conform. One important
constraint is mapping cardinalities, which express the number of entities to
whichanotherentitycanbeassociatedviaarelationshipset.Forexample,ifeach
instructor must be associated with only a single department, the E-R model can
expressthat constraint.
Theentity-relationshipmodeliswidelyusedindatabasedesign,andChapter
7 exploresit in detail.
1.6.4 Normalization
Anothermethodfordesigningarelationaldatabaseistouseaprocesscommonly
known as normalization. The goal is to generate a set of relation schemas that
allows us to store information without unnecessary redundancy, yet also allows
us to retrieve information easily. The approach is to design schemas that are in
an appropriate normal form. To determine whether a relation schema is in one of
thedesirablenormalforms,weneedadditionalinformationaboutthereal-world
enterprise that we are modeling with the database. The most common approach
isto usefunctionaldependencies,which we cover in Section8.4.
To understand the need for normalization, let us look at what can go wrong
in a bad database design. Among the undesirable properties that a bad design
may have are:
• Repetitionofinformation
• Inability to representcertaininformation
1.6 Database Design 19
ID name salary dept name building budget
22222 Einstein 95000 Physics Watson 70000
12121 Wu 90000 Finance Painter 120000
32343 ElSaid 60000 History Painter 50000
45565 Katz 75000 Comp.Sci. Taylor 100000
98345 Kim 80000 Elec.Eng. Taylor 85000
76766 Crick 72000 Biology Watson 90000
10101 Srinivasan 65000 Comp.Sci. Taylor 100000
58583 Cali?eri 62000 History Painter 50000
83821 Brandt 92000 Comp.Sci. Taylor 100000
15151 Mozart 40000 Music Packard 80000
33456 Gold 87000 Physics Watson 70000
76543 Singh 80000 Finance Painter 120000
Figure 1.4 The faculty table.
We shall discuss these problems with the help of a modi?ed database design for
our universityexample.
Suppose that instead of having the two separate tables instructor and depart-
ment,wehaveasingletable,faculty, that combines the information from the two
tables (as shown in Figure 1.4). Notice that there are two rows in faculty that
contain repeated information about the History department, speci?cally, that
department’s building and budget. The repetition of information in our alterna-
tive design is undesirable. Repeating information wastes space. Furthermore, it
complicates updating the database. Suppose that we wish to change the budget
amount of the History department from $50,000 to $46,800. This change must
be re?ected in the two rows; contrast this with the original design, where this
requiresanupdatetoonly asinglerow. Thus, updatesaremore costly underthe
alternative design than under the original design. When we perform the update
inthealternativedatabase,wemustensurethateverytuplepertainingtotheHis-
torydepartmentisupdated,or elseour databasewill show twodifferentbudget
valuesforthe Historydepartment.
Now, let us shift our attention to the issue of “inability to represent certain
information.”Supposewearecreatinganewdepartmentintheuniversity.Inthe
alternativedesignabove,wecannotrepresentdirectlytheinformationconcerning
adepartment(dept name, building, budget)unlessthatdepartmenthasatleastone
instructor at the university. This is because rows in the faculty table require
values for ID, name,andsalary. This means that we cannot record information
about the newly createddepartmentuntil the ?rst instructor ishiredfor thenew
department.
One solution to this problem is to introduce null values. The null value
indicates that the value does not exist (or is not known). An unknown value
maybeeither missing (thevaluedoesexist,butwedonothavethatinformation)
or not known (we do not know whether or not the value actually exists). As we
20 Chapter 1 Introduction
shall see later,null values are dif?cult to handle, and it is preferablenot to resort
to them. If we are not willing to deal with null values, then we can create a
particularitemofdepartmentinformationonlywhenthedepartmenthasatleast
one instructor associated with the department. Furthermore, we would have
to delete this information when the last instructor in the department departs.
Clearly, this situation is undesirable, since, under our original database design,
the department information would be available regardless of whether or not
there is an instructor associated with the department, and without resorting to
null values.
Anextensivetheoryofnormalizationhasbeendevelopedthathelpsformally
de?ne what database designs are undesirable, and how to obtain desirable de-
signs. Chapter 8 covers relational-database design,including normalization.
1.7 Data Storage and Querying
A database system is partitioned into modules that deal with each of the re-
sponsibilities of the overall system. The functional components of a database
systemcanbebroadlydividedintothestoragemanagerandthequeryprocessor
components.
Thestoragemanagerisimportantbecausedatabasestypicallyrequirealarge
amount of storage space. Corporate databases range in size from hundreds of
gigabytes to, for the largest databases, terabytes of data. A gigabyte is approxi-
mately1000megabytes(actually1024)(1billionbytes),andaterabyteis1million
megabytes (1 trillion bytes). Since the main memory of computers cannot store
this much information, the information is stored on disks. Data are moved be-
tween disk storage and main memory as needed. Since the movement of data
to and from disk is slow relative to the speed of the central processing unit, it is
imperativethatthedatabasesystemstructurethedatasoastominimizetheneed
to move databetweendiskand mainmemory.
The query processor is important because it helps the database system to
simplify and facilitate access to data. The query processor allows database users
toobtaingoodperformancewhilebeingabletoworkattheviewlevelandnotbe
burdenedwithunderstandingthephysical-leveldetailsoftheimplementationof
the system. It is the job of the database system to translate updates and queries
writteninanonprocedurallanguage,atthelogicallevel,intoanef?cientsequence
of operationsat the physical level.
1.7.1 Storage Manager
The storage manager is the component of a database system that provides the
interface between the low-level data stored in the database and the application
programs and queries submitted to the system. The storage manager is respon-
sible for the interaction with the ?le manager. The raw data are stored on the
disk using the ?le system provided by the operating system. The storage man-
agertranslatesthevarious DMLstatementsintolow-level?le-systemcommands.
1.7 Data Storage and Querying 21
Thus, the storage manager is responsible for storing, retrieving, and updating
datainthedatabase.
Thestoragemanagercomponentsinclude:
  Authorization and integrity manager, which tests for the satisfaction of
integrityconstraintsandcheckstheauthorityofuserstoaccessdata.
  Transaction manager, which ensures that the database remains in a consis-
tent (correct) state despite system failures, and that concurrent transaction
executionsproceedwithoutcon?icting.
  Filemanager,whichmanagestheallocationofspaceondiskstorageandthe
datastructuresusedtorepresentinformationstoredondisk.
  Buffermanager,whichisresponsibleforfetchingdatafromdiskstorageinto
mainmemory,anddecidingwhatdatatocacheinmainmemory.Thebuffer
managerisacriticalpartofthedatabasesystem,sinceitenablesthedatabase
tohandledatasizesthataremuchlargerthanthesizeofmainmemory.
Thestoragemanagerimplementsseveraldatastructuresaspartofthephys-
icalsystemimplementation:
  Data ?les,whichstorethedatabaseitself.
  Data dictionary, which stores metadataabout the structure of the database,
inparticulartheschemaofthedatabase.
  Indices, which can provide fast access to data items. Like the index in this
textbook,adatabase indexprovidespointerstothose dataitemsthathold a
particular value. For example, we could use an index to ?nd the instructor
record with a particular ID,orallinstructor records with a particular name.
Hashingisanalternativetoindexingthatisfasterinsomebutnotallcases.
Wediscussstoragemedia,?lestructures,andbuffermanagementinChapter10.
Methods of accessing data ef?ciently via indexing or hashing are discussed in
Chapter11.
1.7.2 The Query Processor
Thequeryprocessorcomponentsinclude:
  DDLinterpreter,whichinterpretsDDLstatementsandrecordsthede?nitions
inthedatadictionary.
  DMLcompiler,whichtranslatesDMLstatementsinaquerylanguageintoan
evaluationplanconsistingoflow-levelinstructionsthatthequeryevaluation
engineunderstands.
22 Chapter 1 Introduction
A query can usually be translated into any of a number of alternative
evaluationplansthatallgivethesameresult.TheDMLcompileralsoperforms
query optimization; that is, it picks the lowest cost evaluation plan from
among the alternatives.
• Query evaluation engine, which executes low-level instructions generated
by the DML compiler.
QueryevaluationiscoveredinChapter12,whilethemethodsbywhichthequery
optimizer chooses from among the possible evaluation strategies are discussed
in Chapter 13.
1.8 Transaction Management
Often, several operations on the database form a single logical unit of work. An
example is a funds transfer, as in Section 1.2, in which one department account
(say A) is debited and another department account (say B) is credited. Clearly, it
is essential that either both the credit and debit occur, or that neither occur. That
is, the funds transfer must happen in its entirety or not at all. This all-or-none
requirementiscalledatomicity.Inaddition,itisessentialthattheexecutionofthe
funds transfer preserve the consistency of the database. That is, the value of the
sum of the balances of Aand B must be preserved.Thiscorrectnessrequirement
is called consistency. Finally, after the successful execution of a funds transfer,
the new values of the balances of accounts A and B must persist, despite the
possibilityof systemfailure.This persistencerequirementis calleddurability.
A transaction is a collection of operations that performs a single logical
function in a database application. Each transaction is a unit of both atomicity
and consistency. Thus, we require that transactions do not violate any database-
consistencyconstraints.Thatis,ifthedatabasewasconsistentwhenatransaction
started, the database must be consistent when the transaction successfully ter-
minates. However, during the execution of a transaction, it may be necessary
temporarily to allow inconsistency, since either the debit of A or the credit of B
mustbedonebeforetheother.Thistemporaryinconsistency,althoughnecessary,
may leadto dif?culty if a failure occurs.
It is the programmer’s responsibility to de?ne properly the various transac-
tions, so that each preserves the consistency of the database. For example, the
transaction to transfer funds from the account of department A to the account of
department Bcouldbede?nedtobecomposedoftwoseparateprograms:one
that debits account A, and another that credits account B. The execution of these
twoprogramsoneaftertheotherwillindeedpreserveconsistency.However,each
programbyitselfdoesnottransformthedatabasefromaconsistentstatetoanew
consistent state. Thus, those programs are not transactions.
Ensuring the atomicity and durability properties is the responsibility of the
database system itself—speci?cally, of the recovery manager. In the absence of
failures, all transactions complete successfully, and atomicity is achieved easily.
1.9 Database Architecture 23
However,because ofvarioustypesoffailure,atransaction maynot always com-
pleteitsexecutionsuccessfully.Ifwearetoensuretheatomicityproperty,afailed
transaction must have no effect on the state of the database. Thus, the database
must be restored to the state in which it was before the transaction in question
startedexecuting.Thedatabasesystemmustthereforeperformfailurerecovery,
that is, detect system failures and restore the database to the state that existed
prior to the occurrence of the failure.
Finally,whenseveraltransactionsupdatethedatabaseconcurrently,thecon-
sistencyofdatamaynolongerbepreserved,eventhougheachindividualtransac-
tioniscorrect.Itistheresponsibilityoftheconcurrency-controlmanagertocon-
trol the interaction among the concurrent transactions, to ensure the consistency
of the database. The transaction manager consists of the concurrency-control
manager and the recoverymanager.
The basic concepts of transaction processing are covered in Chapter 14. The
management of concurrent transactions is covered in Chapter 15. Chapter 16
coversfailure recoveryin detail.
The concept of a transaction has been applied broadly in database systems
and applications. While the initial use of transactions was in ?nancial applica-
tions,theconceptisnowusedinreal-timeapplicationsintelecommunication,as
well as in the management of long-duration activities such as product design or
administrativework?ows. Thesebroaderapplicationsofthetransactionconcept
are discussedin Chapter 26.
1.9 Database Architecture
We are now in a position to provide a single picture (Figure 1.5) of the various
components of a database system and the connections among them.
Thearchitectureofadatabasesystemisgreatlyin?uencedbytheunderlying
computer system on which the database system runs. Database systems can be
centralized, or client-server, where one server machine executes work on behalf
ofmultipleclientmachines.Databasesystemscanalsobedesignedtoexploitpar-
allelcomputerarchitectures.Distributeddatabasesspanmultiplegeographically
separatedmachines.
In Chapter 17 we cover the general structure of modern computer systems.
Chapter 18 describes how various actions of a database, in particular query pro-
cessing,canbeimplementedtoexploitparallelprocessing.Chapter19presentsa
number of issues that arise in a distributed database, and describes how to deal
witheachissue.Theissuesincludehowtostoredata,howtoensureatomicityof
transactions that execute at multiple sites, how to perform concurrency control,
andhowtoprovidehighavailabilityinthepresenceoffailures.Distributedquery
processing and directorysystems are alsodescribedin this chapter.
Most users of a database system today are not present at the site of the
database system, but connect to it through a network. We can therefore differen-
tiatebetweenclientmachines,onwhichremotedatabaseuserswork,andserver
machines, on which the database system runs.
24 Chapter 1 Introduction
Database applications are usually partitioned into two or three parts, as in
Figure1.6.Inatwo-tierarchitecture,theapplicationresidesattheclientmachine,
where it invokes database system functionality at the server machine through
naive users
(tellers, agents, 
web users)
query processor
storage manager
disk storage
indices
statistical data data
data dictionary
application
programmers
application
interfaces
application
program
object code
compiler and
linker
bu?er manager ?le manager
authorization
and integrity
 manager
transaction
manager
DML compiler 
and organizer
query evaluation
engine
DML queries DDL interpreter
application
programs
query
tools
administration
tools
sophisticated
users
(analysts)
database
administrators
use write use use
Figure 1.5 System structure.
1.10 Data Mining and InformationRetrieval 25
user
application
database system
network
(a) Two-tier architecture
client
server
user
application client
database system
network
application server
(b) Three-tier architecture
Figure 1.6 Two-tier and three-tier architectures.
query language statements. Application program interface standards like ODBC
and JDBC are used forinteraction betweenthe clientand the server.
In contrast, in a three-tier architecture, the client machine acts as merely a
front end and does not contain any direct database calls. Instead, the client end
communicates with an application server, usually through a forms interface.
The application server in turn communicates with a database system to access
data. Thebusinesslogic of the application,which says what actions to carry out
under what conditions, is embedded in the application server, instead of being
distributed across multiple clients. Three-tier applications are more appropriate
for largeapplications, and for applications that run on the World Wide Web.
1.10 Data Mining and Information Retrieval
Thetermdataminingreferslooselytotheprocessofsemiautomaticallyanalyzing
large databases to ?nd useful patterns. Like knowledge discovery in arti?cial
intelligence (also called machine learning) or statistical analysis, data mining
attempts to discover rulesand patterns from data. However, data mining differs
from machine learning and statistics in that it deals with large volumes of data,
stored primarily on disk. That is, data mining deals with “knowledge discovery
in databases.”
Some types of knowledge discoveredfrom a database can be representedby
asetofrules. The following is an example of a rule, stated informally: “Young
womenwithannualincomesgreaterthan$50,000arethemostlikelypeopletobuy
small sports cars.” Of course such rules are not universally true, but rather have
26 Chapter 1 Introduction
degreesof“support”and“con?dence.”Othertypesofknowledgearerepresented
by equations relating different variables to each other, or by other mechanisms
for predictingoutcomes when the valuesof some variablesare known.
There are a variety of possible types of patterns that may be useful, and
differenttechniquesareusedto?nddifferenttypesofpatterns.InChapter20we
studyafewexamplesofpatternsandseehowtheymaybeautomaticallyderived
from a database.
Usuallythereisamanualcomponenttodatamining,consistingofpreprocess-
ingdatatoaformacceptabletothealgorithms,andpostprocessingofdiscovered
patterns to ?nd novel ones that could be useful. There may also be more than
one type of pattern that can be discovered from a given database, and manual
interaction may be needed to pick useful types of patterns. For this reason, data
miningisreallyasemiautomaticprocessinreallife.However,inourdescription
we concentrate on the automatic aspect of mining.
Businesses have begun to exploit the burgeoning data online to make better
decisions about their activities, such as what items to stock and how best to
target customers to increase sales. Many of their queries are rather complicated,
however,andcertaintypesofinformationcannotbeextractedevenbyusingSQL.
Several techniques and tools are available to help with decision support.
Several tools for data analysis allow analysts to view data in different ways.
Other analysis tools precompute summaries of very large amounts of data, in
order to give fast responses to queries. The SQL standard contains additional
constructs to support dataanalysis.
Largecompanieshavediversesourcesofdatathattheyneedtouseformaking
businessdecisions.Toexecutequeriesef?cientlyonsuchdiversedata,companies
have built data warehouses. Data warehouses gather data from multiple sources
under a uni?ed schema, at a single site. Thus, they provide the user a single
uniform interface to data.
Textualdata,too,hasgrownexplosively.Textualdataisunstructured,unlike
the rigidly structured data in relational databases. Querying of unstructured
textual data is referred to as information retrieval. Information retrieval systems
have much in common with database systems—in particular, the storage and
retrieval of data on secondary storage. However, the emphasis in the ?eld of
information systems is differentfrom that in database systems, concentrating on
issues such as querying based on keywords; the relevance of documents to the
query;andtheanalysis,classi?cation,andindexingofdocuments.InChapters20
and 21, we cover decision support, including online analytical processing, data
mining, data warehousing, and information retrieval.
1.11 Specialty Databases
Severalapplicationareasfordatabasesystemsarelimitedbytherestrictionsofthe
relationaldatamodel.Asaresult,researchershavedevelopedseveraldatamodels
to deal with these application domains, including object-based data models and
semistructureddata models.
1.12 Database Usersand Administrators 27
1.11.1 Object-Based Data Models
Object-oriented programming has become the dominant software-development
methodology.Thisledtothedevelopmentofanobject-orienteddatamodelthat
can be seen as extending the E-R model with notions of encapsulation, methods
(functions), and object identity. Inheritance, object identity, and encapsulation
(informationhiding),withmethodstoprovideaninterfacetoobjects,areamong
thekeyconceptsofobject-orientedprogrammingthathavefoundapplicationsin
datamodeling. The object-oriented data modelalso supportsa rich type system,
including structuredand collection types.In the 1980s, severaldatabase systems
based on the object-orienteddata modelwere developed.
The major database vendors presently support the object-relational data
model,adatamodelthatcombinesfeaturesoftheobject-orienteddatamodeland
relational data model. It extends the traditional relational model with a variety
of features such as structured and collection types, as well as object orientation.
Chapter 22 examines the object-relational data model.
1.11.2 Semistructured Data Models
Semistructured data models permit the speci?cation of data where individual
dataitemsofthesametypemayhavedifferentsetsofattributes.Thisisincontrast
with the data models mentioned earlier, where every data item of a particular
type must have the same set of attributes.
The XML language was initially designed as a way of adding markup infor-
mationtotextdocuments,buthasbecomeimportantbecauseofitsapplicationsin
dataexchange. XML providesa way to representdata that have nestedstructure,
and furthermore allows a great deal of ?exibility in structuring of data, which is
important for certain kinds of nontraditional data. Chapter 23 describes the XML
language, different ways of expressing queries on data represented in XML,and
transforming XML data from one form to another.
1.12 Database Users and Administrators
A primary goal of a database system is to retrieve information from and store
new information into the database. People who work with a database can be
categorizedas database users or database administrators.
1.12.1 Database Users and User Interfaces
Therearefourdifferenttypesofdatabase-systemusers,differentiatedbytheway
they expect to interact with the system. Different types of user interfaces have
been designedfor the differenttypesof users.
• Na¨ ?ve users are unsophisticated users who interact with the system by in-
voking one of the application programs that have been written previously.
For example, a clerk in the university who needs to add a new instructor to
28 Chapter 1 Introduction
departmentAinvokesaprogramcallednew hire.Thisprogramaskstheclerk
for the name of the new instructor, her new ID, the name of the department
(thatis, A),andthesalary.
The typical user interface for na¨ ?ve users is a forms interface, where the
user can ?ll in appropriate ?elds of the form. Na¨ ?ve users may also simply
read reportsgeneratedfromthedatabase.
As another example, consider a student, who during class registration
period, wishes to register for a class by using a Web interface. Such a user
connectstoaWebapplicationprogramthatrunsataWebserver.Theappli-
cation ?rst veri?es the identity of the user, and allows her to access a form
wheresheentersthedesiredinformation.Theforminformationissentback
to the Web application at the server,which then determinesif there is room
inthe class (by retrievinginformation from the database) and if so addsthe
studentinformationtotheclassrosterinthedatabase.
  Applicationprogrammersarecomputerprofessionalswhowriteapplication
programs.Applicationprogrammerscanchoosefrommanytoolstodevelop
userinterfaces.Rapidapplicationdevelopment(RAD)toolsaretoolsthaten-
ableanapplicationprogrammertoconstructformsandreportswithminimal
programmingeffort.
  Sophisticated users interact with the system without writing programs. In-
stead,theyformtheirrequestseitherusingadatabasequerylanguageorby
usingtoolssuchasdataanalysissoftware.Analystswhosubmitqueriesto
exploredatainthedatabasefallinthiscategory.
  Specialized users are sophisticated users who write specialized database
applications that do not ?t into the traditional data-processing framework.
Among these applications are computer-aided design systems, knowledge-
baseandexpertsystems,systemsthatstoredatawithcomplexdatatypes(for
example,graphicsdataandaudiodata),andenvironment-modelingsystems.
Chapter22coversseveraloftheseapplications.
1.12.2 Database Administrator
OneofthemainreasonsforusingDBMSsistohavecentralcontrolofboththedata
and the programs that access those data. A person who has such central control
overthesystemiscalledadatabase administrator(DBA).Thefunctionsofa DBA
include:
  Schema de?nition.TheDBAcreatestheoriginaldatabaseschemabyexecut-
ingasetofdatade?nitionstatementsinthe DDL.
  Storage structure and access-method de?nition.
  Schemaandphysical-organizationmodi?cation.TheDBAcarriesoutchanges
totheschemaandphysicalorganizationtore?ectthechangingneedsofthe
organization,ortoalterthephysicalorganizationtoimproveperformance.
1.13 History ofDatabase Systems 29
• Granting of authorization for data access. By granting different types of
authorization, the database administrator can regulate which parts of the
databasevarioususerscanaccess.Theauthorizationinformationiskeptina
specialsystemstructurethatthedatabasesystemconsultswheneversomeone
attemptsto access the datain the system.
• Routine maintenance. Examples of the database administrator’s routine
maintenance activitiesare:
?
Periodically backing up the database, either onto tapes or onto remote
servers,topreventlossof data incase of disasterssuch as ?ooding.
?
Ensuring that enough free disk space is available for normal operations,
and upgradingdiskspace as required.
?
Monitoring jobs running on the database and ensuring that performance
isnot degradedby veryexpensivetaskssubmittedby some users.
1.13 History of Database Systems
Information processing drives the growth of computers, as it has from the earli-
est days of commercial computers. In fact, automation of data processing tasks
predates computers. Punched cards, invented by Herman Hollerith, were used
at the very beginning of the twentieth century to record U.S. census data, and
mechanicalsystemswereusedtoprocessthecardsandtabulateresults.Punched
cardswere laterwidelyused as ameans of enteringdatainto computers.
Techniquesfor datastorage and processinghave evolvedoverthe years:
• 1950sandearly1960s:Magnetictapesweredevelopedfordatastorage.Data
processing tasks such as payroll were automated, with data stored on tapes.
Processing of data consisted of reading data from one or more tapes and
writing data to a new tape. Data could also be input from punched card
decks, and output to printers. For example, salary raises were processed by
entering the raises on punched cards and reading the punched card deck in
synchronizationwithatapecontainingthemastersalarydetails.Therecords
had to be in the same sorted order. The salary raises would be added to the
salary read from the master tape, and written to a new tape; the new tape
would become the new master tape.
Tapes(andcarddecks)couldbereadonlysequentially,anddatasizeswere
muchlargerthanmainmemory;thus,dataprocessingprogramswereforced
toprocessdatainaparticularorder,byreadingandmergingdatafromtapes
and card decks.
• Late1960sand1970s:Widespreaduseofharddisksinthelate1960schanged
thescenariofordataprocessinggreatly,sinceharddisksalloweddirectaccess
to data. The position of data on disk was immaterial, since any location on
diskcouldbeaccessedinjusttensofmilliseconds.Datawerethusfreedfrom
30 Chapter 1 Introduction
the tyranny of sequentiality. With disks, network and hierarchical databases
could be created that allowed data structures such as lists and trees to be
stored on disk. Programmers could construct and manipulate these data
structures.
A landmark paper by Codd [1970] de?ned the relational model and
nonprocedural ways of queryingdata in the relationalmodel,and relational
databaseswereborn.Thesimplicityoftherelationalmodelandthepossibility
of hiding implementation details completely from the programmer were
enticing indeed. Codd later won the prestigious Association of Computing
Machinery Turing Award for his work.
• 1980s:Althoughacademicallyinteresting,therelationalmodelwasnotused
inpracticeinitially,becauseofitsperceivedperformancedisadvantages;rela-
tionaldatabasescouldnotmatchtheperformanceofexistingnetworkandhi-
erarchicaldatabases.ThatchangedwithSystemR,agroundbreakingproject
atIBMResearchthatdevelopedtechniquesfortheconstructionofanef?cient
relationaldatabasesystem.ExcellentoverviewsofSystemRareprovidedby
Astrahan et al. [1976] and Chamberlin et al. [1981]. The fully functional Sys-
tem R prototype led to IBM’s ?rst relational database product, SQL/DS.At
the same time, the Ingres system was being developed at the University of
California at Berkeley. It led to a commercial product of the same name. Ini-
tial commercial relational database systems, such as IBM DB2, Oracle, Ingres,
and DEC Rdb, played a major role in advancing techniques for ef?cient pro-
cessing of declarative queries. By the early 1980s, relational databases had
becomecompetitivewithnetworkandhierarchicaldatabasesystemsevenin
the area of performance. Relational databases were so easy to use that they
eventuallyreplacednetworkandhierarchicaldatabases;programmersusing
suchdatabaseswereforcedtodealwithmanylow-levelimplementationde-
tails,andhadtocodetheirqueriesinaproceduralfashion.Mostimportantly,
they had to keep ef?ciency in mind when designing their programs, which
involved a lot of effort. In contrast, in a relational database, almost all these
low-level tasks are carried out automatically by the database, leaving the
programmer free to work at a logical level.Since attaining dominance in the
1980s, the relational model has reignedsupremeamong data models.
The 1980s also saw much research on parallel and distributed databases,
as well as initial work on object-orienteddatabases.
• Early 1990s:TheSQL language was designed primarily for decision support
applications,whicharequery-intensive,yetthemainstayofdatabasesinthe
1980s was transaction-processing applications, which are update-intensive.
Decision support and querying re-emerged as a major application area for
databases. Tools for analyzing large amounts of data saw large growths in
usage.
Many database vendors introduced parallel database products in this
period.Databasevendorsalsobegantoaddobject-relationalsupporttotheir
databases.
1.14 Summary 31
• 1990s: The major event of the 1990s was the explosive growth of the World
WideWeb.Databasesweredeployedmuchmoreextensivelythaneverbefore.
Databasesystemsnowhadtosupportveryhightransaction-processingrates,
as well as very high reliability and 24 × 7 availability (availability 24 hours
a day, 7 days a week, meaning no downtime for scheduled maintenance
activities).Database systems alsohad to support Web interfaces todata.
• 2000s:The?rsthalfofthe2000ssawtheemergingof XMLandtheassociated
query language XQuery as a new database technology. Although XML is
widely used for data exchange, as well as for storing certain complex data
types,relationaldatabases stillform the coreofavast majorityoflarge-scale
databaseapplications.Inthistimeperiodwehavealsowitnessedthegrowth
in “autonomic-computing/auto-admin” techniques for minimizing system
administration effort. This period also saw a signi?cant growth in use of
open-source database systems,particularly PostgreSQL and MySQL.
The latter part of the decadehas seen growth in specializeddatabases for
data analysis, in particular column-stores, which in effect store each column
of a table as a separate array, and highly parallel database systems designed
for analysis of very large data sets. Several novel distributed data-storage
systemshavebeenbuilttohandlethedatamanagementrequirementsofvery
large Web sites such as Amazon, Facebook, Google, Microsoft and Yahoo!,
and some of these are now offered as Web services that can be used by
applicationdevelopers.Therehasalsobeensubstantialworkonmanagement
andanalysisofstreamingdata,suchasstock-markettickerdataorcomputer
networkmonitoringdata.Data-miningtechniquesarenowwidelydeployed;
example applications include Web-based product-recommendation systems
and automatic placementofrelevantadvertisementson Webpages.
1.14 Summary
• A database-management system (DBMS)consistsofacollectionofinterre-
lateddataandacollectionofprogramstoaccessthatdata.Thedatadescribe
one particular enterprise.
• Theprimarygoalofa DBMSistoprovideanenvironmentthatisbothconve-
nient and ef?cient for peopleto use in retrievingand storing information.
• Database systems are ubiquitous today, and most people interact, either di-
rectlyorindirectly,with databasesmany timeseveryday.
• Databasesystemsaredesignedtostorelargebodiesofinformation.Theman-
agement of data involves both the de?nition of structures for the storage of
information and the provision of mechanisms for the manipulation of infor-
mation. In addition, the database system must provide for the safety of the
informationstored,inthefaceofsystemcrashesorattemptsatunauthorized
access. If data are to be shared among several users, the system must avoid
possible anomalous results.
32 Chapter 1 Introduction
• A major purpose of a database system is to provide users with an abstract
viewofthedata.Thatis,thesystemhidescertaindetailsofhow thedataare
storedand maintained.
• Underlying the structure of a database is the data model: a collection of
conceptual tools for describing data, data relationships, data semantics, and
data constraints.
• Therelationaldatamodelisthemostwidelydeployedmodelforstoringdata
in databases. Other data models are the object-oriented model, the object-
relational model,and semistructureddatamodels.
• A data-manipulation language (DML) is a language that enables users to
access or manipulate data. Nonprocedural DMLs, which require a user to
specify only what data are needed, without specifying exactly how to get
those data, are widely usedtoday.
• A data-de?nition language (DDL) is a language for specifying the database
schema and aswell as other propertiesof the data.
• Database design mainly involves the design of the database schema. The
entity-relationship(E-R)datamodelisawidelyuseddatamodelfordatabase
design. It provides a convenient graphical representation to view data, rela-
tionships, and constraints.
• A database system has severalsubsystems.
?
The storage manager subsystem provides the interface between the low-
leveldatastoredinthedatabaseandtheapplicationprogramsandqueries
submitted tothe system.
?
The query processor subsystem compiles and executes DDL and DML
statements.
• Transaction management ensures that the database remains in a consistent
(correct) state despite system failures. The transaction manager ensures that
concurrent transaction executions proceedwithout con?icting.
• Thearchitectureofadatabasesystemisgreatlyin?uencedbytheunderlying
computer system on which the database system runs. Database systems can
be centralized, or client-server, where one server machine executes work on
behalfofmultipleclientmachines.Databasesystemscanalsobedesignedto
exploitparallelcomputer architectures.Distributeddatabasesspan multiple
geographically separatedmachines.
• Databaseapplicationsaretypicallybrokenupintoafront-endpartthatrunsat
clientmachinesandapartthatrunsatthebackend.Intwo-tierarchitectures,
the front end directly communicates with a database running at the back
end. In three-tier architectures, the back end part is itself broken up into an
application serverand a database server.
PracticeExercises 33
• Knowledge-discovery techniques attempt to discover automatically statisti-
calrulesandpatternsfromdata.The?eldofdataminingcombinesknowledge-
discovery techniques invented by arti?cial intelligence researchers and sta-
tistical analysts, with ef?cient implementation techniques that enable them
tobe used on extremelylarge databases.
• Therearefourdifferenttypesofdatabase-systemusers,differentiatedbythe
waytheyexpecttointeractwiththesystem.Differenttypesofuserinterfaces
have been designedfor the differenttypesof users.
Review Terms
• Database-management system
(DBMS)
• Database-system applications
• File-processingsystems
• Data inconsistency
• Consistency constraints
• Data abstraction
• Instance
• Schema
?
Physical schema
?
Logical schema
• Physical dataindependence
• Data models
?
Entity-relationshipmodel
?
Relational datamodel
?
Object-based data model
?
Semistructureddatamodel
• Database languages
?
Data-de?nitionlanguage
?
Data-manipulation language
?
Query language
• Metadata
• Applicationprogram
• Normalization
• Data dictionary
• Storage manager
• Query processor
• Transactions
?
Atomicity
?
Failure recovery
?
Concurrency control
• Two-andthree-tierdatabasearchi-
tectures
• Data mining
• Database administrator (DBA)
Practice Exercises
1.1 Thischapterhasdescribedseveralmajoradvantagesofadatabasesystem.
What are two disadvantages?
1.2 List ?ve ways in which the type declaration system of a language such as
Javaor C++differs from the data de?nitionlanguage used ina database.
34 Chapter 1 Introduction
1.3 List six major steps that you would take in setting up a database for a
particular enterprise.
1.4 List at least 3 differenttypes of information that a universitywould main-
tain, beyond those listedin Section1.6.2.
1.5 Suppose you want to build a videosite similar to YouTube. Considereach
of the points listed in Section 1.2, as disadvantages of keeping data in a
?le-processing system. Discuss the relevance of each of these points to the
storageofactualvideodata,andtometadataaboutthevideo,suchastitle,
the user who uploadedit, tags, and which users viewedit.
1.6 Keyword queries used in Web search are quite different from database
queries. List key differences between the two, in terms of the way the
queriesare speci?ed,and interms of what is the resultof a query.
Exercises
1.7 Listfourapplicationsyouhaveusedthatmostlikelyemployedadatabase
systemtostore persistentdata.
1.8 List four signi?cant differences between a ?le-processing system and a
DBMS.
1.9 Explain the concept of physical data independence, and its importance in
database systems.
1.10 List ?ve responsibilities of a database-management system. For each re-
sponsibility, explain the problems that would arise if the responsibility
were not discharged.
1.11 Listatleasttworeasonswhydatabasesystemssupportdatamanipulation
using a declarative query language such as SQL, instead of just providing
a a library of C or C++ functions to carry out data manipulation.
1.12 Explainwhat problemsare caused by the designof the table in Figure1.4.
1.13 What are ?ve main functions of a database administrator?
1.14 Explainthedifferencebetweentwo-tierandthree-tierarchitectures.Which
is better suitedfor Webapplications? Why?
1.15 Describe at least 3 tables that might be used to store information in a
social-networking system such as Facebook.
Tools
Therearealargenumberofcommercialdatabasesystemsinusetoday.Themajor
ones include: IBM DB2 (www.ibm.com/software/data/db2), Oracle (www.oracle.com),
Microsoft SQL Server (www.microsoft.com/sql), Sybase (www.sybase.com), and IBM
Informix (www.ibm.com/software/data/informix). Some of these systems are available
Bibliographical Notes 35
free for personal or noncommercial use, or for development, but are not free for
actual deployment.
There are also a number of free/public domain database systems; widely
used onesinclude MySQL (www.mysql.com) and PostgreSQL(www.postgresql.org).
A more complete list of links to vendor Web sites and other information is
available from the home page of this book, at www.db-book.com.
Bibliographical Notes
We list below general-purpose books, research paper collections, and Web sites
on databases. Subsequent chapters provide references to material on each topic
outlined in this chapter.
Codd[1970] is the landmark paperthat introduced the relational model.
Textbooks covering database systems include Abiteboul et al. [1995], O’Neil
and O’Neil [2000], Ramakrishnan and Gehrke [2002], Date [2003], Kifer et al.
[2005], Elmasri and Navathe [2006], and Garcia-Molina et al. [2008]. Textbook
coverageoftransactionprocessingisprovidedbyBernsteinandNewcomer[1997]
andGrayandReuter[1993].Abookcontainingacollectionofresearchpaperson
database management is offeredby Hellersteinand Stonebraker [2005].
A review of accomplishments in database management and an assessment
of future research challenges appears in Silberschatz et al. [1990], Silberschatz
et al. [1996], Bernstein et al. [1998], Abiteboul et al. [2003], and Agrawal et al.
[2009].ThehomepageoftheACMSpecialInterestGrouponManagementofData
(www.acm.org/sigmod) provides a wealth of information about database research.
Database vendor Web sites (see the Tools section above) provide details about
theirrespectiveproducts.
This page intentionally left blank 
PART
1
RELATIONAL
DATABASES
Adatamodelisacollectionofconceptualtoolsfordescribingdata,datarelation-
ships, data semantics, and consistency constraints. In this part, we focus on the
relationalmodel.
Therelationalmodel,whichiscoveredinChapter2,usesacollectionoftables
to represent both data and the relationships among those data. Its conceptual
simplicity has led to its widespread adoption; today a vast majority of database
productsarebasedontherelationalmodel.Therelationalmodeldescribesdataat
thelogicalandviewlevels,abstractingawaylow-leveldetailsofdatastorage.The
entity-relationshipmodel,discussedlaterinChapter7(inPart2),isahigher-level
datamodelwhichiswidelyusedfordatabasedesign.
To make data from a relational database available to users, we have to ad-
dress several issues. The most important issue is how users specify requests for
retrieving and updating data; several query languages have been developed for
this task. A second, but still important, issue is data integrity and protection;
databases need to protect data from damage by user actions, whether uninten-
tionalorintentional.
Chapters 3, 4 and 5 cover the SQL language, which is the most widely used
query language today. Chapters 3 and 4 provide introductory and intermediate
level descriptions of SQL. Chapter 4 also covers integrity constraints which are
enforced by the database, and authorization mechanisms, which control what
access and update actions can be carried out by a user. Chapter 5 covers more
advanced topics, including access to SQL from programming languages, and the
useof SQL fordataanalysis.
Chapter 6 covers three formal query languages, the relational algebra, the
tuplerelationalcalculusandthedomainrelationalcalculus,whicharedeclarative
query languages based on mathematical logic. These formal languages form the
basisfor SQL,andfortwootheruser-friendlylanguages, QBEandDatalog,which
aredescribedinAppendixB(availableonlineat db-book.com).
37
This page intentionally left blank 
CHAPTER
2
Introduction to the Relational
Model
The relational model is today the primary data model for commercial data-
processing applications. It attained its primary position because of its simplicity,
which eases the job of the programmer, compared to earlier data modelssuch as
the network model or the hierarchical model.
In this chapter, we ?rst study the fundamentals of the relational model. A
substantialtheoryexistsforrelationaldatabases.Westudythepartofthistheory
dealing with queries in Chapter 6. In Chapters 7 through 8, we shall examine
aspectsofdatabasetheorythathelpinthedesignofrelationaldatabaseschemas,
whileinChapters12and13wediscussaspectsofthetheorydealingwithef?cient
processing of queries.
2.1 StructureofRelationalDatabases
Arelationaldatabaseconsistsofacollectionoftables,eachofwhichisassigneda
uniquename.Forexample,considertheinstructortableofFigure2.1,whichstores
information about instructors. The table has four column headers: ID, name, dept
name,andsalary. Each row of this table records information about an instructor,
consisting of the instructor’s ID, name, dept name,andsalary. Similarly, the course
tableofFigure2.2storesinformationaboutcourses,consistingofacourse id,title,
dept name,andcredits, for each course. Note that each instructor is identi?ed by
the value of the column ID, while each course is identi?ed by the value of the
column course id.
Figure2.3showsathirdtable,prereq,whichstorestheprerequisitecoursesfor
eachcourse.Thetablehastwocolumns, course idandprereq id.Eachrowconsists
of a pair of course identi?erssuchthat the second courseis a prerequisitefor the
?rst course.
Thus, a row in the prereq table indicates that two courses are related in the
sense that one course is a prerequisite for the other. As another example, we
consider the table instructor, a row in the table can be thought of as representing
39
40 Chapter2 IntroductiontotheRelationalModel
ID name dept name salary
10101 Srinivasan Comp. Sci. 65000
12121 Wu Finance 90000
15151 Mozart Music 40000
22222 Einstein Physics 95000
32343 ElSaid History 60000
33456 Gold Physics 87000
45565 Katz Comp. Sci. 75000
58583 Cali?eri History 62000
76543 Singh Finance 80000
76766 Crick Biology 72000
83821 Brandt Comp. Sci. 92000
98345 Kim Elec.Eng. 80000
Figure2.1 The instructor relation.
the relationship between a speci?ed ID and the corresponding values for name,
dept name,andsalary values.
In general, a row in a table represents a relationship among a set of values.
Since a table is a collection of such relationships, there is a close correspondence
betweentheconceptoftableandthemathematicalconceptofrelation,fromwhich
the relational data model takes its name. In mathematical terminology, a tuple is
simply a sequence (or list) of values. A relationship between n values is repre-
sented mathematically by an n-tuple of values, i.e., a tuple with n values, which
correspondsto a row ina table.
course id title dept name credits
BIO-101 Intro. to Biology Biology 4
BIO-301 Genetics Biology 4
BIO-399 Computational Biology Biology 3
CS-101 Intro. to ComputerScience Comp. Sci. 4
CS-190 Game Design Comp. Sci. 4
CS-315 Robotics Comp. Sci. 3
CS-319 Image Processing Comp. Sci. 3
CS-347 Database SystemConcepts Comp. Sci. 3
EE-181 Intro. to DigitalSystems Elec. Eng. 3
FIN-201 InvestmentBanking Finance 3
HIS-351 World History History 3
MU-199 MusicVideoProduction Music 3
PHY-101 Physical Principles Physics 4
Figure2.2 The course relation.
2.1 StructureofRelationalDatabases 41
course id prereq id
BIO-301 BIO-101
BIO-399 BIO-101
CS-190 CS-101
CS-315 CS-101
CS-319 CS-101
CS-347 CS-101
EE-181 PHY-101
Figure2.3 The prereq relation.
Thus,intherelationalmodelthetermrelationisusedtorefertoatable,while
the term tuple is used to refer to a row. Similarly, the term attribute refers to a
column of a table.
ExaminingFigure2.1,wecanseethattherelationinstructorhasfourattributes:
ID, name, dept name,andsalary.
We use the term relationinstance to refer to a speci?c instance of a relation,
i.e.,containingaspeci?csetofrows.TheinstanceofinstructorshowninFigure2.1
has 12 tuples,corresponding to 12 instructors.
Inthischapter,weshallbeusinganumberofdifferentrelationstoillustratethe
variousconceptsunderlyingthe relationaldatamodel.Theserelationsrepresent
partofauniversity.Theydonotincludeallthedataanactualuniversitydatabase
wouldcontain,inordertosimplifyourpresentation.Weshalldiscusscriteriafor
the appropriatenessof relational structures ingreat detailin Chapters 7 and 8.
The order in which tuples appear in a relation is irrelevant, since a relation
is a set of tuples. Thus, whether the tuples of a relation are listed in sorted order,
asinFigure2.1,orareunsorted,asinFigure2.4,doesnotmatter;therelationsin
ID name dept name salary
22222 Einstein Physics 95000
12121 Wu Finance 90000
32343 ElSaid History 60000
45565 Katz Comp.Sci. 75000
98345 Kim Elec.Eng. 80000
76766 Crick Biology 72000
10101 Srinivasan Comp.Sci. 65000
58583 Cali?eri History 62000
83821 Brandt Comp.Sci. 92000
15151 Mozart Music 40000
33456 Gold Physics 87000
76543 Singh Finance 80000
Figure2.4 Unsorted display of the instructor relation.
42 Chapter2 IntroductiontotheRelationalModel
the two ?gures are the same, since both contain the same set of tuples. For ease
of exposition, we will mostlyshow the relationssorted by their ?rst attribute.
For each attribute of a relation, there is a set of permitted values, called the
domainofthat attribute.Thus,thedomainofthe salary attributeof the instructor
relation is the set of all possible salary values, while the domain of the name
attribute is the set ofall possibleinstructor names.
Werequirethat,forallrelationsr,thedomainsofallattributesofrbeatomic.
Adomainisatomic if elements of the domain are considered to be indivisible
units. For example, suppose the table instructor had an attribute phone number,
whichcanstoreasetofphonenumberscorrespondingtotheinstructor.Thenthe
domainofphone numberwouldnotbeatomic,sinceanelementofthedomainisa
setofphonenumbers,andithassubparts,namelytheindividualphonenumbers
in the set.
The important issue is not what the domain itself is, but rather how we use
domain elements in our database. Suppose now that the phone number attribute
stores a single phone number. Even then, if we split the value from the phone
numberattributeintoacountrycode,anareacodeandalocalnumber,wewould
be treating it as a nonatomic value. If we treat each phone number as a single
indivisibleunit, then the attribute phone numberwould have an atomic domain.
Inthischapter,aswellasinChapters3through6,weassumethatallattributes
haveatomicdomains.InChapter22,weshalldiscussextensionstotherelational
datamodel to permitnonatomic domains.
The null value is a special value that signi?es that the value is unknown or
doesnotexist.Forexample,supposeasbeforethatweincludetheattributephone
number in the instructor relation. It may be that an instructor does not have a
phone number at all, or that the telephone number is unlisted. We would then
have to use the null value to signify that the value is unknown or does not exist.
We shall see later that null values cause a number of dif?culties when we access
or update the database, and thus should be eliminatedifat all possible.We shall
assume null values are absent initially, and in Section 3.6 we describe the effect
of nulls on differentoperations.
2.2 DatabaseSchema
When we talk about a database, we must differentiate between the database
schema, which is the logical design of the database, and the database instance,
which is a snapshot of the data in the database at a giveninstant in time.
The concept of a relation corresponds to the programming-language no-
tion of a variable, while the concept of a relation schema corresponds to the
programming-language notion of type de?nition.
In general, a relation schema consists of a list of attributes and their corre-
sponding domains. We shall not be concerned about the precise de?nition of the
domain of each attribute until we discuss the SQL language in Chapter3.
Theconceptofarelationinstancecorrespondstotheprogramming-language
notionofavalueofavariable.Thevalueofagivenvariablemaychangewithtime;
2.2 DatabaseSchema 43
dept name building budget
Biology Watson 90000
Comp. Sci. Taylor 100000
Elec.Eng. Taylor 85000
Finance Painter 120000
History Painter 50000
Music Packard 80000
Physics Watson 70000
Figure2.5 The department relation.
similarlythe contents of a relationinstance may change withtimeas therelation
is updated.In contrast, the schema of a relationdoesnot generally change.
Although it is important to know the difference between a relation schema
and a relation instance, we often use the same name, such as instructor, to refer
to both the schema and the instance. Where required, we explicitly refer to the
schemaortotheinstance,forexample “the instructor schema,”or “aninstance of
the instructor relation.” However, where it is clear whether we mean the schema
or the instance, we simply use the relationname.
Considerthe department relationof Figure2.5.The schema for that relationis
department (dept name, building, budget)
Notethattheattributedept nameappearsinboththeinstructorschemaandthe
department schema. This duplication is not a coincidence. Rather, using common
attributes in relation schemas is one way of relating tuples of distinct relations.
For example, suppose we wish to ?nd the information about all the instructors
who work in the Watson building. We look ?rst at the department relation to
?nd the dept name of all the departments housed in Watson. Then, for each such
department, we look in the instructor relation to ?nd the information about the
instructor associated with the corresponding dept name.
Letus continue with our universitydatabase example.
Each course in a university may be offered multiple times, across different
semesters,orevenwithinasemester.Weneedarelationtodescribeeachindivid-
ual offering,or section, of the class. The schema is
section (course id, sec id, semester, year, building, room number, time slot id)
Figure 2.6 shows a sample instance of the section relation.
We need a relation to describe the association between instructors and the
class sections that they teach. The relationschema to describe this association is
teaches (ID, course id, sec id, semester, year)
44 Chapter2 IntroductiontotheRelationalModel
course id sec id semester year building room number time slot id
BIO-101 1 Summer 2009 Painter 514 B
BIO-301 1 Summer 2010 Painter 514 A
CS-101 1 Fall 2009 Packard 101 H
CS-101 1 Spring 2010 Packard 101 F
CS-190 1 Spring 2009 Taylor 3128 E
CS-190 2 Spring 2009 Taylor 3128 A
CS-315 1 Spring 2010 Watson 120 D
CS-319 1 Spring 2010 Watson 100 B
CS-319 2 Spring 2010 Taylor 3128 C
CS-347 1 Fall 2009 Taylor 3128 A
EE-181 1 Spring 2009 Taylor 3128 C
FIN-201 1 Spring 2010 Packard 101 B
HIS-351 1 Spring 2010 Painter 514 C
MU-199 1 Spring 2010 Packard 101 D
PHY-101 1 Fall 2009 Watson 100 A
Figure2.6 The section relation.
Figure 2.7 shows a sampleinstance of the teaches relation.
As you can imagine, there are many more relations maintained in a real uni-
versity database. In addition to those relations we have listed already, instructor,
department,course,section,prereq,andteaches,weusethefollowingrelationsinthis
text:
ID course id sec id semester year
10101 CS-101 1 Fall 2009
10101 CS-315 1 Spring 2010
10101 CS-347 1 Fall 2009
12121 FIN-201 1 Spring 2010
15151 MU-199 1 Spring 2010
22222 PHY-101 1 Fall 2009
32343 HIS-351 1 Spring 2010
45565 CS-101 1 Spring 2010
45565 CS-319 1 Spring 2010
76766 BIO-101 1 Summer 2009
76766 BIO-301 1 Summer 2010
83821 CS-190 1 Spring 2009
83821 CS-190 2 Spring 2009
83821 CS-319 2 Spring 2010
98345 EE-181 1 Spring 2009
Figure2.7 The teaches relation.
2.3 Keys 45
• student(ID, name, dept name, tot cred)
• advisor (s id, i id)
• takes (ID, course id, sec id, semester, year, grade)
• classroom (building, room number, capacity)
• time slot (time slot id, day, start time, end time)
2.3 Keys
We must have a way to specify how tuples within a given relation are distin-
guished. This is expressed in terms of their attributes. That is, the values of the
attribute values of a tuple must be such that they can uniquely identify the tuple.
In other words, no two tuples in a relation are allowed to have exactly the same
value for all attributes.
Asuperkeyisasetofoneormoreattributesthat,takencollectively,allowus
to identify uniquely a tuple in the relation. For example, the ID attribute of the
relation instructor is suf?cient to distinguish one instructor tuple from another.
Thus, ID is a superkey. The name attribute of instructor, on the other hand, is not
a superkey,because severalinstructorsmight have the same name.
Formally, let R denote the set of attributes in the schema of relation r.Ifwe
say that a subset K of R is a superkey for r, we are restricting consideration to
instances of relations r in which no two distinct tuples have the same values on
all attributesin K.Thatis,ift
1
and t
2
are in r and t
1
null t
2
,thent
1
.K null t
2
.K.
Asuperkeymaycontainextraneousattributes.Forexample,thecombination
of ID and name is a superkeyfor the relation instructor.IfK is a superkey,then so
is any superset of K. We are often interested in superkeys for which no proper
subset is asuperkey.Such minimal superkeysare calledcandidatekeys.
Itispossiblethat severaldistinctsetsof attributescould serveasacandidate
key.Supposethatacombinationofnameanddept nameissuf?cienttodistinguish
amongmembersoftheinstructorrelation.Then,both{ID}and{name,dept name}
arecandidatekeys.Althoughtheattributes IDand nametogethercandistinguish
instructor tuples, their combination, {ID, name}, does not form a candidate key,
since the attribute ID alone is acandidate key.
We shall use the termprimarykey to denote a candidate key that is chosen
by the database designer as the principal means of identifying tuples within a
relation. A key (whether primary, candidate, or super) is a property of the entire
relation, rather than of the individual tuples. Any two individual tuples in the
relation are prohibited from having the same value on the key attributes at the
same time. The designation of a key represents a constraint in the real-world
enterprisebeingmodeled.
Primarykeysmustbechosenwithcare.Aswenoted,thenameofapersonis
obviouslynotsuf?cient,becausetheremaybemanypeoplewiththesamename.
In the United States, the social-security number attribute of a person would be
a candidate key. Since non-U.S. residents usually do not have social-security
46 Chapter2 IntroductiontotheRelationalModel
numbers, international enterprises must generate their own unique identi?ers.
An alternativeis to use some unique combination of other attributesas a key.
The primary key should be chosen such that its attribute values are never,
or very rarely, changed. For instance, the address?eld of a person should not be
part of the primary key, since it is likely to change. Social-security numbers, on
the other hand, areguaranteed nevertochange. Unique identi?ersgeneratedby
enterprisesgenerallydonotchange,exceptiftwoenterprisesmerge;insuchacase
the same identi?ermay have been issued by both enterprises, and a reallocation
of identi?ersmay be requiredto make sure they are unique.
It is customary to list the primary key attributes of a relation schema before
the other attributes; for example, the dept name attribute of department is listed
?rst, since it is the primarykey.Primarykeyattributes are also underlined.
Arelation,sayr
1
, may include among its attributes the primary key of an-
otherrelation,sayr
2
.Thisattributeiscalledaforeignkeyfromr
1
,referencingr
2
.
The relation r
1
is also called the referencing relation of the foreign key depen-
dency,andr
2
iscalledthereferencedrelationoftheforeignkey.Forexample,the
attributedept nameininstructorisaforeignkeyfrominstructor,referencingdepart-
ment,sincedept name is the primary key of department. In any database instance,
givenanytuple,sayt
a
,fromtheinstructorrelation,theremustbesometuple,say
t
b
,inthedepartment relation such that the value of the dept name attribute of t
a
is
the same as the valueof the primary key, dept name,oft
b
.
Now consider the section and teaches relations. It would be reasonable to
require that if a section exists for a course, it must be taught by at least one
instructor; however, it could possibly be taught by more than one instructor.
To enforce this constraint, we would require that if a particular (course id, sec id,
semester, year) combination appears in section, then the same combination must
appear in teaches. However, this set of values does not form a primary key for
teaches, since more than one instructor may teach one such section. As a result,
we cannot declare a foreign key constraint from section to teaches (although we
can de?ne a foreignkey constraint in the other direction,from teaches to section).
The constraint from section to teaches is an example of areferentialintegrity
constraint;a referentialintegrityconstraint requiresthat thevaluesappearingin
speci?edattributesofanytupleinthereferencingrelationalsoappearinspeci?ed
attributes of at least one tuple inthe referencedrelation.
2.4 SchemaDiagrams
A database schema, along with primary key and foreign key dependencies, can
be depicted byschemadiagrams. Figure 2.8 shows the schema diagram for our
universityorganization.Eachrelationappearsasabox,withtherelationnameat
thetopinblue,andtheattributeslistedinsidethebox.Primarykeyattributesare
shown underlined.Foreignkeydependenciesappearas arrowsfromthe foreign
key attributes of the referencing relation to the primary key of the referenced
relation.
2.5 RelationalQueryLanguages 47
ID
course_id
sec_id
semester
year
grade
ID
name
dept_name
tot_cred
building
room_no
capacity
s_id
i_id
ID
course_id
sec_id
semester
year
takes
section
classroom
teaches
prereq
course_id
prereq_id
course_id
title
dept_name
credits
course
student
dept_name
building
budget
department
instructor
ID
name
dept_name
salary
advisor
time_slot
time_slot_id
day
start_time
end_time
course_id
sec_id
semester
year
building
room_no
time_slot_id
Figure2.8 Schema diagram for the university database.
Referential integrity constraints other than foreign key constraints are not
shown explicitly in schema diagrams. We will study a different diagrammatic
representation called the entity-relationship diagram later, in Chapter 7. Entity-
relationship diagrams let us represent several kinds of constraints, including
general referentialintegrityconstraints.
Many database systems provide design tools with a graphical user interface
for creating schema diagrams. We shall discuss diagrammatic representation of
schemas at lengthin Chapter 7.
The enterprise that we use in the examples in later chapters is a university.
Figure2.9givestherelationalschemathatweuseinourexamples,withprimary-
key attributes underlined. As we shall see in Chapter 3, this corresponds to the
approach to de?ning relations inthe SQL data-de?nitionlanguage.
2.5 RelationalQueryLanguages
A query language is a language in which a user requests information from the
database. These languages are usually on a level higher than that of a standard
programminglanguage.Querylanguagescanbecategorizedaseitherprocedural
or nonprocedural. In a procedural language, the user instructs the system to
perform a sequence of operations on the database to compute the desiredresult.
Inanonprocedurallanguage,theuserdescribesthedesiredinformationwithout
giving a speci?c procedurefor obtaining that information.
48 Chapter2 IntroductiontotheRelationalModel
classroom(building, room number, capacity)
department(dept name, building, budget)
course(course id, title, dept name, credits)
instructor(ID, name, dept name, salary)
section(course id, sec id, semester, year, building, room number, time slot id)
teaches(ID, course id, sec id, semester, year)
student(ID, name, dept name, tot cred)
takes(ID, course id, sec id, semester, year, grade)
advisor(s ID, i ID)
time slot(time slot id, day, start time, end time)
prereq(course id, prereq id)
Figure2.9 Schema of the university database.
Querylanguagesusedinpracticeincludeelementsofboththeproceduraland
the nonprocedural approaches. We study the very widely used query language
SQL in Chapters 3 through 5.
Thereare anumber of “pure”querylanguages: The relationalalgebra is pro-
cedural,whereas the tuplerelationalcalculus and domainrelationalcalculus are
nonprocedural.Thesequerylanguagesareterseandformal,lackingthe“syntactic
sugar” of commercial languages, but they illustrate the fundamental techniques
forextractingdatafromthedatabase.InChapter6,weexamineindetailtherela-
tional algebra and the two versionsof therelational calculus, the tuplerelational
calculus and domain relational calculus. The relational algebra consists of a set
of operations that take one or two relations as input and produce a new relation
as their result. The relational calculus uses predicate logic to de?ne the result
desiredwithoutgivinganyspeci?calgebraicprocedureforobtainingthatresult.
2.6 RelationalOperations
Allproceduralrelationalquerylanguagesprovideasetofoperationsthatcanbe
applied to either a single relation or a pair of relations. These operations have
the nice and desired property that their result is always a single relation. This
property allows one to combine several of these operations in a modular way.
Speci?cally, since the result of a relational query is itself a relation, relational
operations can be applied to the results of queries as well as to the given set of
relations.
Thespeci?crelationaloperations areexpresseddifferentlydependingonthe
language,but?tthegeneralframeworkwedescribeinthissection.InChapter3,
we show the speci?cway the operations are expressedin SQL.
The most frequent operation is the selection of speci?c tuples from a sin-
gle relation (say instructor) that satis?es some particular predicate (say salary >
$85,000). The result is a new relation that is a subset of the original relation (in-
2.6 RelationalOperations 49
ID name dept name salary
12121 Wu Finance 90000
22222 Einstein Physics 95000
33456 Gold Physics 87000
83821 Brandt Comp. Sci. 92000
Figure2.10 Result of query selecting instructor tuples with salary greater than $85000.
structor).Forexample,ifweselecttuplesfromtheinstructorrelationofFigure2.1,
satisfyingthepredicate“salaryisgreaterthan$85000”,wegettheresultshownin
Figure 2.10.
Another frequent operation is to select certain attributes (columns) from a
relation. The result is a new relation having only those selected attributes. For
example, suppose we want a list of instructor IDs and salaries without listing
the name and dept name values from the instructor relation of Figure 2.1, then the
result, shown in Figure 2.11, has the two attributes ID and salary. Each tuple in
the result is derived from a tuple of the instructor relation but with only selected
attributes shown.
The join operationallowsthecombining of tworelationsby mergingpairsof
tuples,onefromeachrelation,intoasingletuple.Thereareanumberofdifferent
waystojoinrelations(asweshallseeinChapter3).Figure2.12showsanexample
ofjoiningthetuplesfromtheinstructoranddepartmenttableswiththenewtuples
showing the information about each instructor and the departmentin which she
is working. This result was formed by combining each tuple in the instructor
relationwith the tuple in the department relationfor the instructor’s department.
IntheformofjoinshowninFigure2.12,whichiscalleda natural join,atuple
fromtheinstructorrelationmatchesatupleinthedepartmentrelationifthevalues
ID salary
10101 65000
12121 90000
15151 40000
22222 95000
32343 60000
33456 87000
45565 75000
58583 62000
76543 80000
76766 72000
83821 92000
98345 80000
Figure2.11 Result of query selecting attributes ID and salary from the instructor relation.
50 Chapter2 IntroductiontotheRelationalModel
ID name salary dept name building budget
10101 Srinivasan 65000 Comp.Sci. Taylor 100000
12121 Wu 90000 Finance Painter 120000
15151 Mozart 40000 Music Packard 80000
22222 Einstein 95000 Physics Watson 70000
32343 ElSaid 60000 History Painter 50000
33456 Gold 87000 Physics Watson 70000
45565 Katz 75000 Comp.Sci. Taylor 100000
58583 Cali?eri 62000 History Painter 50000
76543 Singh 80000 Finance Painter 120000
76766 Crick 72000 Biology Watson 90000
83821 Brandt 92000 Comp.Sci. Taylor 100000
98345 Kim 80000 Elec.Eng. Taylor 85000
Figure2.12 Result of natural join of the instructor and department relations.
of their dept name attributes are the same. All such matching pairs of tuples are
present in the join result. In general, the natural join operation on two relations
matchestupleswhosevaluesarethesameonallattributenamesthatarecommon
to both relations.
TheCartesianproductoperationcombinestuplesfromtworelations,butunlike
the join operation, its result contains all pairs of tuples from the two relations,
regardlessof whether their attribute valuesmatch.
Becauserelationsaresets,wecanperformnormalsetoperationsonrelations.
The union operation performs a set union of two “similarly structured” tables
(say a table of all graduate students and a table of all undergraduate students).
For example, one can obtain the set of all students in a department. Other set
operations, such as intersection and set difference can be performedaswell.
Aswe notedearlier,we can performoperationsonthe resultsofqueries.For
example,ifwewantto?ndtheIDandsalaryforthoseinstructorswhohavesalary
greater than $85,000, we would perform the ?rst two operations in our example
above. First we select those tuples from the instructor relation where the salary
value is greater than $85,000 and then, from that result, select the two attributes
ID and salary, resulting in the relation shown in Figure 2.13 consisting of the ID
ID salary
12121 90000
22222 95000
33456 87000
83821 92000
Figure2.13 Result of selecting attributes ID and salary of instructors with salary greater
than $85,000.
2.6 RelationalOperations 51
RELATIONALALGEBRA
The relational algebra de?nes a set of operations on relations, paralleling the
usualalgebraicoperationssuchasaddition,subtractionormultiplication,which
operate on numbers. Just as algebraic operations on numbers take one or more
numbers as input and return a number as output, the relational algebra op-
erations typically take one or two relations as input and return a relation as
output.
Relational algebra is covered in detail in Chapter 6, but we outline a few of
the operationsbelow.
Symbol(Name) Example of Use
    salary>=85000
(instructor)
(Selection) Return rows of the input relation that satisfy
the predicate.
null null ID,salary
(instructor)
(Projection) Output speci?ed attributes from all rows of
the input relation. Remove duplicate tuples
from the output.
  instructor   department
(Natural join) Outputpairsofrowsfromthetwoinputrela-
tionsthathavethesamevalueonallattributes
that havethe same name.
× instructor ×department
(Cartesian product) Output all pairs of rows from the two input
relations (regardless of whether or not they
have the same valueson commonattributes)
? null name
(instructor) ? null name
(student)
(Union) Outputtheunionoftuplesfromthetwoinput
relations.
and salary. In this example, we could have performed the operations in either
order,but that is not the case for all situations, as we shall see.
Sometimes,theresultofaquerycontainsduplicatetuples.Forexample,ifwe
select the dept name attribute from the instructor relation, there are several cases
ofduplication,including “Comp.Sci.”,whichshowsupthreetimes.Certainrela-
tionallanguagesadherestrictlytothemathematicalde?nitionofasetandremove
duplicates. Others, in consideration of the relatively large amount of processing
required to remove duplicates from large result relations, retain duplicates. In
these latter cases, the relations are not truly relations in the pure mathematical
sense of the term.
Of course, data in a database must be changed over time. A relation can be
updatedbyinsertingnewtuples,deletingexistingtuples,ormodifyingtuplesby
52 Chapter2 IntroductiontotheRelationalModel
changingthevaluesofcertainattributes.Entirerelationscanbedeletedandnew
ones created.
We shall discuss relational queries and updates using the SQL language in
Chapters 3 through 5.
2.7 Summary
• Therelationaldatamodel is based on a collection of tables. The user of the
database system may query these tables, insert new tuples, delete tuples,
andupdate(modify)tuples.Thereareseverallanguagesforexpressingthese
operations.
• Theschema of a relation refers to its logical design, while aninstance of the
relationreferstoitscontentsatapointintime.Theschemaofadatabaseand
an instance of a database are similarly de?ned. The schema of a relation in-
cludesitsattributes,andoptionallythetypesoftheattributesandconstraints
on the relationsuch as primary and foreign key constraints.
• Asuperkey of a relation is a set of one or more attributes whose values are
guaranteed to identify tuples in the relation uniquely. A candidate key is a
minimal superkey, that is, a set of attributes that forms a superkey, but none
of whose subsets is a superkey. One of the candidate keys of a relation is
chosen as itsprimarykey.
• Aforeignkeyisasetofattributesinareferencingrelation,suchthatforeach
tuple in the referencing relation, the values of the foreign key attributes are
guaranteed to occur as the primary key value of a tuple in the referenced
relation.
• A schema diagram is a pictorial depiction of the schema of a database that
shows the relations in the database, their attributes, and primary keys and
foreignkeys.
• The relational query languages de?ne a set of operations that operate on
tables, and output tables as their results. These operations can be combined
to getexpressionsthat expressdesiredqueries.
• The relational algebra provides a set of operations that take one or more
relationsasinputandreturnarelationasanoutput.Practicalquerylanguages
such as SQL are based on the relational algebra, but add a number of useful
syntactic features.
ReviewTerms
• Table
• Relation
• Tuple
• Attribute
• Domain
• Atomic domain
PracticeExercises 53
• Null value
• Database schema
• Database instance
• Relationschema
• Relationinstance
• Keys
?
Superkey
?
Candidatekey
?
Primary key
• Foreignkey
?
Referencing relation
?
Referenced relation
• Referential integrityconstraint
• Schema diagram
• Query language
?
Procedural language
?
Nonprocedural language
• Operations on relations
?
Selectionof tuples
?
Selectionof attributes
?
Natural join
?
Cartesianproduct
?
Setoperations
• Relational algebra
PracticeExercises
2.1 Consider the relational database of Figure 2.14. What are the appropriate
primarykeys?
2.2 Considertheforeignkeyconstraintfromthedept nameattributeofinstructor
to the department relation. Give examples of inserts and deletes to these
relations,which can cause a violation of the foreignkey constraint.
2.3 Consider the time slot relation. Given that a particular time slot can meet
more than once in a week, explain why day and start time are part of the
primary key ofthis relation, while end timeis not.
2.4 In the instance of instructor shown in Figure 2.1, no two instructors have
the same name. From this, can we conclude that name can be used as a
superkey(or primarykey)of instructor?
2.5 What is the result of ?rst performing the cross product of student and
advisor, and then performing a selection operation on the result with the
predicate s id = ID? (Using the symbolic notation of relational algebra, this
querycan be writtenas  s id=ID
(student ×advisor).)
employee (person name, street, city)
works (person name, company name, salary)
company (company name, city)
Figure2.14 Relational database for Exercises 2.1, 2.7, and 2.12.
54 Chapter2 IntroductiontotheRelationalModel
branch(branch name, branch city, assets)
customer (customer name, customer street, customer city)
loan (loan number, branch name, amount)
borrower (customer name, loan number)
account (account number, branch name, balance)
depositor (customer name, account number)
Figure2.15 Banking database for Exercises 2.8, 2.9, and 2.13.
2.6 Consider the following expressions, which use the result of a relational
algebra operation as the input to another operation. For each expression,
explainin words what the expressiondoes.
a.   year?2009
(takes)  student
b.   year?2009
(takes   student)
c. null ID,name,course id
(student   takes)
2.7 Consider the relational database of Figure 2.14. Give an expression in the
relational algebra to expresseach of the following queries:
a. Find the names of all employeeswho livein city “Miami”.
b. Findthenamesofallemployeeswhosesalaryisgreaterthan$100,000.
c. Find the names of all employees who live in “Miami” and whose
salary is greater than $100,000.
2.8 Consider the bank database of Figure 2.15. Give an expression in the rela-
tional algebra for each of the following queries.
a. Find the names of all branches located in “Chicago”.
b. Find the names of all borrowers who have a loan in branch “Down-
town”.
Exercises
2.9 Considerthe bank database ofFigure 2.15.
a. What are the appropriateprimary keys?
b. Givenyourchoiceofprimarykeys,identifyappropriateforeignkeys.
2.10 Consider the advisor relation shown in Figure 2.8, with s id as the primary
key of advisor. Suppose a student can have more than one advisor. Then,
woulds idstillbeaprimarykeyofthe advisorrelation?Ifnot,whatshould
the primarykeyof advisor be?
2.11 Describethedifferencesinmeaningbetweentheterms relation and relation
schema.
BibliographicalNotes 55
2.12 Consider the relational database of Figure 2.14. Give an expression in the
relational algebrato expresseach of the following queries:
a. Find the names of all employees who work for “First Bank Corpora-
tion”.
b. Findthenamesandcitiesofresidenceofallemployeeswhoworkfor
“First Bank Corporation”.
c. Findthenames,streetaddress,andcitiesofresidenceofallemployees
who work for “First Bank Corporation” and earnmore than $10,000.
2.13 Consider the bank database of Figure 2.15. Give an expression in the rela-
tional algebra for each of the following queries:
a. Find all loan numbers with a loan value greaterthan $10,000.
b. Find the names of all depositors who have an account with a value
greaterthan $6,000.
c. Find the names of all depositors who have an account with a value
greaterthan $6,000 at the “Uptown” branch.
2.14 Listtwo reasons why null valuesmight be introducedinto the database.
2.15 Discuss the relativemeritsof procedural and nonprocedural languages.
BibliographicalNotes
E.F.CoddoftheIBMSanJoseResearchLaboratoryproposedtherelationalmodel
in the late 1960s (Codd [1970]). This work led to the prestigious ACM Turing
Award to Coddin 1981 (Codd [1982]).
AfterCoddpublishedhisoriginalpaper,severalresearchprojectswereformed
with the goal of constructing practical relational database systems, including
System R at the IBM San Jose Research Laboratory, Ingres at the University of
California at Berkeley, and Query-by-Example at the IBM T. J. Watson Research
Center.
Many relational database products are now commercially available. These
include IBM’s DB2 and Informix, Oracle, Sybase, and Microsoft SQL Server.Open
source relational database systems include MySQL and PostgreSQL.Microsoft
Access isa single-userdatabaseproduct that ispart oftheMicrosoft Of?ce suite.
Atzeni and Antonellis [1993], Maier [1983], and Abiteboul et al. [1995] are
textsdevotedexclusivelytothe theoryof the relational datamodel.
This page intentionally left blank 
CHAPTER
3
Introduction to SQL
There are a number of database query languages in use, either commercially or
experimentally.Inthischapter,aswellasinChapters4and5,westudythemost
widelyusedquerylanguage, SQL.
Althoughwerefertothe SQLlanguageasa“querylanguage,”itcandomuch
more than just query a database. It can de?ne the structure of the data, modify
datain the database,and specifysecurityconstraints.
It is not our intention to provide a complete users’ guide for SQL.Rather,we
present SQL’sfundamentalconstructsandconcepts.Individualimplementations
of SQL maydifferindetails,or maysupportonly a subsetof the fulllanguage.
3.1 Overview of the SQL Query Language
IBMdevelopedtheoriginalversionof SQL,originallycalledSequel,aspartofthe
SystemRprojectintheearly1970s. TheSequellanguagehas evolvedsince then,
and its name has changed to SQL (Structured Query Language). Many products
now support the SQL language. SQL has clearly established itself as the standard
relationaldatabase language.
In 1986, the American National Standards Institute (ANSI) and the Interna-
tional Organization for Standardization (ISO) published an SQL standard, called
SQL-86.ANSIpublishedanextendedstandardforSQL,SQL-89,in1989.Thenextver-
sionofthestandardwasSQL-92standard,followedbySQL:1999,SQL:2003,SQL:2006,
and most recently SQL:2008. The bibliographic notes provide references to these
standards.
The SQL language has severalparts:
• Data-de?nitionlanguage(DDL).TheSQL DDLprovidescommandsforde?n-
ing relationschemas, deletingrelations,and modifyingrelationschemas.
• Data-manipulation language (DML). The SQL DML provides the ability to
query information from the database and to insert tuples into, delete tuples
from,and modifytuplesinthedatabase.
57
58 Chapter 3 Introduction to SQL
• Integrity.TheSQL DDL includes commands for specifying integrity con-
straintsthatthedatastoredinthedatabasemustsatisfy.Updatesthatviolate
integrityconstraints aredisallowed.
• View de?nition.TheSQL DDLincludescommands for de?ning views.
• Transaction control. SQL includes commands for specifying the beginning
and endingof transactions.
• Embedded SQL and dynamic SQL.Embeddedanddynamic SQL de?nehow
SQLstatementscanbeembeddedwithingeneral-purposeprogramminglan-
guages,such as C,C++,and Java.
• Authorization.TheSQL DDLincludescommandsforspecifyingaccessrights
to relationsand views.
Inthischapter,wepresentasurveyofbasic DMLandthe DDLfeaturesof SQL.
Featuresdescribedherehave beenpartof the SQL standard since SQL-92.
InChapter4,weprovideamoredetailedcoverageoftheSQLquerylanguage,
including (a) various join expressions; (b) views; (c) transactions; (d) integrity
constraints; (e)type system;and (f) authorization.
InChapter5,wecovermoreadvancedfeaturesoftheSQLlanguage,including
(a) mechanisms to allow accessing SQL from a programming language; (b) SQL
functions and procedures;(c) triggers;(d) recursivequeries;(e) advanced aggre-
gation features; and (f) several features designed for data analysis, which were
introduced in SQL:1999, and subsequent versions of SQL. Later, in Chapter 22, we
outlineobject-orientedextensionsto SQL, which were introducedin SQL:1999.
Although most SQL implementations support the standard features we de-
scribehere,youshouldbeawarethattherearedifferencesbetweenimplementa-
tions.Mostimplementationssupportsomenonstandardfeatures,whileomitting
support for some of the more advanced features. In case you ?nd that some lan-
guage features described here do not work on the database system that you use,
consult the user manuals for your database system to ?nd exactly what features
itsupports.
3.2 SQL Data De?nition
The set of relations in a database must be speci?ed to the system by means of a
data-de?nition language (DDL). The SQL DDL allows speci?cation of not only a
setof relations,but also information about each relation,including:
• Theschema for eachrelation.
• Thetypesof valuesassociatedwitheach attribute.
• The integrityconstraints.
• Thesetofindicestobemaintainedforeachrelation.
3.2 SQL Data De?nition 59
• The securityand authorization information foreach relation.
• The physical storage structure ofeach relationon disk.
We discuss here basic schema de?nition and basic types; we defer discussion of
theother SQL DDLfeaturesto Chapters4 and5.
3.2.1 Basic Types
The SQL standardsupportsa varietyof built-intypes,including:
• char(n):A?xed-lengthcharacterstringwithuser-speci?edlengthn.Thefull
form, character, canbe usedinstead.
• varchar(n): A variable-length character string with user-speci?ed maximum
lengthn. The full form, character varying,isequivalent.
• int:Aninteger(a?nitesubsetoftheintegersthatismachinedependent).The
fullform, integer,isequivalent.
• smallint:A smallinteger(a machine-dependentsubset of theintegertype).
• numeric(p,d):A?xed-pointnumberwithuser-speci?edprecision.Thenum-
ber consists of p digits (plus a sign), and d of the p digits are to the right of
the decimal point. Thus, numeric(3,1) allows 44.5tobestoredexactly ,but
neither 444.5or0.32 can be storedexactlyin a ?eld of thistype.
• real, double precision: Floating-point and double-precision ?oating-point
numbers with machine-dependentprecision.
• ?oat(n): A ?oating-point number, with precisionof atleast ndigits.
AdditionaltypesarecoveredinSection4.5.
Each type may include a special value called the null value. A null value
indicatesanabsentvaluethatmayexistbutbe unknown orthatmaynotexistat
all. In certain cases, we may wish to prohibit null values from being entered, as
we shallsee shortly.
The char data type stores ?xed length strings. Consider, for example, an
attribute A of type char(10). If we store a string “Avi” in this attribute, 7 spaces
areappendedtothestringtomakeit10characterslong.Incontrast,ifattributeB
were of type varchar(10), and we store “Avi” in attribute B,nospaceswouldbe
added. When comparing two values of type char, if they are of different lengths
extra spaces are automatically added to the shorter one to make them the same
size,beforecomparison.
Whencomparingachartypewithavarchartype,onemayexpectextraspaces
to be added to the varchar type to make the lengths equal, before comparison;
however, this may or may not be done, depending on the database system. As a
result, even if the same value “Avi” is stored in the attributes A and B above, a
comparison A=B may return false. We recommend you always use the varchar
typeinsteadof the chartypetoavoidtheseproblems.
60 Chapter 3 Introduction to SQL
SQL also provides the nvarchar type to store multilingual data using the
Unicode representation. However, many databases allow Unicode (in the UTF-8
representation)to bestoredevenin varchartypes.
3.2.2 Basic Schema De?nition
We de?ne an SQL relation by using the create table command. The following
command createsa relation departmentinthe database.
create table department
(dept name varchar (20),
building varchar (15),
budget numeric (12,2),
primarykey(dept name));
The relation created above has three attributes, dept name, which is a character
string of maximum length 20, building, which is a character string of maximum
length 15, and budget, which is a number with 12 digits in total, 2 of which are
after the decimal point. The create table command also speci?es that the dept
name attributeistheprimarykeyof the department relation.
Thegeneralform of the create tablecommand is:
create tabler
(A
1
D
1
,
A
2
D
2
,
...,
A
n
D
n
,
nullintegrity-constraint
1
null,
...,
nullintegrity-constraint
k
null);
where risthenameoftherelation,eachA
i
is the name of an attribute in the
schema of relationr,andD
i
is thedomainofattribute A
i
;thatis,D
i
speci?esthe
typeofattribute A
i
alongwithoptionalconstraintsthatrestrictthesetofallowed
valuesfor A
i
.
The semicolon shown at the end of the create table statements, as well as
at the end of other SQL statements later in this chapter, is optional in many SQL
implementations.
SQL supports a number of different integrity constraints. In this section, we
discussonly a few ofthem:
• primary key (A
j
1
, A
j
2
,...,A
j
m
): The primary-key speci?cation says that at-
tributes A
j
1
, A
j
2
,...,A
j
m
formtheprimarykeyfortherelation.Theprimary-
keyattributesarerequiredtobenonnullandunique;thatis,notuplecanhave
a null value for a primary-key attribute, and no two tuples in the relation
can be equal on all the primary-key attributes. Although the primary-key
3.2 SQL Data De?nition 61
speci?cation is optional, it is generally a good idea to specify a primary key
for each relation.
• foreignkey(A
k
1
, A
k
2
,...,A
k
n
)referencess:Theforeignkeyspeci?cationsays
that the values of attributes (A
k
1
, A
k
2
,...,A
k
n
) for any tuple in the relation
must correspond to values of the primary key attributes of some tuple in
relation s.
Figure3.1presentsapartialSQLDDLde?nitionoftheuniversitydatabasewe
useinthetext.Thede?nitionofthecoursetablehasadeclaration“foreignkey
(dept name)referencesdepartment”.Thisforeign-keydeclarationspeci?esthat
for each course tuple, the departmentname speci?edin the tuplemust exist
in the primary key attribute (dept name)ofthedepartment relation. Without
thisconstraint,itispossibleforacoursetospecifyanonexistentdepartment
name.Figure3.1alsoshowsforeignkeyconstraintsontablessection,instructor
and teaches.
• not null:Thenot null constraint on an attribute speci?es that the null value
is not allowed for that attribute; in other words, the constraint excludes the
null value from the domain of that attribute. For example, in Figure 3.1, the
notnullconstraintonthenameattributeoftheinstructorrelationensuresthat
the name of an instructorcannot be null.
Moredetailsontheforeign-keyconstraint,aswellasonotherintegrityconstraints
that the create tablecommand mayinclude,areprovidedlater,inSection4.4.
SQLpreventsanyupdatetothedatabasethatviolatesanintegrityconstraint.
Forexample,ifanewlyinsertedormodi?edtupleinarelationhasnullvaluesfor
any primary-keyattribute,or ifthe tuplehas the samevalueonthe primary-key
attributesasdoesanothertupleintherelation,SQL?agsanerrorandpreventsthe
update. Similarly, an insertion of a course tuple with a dept name value that does
not appearin the departmentrelationwould violatethe foreign-keyconstraint on
course,andSQL preventssuch aninsertionfrom taking place.
A newly created relation is empty initially. We can use the insert command
to loaddatainto therelation.Forexample,ifwe wish to insertthe fact that there
is an instructor named Smith in the Biology department with instructor id 10211
and a salaryof $66,000, we write:
insertinto instructor
values (10211, ’Smith’, ’Biology’, 66000);
The values are speci?ed in the order in which the corresponding attributes are
listedintherelationschema.Theinsertcommandhasanumberofusefulfeatures,
and iscoveredinmoredetaillater,inSection3.9.2.
We can use the deletecommandtodeletetuplesfromarelation.Thecommand
deletefrom student;
62 Chapter 3 Introduction to SQL
create table department
(dept name varchar(20),
building varchar(15),
budget numeric(12,2),
primarykey(dept name));
create table course
(course id varchar(7),
title varchar(50),
dept name varchar(20),
credits numeric(2,0),
primarykey(course id),
foreign key(dept name)references department);
create table instructor
(ID varchar(5),
name varchar(20) notnull,
dept name varchar(20),
salary numeric(8,2),
primarykey(ID),
foreign key(dept name)references department);
create table section
(course id varchar(8),
sec id varchar(8),
semester varchar(6),
year numeric(4,0),
building varchar(15),
room number varchar(7),
time slot id varchar(4),
primarykey(course id, sec id, semester,year),
foreign key(course id) references course);
create table teaches
(ID varchar(5),
course id varchar(8),
sec id varchar(8),
semester varchar(6),
year numeric(4,0),
primarykey(ID, course id, sec id, semester,year),
foreign key(course id, sec id, semester,year) references section,
foreign key(ID)references instructor);
Figure 3.1 SQL data de?nition for part of the university database.
3.3 Basic Structure of SQL Queries 63
would delete all tuples from the student relation. Other forms of the delete com-
mandallowspeci?ctuplestobedeleted;thedeletecommandiscoveredinmore
detaillater,inSection3.9.1.
Toremovearelationfroman SQLdatabase,weusethedroptablecommand.
Thedroptablecommanddeletesallinformationaboutthedroppedrelationfrom
the database. The command
droptable r;
isa more drasticaction than
delete from r;
Thelatterretainsrelationr,butdeletesalltuplesinr.Theformerdeletesnotonly
all tuples of r,butalsotheschemaforr.Afterr is dropped, no tuples can be
insertedinto runlessit isre-createdwiththe create table command.
We use the alter table command to add attributes to an existing relation. All
tuplesintherelationareassignednullasthevalueforthenewattribute.Theform
of the alter table command is
alter tabler addAD ;
where r is the name of an existing relation, A is the name of the attribute to be
added, and D is the type of the added attribute. We can drop attributes from a
relationby the command
alter table r drop A;
whereristhenameofanexistingrelation,andAisthenameofanattributeofthe
relation.Manydatabasesystemsdonotsupportdroppingofattributes,although
theywill allowan entiretableto bedropped.
3.3 Basic Structure of SQL Queries
The basic structure of an SQL query consists of three clauses: select, from,and
where.Thequerytakesasitsinputtherelationslistedinthefromclause,operates
onthemasspeci?edinthewhereandselectclauses,andthenproducesarelation
as the result. We introduce the SQL syntax through examples, and describe the
generalstructureof SQL querieslater.
3.3.1 Queries on a Single Relation
Let us consider a simple query using our university example, “Find the names
of all instructors.” Instructor names are found in the instructor relation, so we
64 Chapter 3 Introduction to SQL
name
Srinivasan
Wu
Mozart
Einstein
ElSaid
Gold
Katz
Cali?eri
Singh
Crick
Brandt
Kim
Figure 3.2 Result of “select name from instructor”.
putthatrelationinthefrom clause. The instructor’s name appears in the name
attribute, sowe putthat in the select clause.
select name
from instructor;
The result is a relation consisting of a single attribute with the heading name.If
theinstructorrelationisasshowninFigure2.1,thentherelationthatresultsfrom
the precedingqueryis shown in Figure3.2.
Nowconsideranotherquery,“Findthedepartmentnamesofallinstructors,”
which can be written as:
select dept name
from instructor;
Since more than one instructor can belong to a department, a department name
could appear more than once in the instructor relation. The result of the above
queryisa relationcontaining the departmentnames, shown inFigure 3.3.
In the formal, mathematical de?nition of the relational model, a relation is a
set.Thus,duplicatetupleswouldneverappearinrelations.Inpractice,duplicate
elimination is time-consuming. Therefore, SQL allows duplicates in relations as
well as in the results of SQL expressions. Thus, the preceding SQL query lists
each department name once for every tuple in which it appears in the instructor
relation.
Inthosecaseswherewewanttoforcetheeliminationofduplicates,weinsert
the keyworddistinct after select. Wecan rewritetheprecedingqueryas:
select distinct dept name
from instructor;
3.3 Basic Structure of SQL Queries 65
dept name
Comp.Sci.
Finance
Music
Physics
History
Physics
Comp.Sci.
History
Finance
Biology
Comp.Sci.
Elec.Eng.
Figure 3.3 Result of “select dept name from instructor”.
ifwewantduplicatesremoved.Theresultoftheabovequerywouldcontaineach
departmentname atmost once.
SQL allows us to use the keyword all to specify explicitly that duplicates are
not removed:
select all dept name
from instructor;
Since duplicate retention is the default, we shall not use all in our examples. To
ensure the elimination of duplicates in the results of our example queries, we
shall use distinctwheneveritis necessary.
The select clause may also contain arithmetic expressions involving the op-
erators +, ?, ?,and/operatingonconstantsorattributesoftuples.Forexample,
thequery:
select ID, name, dept name, salary*1.1
from instructor;
returnsarelationthatisthesameastheinstructorrelation,exceptthattheattribute
salary is multiplied by 1.1. This shows what would result if we gave a 10% raise
to each instructor; note, however, that it does not result in any change to the
instructor relation.
SQL also provides special data types, such as various forms of the date type,
andallowsseveralarithmeticfunctionstooperateonthesetypes.Wediscussthis
furtherin Section4.5.1.
The where clause allows us to select only those rows in the result relation of
the from clause that satisfy a speci?ed predicate. Consider the query “Find the
names of all instructors in the Computer Science department who have salary
greaterthan $70,000.” Thisquerycan bewrittenin SQL as:
66 Chapter 3 Introduction to SQL
name
Katz
Brandt
Figure 3.4 Result of “Find the names of all instructors in the Computer Science department
who have salary greater than $70,000.”
select name
from instructor
wheredept name =’Comp.Sci.’ and salary> 70000;
If the instructor relation is as shown in Figure 2.1, then the relation that results
from the precedingqueryisshown in Figure 3.4.
SQL allows the use of the logical connectives and, or,andnot in the where
clause. The operands of the logical connectives can be expressions involving
the comparison operators <, <=, >, >=, =,and<>. SQL allows us to use the
comparison operators to compare strings and arithmetic expressions, as well as
specialtypes,such asdatetypes.
Weshallexploreotherfeaturesofwhereclausepredicateslaterinthischapter.
3.3.2 Queries on Multiple Relations
Sofarourexamplequerieswereonasinglerelation.Queriesoftenneedtoaccess
informationfrom multiplerelations.We now studyhow to writesuchqueries.
An an example, suppose we want to answer the query “Retrieve the names
of all instructors, along with their department names and department building
name.”
Looking at the schema of the relation instructor,werealizethatwecanget
the department name from the attribute dept name, but the department building
name is presentin the attribute building of the relation department. To answer the
query, each tuple in the instructor relation must be matched with the tuple in
thedepartmentrelationwhose dept namevaluematchesthe dept namevalueofthe
instructor tuple.
InSQL,toanswertheabovequery,welisttherelationsthatneedtobeaccessed
in the from clause, and specify the matching condition in the where clause. The
above querycan bewrittenin SQL as
select name, instructor.dept name, building
from instructor, department
whereinstructor.dept name= department.dept name;
If the instructor and department relations are as shown in Figures 2.1 and 2.5
respectively,thentheresultof thisqueryisshown inFigure3.5.
Note that the attribute dept name occurs in both the relations instructor and
department, and the relation name is used as a pre?x (in instructor.dept name,and
3.3 Basic Structure of SQL Queries 67
name dept name building
Srinivasan Comp.Sci. Taylor
Wu Finance Painter
Mozart Music Packard
Einstein Physics Watson
ElSaid History Painter
Gold Physics Watson
Katz Comp.Sci. Taylor
Cali?eri History Painter
Singh Finance Painter
Crick Biology Watson
Brandt Comp.Sci. Taylor
Kim Elec.Eng. Taylor
Figure 3.5 The result of “Retrieve the names of all instructors, along with their department
names and department building name.”
department.dept name)tomakecleartowhichattributewearereferring.Incontrast,
theattributesnameandbuildingappearinonlyoneoftherelations,andtherefore
donot need to be pre?xedby the relationname.
Thisnamingconventionrequiresthattherelationsthatarepresentinthefrom
clause have distinct names. This requirement causes problems in some cases,
suchaswheninformationfromtwodifferenttuplesinthesamerelationneedsto
be combined. In Section 3.4.1, we see how to avoid these problems by using the
rename operation.
WenowconsiderthegeneralcaseofSQLqueriesinvolvingmultiplerelations.
As we have seen earlier, an SQL query can contain three types of clauses, the
select clause, the fromclause, and the whereclause. The role of each clause isas
follows:
• The select clause isusedto listtheattributesdesiredintheresultof aquery.
• The from clause is a list of the relations to be accessed in the evaluation of
thequery.
• The where clause is a predicate involving attributes of the relation in the
fromclause.
AtypicalSQL queryhas theform
select A
1
, A
2
,...,A
n
fromr
1
, r
2
,...,r
m
whereP;
Each A
i
representsanattribute,andeachr
i
arelation.Pisapredicate.Ifthewhere
clauseis omitted,the predicatePistrue.
68 Chapter 3 Introduction to SQL
Although the clauses must be written in the order select, from, where,the
easiestwaytounderstandtheoperationsspeci?edbythequeryistoconsiderthe
clauses inoperational order:?rst from,thenwhere,andthenselect.
1
The from clause by itself de?nes a Cartesian product of the relations listed
in the clause. It is de?ned formally in terms of set theory, but is perhaps best
understood as an iterative process that generatestuplesfortheresultrelationof
the fromclause.
for each tuple t
1
in relationr
1
for each tuple t
2
in relationr
2
...
for each tuple t
m
in relationr
m
Concatenate t
1
, t
2
,...,t
m
into a singletuplet
Add t into the resultrelation
The result relation has all attributes from all the relations in the from clause.
Since the same attribute name may appear in both r
i
and r
j
, as we saw earlier,
we pre?x the the name of the relation from which the attribute originally came,
beforethe attributename.
For example, the relation schema for the Cartesian product of relations in-
structor and teaches is:
(instructor.ID, instructor.name, instructor.dept name, instructor.salary
teaches.ID, teaches.course id, teaches.sec id, teaches.semester,teaches.year)
With this schema, we can distinguish instructor.ID from teaches.ID. For those at-
tributes that appear in only one of the two schemas, we shall usually drop the
relation-name pre?x. This simpli?cation does not lead to any ambiguity. We can
then write the relationschema as:
(instructor.ID, name, dept name, salary
teaches.ID, course id, sec id, semester,year)
To illustrate, consider the instructor relation in Figure 2.1 and the teaches
relation in Figure 2.7. Their Cartesian product is shown in Figure 3.6, which
includesonly a portion ofthe tuplesthat makeup theCartesianproductresult.
2
The Cartesian product by itself combines tuples from instructor and teaches
that are unrelated to each other. Each tuple in instructor is combined with every
tuple in teaches, even those that refer to a different instructor. The result can be
an extremely large relation, and it rarely makes sense to create such a Cartesian
product.
1
In practice,SQL may convert the expression into an equivalent form that can be processed more ef?ciently. However,
we shall defer concernsabout ef?ciency to Chapters 12 and 13.
2
Note that werenamed instructor.IDas inst.IDto reduce the width ofthe table inFigure3.6.
3.3 Basic Structure of SQL Queries 69
inst.ID name dept name salary teaches.ID course id sec id semester year
10101 Srinivasan Physics 95000 10101 CS-101 1 Fall 2009
10101 Srinivasan Physics 95000 10101 CS-315 1 Spring 2010
10101 Srinivasan Physics 95000 10101 CS-347 1 Fall 2009
10101 Srinivasan Physics 95000 10101 FIN-201 1 Spring 2010
10101 Srinivasan Physics 95000 15151 MU-199 1 Spring 2010
10101 Srinivasan Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
12121 Wu Physics 95000 10101 CS-101 1 Fall 2009
12121 Wu Physics 95000 10101 CS-315 1 Spring 2010
12121 Wu Physics 95000 10101 CS-347 1 Fall 2009
12121 Wu Physics 95000 10101 FIN-201 1 Spring 2010
12121 Wu Physics 95000 15151 MU-199 1 Spring 2010
12121 Wu Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
15151 Mozart Physics 95000 10101 CS-101 1 Fall 2009
15151 Mozart Physics 95000 10101 CS-315 1 Spring 2010
15151 Mozart Physics 95000 10101 CS-347 1 Fall 2009
15151 Mozart Physics 95000 10101 FIN-201 1 Spring 2010
15151 Mozart Physics 95000 15151 MU-199 1 Spring 2010
15151 Mozart Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
22222 Einstein Physics 95000 10101 CS-101 1 Fall 2009
22222 Einstein Physics 95000 10101 CS-315 1 Spring 2010
22222 Einstein Physics 95000 10101 CS-347 1 Fall 2009
22222 Einstein Physics 95000 10101 FIN-201 1 Spring 2010
22222 Einstein Physics 95000 15151 MU-199 1 Spring 2010
22222 Einstein Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
Figure 3.6 The Cartesian product of the instructor relation with the teaches relation.
Instead,thepredicateinthewhereclauseisusedtorestrictthecombinations
created by the Cartesian product to those that are meaningful for the desired
answer. We would expect a query involving instructor and teaches to combine a
particular tuple t in instructor with only those tuples in teaches that refer to the
sameinstructortowhichtrefers.Thatis,wewishonlytomatchteachestupleswith
instructor tuples that have the same ID value. The following SQL query ensures
this condition, and outputs the instructor name and course identi?ersfrom such
matching tuples.
70 Chapter 3 Introduction to SQL
select name, course id
from instructor, teaches
whereinstructor.ID= teaches.ID;
Notethattheabovequeryoutputsonlyinstructorswhohavetaughtsomecourse.
Instructors who have not taught any course are not output; if we wish to output
suchtuples,wecoulduseanoperationcalledtheouterjoin,whichisdescribedin
Section4.1.2.
If the instructor relation is as shown in Figure 2.1 and the teaches relation is
as shown in Figure 2.7, then the relation that results from the preceding query
is shown in Figure 3.7. Observe that instructors Gold, Cali?eri, and Singh, who
have not taught any course, donot appearin the above result.
Ifweonlywished to?nd instructornamesand course identi?ersforinstruc-
torsintheComputerSciencedepartment,wecouldaddanextrapredicatetothe
where clause, asshown below.
select name, course id
from instructor, teaches
where instructor.ID= teaches.IDand instructor.dept name =’Comp. Sci.’;
Note that since the dept name attribute occurs only in the instructor relation, we
couldhaveusedjustdept name,insteadofinstructor.dept nameintheabovequery.
In general,the meaningof an SQL querycan be understood as follows:
name course id
Srinivasan CS-101
Srinivasan CS-315
Srinivasan CS-347
Wu FIN-201
Mozart MU-199
Einstein PHY-101
ElSaid HIS-351
Katz CS-101
Katz CS-319
Crick BIO-101
Crick BIO-301
Brandt CS-190
Brandt CS-190
Brandt CS-319
Kim EE-181
Figure 3.7 Result of “For all instructors in the university who have taught some course, ?nd
their names and the course ID of all courses they taught.”
3.3 Basic Structure of SQL Queries 71
1. GenerateaCartesianproduct ofthe relationslistedinthe fromclause
2. Applythepredicatesspeci?edinthewhere clauseon theresultofStep1.
3. For each tuple in the result of Step 2, output the attributes (or results of
expressions)speci?edintheselect clause.
The above sequence of steps helps make clear what the result of an SQL query
should be, not how it should be executed. A real implementation of SQL would
not execute the query in this fashion; it would instead optimize evaluation by
generating(asfaraspossible)onlyelementsoftheCartesianproductthatsatisfy
the where clause predicates. We study such implementation techniques later, in
Chapters12 and 13.
When writing queries, you should be careful to include appropriate where
clause conditions. If you omit the where clause condition in the preceding SQL
query, it would output the Cartesian product, which could be a huge relation.
For the example instructor relation in Figure 2.1 and the example teaches relation
in Figure 2.7, their Cartesian product has 12 ?13 = 156 tuples — more than we
can show in the text! To make matters worse, suppose we have a more realistic
numberofinstructorsthanweshowinoursamplerelationsinthe?gures,say200
instructors.Let’sassumeeachinstructorteaches3courses,sowehave600tuples
in the teaches relation. Then the above iterative process generates 200 ? 600 =
120,000 tuplesintheresult.
3.3.3 The Natural Join
In our example query that combined information from the instructor and teaches
table,thematchingconditionrequiredinstructor.IDtobeequaltoteaches.ID.These
are the only attributes in the two relations that have the same name. In fact this
is a common case; that is, the matching condition in the from clause most often
requiresall attributeswithmatching names tobe equated.
To make the life of an SQL programmer easier for this common case, SQL
supportsanoperationcalledthenaturaljoin,whichwedescribebelow.InfactSQL
supports several other ways in which information from two or more relations
can be joined together. We have already seen how a Cartesian product along
with a where clause predicate can be used to join information from multiple
relations.Otherwaysofjoininginformationfrommultiplerelationsarediscussed
inSection4.1.
Thenaturaljoinoperationoperatesontworelationsandproducesarelation
as the result. Unlike the Cartesian product of two relations, which concatenates
eachtupleofthe?rstrelationwitheverytupleofthesecond,naturaljoinconsiders
only those pairs of tuples with the same value on those attributes that appear in
the schemas of both relations. So, going back to the example of the relations
instructor and teaches,computinginstructor natural join teaches considers only
those pairs of tuples where both the tuple from instructor and the tuple from
teaches havethesamevalueon the common attribute, ID.
72 Chapter 3 Introduction to SQL
ID name dept name salary course id sec id semester year
10101 Srinivasan Comp.Sci. 65000 CS-101 1 Fall 2009
10101 Srinivasan Comp.Sci. 65000 CS-315 1 Spring 2010
10101 Srinivasan Comp.Sci. 65000 CS-347 1 Fall 2009
12121 Wu Finance 90000 FIN-201 1 Spring 2010
15151 Mozart Music 40000 MU-199 1 Spring 2010
22222 Einstein Physics 95000 PHY-101 1 Fall 2009
32343 ElSaid History 60000 HIS-351 1 Spring 2010
45565 Katz Comp.Sci. 75000 CS-101 1 Spring 2010
45565 Katz Comp.Sci. 75000 CS-319 1 Spring 2010
76766 Crick Biology 72000 BIO-101 1 Summer 2009
76766 Crick Biology 72000 BIO-301 1 Summer 2010
83821 Brandt Comp.Sci. 92000 CS-190 1 Spring 2009
83821 Brandt Comp.Sci. 92000 CS-190 2 Spring 2009
83821 Brandt Comp.Sci. 92000 CS-319 2 Spring 2010
98345 Kim Elec.Eng. 80000 EE-181 1 Spring 2009
Figure 3.8 The natural join of the instructor relation with the teaches relation.
The result relation, shown in Figure 3.8, has only 13 tuples, the ones that
give information about an instructor and a course that that instructor actually
teaches. Notice thatwe donot repeatthoseattributesthat appearin theschemas
ofbothrelations;rathertheyappearonlyonce.Noticealsotheorderinwhichthe
attributesarelisted:?rstthe attributescommon totheschemas of bothrelations,
secondthoseattributesuniquetotheschemaofthe?rstrelation,and?nally,those
attributesunique to the schema of the second relation.
Consider the query “For all instructors in the university who have taught
some course, ?nd their names and the course ID of all courses they taught”,
which we wroteearlieras:
select name, course id
from instructor, teaches
whereinstructor.ID= teaches.ID;
This query can be written more concisely using the natural-join operation in
SQL as:
select name, course id
from instructor naturaljoin teaches;
Bothof theabovequeriesgeneratethesameresult.
Aswesawearlier,theresultofthenaturaljoinoperationisarelation.Concep-
tually, expression “instructor natural join teaches” in the from clause is replaced
3.3 Basic Structure of SQL Queries 73
by the relation obtained by evaluating the natural join.
3
The where and select
clausesarethenevaluatedonthis relation,as we saw earlierinSection3.3.2.
A from clause in an SQL query can have multiple relations combined using
natural join, as shown here:
select A
1
, A
2
,...,A
n
fromr
1
naturaljoinr
2
naturaljoin...naturaljoinr
m
whereP;
Moregenerally,afromclause can be of the form
from E
1
, E
2
,...,E
n
where each E
i
can be a single relation or an expression involving natural joins.
Forexample,supposewewishtoanswerthequery“Listthenamesofinstructors
along with the the titles of courses that they teach.”Thequerycanbewrittenin
SQL as follows:
select name, title
from instructor naturaljoin teaches, course
where teaches.course id= course.course id;
The natural joinof instructor and teaches is?rst computed,as we saw earlier,and
aCartesianproductofthisresultwithcourseiscomputed,fromwhichthewhere
clause extracts only those tuples where the course identi?er from the join result
matches the course identi?er from the course relation. Note that teaches.course id
inthewhereclausereferstothecourse id?eldofthenaturaljoinresult,sincethis
?eldin turn camefrom the teaches relation.
In contrast the following SQL querydoesnot compute the same result:
select name, title
from instructor naturaljoin teaches naturaljoin course;
Toseewhy,notethatthenaturaljoinofinstructorandteachescontainstheattributes
(ID, name, dept name, salary, course id, sec id), while the course relationcontains the
attributes (course id, title, dept name, credits). As a result, the natural join of these
two would requirethat the dept name attribute valuesfrom the two inputs be the
same, in addition to requiring that the course id values be the same. This query
would then omit all (instructor name, course title) pairs where the instructor
teaches a course in a department other than the instructor’s own department.
Thepreviousquery,on theother hand,correctly outputssuch pairs.
3
As a consequence, it is not possible to use attribute names containing the original relation names, for instance instruc-
tor.nameor teaches.courseid, to refer to attributes in the natural join result; we can, however, use attribute names such as
name and courseid, withoutthe relationnames.
74 Chapter 3 Introduction to SQL
To provide the bene?t of natural join while avoiding the danger of equating
attributes erroneously, SQL provides a form of the natural join construct that
allows you to specify exactly which columns should be equated. This feature is
illustratedby thefollowing query:
select name, title
from(instructor naturaljoin teaches) join course using(course id);
Theoperationjoin...usingrequiresalistofattributenamestobespeci?ed.Both
inputs must have attributes with the speci?ed names. Consider the operation r
1
join r
2
using(A
1
, A
2
). The operation is similar to r
1
natural join r
2
,exceptthata
pairoftuplest
1
fromr
1
andt
2
fromr
2
matchift
1
.A
1
= t
2
.A
1
andt
1
.A
2
= t
2
.A
2
;even
ifr
1
andr
2
bothhave anattributenamed A
3
,itisnot required that t
1
.A
3
= t
2
.A
3
.
Thus,intheprecedingSQLquery,thejoinconstructpermitsteaches.dept name
and course.dept name todiffer,and the SQL querygivesthecorrect answer.
3.4 Additional Basic Operations
Therearenumber of additionalbasic operationsthat aresupportedin SQL.
3.4.1 The Rename Operation
Consideragain thequerythat weusedearlier:
select name, course id
from instructor, teaches
whereinstructor.ID= teaches.ID;
The resultof thisqueryisa relationwiththe following attributes:
name, course id
The names of the attributes in the result are derived from the names of the
attributesintherelationsinthe fromclause.
We cannot, however, always derive names in this way, for several reasons:
First, two relations in the from clause may have attributes with the same name,
inwhichcaseanattributenameisduplicatedintheresult.Second,ifweusedan
arithmetic expression in the select clause, the resultant attribute does not have
a name. Third, even if an attribute name can be derived from the base relations
as in the preceding example, we may want to change the attribute name in the
result. Hence, SQL provides a way of renaming the attributes of a result relation.
Itusesthe as clause, taking the form:
old-name as new-name
3.4 Additional Basic Operations 75
The as clause can appearinboth theselect and fromclauses.
4
Forexample,ifwewanttheattributenamenametobereplacedwiththename
instructor name,we canrewritethe precedingqueryas:
select name as instructor name, course id
from instructor, teaches
where instructor.ID= teaches.ID;
The as clause is particularly useful in renaming relations. One reason to
renamearelationistoreplacealongrelationnamewithashortenedversionthat
is more convenient to use elsewhere in the query. To illustrate, we rewrite the
query “For all instructors in the university who have taught some course, ?nd
theirnames andthe course ID of allcourses theytaught.”
select T.name,S.course id
from instructor as T, teaches as S
where T.ID=S.ID;
Another reason to rename a relation is a case where we wish to compare
tuples in the same relation. We then need to take the Cartesian product of a
relation with itself and, without renaming, it becomes impossible to distinguish
one tuple from the other. Suppose that we want to write the query “Find the
namesof allinstructorswhose salaryisgreaterthan atleastone instructorinthe
Biologydepartment.”We can write the SQL expression:
select distinct T.name
from instructor as T, instructor as S
where T.salary> S.salary and S.dept name = ’Biology’;
Observethatwecouldnotusethenotationinstructor.salary,sinceitwouldnotbe
clear which referenceto instructor isintended.
Intheabovequery,TandScanbethoughtofascopiesoftherelationinstructor,
butmoreprecisely,theyaredeclaredasaliases,thatisasalternativenames,forthe
relationinstructor.Anidenti?er,suchasT andS,thatisusedtorenamearelation
is referred to as a correlation name in the SQL standard, but is also commonly
referred to as a table alias,oracorrelation variable,oratuplevariable.
NotethatabetterwaytophrasethepreviousqueryinEnglishwouldbe“Find
thenamesofallinstructorswhoearnmorethanthelowestpaidinstructorinthe
Biology department.” Our original wording ?ts more closely with the SQL that
we wrote, but the latter wording is more intuitive, and can in fact be expressed
directlyin SQL aswe shallseeinSection3.8.2.
4
Early versions ofSQL did not include the keywordas. As a result, some implementations ofSQL, notably Oracle, do
notpermitthekeywordasinthefromclause.InOracle,“old-nameasnew-name”iswritteninsteadas“old-namenew-name”
in the from clause. The keyword as is permitted for renaming attributes in the select clause, but it is optional and may
be omitted in Oracle.
76 Chapter 3 Introduction to SQL
3.4.2 String Operations
SQLspeci?esstringsbyenclosingtheminsinglequotes,forexample,’Computer’.
Asinglequotecharacterthatispartofastringcanbespeci?edbyusingtwosingle
quotecharacters;forexample,thestring“It’sright”canbespeci?edby“It”sright”.
The SQL standard speci?es that the equality operation on strings is case sen-
sitive; as a result the expression “’comp. sci.’ = ’Comp. Sci.’” evaluates to false.
However, some database systems, such as MySQL and SQL Server, do not distin-
guish uppercase from lowercase when matching strings; as a result “’comp. sci.’
=’Comp.Sci.’ ” would evaluateto trueon these databases. This default behavior
can, however, be changed, either at the database level or at the level of speci?c
attributes.
SQL also permits a variety of functions on character strings, such as concate-
nating(using “null ”),extractingsubstrings,?ndingthelengthofstrings,converting
strings to uppercase (using the function upper(s)wheres is a string) and low-
ercase (using the function lower(s)), removing spaces at the end of the string
(usingtrim(s))andsoon. Therearevariationsontheexactsetofstringfunctions
supportedbydifferentdatabasesystems.Seeyourdatabasesystem’smanualfor
moredetailson exactly what stringfunctions itsupports.
Pattern matching can be performed on strings, using the operator like.W e
describe patternsby usingtwo specialcharacters:
• Percent (%): The % character matches any substring.
• Underscore( ): The character matches any character.
Patterns are case sensitive; that is, uppercase characters do not match lowercase
characters,orviceversa.Toillustratepatternmatching,weconsiderthefollowing
examples:
• ’Intro%’ matches any stringbeginning with “Intro”.
• ’%Comp%’ matches any string containing “Comp” as a substring, for exam-
ple,’Intro. toComputerScience’, and ’Computational Biology’.
• ’ ’ matches any stringof exactlythree characters.
• ’ %’ matchesany stringof atleastthree characters.
SQLexpressespatternsbyusingthelikecomparisonoperator.Considerthequery
“Find the names of alldepartmentswhose building name includesthe substring
‘Watson’.” Thisquerycanbe writtenas:
select dept name
fromdepartment
wherebuilding like ’%Watson%’;
3.4 Additional Basic Operations 77
Forpatternstoincludethespecialpatterncharacters(thatis,%and ),SQLallows
thespeci?cationofanescapecharacter.Theescapecharacterisusedimmediately
before a special pattern character to indicate that the special pattern character
is to be treated like a normal character. We de?ne the escape character for a
like comparison using the escape keyword. To illustrate, consider the following
patterns, which use a backslash (\) asthe escape character:
• like’ab\%cd%’ escape’\’ matches all strings beginning with “ab%cd”.
• like’ab\\cd%’ escape’\’ matches all strings beginning with “ab\cd”.
SQL allows us to search for mismatches instead of matches by using the not
likecomparisonoperator.Somedatabasesprovidevariantsofthelikeoperation
which donot distinguishlower and uppercase.
SQL:1999 also offers a similar to operation, which provides more powerful
pattern matching than the like operation; the syntax for specifying patterns is
similartothat usedinUnixregularexpressions.
3.4.3 Attribute Speci?cation in Select Clause
Theasterisksymbol“*”canbeusedintheselectclausetodenote“allattributes.”
Thus, the use of instructor.*inthe select clause of the query:
select instructor.*
from instructor, teaches
whereinstructor.ID= teaches.ID;
indicates that all attributes of instructor are to be selected. A select clause of the
form select * indicates that all attributes of the result relation of the from clause
areselected.
3.4.4 Ordering the Display of Tuples
SQL offers the user some control over the order in which tuples in a relation
are displayed. The order by clause causes the tuples in the result of a query to
appear in sorted order. To list in alphabetic order all instructors in the Physics
department,wewrite:
select name
from instructor
wheredept name =’Physics’
orderbyname;
By default, the order by clause lists items in ascending order. To specify the
sort order, we may specify desc for descending order or asc for ascending order.
Furthermore,orderingcanbeperformedonmultipleattributes.Supposethatwe
wish to list the entire instructor relation in descending order of salary. If several
78 Chapter 3 Introduction to SQL
instructorshavethesamesalary,weordertheminascendingorderbyname.We
expressthis queryin SQL as follows:
select *
from instructor
orderbysalary desc, name asc;
3.4.5 Where Clause Predicates
SQL includes a between comparison operator to simplify where clauses that
specify that a value be less than or equal to some value and greater than or
equaltosomeothervalue.Ifwewishto?ndthenamesofinstructorswithsalary
amounts between $90,000 and $100,000, we can use the between comparison to
write:
select name
from instructor
where salary between 90000 and100000;
insteadof:
select name
from instructor
wheresalary<=100000 and salary>= 90000;
Similarly,we can usethe not between comparison operator.
We can extend the preceding query that ?nds instructor names along with
course identi?ers, which we saw earlier, and consider a more complicated case
in which we require also that the instructors be from the Biology department:
“Find the instructor names and the courses they taught for all instructors in the
Biology department who have taught some course.” To write this query, we can
modify either of the SQL queries we saw earlier, by adding an extra condition in
the where clause. We show below the modi?ed form of the SQL query that does
not use natural join.
select name, course id
from instructor, teaches
where instructor.ID= teaches.IDand dept name =’Biology’;
SQL permits us to use the notation (v
1
,v
2
,...,v
n
) to denote a tuple of arity n
containingvaluesv
1
,v
2
,...,v
n
.Thecomparisonoperatorscanbeusedontuples,
and the ordering is de?ned lexicographically. For example, (a
1
,a
2
) <= (b
1
,b
2
)
3.5 Set Operations 79
course id
CS-101
CS-347
PHY-101
Figure 3.9 The c1 relation, listing courses taught in Fall 2009.
is true if a
1
<= b
1
and a
2
<= b
2
; similarly, the two tuples are equal if all their
attributesareequal.Thus, thepreceding SQL querycan berewrittenas follows:
5
select name, course id
from instructor, teaches
where (instructor.ID, dept name) = (teaches.ID,’Biology’);
3.5 Set Operations
The SQL operations union, intersect,andexcept operate on relations and cor-
respond to the mathematical set-theory operations ?, ?,and?. We shall now
constructqueriesinvolvingtheunion,intersect,andexceptoperationsovertwo
sets.
• Thesetof allcourses taught inthe Fall2009 semester:
select course id
from section
wheresemester = ’Fall’ and year= 2009;
• Thesetof allcourses taught inthe Spring2010 semester:
select course id
from section
where semester =’Spring’ and year= 2010;
Inourdiscussionthatfollows,weshallrefertotherelationsobtainedastheresult
of the preceding queries as c1 and c2, respectively, and show the results when
these queries are run on the section relation of Figure 2.6 in Figures 3.9 and 3.10.
Observe that c2 contains two tuples corresponding to course id CS-319, since two
sections ofthe coursehave beenofferedin Spring2010.
5
Although itispart oftheSQL-92 standard, someSQL implementations may notsupportthis syntax.
80 Chapter 3 Introduction to SQL
course id
CS-101
CS-315
CS-319
CS-319
FIN-201
HIS-351
MU-199
Figure 3.10 The c2 relation, listing courses taught in Spring 2010.
3.5.1 The Union Operation
To ?nd the set of all courses taught either in Fall 2009 or in Spring 2010, or both,
we write:
6
(select course id
from section
where semester = ’Fall’ andyear= 2009)
union
(select course id
from section
where semester = ’Spring’and year= 2010);
Theunionoperationautomaticallyeliminatesduplicates,unliketheselectclause.
Thus, using the section relation of Figure 2.6, where two sections of CS-319 are
offeredinSpring2010,andasectionof CS-101isofferedintheFall2009aswellas
intheFall2010semester,CS-101andCS-319appearonlyonceintheresult,shown
inFigure3.11.
Ifwewanttoretainallduplicates,wemustwriteunionallinplaceofunion:
(select course id
from section
where semester = ’Fall’ andyear= 2009)
unionall
(select course id
from section
where semester = ’Spring’and year= 2010);
The number of duplicate tuples in the result is equal to the total number of
duplicates that appear in both c1 and c2. So, in the above query, each of CS-319
and CS-101 would be listed twice. As a further example, if it were the case that 4
sectionsofECE-101weretaughtintheFall2009semesterand2sectionsofECE-101
6
The parentheses weinclude around each select-from-where statement are optional,but useful for ease ofreading.
3.5 Set Operations 81
course id
CS-101
CS-315
CS-319
CS-347
FIN-201
HIS-351
MU-199
PHY-101
Figure 3.11 The result relation for c1 union c2.
weretaught in theFall 2010 semester,thentherewould be 6 tupleswith ECE-101
intheresult.
3.5.2 The Intersect Operation
To ?nd the set of all courses taught in the Fall 2009 as well as in Spring 2010 we
write:
(select course id
from section
where semester =’Fall’ and year= 2009)
intersect
(select course id
from section
where semester =’Spring’ and year= 2010);
Theresultrelation,showninFigure3.12,containsonlyonetuplewithCS-101.The
intersect operation automatically eliminates duplicates. For example, if it were
the case that 4 sections of ECE-101 were taught in the Fall 2009 semester and 2
sectionsof ECE-101weretaughtintheSpring2010semester,thentherewouldbe
only 1 tuplewith ECE-101 intheresult.
If we want to retain all duplicates, we must write intersect all in place of
intersect:
course id
CS-101
Figure 3.12 The result relation for c1 intersect c2.
82 Chapter 3 Introduction to SQL
(select course id
from section
where semester = ’Fall’ andyear= 2009)
intersect all
(select course id
from section
where semester = ’Spring’and year= 2010);
Thenumberofduplicatetuplesthatappearintheresultisequaltotheminimum
number of duplicates in both c1 and c2. For example, if 4 sections of ECE-101
weretaughtintheFall2009semesterand2sectionsof ECE-101weretaughtinthe
Spring2010 semester,thentherewould be2 tupleswith ECE-101 intheresult.
3.5.3 The Except Operation
To ?nd all courses taught in the Fall 2009 semester but not in the Spring 2010
semester,we write:
(select course id
from section
where semester = ’Fall’ andyear= 2009)
except
(select course id
from section
where semester = ’Spring’and year= 2010);
The result of this query is shown in Figure 3.13. Note that this is exactly relation
c1 of Figure 3.9 except that the tuple for CS-101 does not appear. The except
operation
7
outputs all tuples from its ?rst input that do not occur in the second
input; that is, it performs set difference. The operation automatically eliminates
duplicatesintheinputsbeforeperformingsetdifference.Forexample,if4sections
of ECE-101 were taught in the Fall 2009 semester and 2 sections of ECE-101 were
taught in the Spring 2010 semester, the result of the except operation would not
have any copyof ECE-101.
Ifwe want to retainduplicates,we mustwrite except all inplaceof except:
(select course id
from section
where semester = ’Fall’ andyear= 2009)
except all
(select course id
from section
where semester = ’Spring’and year= 2010);
7
SomeSQL implementations, notably Oracle, use the keywordminus inplaceofexcept.
3.6 Null Values 83
course id
CS-347
PHY-101
Figure 3.13 The result relation for c1 except c2.
The number of duplicate copies of a tuple in the result is equal to the number of
duplicate copies in c1 minus the number of duplicate copies in c2,providedthat
the difference is positive. Thus, if 4 sections of ECE-101 were taught in the Fall
2009semesterand2sectionsofECE-101weretaughtinSpring2010,thenthereare
2 tuples with ECE-101 in the result.If, however, there were two or fewer sections
of ECE-101inthetheFall2009semester,andtwosectionsof ECE-101intheSpring
2010 semester,thereisno tuplewith ECE-101 intheresult.
3.6 Null Values
Null values present special problems in relational operations, including arith-
meticoperations,comparisonoperations,and setoperations.
Theresultofanarithmeticexpression(involving,forexample +, ?, ?,or/)is
null if any of the input values is null. For example, if a query has an expression
r.A+5,andr.Aisnullforaparticulartuple,thentheexpressionresultmustalso
be null for that tuple.
Comparisons involving nulls are more of a problem. For example, consider
the comparison “1 < null”. It would be wrong to say this is true since we do not
knowwhatthenullvaluerepresents.Butitwouldlikewisebewrongtoclaimthis
expressionisfalse;ifwedid,“not(1 <null)”wouldevaluatetotrue,whichdoes
not make sense. SQL therefore treats as unknown the result of any comparison
involving a null value (other than predicates is null and is not null,whichare
describedlaterinthissection).Thiscreatesathirdlogicalvalueinadditiontotrue
and false.
Since the predicate in a where clause can involve Boolean operations such
as and, or,andnot on the results of comparisons, the de?nitions of the Boolean
operationsareextendedtodealwiththevalue unknown.
• and:Theresultoftrue and unknown is unknown, false and unknown is false,
while unknown and unknown is unknown.
• or:Theresultoftrue or unknown is true, false or unknown is unknown, while
unknown or unknown is unknown.
• not:Theresultofnotunknown is unknown.
You can verify that if r.A is null, then “1 < r.A” as well as “not (1 < r.A)”
evaluateto unknown.
84 Chapter 3 Introduction to SQL
Ifthewhereclausepredicateevaluatestoeitherfalseorunknownforatuple,
that tupleisnot addedtothe result.
SQLusesthespecialkeywordnullinapredicatetotestforanullvalue.Thus,
to ?nd all instructors who appear in the instructor relation with null values for
salary,wewrite:
select name
from instructor
wheresalary is null;
The predicateis not nullsucceedsif thevalueonwhich itisappliedisnot null.
SomeimplementationsofSQLalsoallowustotestwhethertheresultofacom-
parison is unknown, rather than true or false, by using the clauses is unknown
and is notunknown.
When a query uses the select distinct clause, duplicate tuples must be elim-
inated. For this purpose, when comparing values of corresponding attributes
fromtwotuples,thevaluesaretreatedasidenticalifeitherbotharenon-nulland
equal in value, or both are null. Thus two copies of a tuple, such as {(’A’,null),
(’A’,null)}, are treated as being identical, even if some of the attributes have a
null value. Using the distinct clause then retains only one copy of such identical
tuples. Note that the treatment of null above is different from the way nulls are
treated in predicates, where a comparison “null=null” would return unknown,
ratherthan true.
The above approach of treating tuples as identical if they have the same
values for all attributes, even if some of the values are null, is also used for the
setoperationsunion, intersection and except.
3.7 Aggregate Functions
Aggregatefunctionsarefunctionsthattakeacollection(asetormultiset)ofvalues
as inputand returna single value. SQL offers?vebuilt-in aggregatefunctions:
• Average:avg
• Minimum: min
• Maximum: max
• Total: sum
• Count: count
Theinputtosumandavgmustbeacollectionofnumbers,buttheotheroperators
can operateon collectionsof nonnumeric datatypes,such as strings,as well.
3.7 Aggregate Functions 85
3.7.1 Basic Aggregation
Consider the query “Find the average salary of instructors in the Computer Sci-
ence department.”Wewritethisqueryas follows:
select avg (salary)
from instructor
wheredept name= ’Comp. Sci.’;
The result of this query is a relation with a single attribute, containing a single
tuple with a numerical value corresponding to the average salary of instructors
intheComputerSciencedepartment.Thedatabasesystemmaygiveanarbitrary
name to the result relation attribute that is generated by aggregation; however,
wecangiveameaningfulnametotheattributebyusingtheasclauseasfollows:
select avg (salary) as avg salary
from instructor
wheredept name= ’Comp. Sci.’;
In the instructor relation of Figure 2.1, the salaries in the Computer Science
department are $75,000, $65,000, and $92,000. The average balance is $232,000/3
= $77,333.33.
Retaining duplicates is important in computing an average. Suppose the
ComputerScience departmentadds a fourth instructor whose salary happens to
be $75,000. If duplicates were eliminated, we would obtain the wrong answer
($232,000/4 =$58.000) rather than the correctanswer of $76,750.
There are cases where we must eliminate duplicates before computing an
aggregate function. If we do want to eliminate duplicates, we use the keyword
distinct in the aggregate expression. An example arises in the query “Find the
total number of instructors who teach a course in the Spring 2010 semester.”
In this case, an instructor counts only once, regardless of the number of course
sections that the instructor teaches. The required information is contained in the
relation teaches, and we write this queryasfollows:
select count(distinct ID)
fromteaches
wheresemester = ’Spring’ andyear = 2010;
Becauseofthe keyworddistinctpreceding ID,evenifaninstructorteachesmore
than one course, she iscounted only once in the result.
Weusetheaggregatefunctioncountfrequentlytocountthenumberoftuples
in a relation. The notation for this function in SQL is count (*). Thus, to ?nd the
number of tuplesinthe course relation,we write
select count(*)
from course;
86 Chapter 3 Introduction to SQL
ID name dept name salary
76766 Crick Biology 72000
45565 Katz Comp.Sci. 75000
10101 Srinivasan Comp.Sci. 65000
83821 Brandt Comp.Sci. 92000
98345 Kim Elec.Eng. 80000
12121 Wu Finance 90000
76543 Singh Finance 80000
32343 ElSaid History 60000
58583 Cali?eri History 62000
15151 Mozart Music 40000
33456 Gold Physics 87000
22222 Einstein Physics 95000
Figure 3.14 Tuples of the instructor relation, grouped by the dept name attribute.
SQLdoesnotallowtheuseofdistinctwithcount(*).Itislegaltousedistinct
with max and min, even though the result does not change. We can use the
keywordallinplaceofdistincttospecifyduplicateretention,but,sinceallisthe
default,thereisno needto doso.
3.7.2 Aggregation with Grouping
There are circumstances where we would like to apply the aggregate function
not only to a single set of tuples, but also to a group of sets of tuples; we specify
this wish in SQL using the group by clause. The attribute or attributes given in
the group by clause are used to form groups. Tuples with the same value on all
attributesinthegroup byclause are placed in one group.
Asanillustration,considerthequery“Findtheaveragesalaryineachdepart-
ment.” Wewritethisqueryas follows:
select dept name, avg(salary) as avg salary
from instructor
groupby dept name;
Figure 3.14 shows the tuples in the instructor relation grouped by the dept
nameattribute,whichisthe?rststepincomputingthequeryresult.Thespeci?ed
aggregate is computed for each group, and the result of the query is shown in
Figure3.15.
Incontrast,considerthequery“Findtheaveragesalaryofallinstructors.”We
writethis queryas follows:
select avg (salary)
from instructor;
3.7 Aggregate Functions 87
dept name avg salary
Biology 72000
Comp.Sci. 77333
Elec.Eng. 80000
Finance 85000
History 61000
Music 40000
Physics 91000
Figure 3.15 The result relation for the query “Find the average salary in each department”.
Inthiscasethegroupbyclausehasbeenomitted,sotheentirerelationistreated
as asingle group.
As another example of aggregation on groups of tuples, consider the query
“Find the number of instructors in each department who teach a course in the
Spring 2010 semester.” Information about which instructors teach which course
sections in which semester is available in the teaches relation. However, this in-
formationhastobejoinedwithinformationfromtheinstructorrelationtogetthe
departmentname of each instructor. Thus, we write thisqueryas follows:
select dept name, count(distinct ID)as instr count
from instructor naturaljoin teaches
wheresemester =’Spring’ and year =2010
groupby dept name;
The resultisshown in Figure 3.16.
When an SQL query uses grouping, it is important to ensure that the only
attributesthatappearintheselectstatementwithoutbeingaggregatedarethose
that are present in the group by clause. In other words, any attribute that is not
present in the group by clause must appear only inside an aggregate function if
it appears in the select clause, otherwise the query is treated as erroneous. For
example,the following queryis erroneoussince ID doesnot appear inthe group
byclause, and yetitappearsinthe select clausewithout being aggregated:
dept name instr count
Comp.Sci. 3
Finance 1
History 1
Music 1
Figure 3.16 The result relation for the query “Find the number of instructors in each
department who teach a course in the Spring 2010 semester.”
88 Chapter 3 Introduction to SQL
/* erroneousquery*/
select dept name, ID, avg(salary)
from instructor
group by dept name;
Each instructor in a particular group (de?ned by dept name) can have a different
ID, and since only one tuple is output for each group, there is no unique way of
choosingwhich IDvaluetooutput.Asaresult,suchcasesaredisallowedby SQL.
3.7.3 The Having Clause
At times, it is useful to state a condition that applies to groups rather than to
tuples.Forexample,wemightbeinterestedinonlythosedepartmentswherethe
average salary of the instructors is more than $42,000. This condition does not
apply to a single tuple; rather, it applies to each group constructed by the group
by clause. To expresssuch a query, we use the having clause of SQL. SQL applies
predicates in the having clause after groups have been formed, so aggregate
functions may beused.We expressthisqueryin SQL asfollows:
select dept name, avg(salary) as avg salary
from instructor
groupby dept name
havingavg(salary)> 42000;
The resultisshown in Figure 3.17.
Aswasthecasefortheselectclause,anyattributethatispresentinthehaving
clause without being aggregated must appear in the group by clause, otherwise
the queryistreatedas erroneous.
Themeaningofaquerycontainingaggregation,groupby,orhavingclauses
isde?ned by the following sequence of operations:
1. As was the case for queries without aggregation, the from clause is ?rst
evaluatedto getarelation.
dept name avg(avg salary)
Physics 91000
Elec.Eng. 80000
Finance 85000
Comp.Sci. 77333
Biology 72000
History 61000
Figure 3.17 The result relation for the query “Find the average salary of instructors in those
departments where the average salary is more than $42,000.”
3.7 Aggregate Functions 89
2. If a where clauseis present,the predicateinthewhere clause isappliedon
the resultrelationof the fromclause.
3. Tuples satisfying the where predicate are then placed into groups by the
group by clause if it is present. If the group by clause is absent, the entire
setoftuplessatisfyingthewherepredicateistreatedasbeinginonegroup.
4. The having clause, if it is present, is appliedto each group; the groups that
do not satisfythe havingclausepredicateareremoved.
5. The select clauseusestheremaininggroupstogeneratetuplesoftheresult
ofthequery,applyingtheaggregatefunctionstogetasingleresulttuplefor
eachgroup.
To illustrate the use of both a having clause and a where clause in the same
query, we consider the query “For each course section offered in 2009, ?nd the
averagetotalcredits(tot cred)ofallstudentsenrolledinthesection,ifthesection
had atleast2 students.”
select course id, semester,year, sec id, avg (tot cred)
from takes naturaljoin student
where year = 2009
groupby course id, semester,year, sec id
havingcount(ID)>= 2;
Note that all the required information for the preceding query is available from
therelationstakesandstudent,andthatalthoughthequerypertainstosections,a
joinwith section isnot needed.
3.7.4 Aggregation with Null and Boolean Values
Null values, when they exist, complicate the processing of aggregate operators.
For example,assumethat sometuplesinthe instructor relationhave a null value
for salary. Considerthe following querytototal allsalaryamounts:
select sum (salary)
from instructor;
Thevaluestobesummedintheprecedingqueryincludenullvalues,sincesome
tuples have a null value for salary. Rather than say that the overall sum is itself
null,theSQL standardsaysthatthe sumoperatorshould ignorenull valuesinits
input.
Ingeneral,aggregatefunctionstreatnullsaccordingtothefollowingrule:All
aggregate functions except count (*) ignore null values in their input collection.
Asaresultofnullvaluesbeingignored,thecollectionofvaluesmaybeempty.The
countofanemptycollectionisde?nedtobe0,andallotheraggregateoperations
90 Chapter 3 Introduction to SQL
return a value of null when applied on an empty collection. The effect of null
valuesonsomeof themorecomplicated SQL constructs can be subtle.
A Boolean data type that can take values true, false,andunknown,was
introduced in SQL:1999. The aggregate functions some and every,whichmean
exactly what you would intuitively expect, can be applied on a collection of
Booleanvalues.
3.8 Nested Subqueries
SQL provides a mechanism for nesting subqueries. A subquery is a select-from-
where expression that is nested within another query. A common use of sub-
queriesistoperformtestsforsetmembership,makesetcomparisons,anddeter-
mine set cardinality, by nesting subqueries in the where clause. We study such
uses of nested subqueries in the where clause in Sections 3.8.1 through 3.8.4. In
Section 3.8.5, we study nesting of subqueries in the from clause. In Section 3.8.7,
we see how a class of subqueries called scalar subqueries can appear wherever
anexpressionreturninga valuecan occur.
3.8.1 Set Membership
SQL allows testing tuples for membership in a relation. The in connective tests
for set membership, where the set is a collection of values produced by a select
clause. The notinconnective testsfor the absence of setmembership.
As an illustration, reconsider the query “Find all the courses taught in the
both the Fall2009 and Spring2010 semesters.”Earlier,we wrote such aqueryby
intersecting two sets: the set of courses taught in Fall 2009 and the set of courses
taughtinSpring2010.Wecantakethealternativeapproachof?ndingallcourses
that were taught in Fall 2009 and that are also members of the set of courses
taught in Spring2010. Clearly,this formulation generatesthe same resultsas the
previousonedid,butitleadsustowriteourqueryusingtheinconnectiveofSQL.
Webeginby?ndingallcoursestaughtinSpring2010,andwewritethesubquery
(select course id
from section
where semester =’Spring’ and year= 2010)
We then need to ?nd those courses that were taught in the Fall 2009 and that
appear in the set of courses obtained in the subquery. We do so by nesting the
subqueryinthewhere clauseof an outerquery.Theresultingqueryis
select distinct course id
from section
wheresemester = ’Fall’ and year= 2009 and
course id in (select course id
from section
where semester =’Spring’ and year= 2010);
3.8 Nested Subqueries 91
Thisexampleshowsthatitispossibletowritethesamequeryseveralwaysin
SQL.This?exibilityisbene?cial,sinceitallowsausertothinkaboutthequeryin
the way that seems most natural. We shall see that there is a substantial amount
of redundancyin SQL.
We use the notinconstructinawaysimilartotheinconstruct.Forexample,
to ?nd all the courses taught in the Fall 2009 semesterbut not in the Spring 2010
semester,we canwrite:
select distinct course id
from section
wheresemester = ’Fall’ and year= 2009 and
course id notin(select course id
from section
wheresemester =’Spring’ and year= 2010);
Theinandnotinoperatorscanalsobeusedonenumeratedsets.Thefollow-
ingqueryselectsthenamesofinstructorswhosenamesareneither “Mozart”nor
“Einstein”.
select distinct name
from instructor
where namenotin(’Mozart’, ’Einstein’);
Intheprecedingexamples,wetestedmembershipinaone-attributerelation.
It is also possible to test for membership in an arbitrary relation in SQL.For
example,wecanwritethequery“?ndthetotalnumberof(distinct)studentswho
have taken course sections taught by the instructor with ID110011” as follows:
select count(distinct ID)
from takes
where(course id, sec id, semester,year)in (select course id, sec id, semester,year
fromteaches
whereteaches.ID= 10101);
3.8.2 Set Comparison
As an example of the ability of a nested subquery to compare sets, consider the
query “Find the names of all instructors whose salary is greater than at least one
instructor in the Biology department.” In Section 3.4.1, we wrote this query as
follows:
select distinct T.name
from instructor as T, instructor as S
where T.salary> S.salary and S.dept name = ’Biology’;
92 Chapter 3 Introduction to SQL
SQLdoes,however,offeranalternativestyleforwritingtheprecedingquery.The
phrase“greaterthanatleastone”isrepresentedinSQLby>some.Thisconstruct
allows us to rewrite the query in a form that resembles closely our formulation
of thequeryin English.
select name
from instructor
where salary> some (select salary
from instructor
where dept name = ’Biology’);
The subquery:
(select salary
from instructor
where dept name = ’Biology’)
generatesthe set of all salary valuesof all instructors inthe Biology department.
The>somecomparisoninthewhereclauseoftheouterselectistrueifthesalary
valueofthetupleisgreaterthanatleastonememberofthesetofallsalaryvalues
for instructorsinBiology.
SQL also allows < some, <= some, >= some, = some,and<> some com-
parisons. As an exercise, verify that = some is identical to in,whereas<> some
is not the same as notin.
8
Now we modify our query slightly. Let us ?nd the names of all instructors
thathaveasalaryvaluegreaterthanthatofeachinstructorintheBiologydepart-
ment.Theconstruct>allcorrespondstothephrase “greaterthanall.”Usingthis
construct, we writethe queryasfollows:
select name
from instructor
wheresalary> all (select salary
from instructor
wheredept name =’Biology’);
As it does for some, SQL also allows < all, <= all, >= all, = all,and<> all
comparisons. As an exercise, verify that <> all is identical to not in,whereas=
all is not the same as in.
Asanotherexampleofsetcomparisons,considerthequery“Findthedepart-
ments that have the highest average salary.”We begin by writing a queryto ?nd
all average salaries, and then nest it as a subquery of a larger query that ?nds
8
The keyword any is synonymous to some inSQL. Early versions ofSQL allowed only any. Later versions added the
alternative some to avoidthe linguistic ambiguity ofthe word anyin English.
3.8 Nested Subqueries 93
those departments for which the average salary is greater than or equal to all
averagesalaries:
select dept name
from instructor
groupby dept name
havingavg (salary)>= all (select avg (salary)
from instructor
group by dept name);
3.8.3 Test for Empty Relations
SQLincludesafeaturefortestingwhetherasubqueryhasanytuplesinitsresult.
Theexistsconstructreturnsthevaluetrueiftheargumentsubqueryisnonempty.
Usingtheexistsconstruct,wecanwritethequery“Findallcoursestaughtinboth
theFall 2009 semesterand intheSpring2010 semester”instillanother way:
select course id
from section as S
wheresemester =’Fall’ and year= 2009 and
exists (select *
from section as T
where semester = ’Spring’ andyear= 2010 and
S.course id= T.course id);
The above query also illustrates a feature of SQL where a correlation name
from an outer query (S in the above query), can be used in a subquery in the
where clause. A subquery that uses a correlation name from an outer query is
calledacorrelated subquery.
In queries that contain subqueries, a scoping rule applies for correlation
names. In a subquery, according to the rule, it is legal to use only correlation
names de?ned in the subquery itself or in any query that contains the subquery.
If a correlation name is de?ned both locally in a subquery and globally in a
containing query, the local de?nition applies. This rule is analogous to the usual
scoping rulesusedfor variablesinprogramminglanguages.
Wecantestforthenonexistenceoftuplesinasubquerybyusingthenotexists
construct.Wecanusethenotexistsconstructtosimulatethesetcontainment(that
is,superset)operation:Wecanwrite“relation Acontainsrelation B”as“notexists
(B except A).” (Although it is not partof the current SQL standards, the contains
operatorwaspresentinsomeearlyrelationalsystems.)Toillustratethenotexists
operator,considerthequery“Findallstudentswhohavetakenallcoursesoffered
in the Biology department.” Using the except construct, we can write the query
as follows:
94 Chapter 3 Introduction to SQL
select distinct S.ID, S.name
fromstudent as S
where notexists ((select course id
fromcourse
where dept name = ’Biology’)
except
(select T.course id
from takes as T
where S.ID = T.ID));
Here,the subquery:
(select course id
from course
where dept name = ’Biology’)
?nds the setof allcourses offeredin the Biologydepartment.The subquery:
(select T.course id
from takes as T
where S.ID = T.ID)
?ndsallthecoursesthatstudentS.IDhastaken.Thus,theouterselecttakeseach
studentandtestswhetherthesetofallcoursesthatthestudenthastakencontains
the setof allcourses offeredintheBiologydepartment.
3.8.4 Test for the Absence of Duplicate Tuples
SQL includes a boolean function for testing whether a subquery has duplicate
tuples in its result. The unique construct
9
returns the value true if the argument
subquerycontainsnoduplicatetuples.Usingtheuniqueconstruct,wecanwrite
the query “Find all courses that wereofferedat mostonce in2009” as follows:
select T.course id
from course as T
where unique(select R.course id
from section as R
whereT.course id= R.course id and
R.year = 2009);
Note that if a course is not offered in 2009, the subquery would return an empty
result, and the uniquepredicatewould evaluatetotrueonthe emptyset.
Anequivalentversionof the abovequerynot using the uniqueconstruct is:
9
This construct isnotyetwidelyimplemented.
3.8 Nested Subqueries 95
select T.course id
fromcourse as T
where1<=(select count(R.course id)
from section as R
whereT.course id= R.course id and
R.year =2009);
We can test for the existence of duplicate tuples in a subquery by using the
not unique construct. To illustrate this construct, consider the query “Find all
courses thatwereofferedat leasttwicein2009” asfollows:
select T.course id
fromcourse as T
wherenotunique(select R.course id
from section as R
where T.course id= R.course id and
R.year =2009);
Formally, the unique test on a relation is de?ned to fail if and only if the
relation contains two tuples t
1
and t
2
such that t
1
= t
2
.Sincethetestt
1
= t
2
fails
if any of the ?elds of t
1
or t
2
are null, it is possible for unique to be true even if
thereare multiplecopies of a tuple,as long as at least one of the attributesof the
tupleisnull.
3.8.5 Subqueries in the From Clause
SQLallowsasubqueryexpressiontobeusedinthefromclause.Thekeyconcept
appliedhereisthatanyselect-from-whereexpressionreturnsarelationasaresult
and, therefore, can be inserted into another select-from-where anywhere that a
relationcan appear.
Consider the query “Find the average instructors’ salaries of those depart-
ments where the average salary is greater than $42,000.” We wrote this query in
Section 3.7 by using the having clause. We can now rewrite this query, without
using the havingclause, by using asubquery inthe fromclause, as follows:
select dept name, avg salary
from(select dept name, avg(salary) as avg salary
from instructor
group by dept name)
whereavg salary> 42000;
Thesubquerygeneratesarelationconsistingofthenamesofalldepartmentsand
their corresponding average instructors’ salaries. The attributes of the subquery
resultcan be usedintheouter query,ascan beseeninthe aboveexample.
96 Chapter 3 Introduction to SQL
Note that we do not need to use the having clause, since the subquery in
the from clause computes the average salary, and the predicate that was in the
havingclauseearlieris now inthe where clauseof the outerquery.
We can give the subquery result relation a name, and rename the attributes,
using the as clause,asillustratedbelow.
select dept name, avg salary
from(select dept name, avg (salary)
from instructor
groupby dept name)
as dept avg (dept name, avg salary)
whereavg salary>42000;
Thesubqueryresultrelationisnamed dept avg,withtheattributes dept nameand
avg salary.
Nested subqueries in the from clause are supported by most but not all SQL
implementations. However, some SQL implementations, notably Oracle, do not
supportrenaming of the resultrelationin the fromclause.
As another example, suppose we wish to ?nd the maximum across all de-
partmentsofthetotalsalaryateachdepartment.Thehavingclausedoesnothelp
usinthistask,butwecanwritethisqueryeasilybyusingasubqueryinthefrom
clause, as follows:
select max (tot salary)
from(select dept name, sum(salary)
from instructor
groupby dept name) as dept total (dept name, tot salary);
We note that nested subqueries in the from clause cannot use correlation
variables from other relations in the from clause. However, SQL:2003 allows a
subquery in the from clause that is pre?xed by the lateral keyword to access
attributes of preceding tables or subqueries in the from clause. For example, if
we wish to print the names of each instructor, along with their salary and the
averagesalary intheirdepartment,we couldwritethe queryas follows:
select name, salary, avg salary
from instructor I1, lateral (select avg(salary)asavg salary
from instructor I2
whereI2.dept name= I1.dept name);
Without the lateral clause, the subquery cannot access the correlation variable
I1 from the outer query. Currently, only a few SQL implementations, such as IBM
DB2,supportthe lateral clause.
3.8 Nested Subqueries 97
3.8.6 The with Clause
Thewithclauseprovidesawayofde?ningatemporaryrelationwhosede?nition
is available only to the query in which the with clause occurs. Consider the
following query,which ?nds those departmentswith the maximumbudget.
with max budget(value) as
(select max(budget)
fromdepartment)
select budget
fromdepartment, max budget
where department.budget = max budget.value;
The with clause de?nes the temporary relation max budget, which is used in
the immediately following query. The with clause, introduced in SQL:1999,is
supportedby many,but not all,database systems.
We could have writtenthe above queryby using a nestedsubquery ineither
the from clause or the where clause. However, using nested subqueries would
have made the queryharderto read and understand.The with clause makesthe
querylogicclearer;italsopermitsaviewde?nitiontobeusedinmultipleplaces
within a query.
Forexample,supposewewantto?ndalldepartmentswherethetotalsalary
isgreaterthantheaverageofthetotalsalaryatalldepartments.Wecanwritethe
queryusingthe with clause asfollows.
with dept total (dept name, value) as
(select dept name, sum(salary)
from instructor
groupby dept name),
dept total avg(value) as
(select avg(value)
from dept total)
select dept name
from dept total, dept total avg
wheredept total.value >= dept total avg.value;
Wecan,ofcourse,createanequivalentquerywithoutthewithclause,butitwould
be more complicated and harder to understand. You can write the equivalent
queryasan exercise.
3.8.7 Scalar Subqueries
SQL allows subqueries to occur wherever an expression returning a value is
permitted, provided the subquery returns only one tuple containing a single
attribute;suchsubqueriesarecalledscalar subqueries.Forexample,asubquery
98 Chapter 3 Introduction to SQL
can be used in the select clause as illustrated in the following example that lists
alldepartmentsalong with the number of instructorsin each department:
select dept name,
(select count(*)
from instructor
wheredepartment.dept name= instructor.dept name)
as num instructors
fromdepartment;
The subquery in the above example is guaranteed to return only a single value
sinceithasacount(*)aggregatewithoutagroupby.Theexamplealsoillustrates
theusageofcorrelationvariables,thatis,attributesofrelationsinthefromclause
of theouter query,such as department.dept name inthe aboveexample.
Scalarsubqueriescan occurinselect, where,andhavingclauses.Scalarsub-
queriesmayalsobede?nedwithoutaggregates.Itisnotalwayspossibleto?gure
out at compile time if a subquery can return more than one tuple in its result;
if the result has more than one tuple when the subquery is executed, a run-time
erroroccurs.
Note that technically the type of a scalar subquery result is still a relation,
even if it contains a single tuple. However, when a scalar subquery is used in an
expression where a value is expected, SQL implicitly extracts the value from the
single attribute of the single tuple inthe relation,and returnsthat value.
3.9 Modi?cation of the Database
We have restricted our attention until now to the extraction of information from
thedatabase.Now,weshowhowtoadd,remove,orchangeinformationwithSQL.
3.9.1 Deletion
Adeleterequestisexpressedinmuchthesamewayasaquery.Wecandeleteonly
wholetuples;wecannotdeletevaluesononlyparticularattributes.SQLexpresses
a deletionby
delete from r
where P;
where P represents a predicate and r represents a relation. The delete statement
?rst?ndsalltuplestinrforwhich P(t)istrue,andthendeletesthemfromr.The
where clausecan beomitted,inwhich case alltuplesinraredeleted.
Notethatadeletecommandoperatesononlyonerelation.Ifwewanttodelete
tuplesfromseveralrelations,wemustuseonedeletecommandforeachrelation.
The predicate in the whereclausemaybeascomplexasaselect command’s
whereclause.Attheotherextreme,the whereclausemaybeempty.Therequest
3.9 Modi?cation of the Database 99
delete from instructor;
deletes all tuples from the instructor relation. The instructor relation itself still
exists,but itisempty.
Hereareexamplesof SQL deleterequests:
• Delete all tuples in the instructor relation pertaining to instructors in the
Finance department.
delete from instructor
where dept name=’Finance’;
• Deleteall instructorswith a salarybetween$13,000 and$15,000.
deletefrom instructor
wheresalary between 13000 and15000;
• Deletealltuplesintheinstructorrelationforthoseinstructorsassociatedwith
a departmentlocatedinthe Watsonbuilding.
delete from instructor
where dept namein (select dept name
fromdepartment
wherebuilding = ’Watson’);
This delete request ?rst ?nds all departments located in Watson, and then
deletesall instructor tuplespertainingtothose departments.
Note that, although we may delete tuples from only one relation at a time,
we may reference any number of relations in a select-from-where nested in the
where clause of a delete.Thedelete request can contain a nested select that
referencestherelationfromwhichtuplesaretobedeleted.Forexample,suppose
thatwewanttodeletetherecordsofallinstructorswithsalarybelowtheaverage
at theuniversity.We couldwrite:
delete from instructor
wheresalary< (select avg (salary)
from instructor);
The delete statement ?rst tests each tuple in the relation instructor to check
whether the salary is less than the average salary of instructors in the univer-
sity. Then, all tuples that fail the test—that is, represent an instructor with a
lower-than-averagesalary—aredeleted.Performingallthetestsbeforeperform-
ing any deletion is important—if some tuples are deleted before other tuples
100 Chapter 3 Introduction to SQL
havebeentested,theaveragesalarymaychange,andthe?nalresultofthedelete
would dependontheorderinwhich thetupleswereprocessed!
3.9.2 Insertion
To insert data into a relation, we either specify a tuple to be inserted or write a
querywhoseresultisasetoftuplestobeinserted.Obviously,theattributevalues
for inserted tuples must be members of the corresponding attribute’s domain.
Similarly,tuplesinsertedmusthave thecorrectnumber of attributes.
The simplest insert statement is a request to insert one tuple. Suppose that
we wish to insert the fact that there is a course CS-437 in the Computer Science
departmentwithtitle “Database Systems”,and 4credithours.We write:
insert intocourse
values (’CS-437’, ’Database Systems’, ’Comp. Sci.’, 4);
Inthis example,the valuesarespeci?edintheorderinwhich thecorresponding
attributes are listed in the relation schema. For the bene?t of users who may not
remember the order of the attributes, SQL allows the attributes to be speci?ed as
partoftheinsertstatement.Forexample,thefollowing SQLinsertstatementsare
identicalinfunction tothe precedingone:
insert intocourse(course id, title, dept name, credits)
values (’CS-437’, ’Database Systems’, ’Comp. Sci.’, 4);
insert intocourse(title, course id, credits, dept name)
values (’Database Systems’,’CS-437’, 4, ’Comp. Sci.’);
More generally, we might want to insert tuples on the basis of the result of a
query.SupposethatwewanttomakeeachstudentintheMusicdepartmentwho
has earned more than 144 credit hours, an instructor in the Music department,
with a salaryof $18,000. Wewrite:
insertinto instructor
select ID, name, dept name, 18000
fromstudent
where dept name = ’Music’ and tot cred>144;
Instead of specifying a tuple as we did earlier in this section, we use a select to
specify a set of tuples. SQL evaluates the select statement ?rst, giving a set of
tuples that is then inserted into the instructor relation. Each tuple has an ID,a
name,adept name(Music), and an salaryof $18,000.
It is important that we evaluate the select statement fully before we carry
outanyinsertions.Ifwecarryoutsomeinsertionsevenastheselectstatementis
being evaluated,a requestsuchas:
3.9 Modi?cation of the Database 101
insert intostudent
select *
fromstudent;
mightinsertanin?nitenumberoftuples,iftheprimarykeyconstraintonstudent
were absent. Without the primary key constraint, the request would insert the
?rst tuple in student again, creating a second copy of the tuple. Since this second
copy is part of student now, the select statement may ?nd it, and a third copy
would be inserted into student.Theselect statement may then ?nd this third
copyandinsertafourthcopy,andsoon,forever.Evaluatingtheselectstatement
completely before performing insertions avoids such problems. Thus, the above
insertstatementwouldsimplyduplicateeverytupleinthestudentrelation,ifthe
relationdidnot have aprimarykeyconstraint.
Our discussion of the insert statement considered only examples in which
a value is given for every attribute in inserted tuples. It is possible for inserted
tuples to be given values on only some attributes of the schema. The remaining
attributesareassignedanull valuedenotedby null.Considertherequest:
insertinto student
values (’3003’, ’Green’, ’Finance’, null);
Thetupleinsertedbythisrequestspeci?edthatastudentwith ID “3003”isinthe
Financedepartment,butthetot credvalueforthisstudentisnotknown.Consider
thequery:
select student
fromstudent
where tot cred> 45;
Since the tot cred value of student “3003” is not known, we cannot determine
whether itisgreaterthan 45.
Mostrelationaldatabaseproductshavespecial“bulkloader”utilitiestoinsert
a large set of tuples into a relation. These utilities allow data to be read from
formatted text ?les, and can execute much faster than an equivalent sequence of
insertstatements.
3.9.3 Updates
Incertainsituations,wemaywishtochangeavalueinatuplewithoutchanging
allvaluesinthetuple.Forthispurpose,theupdatestatementcanbeused.Aswe
could for insert and delete, we can choose the tuples to be updated by using a
query.
Suppose that annual salary increases are being made, and salaries of all in-
structorsare to be increased by 5 percent.We write:
102 Chapter 3 Introduction to SQL
updateinstructor
set salary= salary * 1.05;
Theprecedingupdatestatementisappliedoncetoeachofthetuplesininstructor
relation.
If a salary increase is to be paid only to instructors with salary of less than
$70,000, we can write:
updateinstructor
set salary = salary*1.05
wheresalary<70000;
In general, the where clause of the update statement may contain any construct
legal in the where clause of the select statement (including nested selects). As
withinsertanddelete,anestedselectwithinanupdatestatementmayreference
therelationthatisbeingupdated.Asbefore,SQL?rsttestsalltuplesintherelation
to see whether they should be updated, and carries out the updates afterward.
Forexample,wecanwritetherequest“Givea5percentsalaryraisetoinstructors
whose salary islessthan average”asfollows:
update instructor
set salary = salary*1.05
where salary< (select avg (salary)
from instructor);
Let us now suppose that all instructors with salary over $100,000 receive a
3 percent raise, whereas all others receive a 5 percent raise. We could write two
updatestatements:
updateinstructor
set salary = salary*1.03
wheresalary>100000;
updateinstructor
set salary = salary*1.05
wheresalary<= 100000;
Notethattheorderofthetwoupdatestatementsisimportant.Ifwechangedthe
orderofthetwostatements,aninstructorwithasalaryjustunder$100,000would
receiveanover8percentraise.
SQL provides a case construct that we can use to perform both the updates
withasingle updatestatement,avoidingtheproblemwiththeorderof updates.
3.9 Modi?cation of the Database 103
update instructor
set salary = case
when salary<= 100000 thensalary*1.05
else salary*1.03
end
The generalform of the case statementisas follows.
case
when pred
1
then result
1
when pred
2
then result
2
...
when pred
n
then result
n
else result
0
end
The operation returns result
i
,wherei is the ?rst of pred
1
, pred
2
,...,pred
n
that is
satis?ed; if none of the predicates is satis?ed, the operation returns result
0
.Case
statementscanbe usedinany place wherea valueisexpected.
ScalarsubqueriesarealsousefulinSQLupdatestatements,wheretheycanbe
used in the set clause. Consider an update where we set the tot cred attribute of
each student tuple to the sum of the credits of courses successfully completed by
thestudent.Weassumethatacourseissuccessfullycompletedifthestudenthas
a grade that is not ’F’ or null. To specify this update, we need to use a subquery
inthe set clause, as shown below:
updatestudent S
set tot cred = (
select sum(credits)
from takes naturaljoin course
whereS.ID= takes.ID and
takes.grade<> ’F’ and
takes.grade is notnull);
ObservethatthesubqueryusesacorrelationvariableSfromtheupdatestatement.
In case a student has not successfully completed any course, the above update
statement would set the tot cred attribute value to null. To set the value to 0
instead, we could use another update statement to replace null values by 0; a
better alternative is to replace the clause “select sum(credits)” in the preceding
subqueryby the following select clause using acase expression:
select case
when sum(credits) is notnullthensum(credits)
else 0
end
104 Chapter 3 Introduction to SQL
3.10 Summary
• SQListhemostin?uentialcommerciallymarketedrelationalquerylanguage.
The SQL language has severalparts:
?
Data-de?nition language(DDL), which providescommands for de?ning
relationschemas, deletingrelations,and modifyingrelationschemas.
?
Data-manipulation language (DML), which includes a query language
andcommandstoinserttuplesinto,deletetuplesfrom,andmodifytuples
inthe database.
• The SQL data-de?nition language is used to create relations with speci?ed
schemas.Inadditiontospecifyingthenamesandtypesofrelationattributes,
SQLalsoallowsthespeci?cationofintegrityconstraintssuchasprimary-key
constraints and foreign-keyconstraints.
• SQL includes a variety of language constructs for queries on the database.
Theseincludetheselect,from,andwhereclauses,andsupportforthenatural
join operation.
• SQL also provides mechanisms to rename both attributes and relations, and
to orderqueryresultsby sortingon speci?edattributes.
• SQL supports basic set operations on relations including union, intersect,
and except, which correspond to the mathematical set-theory operations ?,
?,and?.
• SQL handles queries on relations containing null values by adding the truth
value “unknown” to theusual truthvaluesoftrueand false.
• SQL supports aggregation, including the ability to divide a relation into
groups, applying aggregation separately on each group. SQL also supports
setoperationsongroups.
• SQL supports nested subqueries in the where,andfrom clauses of an outer
query. It also supports scalar subqueries, wherever an expression returning
a valueis permitted.
• SQL providesconstructs for updating,inserting,anddeletinginformation.
Review Terms
• Data-de?nition language
• Data-manipulation language
• Database schema
• Database instance
• Relation schema
• Relation instance
• Primary key
• Foreignkey
?
Referencingrelation
?
Referencedrelation
PracticeExercises 105
• Nullvalue
• Querylanguage
• SQL querystructure
?
select clause
?
fromclause
?
where clause
• Naturaljoinoperation
• as clause
• orderby clause
• Correlationname(correlationvari-
able,tuplevariable)
• Setoperations
?
union
?
intersect
?
except
• Nullvalues
?
Truth value “unknown”
• Aggregatefunctions
?
avg, min, max, sum, count
?
groupby
?
having
• Nestedsubqueries
• Setcomparisons
?
{<,<=,>,> =}{some, all }
?
exists
?
unique
• lateral clause
• with clause
• Scalar subquery
• Database modi?cation
?
Deletion
?
Insertion
?
Updating
Practice Exercises
3.1 Write the following queries in SQL, using the university schema. (We sug-
gest you actually run these queries on a database, using the sample data
that we provide on the Web site of the book, db-book.com. Instructions for
settingupadatabase,andloadingsampledata,areprovidedontheabove
Web site.)
a. Find the titles of courses in the Comp. Sci. department that have 3
credits.
b. Find the IDs of all studentswho weretaught by an instructor named
Einstein;makesuretherearenoduplicatesintheresult.
c. Find the highest salaryof any instructor.
d. Find all instructors earning the highest salary (there may be more
than one with the same salary).
e. FindtheenrollmentofeachsectionthatwasofferedinAutumn2009.
f. Find the maximum enrollment,across allsections, in Autumn 2009.
g. FindthesectionsthathadthemaximumenrollmentinAutumn2009.
106 Chapter 3 Introduction to SQL
person (driver id, name, address)
car (license, model, year)
accident (report number, date, location)
owns (driver id, license)
participated (report number, license, driver id, damage amount)
Figure 3.18 Insurance database for Exercises 3.4 and 3.14.
3.2 Supposeyouaregivenarelationgrade points(grade,points),whichprovides
a conversion from letter grades in the takes relation to numeric scores; for
examplean“A”gradecouldbespeci?edtocorrespondto4points,an“A?”
to 3.7 points, a “B+” to 3.3 points, a “B” to 3 points, and so on. The grade
points earned by a student for a course offering (section) is de?ned as the
number of credits for the course multiplied by the numeric points for the
gradethat the studentreceived.
Given the above relation, and our university schema, write each of the
followingqueriesin SQL. Youcan assumeforsimplicitythatno takestuple
has the null valuefor grade.
a. Findthetotalgrade-pointsearnedbythestudentwithID12345,across
allcourses takenby thestudent.
b. Find the grade-point average (GPA) for the above student, that is,
the total grade-points divided by the total credits for the associated
courses.
c. Find the ID and thegrade-pointaverageof everystudent.
3.3 Writethefollowinginserts,deletesorupdatesin SQL, usingtheuniversity
schema.
a. Increase the salary of each instructor in the Comp. Sci. department
by 10%.
b. Delete all courses that have never been offered (that is, do not occur
inthe section relation).
c. Inserteverystudentwhose tot cred attribute is greaterthan 100 as an
instructorin the same department,with a salaryof $10,000.
3.4 Consider the insurance database of Figure 3.18, where the primary keys
are underlined. Construct the following SQL queries for this relational
database.
a. Findthe total number of peoplewho owned cars that wereinvolved
in accidentsin 2009.
b. Adda new accident to the database; assume any values for required
attributes.
c. Deletethe Mazdabelonging to “John Smith”.
PracticeExercises 107
branch(branch name, branch city, assets)
customer(customer name, customer street,customer city)
loan (loan number, branch name, amount)
borrower(customer name, loan number)
account (account number, branch name, balance )
depositor (customer name, account number)
Figure 3.19 Banking database for Exercises 3.8 and 3.15.
3.5 Suppose that we have a relation marks(ID, score) and we wish to assign
grades to students based on the score as follows: grade F if score < 40,
grade C if 40 ? score < 60, grade B if 60 ? score < 80, and grade Aif 80 ?
score.WriteSQL queriesto dothe following:
a. Displaythe gradefor each student, based on the marks relation.
b. Find the number of studentswith each grade.
3.6 The SQL like operator is case sensitive, but the lower() function on strings
can be used to perform case insensitive matching. To show how, write a
query that ?nds departments whose names contain the string “sci” as a
substring,regardlessof the case.
3.7 Considerthe SQL query
select distinct p.a1
fromp, r1, r2
where p.a1 = r1.a1 or p.a1 = r2.a1
Underwhatconditionsdoestheprecedingqueryselectvaluesof p.a1that
are either inr1orinr2? Examine carefully the cases where one ofr1orr2
maybe empty.
3.8 ConsiderthebankdatabaseofFigure3.19,wheretheprimarykeysareun-
derlined.Constructthe following SQL queriesfor this relationaldatabase.
a. Find all customersof the bank who have an account but not aloan.
b. Find the names of all customers who live on the same street and in
thesamecityas“Smith”.
c. Find the names of all branches with customers who have an account
inthe bank and who livein “Harrison”.
3.9 ConsidertheemployeedatabaseofFigure3.20,wheretheprimarykeysare
underlined.Giveanexpressionin SQL for each of the following queries.
a. Findthenamesandcitiesofresidenceofallemployeeswhoworkfor
“FirstBank Corporation”.
108 Chapter 3 Introduction to SQL
employee (employee name, street,city)
works (employee name, company name, salary)
company (company name, city)
manages (employee name, manager name)
Figure 3.20 Employee database for Exercises 3.9, 3.10, 3.16, 3.17, and 3.20.
b. Find the names, street addresses, and cities of residence of all em-
ployees who work for “First Bank Corporation” and earn more than
$10,000.
c. Find all employees in the database who do not work for “First Bank
Corporation”.
d. Findallemployeesinthedatabasewhoearnmorethaneachemployee
of “SmallBankCorporation”.
e. Assume that the companies may be located in several cities. Find all
companies located in every city in which “Small Bank Corporation”
islocated.
f. Find the company that has the most employees.
g. Find those companies whose employees earn a higher salary, on av-
erage,thanthe averagesalary at “FirstBank Corporation”.
3.10 Consider the relational database of Figure 3.20. Give an expression in SQL
for each of the following queries.
a. Modifythe database so that “Jones” nowlivesin “Newtown”.
b. Give all managers of “First Bank Corporation”a10percentraise
unless the salary becomes greater than $100,000; in such cases, give
only a3 percentraise.
Exercises
3.11 Writethefollowing queriesin SQL, using the universityschema.
a. FindthenamesofallstudentswhohavetakenatleastoneComp.Sci.
course;makesurethereareno duplicatenames intheresult.
b. FindtheIDsandnamesofallstudentswhohavenottakenanycourse
offeringbefore Spring2009.
c. For each department, ?nd the maximum salary of instructors in that
department.Youmayassumethateverydepartmenthasatleastone
instructor.
d. Findthelowest,acrossalldepartments,oftheper-departmentmaxi-
mumsalary computedby theprecedingquery.
Exercises 109
3.12 Writethefollowing queriesin SQL, using the universityschema.
a. Createanewcourse“CS-001”,titled“WeeklySeminar”,with0credits.
b. Createasection of thiscourse in Autumn 2009,with sec id of 1.
c. Enroll every student in the Comp. Sci. department in the above sec-
tion.
d. Delete enrollments in the above section where the student’s name is
Chavez.
e. Delete the course CS-001. What will happen if you run this delete
statementwithout ?rst deletingofferings(sections) of thiscourse.
f. Deletealltakestuplescorrespondingtoanysectionofanycoursewith
theword “database”asapartofthetitle;ignorecasewhenmatching
thewordwiththe title.
3.13 Write SQL DDL corresponding to the schema in Figure 3.18. Make any
reasonable assumptions about data types, and be sure to declare primary
and foreignkeys.
3.14 Consider the insurance database of Figure 3.18, where the primary keys
are underlined. Construct the following SQL queries for this relational
database.
a. Find the number of accidents in which the cars belonging to “John
Smith”wereinvolved.
b. Update the damage amount for the car with the license number
“AABB2000”in the accident with reportnumber “AR2197” to$3000.
3.15 ConsiderthebankdatabaseofFigure3.19,wheretheprimarykeysareun-
derlined.Constructthe following SQL queriesfor this relationaldatabase.
a. Findallcustomerswhohaveanaccountatallthebrancheslocatedin
“Brooklyn”.
b. Find out the total sum of all loan amounts in the bank.
c. Find the names of all branches that have assets greater than those of
atleastone branch located in “Brooklyn”.
3.16 ConsidertheemployeedatabaseofFigure3.20,wheretheprimarykeysare
underlined.Giveanexpressionin SQL for each of the following queries.
a. Find the names of all employees who work for “First Bank Corpora-
tion”.
b. Find all employees in the database who live in the same cities as the
companiesfor which theywork.
c. Findallemployeesinthedatabasewholiveinthesamecitiesandon
thesamestreetsasdotheirmanagers.
110 Chapter 3 Introduction to SQL
d. Find all employees who earn more than the average salary of all
employeesof theircompany.
e. Find the company that has the smallestpayroll.
3.17 Consider the relational database of Figure 3.20. Give an expression in SQL
for each of the following queries.
a. Giveallemployeesof “FirstBank Corporation” a10 percentraise.
b. Giveallmanagers of “FirstBank Corporation” a10 percentraise.
c. Delete all tuples in the works relation for employees of “Small Bank
Corporation”.
3.18 Listtwo reasonswhy null valuesmightbe introducedinto the database.
3.19 Showthat, in SQL,<> all isidenticalto notin.
3.20 Give an SQL schema de?nition for the employee database of Figure 3.20.
Choose an appropriate domain for each attribute and an appropriate pri-
marykeyfor each relationschema.
3.21 ConsiderthelibrarydatabaseofFigure3.21.Writethefollowingqueriesin
SQL.
a. Printthenamesofmemberswhohaveborrowedanybookpublished
by “McGraw-Hill”.
b. Printthenamesofmemberswhohaveborrowedallbookspublished
by “McGraw-Hill”.
c. Foreachpublisher,printthe namesofmemberswhohaveborrowed
more than ?ve books ofthat publisher.
d. Print the average number of books borrowed per member. Take into
account that if an member does not borrow any books, then that
memberdoesnot appearintheborrowed relationat all.
3.22 Rewrite the where clause
whereunique(select title fromcourse)
without using the uniqueconstruct.
member(memb no, name,age)
book(isbn, title, authors, publisher)
borrowed(memb no, isbn, date)
Figure 3.21 Library database for Exercise 3.21.
Tools 111
3.23 Considerthe query:
select course id, semester,year, sec id, avg(tot cred)
from takes naturaljoin student
where year = 2009
groupby course id, semester,year, sec id
havingcount(ID)>= 2;
Explain why joining section as well in the from clause would not change
theresult.
3.24 Considerthe query:
with dept total (dept name, value) as
(select dept name, sum(salary)
from instructor
groupby dept name),
dept total avg(value) as
(select avg(value)
from dept total)
select dept name
from dept total, dept total avg
where dept total.value >= dept total avg.value;
Rewrite thisquerywithout using the with construct.
Tools
A number of relational database systems are available commercially, including
IBM DB2, IBM Informix, Oracle, Sybase, and Microsoft SQL Server. In addition
several database systems can be downloaded and used free of charge, including
PostgreSQL, MySQL (free except for certain kinds of commercial use), and Oracle
Expressedition.
Mostdatabasesystemsprovideacommandlineinterfaceforsubmitting SQL
commands. In addition, most databases also provide graphical user interfaces
(GUIs),whichsimplifythetaskofbrowsingthedatabase,creatingandsubmitting
queries,andadministeringthedatabase.CommercialIDEsforSQLthatworkacross
multipledatabase platforms, include Embarcadero’s RAD Studio and Aqua Data
Studio.
ForPostgreSQL,thepgAdmintoolprovidesGUIfunctionality,whileforMySQL,
phpMyAdminprovidesGUIfunctionality.TheNetBeansIDEprovidesa GUIfront
endthatworkswithanumberofdifferentdatabases,butwithlimitedfunctional-
ity,whiletheEclipse IDE supportssimilarfunctionalitythroughseveraldifferent
pluginssuch as the Data ToolsPlatform (DTP)and JBuilder.
SQL schema de?nitions and sample data for the university schema are pro-
vided on the Web site for this book, db-book.com. The Web site also contains
112 Chapter 3 Introduction to SQL
instructions on how to set up and access some popular database systems. The
SQL constructs discussed in this chapter are part of the SQL standard, but certain
features are not supported by some databases. The Web site lists these incom-
patibilities, which you will need to take into account when executing queries on
those databases.
Bibliographical Notes
The original version of SQL, called Sequel 2, is described by Chamberlin et al.
[1976]. Sequel 2 was derived from the language Square (Boyce et al. [1975] and
Chamberlin and Boyce [1974]). The American National Standard SQL-86 is de-
scribed in ANSI [1986]. The IBM Systems Application Architecture de?nition of
SQL is de?ned by IBM [1987]. The of?cial standards for SQL-89 and SQL-92 are
availableas ANSI[1989] and ANSI[1992], respectively.
TextbookdescriptionsoftheSQL-92languageincludeDateandDarwen[1997],
MeltonandSimon[1993],andCannanandOtten[1993].DateandDarwen[1997]
and Date [1993a] include a critique of SQL-92 from a programming-languages
perspective.
Textbooks on SQL:1999 include Melton and Simon [2001] and Melton [2002].
Eisenberg and Melton [1999] provide an overview of SQL:1999. Donahoo and
Speegle[2005] covers SQL from a developers’perspective.Eisenberg et al. [2004]
providesanoverviewof SQL:2003.
The SQL:1999, SQL:2003, SQL:2006 and SQL:2008 standards are published as a
collection of ISO/IEC standards documents, which are described in more detail
in Section 24.4. The standards documents are densely packed with information
and hard to read, and of use primarily for database system implementers. The
standards documents are available from the Web site http://webstore.ansi.org,but
only for purchase.
Many database products support SQL features beyond those speci?ed in the
standard,andmay not supportsomefeaturesof thestandard.Moreinformation
onthesefeaturesmaybefoundintheSQLusermanualsoftherespectiveproducts.
Theprocessingof SQLqueries,includingalgorithmsandperformanceissues,
is discussed in Chapters 12 and 13. Bibliographic references on these matters
appearinthosechapters.
CHAPTER
4
Intermediate SQL
In this chapter, we continue our study of SQL. We consider more complex forms
of SQL queries, view de?nition, transactions, integrity constraints, more details
regarding SQL datade?nition, andauthorization.
4.1 Join Expressions
In Section 3.3.3, we introduced the natural join operation. SQL provides other
forms of the join operation, including the ability to specify an explicit join pred-
icate, and the ability to include in the result tuples that are excluded by natural
join.Weshall discusstheseformsof joininthissection.
Theexamplesinthissectioninvolvethetworelationsstudentandtakes,shown
in Figures 4.1 and 4.2, respectively. Observe that the attribute grade has a value
null for the student with ID 98988, for the course BIO-301, section 1, taken in
Summer2010.The nullvalueindicatesthat thegradehas not beenawardedyet.
ID name dept name tot cred
00128 Zhang Comp.Sci. 102
12345 Shankar Comp.Sci. 32
19991 Brandt History 80
23121 Chavez Finance 110
44553 Peltier Physics 56
45678 Levy Physics 46
54321 Williams Comp.Sci. 54
55739 Sanchez Music 38
70557 Snow Physics 0
76543 Brown Comp.Sci. 58
76653 Aoi Elec.Eng. 60
98765 Bourikas Elec.Eng. 98
98988 Tanaka Biology 120
Figure 4.1 The student relation.
113
114 Chapter 4 Intermediate SQL
ID course id sec id semester year grade
00128 CS-101 1 Fall 2009 A
00128 CS-347 1 Fall 2009 A-
12345 CS-101 1 Fall 2009 C
12345 CS-190 2 Spring 2009 A
12345 CS-315 1 Spring 2010 A
12345 CS-347 1 Fall 2009 A
19991 HIS-351 1 Spring 2010 B
23121 FIN-201 1 Spring 2010 C+
44553 PHY-101 1 Fall 2009 B-
45678 CS-101 1 Fall 2009 F
45678 CS-101 1 Spring 2010 B+
45678 CS-319 1 Spring 2010 B
54321 CS-101 1 Fall 2009 A-
54321 CS-190 2 Spring 2009 B+
55739 MU-199 1 Spring 2010 A-
76543 CS-101 1 Fall 2009 A
76543 CS-319 2 Spring 2010 A
76653 EE-181 1 Spring 2009 C
98765 CS-101 1 Fall 2009 C-
98765 CS-315 1 Spring 2010 B
98988 BIO-101 1 Summer 2009 A
98988 BIO-301 1 Summer 2010 null
Figure 4.2 The takes relation.
4.1.1 Join Conditions
In Section 3.3.3, we saw how to express natural joins, and we saw the join...
using clause, which is a form of natural join that only requires values to match
on speci?ed attributes. SQL supports another form of join, in which an arbitrary
joinconditioncan bespeci?ed.
The on condition allows a general predicate over the relations being joined.
This predicate is written like a where clause predicate except for the use of the
keywordonratherthanwhere.Liketheusingcondition,theonconditionappears
at theendof thejoinexpression.
Consider the following query, which has a join expression containing the on
condition.
select *
from studentjoin takes on student.ID= takes.ID;
The on condition above speci?es that a tuple from student matches a tuple from
takesiftheirIDvaluesareequal.Thejoinexpressioninthiscaseisalmostthesame
as the join expression student natural join takes, since the natural join operation
4.1 JoinExpressions 115
alsorequiresthatforastudenttupleandatakestupletomatch.Theonedifference
isthattheresulthasthe IDattributelistedtwice,inthejoinresult,onceforstudent
and once for takes,eventhough their IDvaluesmustbe thesame.
Infact, theabovequeryisequivalenttothe followingquery(inotherwords,
theygenerateexactlythesameresults):
select *
from student, takes
where student.ID= takes.ID;
Aswehaveseenearlier,therelationnameisusedtodisambiguatetheattribute
name ID,andthusthetwooccurrencescanbereferredtoasstudent.IDandtakes.ID
respectively. A version of this query that displays the ID value only once is as
follows:
select student.IDas ID, name, dept name, tot cred,
course id, sec id, semester, year, grade
from studentjoin takes on student.ID= takes.ID;
Theresultof theabove queryisshown inFigure4.3.
The on condition can express any SQL predicate, and thus a join expressions
using the on condition can express a richer class of join conditions than natural
join. However, as illustrated by our preceding example, a query using a join
expression with an on condition can be replaced by an equivalent expression
withouttheoncondition,withthepredicateintheonclausemovedtothewhere
clause.Thus, itmayappearthat the onconditionisa redundantfeatureof SQL.
However,thereare twogoodreasons for introducing the oncondition. First,
we shall see shortly that for a kind of join called an outer join, on conditions do
behave in a manner different from where conditions. Second, an SQL query is
often more readable by humans if the join condition is speci?ed in the on clause
and therestof theconditions appearinthewhere clause.
4.1.2 Outer Joins
Suppose we wish to display a list of all students, displaying their ID,andname,
dept name,andtot cred,alongwiththecoursesthattheyhavetaken.Thefollowing
SQL querymay appeartoretrievethe requiredinformation:
select *
from studentnaturaljoin takes;
Unfortunately, the above query does not work quite as intended. Suppose that
thereissomestudentwhotakesnocourses.Thenthetupleinthe studentrelation
for that particular student would not satisfy the condition of a natural join with
any tuple in the takes relation, and that student’s data would not appear in the
result.Wewouldthusnotseeanyinformationaboutstudentswhohavenottaken
116 Chapter 4 Intermediate SQL
ID name dept name tot cred course id sec id semester year grade
00128 Zhang Comp.Sci. 102 CS-101 1 Fall 2009 A
00128 Zhang Comp.Sci. 102 CS-347 1 Fall 2009 A-
12345 Shankar Comp.Sci. 32 CS-101 1 Fall 2009 C
12345 Shankar Comp.Sci. 32 CS-190 2 Spring 2009 A
12345 Shankar Comp.Sci. 32 CS-315 1 Spring 2010 A
12345 Shankar Comp.Sci. 32 CS-347 1 Fall 2009 A
19991 Brandt History 80 HIS-351 1 Spring 2010 B
23121 Chavez Finance 110 FIN-201 1 Spring 2010 C+
44553 Peltier Physics 56 PHY-101 1 Fall 2009 B-
45678 Levy Physics 46 CS-101 1 Fall 2009 F
45678 Levy Physics 46 CS-101 1 Spring 2010 B+
45678 Levy Physics 46 CS-319 1 Spring 2010 B
54321 Williams Comp.Sci. 54 CS-101 1 Fall 2009 A-
54321 Williams Comp.Sci. 54 CS-190 2 Spring 2009 B+
55739 Sanchez Music 38 MU-199 1 Spring 2010 A-
76543 Brown Comp.Sci. 58 CS-101 1 Fall 2009 A
76543 Brown Comp.Sci. 58 CS-319 2 Spring 2010 A
76653 Aoi Elec.Eng. 60 EE-181 1 Spring 2009 C
98765 Bourikas Elec.Eng. 98 CS-101 1 Fall 2009 C-
98765 Bourikas Elec.Eng. 98 CS-315 1 Spring 2010 B
98988 Tanaka Biology 120 BIO-101 1 Summer 2009 A
98988 Tanaka Biology 120 BIO-301 1 Summer 2010 null
Figure 4.3 The result of student join takes on student.ID= takes.ID with second occurrence
of ID omitted.
anycourses.Forexample,inthe studentand takesrelationsofFigures4.1and4.2,
note that student Snow, with ID 70557, has not taken any courses. Snow appears
in student,butSnow’sIDnumberdoesnotappearinthe IDcolumnof takes.Thus,
Snow doesnot appearintheresultof thenatural join.
Moregenerally,sometuplesineitherorbothoftherelationsbeingjoinedmay
be “lost” in this way. The outer join operation works in a manner similar to the
joinoperationswehavealreadystudied,butpreservethosetuplesthatwouldbe
lostinajoin,by creating tuplesintheresultcontaining nullvalues.
Forexample,toensurethatthestudentnamedSnowfromourearlierexample
appears in the result, a tuple could be added to the join result with all attributes
from the student relation set to the corresponding values for the student Snow,
andalltheremainingattributeswhichcomefromthetakesrelation,namelycourse
id, sec id, semester,andyear,settonull. Thus the tuple for the student Snow is
preservedinthe resultofthe outerjoin.
Thereareinfact threeforms of outerjoin:
• Theleftouterjoinpreservestuplesonlyintherelationnamedbefore(tothe
leftof) the leftouterjoinoperation.
4.1 JoinExpressions 117
• Therightouterjoinpreservestuplesonlyintherelationnamedafter(tothe
rightof) the rightouterjoin operation.
• Thefullouterjoin preservestuplesinbothrelations.
Incontrast,thejoinoperationswestudiedearlierthatdonotpreservenonmatched
tuples are called inner join operations, to distinguish them from the outer-join
operations.
Wenowexplainexactlyhoweachformofouterjoinoperates.Wecancompute
the leftouter-join operationas follows. First, compute the resultof the inner join
asbefore.Then,foreverytupletintheleft-hand-siderelationthatdoesnotmatch
any tuple in the right-hand-side relation in the inner join, add a tuple r to the
resultof the joinconstructed asfollows:
• The attributes of tuple r that are derivedfrom the left-hand-siderelationare
?lledinwiththe valuesfrom tuple t.
• Theremaining attributesof r are?lledwithnull values.
Figure4.4 shows the resultof:
select *
from studentnaturalleftouterjoin takes;
That result includes student Snow (ID 70557), unlike the result of an inner join,
but the tuple for Snow includes nulls for the attributes that appear only in the
schema of the takes relation.
As another example of the use of the outer-join operation, we can write the
query “Findall studentswho havenot taken a course”as:
select ID
from studentnaturalleftouterjoin takes
where course id is null;
Therightouterjoinissymmetrictotheleftouterjoin.Tuplesfromtheright-
hand-side relation that do not match any tuple in the left-hand-side relation are
paddedwith nulls and are addedto the result of the right outer join. Thus, if we
rewriteourabovequeryusingarightouterjoinandswappingtheorderinwhich
we listthe relationsasfollows:
select *
from takes naturalrightouterjoin student;
we get the same result except for the order in which the attributes appear in the
result(seeFigure4.5).
The full outer join is a combination of the left and right outer-join types.
After the operation computes the result of the inner join, it extends with nulls
thosetuplesfromtheleft-hand-siderelationthatdidnotmatchwithanyfromthe
118 Chapter 4 Intermediate SQL
ID name dept name tot cred course id sec id semester year grade
00128 Zhang Comp.Sci. 102 CS-101 1 Fall 2009 A
00128 Zhang Comp.Sci. 102 CS-347 1 Fall 2009 A-
12345 Shankar Comp.Sci. 32 CS-101 1 Fall 2009 C
12345 Shankar Comp.Sci. 32 CS-190 2 Spring 2009 A
12345 Shankar Comp.Sci. 32 CS-315 1 Spring 2010 A
12345 Shankar Comp.Sci. 32 CS-347 1 Fall 2009 A
19991 Brandt History 80 HIS-351 1 Spring 2010 B
23121 Chavez Finance 110 FIN-201 1 Spring 2010 C+
44553 Peltier Physics 56 PHY-101 1 Fall 2009 B-
45678 Levy Physics 46 CS-101 1 Fall 2009 F
45678 Levy Physics 46 CS-101 1 Spring 2010 B+
45678 Levy Physics 46 CS-319 1 Spring 2010 B
54321 Williams Comp.Sci. 54 CS-101 1 Fall 2009 A-
54321 Williams Comp.Sci. 54 CS-190 2 Spring 2009 B+
55739 Sanchez Music 38 MU-199 1 Spring 2010 A-
70557 Snow Physics 0 null null null null null
76543 Brown Comp.Sci. 58 CS-101 1 Fall 2009 A
76543 Brown Comp.Sci. 58 CS-319 2 Spring 2010 A
76653 Aoi Elec.Eng. 60 EE-181 1 Spring 2009 C
98765 Bourikas Elec.Eng. 98 CS-101 1 Fall 2009 C-
98765 Bourikas Elec.Eng. 98 CS-315 1 Spring 2010 B
98988 Tanaka Biology 120 BIO-101 1 Summer 2009 A
98988 Tanaka Biology 120 BIO-301 1 Summer 2010 null
Figure 4.4 Result of student naturalleftouterjoin takes.
right-hand side relation, and adds them to the result. Similarly, it extends with
nulls those tuples from the right-hand-side relation that did not match with any
tuplesfromthe left-hand-siderelationandaddsthem tothe result.
As an example of the use of full outer join, consider the following query:
“DisplayalistofallstudentsintheComp.Sci.department,alongwiththecourse
sections,ifany,thattheyhavetakeninSpring2009;allcoursesectionsfromSpring
2009 must be displayed, even if no student from the Comp. Sci. department has
takenthecourse section.”This querycanbe writtenas:
select *
from (select *
from student
where dept name=’Comp.Sci’)
naturalfullouter join
(select *
from takes
where semester=’Spring’and year =2009);
4.1 JoinExpressions 119
ID course id sec id semester year grade name dept name tot cred
00128 CS-101 1 Fall 2009 A Zhang Comp.Sci. 102
00128 CS-347 1 Fall 2009 A- Zhang Comp.Sci. 102
12345 CS-101 1 Fall 2009 C Shankar Comp.Sci. 32
12345 CS-190 2 Spring 2009 A Shankar Comp.Sci. 32
12345 CS-315 1 Spring 2010 A Shankar Comp.Sci. 32
12345 CS-347 1 Fall 2009 A Shankar Comp.Sci. 32
19991 HIS-351 1 Spring 2010 B Brandt History 80
23121 FIN-201 1 Spring 2010 C+ Chavez Finance 110
44553 PHY-101 1 Fall 2009 B- Peltier Physics 56
45678 CS-101 1 Fall 2009 F Levy Physics 46
45678 CS-101 1 Spring 2010 B+ Levy Physics 46
45678 CS-319 1 Spring 2010 B Levy Physics 46
54321 CS-101 1 Fall 2009 A- Williams Comp.Sci. 54
54321 CS-190 2 Spring 2009 B+ Williams Comp.Sci. 54
55739 MU-199 1 Spring 2010 A- Sanchez Music 38
70557 null null null null null Snow Physics 0
76543 CS-101 1 Fall 2009 A Brown Comp.Sci. 58
76543 CS-319 2 Spring 2010 A Brown Comp.Sci. 58
76653 EE-181 1 Spring 2009 C Aoi Elec.Eng. 60
98765 CS-101 1 Fall 2009 C- Bourikas Elec.Eng. 98
98765 CS-315 1 Spring 2010 B Bourikas Elec.Eng. 98
98988 BIO-101 1 Summer 2009 A Tanaka Biology 120
98988 BIO-301 1 Summer 2010 null Tanaka Biology 120
Figure 4.5 The result of takes naturalrightouter join student.
The on clause can be used with outer joins. The following query is identical
tothe?rst querywesawusing “studentnaturalleftouterjoin takes,”exceptthat
theattribute IDappearstwiceintheresult.
select *
from studentleftouterjoin takes on student.ID= takes.ID;
Aswenotedearlier,onandwherebehavedifferentlyforouterjoin.Thereason
forthisisthatouterjoinaddsnull-paddedtuplesonlyforthosetuplesthatdonot
contribute to the result of the corresponding inner join. The on condition is part
oftheouterjoinspeci?cation,but awhereclauseisnot.Inour example,thecase
of the student tuple for student “Snow” with ID 70557, illustrates this distinction.
Suppose we modify the preceding query by moving the on clause predicate to
thewhere clause,and insteadusing an onconditionof true.
select *
from studentleftouterjoin takes on true
where student.ID= takes.ID;
120 Chapter 4 Intermediate SQL
Join types
innerjoin
lenullouterjoin
rightouterjoin
fullouterjoin
Join conditions
natural
on < predicate>
using (A
1
, A
2
, . . ., A
n
)
Figure 4.6 Join types and join conditions.
Theearlierquery,usingtheleftouterjoinwiththeoncondition,includesatuple
(70557,Snow,Physics,0, null, null, null, null, null, null),becausethereisnotuple
intakeswith ID =70557.Inthelatterquery,however,everytuplesatis?esthejoin
conditiontrue,sononull-paddedtuplesaregeneratedbytheouterjoin.Theouter
join actually generates the Cartesian product of the two relations. Since there is
notupleintakeswith ID =70557,everytimeatupleappearsintheouterjoinwith
name = “Snow”,thevaluesfor student.IDand takes.IDmustbedifferent,andsuch
tuples would be eliminated by the where clause predicate. Thus student Snow
neverappearsinthe resultofthelatterquery.
4.1.3 Join Types and Conditions
Todistinguishnormaljoinsfromouterjoins,normaljoinsarecalledinnerjoinsin
SQL.Ajoinclausecanthusspecifyinnerjoininsteadofouterjointospecifythat
anormaljoinistobeused.Thekeywordinneris,however,optional.Thedefault
join type,when the join clause is usedwithout the outer pre?x is the innerjoin.
Thus,
select *
from studentjoin takes using (ID);
isequivalentto:
select *
from studentinnerjoin takes using(ID);
Similarly,naturaljoinisequivalenttonaturalinnerjoin.
Figure4.6showsafulllistofthevarioustypesofjointhatwehavediscussed.
As can be seen from the ?gure, any form of join (inner, left outer, right outer, or
fullouter) canbe combined withany joincondition(natural,using, oron).
4.2 Views
In our examples up to this point, we have operated at the logical-model level.
That is,we haveassumedthat therelationsinthe collectionwearegivenarethe
actual relationsstoredinthe database.
4.2 Views 121
It is not desirable for all users to see the entire logical model. Security con-
siderations may require that certain data be hidden from users. Consider a clerk
whoneedstoknowaninstructor’s ID,nameanddepartmentname,butdoesnot
have authorization to see the instructor’s salary amount. This person should see
arelationdescribedin SQL,by:
select ID, name, dept name
from instructor;
Aside from security concerns, we may wish to create a personalized collection
of relations that is better matched to a certain user’s intuition than is the logical
model. We may want to have a list of all course sections offered by the Physics
departmentintheFall2009semester,withthebuildingandroomnumberofeach
section.The relationthat wewould createfor obtaining sucha listis:
select course.course id, sec id, building, room number
from course, section
where course.course id = section.course id
and course.dept name =’Physics’
and section.semester = ’Fall’
and section.year =’2009’;
It is possible to compute and store the results of the above queries and then
make the stored relations available to users. However, if we did so, and the
underlying data in the relations instructor, course,orsection changes, the stored
query resultswould then no longer match the result of reexecuting the query on
the relations. In general, it is a bad idea to compute and store query results such
as those in the above examples (although there are some exceptions, which we
studylater).
Instead, SQL allows a “virtual relation” to be de?ned by a query, and the
relation conceptually contains the result of the query. The virtual relation is not
precomputedandstored,butinsteadiscomputedbyexecutingthequerywhen-
everthevirtualrelationisused.
Anysuchrelationthatisnotpartofthelogicalmodel,butismadevisibletoa
userasavirtualrelation,iscalledaview.Itispossibletosupportalargenumber
ofviewsontopofany givensetofactual relations.
4.2.1 View De?nition
Wede?neaviewin SQLbyusingthecreateviewcommand.Tode?neaview,we
mustgivetheviewanameandmuststatethequerythatcomputestheview.The
form ofthe createview command is:
create view vas<queryexpression>;
122 Chapter 4 Intermediate SQL
where <query expression> is any legal query expression. The view name is
representedby v.
Consideragaintheclerkwhoneedstoaccessalldataintheinstructorrelation,
except salary. The clerk should not be authorized to access the instructor relation
(we see later, in Section 4.6, how authorizations can be speci?ed). Instead, a
viewrelation facultycanbemadeavailabletotheclerk,withtheviewde?nedas
follows:
createview faculty as
select ID, name, dept name
from instructor;
Asexplainedearlier,theviewrelationconceptuallycontainsthetuplesinthe
query result, but is not precomputed and stored. Instead, the database system
storesthequeryexpressionassociatedwiththeviewrelation.Whenevertheview
relation is accessed, its tuples are created by computing the query result. Thus,
the viewrelationiscreatedwheneverneeded,ondemand.
TocreateaviewthatlistsallcoursesectionsofferedbythePhysicsdepartment
intheFall2009semesterwiththebuildingandroomnumberofeachsection,we
write:
create view physics fall 2009as
select course.course id, sec id, building, room number
from course, section
where course.course id = section.course id
and course.dept name =’Physics’
and section.semester = ’Fall’
and section.year =’2009’;
4.2.2 Using Views in SQL Queries
Once we have de?ned a view, we can use the view name to refer to the virtual
relation that the view generates. Using the view physics fall 2009, we can ?nd
all Physics courses offered in the Fall 2009 semester in the Watson building by
writing:
select course id
from physics fall 2009
where building= ’Watson’;
Viewnamesmayappearinaqueryanyplacewherearelationnamemayappear,
Theattributenames of aviewcan be speci?edexplicitlyas follows:
createview departments total salary(dept name, total salary)as
select dept name,sum (salary)
from instructor
groupby dept name;
4.2 Views 123
The preceding view gives for each department the sum of the salaries of all the
instructors at that department. Since the expression sum(salary) does not have a
name,the attributenameis speci?edexplicitlyintheviewde?nition.
Intuitively, at any given time, the set of tuples in the view relation is the
resultofevaluationofthequeryexpressionthatde?nestheview.Thus,ifaview
relationiscomputedandstored,itmaybecomeoutofdateiftherelationsusedto
de?ne it are modi?ed. To avoid this, views are usually implemented as follows.
When we de?ne a view, the database system stores the de?nition of the view
itself,ratherthantheresultofevaluationofthequeryexpressionthatde?nesthe
view. Wherever a view relation appears in a query, it is replaced by the stored
query expression. Thus, whenever we evaluate the query, the view relation is
recomputed.
Oneviewmaybeusedintheexpressionde?ninganotherview.Forexample,
we can de?ne a view physics fall 2009 watson that lists the course ID and room
number of all Physics courses offered in the Fall 2009 semester in the Watson
buildingas follows:
createview physics fall 2009 watson as
select course id, room number
from physics fall 2009
where building= ’Watson’;
where physics fall 2009 watson isitselfaviewrelation.Thisisequivalentto:
createview physics fall 2009 watson as
(select course id, room number
from(select course.course id, building, room number
from course, section
where course.course id = section.course id
and course.dept name =’Physics’
and section.semester = ’Fall’
and section.year =’2009’)
where building= ’Watson’;
4.2.3 Materialized Views
Certain database systems allow view relations to be stored, but they make sure
that, if the actual relations used in the view de?nition change, the view is kept
up-to-date.Suchviewsarecalled materialized views.
For example, consider the view departments total salary.Iftheaboveviewis
materialized,itsresultswouldbestoredinthedatabase.However,ifaninstructor
tuple is added to or deleted from the instructor relation, the result of the query
de?ningtheviewwouldchange,andasaresultthematerializedview’scontents
must be updated. Similarly, if an instructor’s salary is updated, the tuple in
departments total salary corresponding to that instructor’s department must be
updated.
124 Chapter 4 Intermediate SQL
The process of keeping the materialized view up-to-date is called material-
ized view maintenance (or often, just view maintenance) and is coveredin Sec-
tion 13.5. View maintenance can be done immediately when any of the relations
on which the view is de?ned is updated. Some database systems, however, per-
form view maintenance lazily, when the view is accessed. Some systems update
materializedviewsonlyperiodically;inthiscase,thecontentsofthematerialized
viewmaybestale,thatis,notup-to-date,whenitisused,andshouldnotbeused
if the application needs up-to-date data. And some database systems permit the
database administrator to control which of the above methods is used for each
materializedview.
Applications that use a view frequently may bene?t if the view is materi-
alized. Applications that demand fast response to certain queries that compute
aggregates over large relations can also bene?t greatly by creating materialized
viewscorrespondingtothequeries.Inthiscase,theaggregatedresultislikelyto
bemuchsmallerthanthelargerelationsonwhichtheviewisde?ned;asaresult
the materialized view can be used to answer the query very quickly, avoiding
reading the large underlying relations. Of course, the bene?ts to queries from
the materialization of a view must be weighed against the storage costs and the
addedoverheadfor updates.
SQL does not de?ne a standard way of specifying that a view is material-
ized, but many database systems providetheir own SQL extensions for this task.
Somedatabasesystemsalwayskeepmaterializedviewsup-to-datewhentheun-
derlying relations change, while others permit them to become out of date, and
periodicallyrecomputethem.
4.2.4 Update of a View
Although views are a useful tool for queries, they present serious problems if
we express updates, insertions, or deletions with them. The dif?culty is that a
modi?cationtothedatabaseexpressedintermsofaviewmustbetranslatedtoa
modi?cationto theactual relationsinthe logicalmodelofthe database.
Suppose the view faculty, which we saw earlier, is made available to a clerk.
Since we allow a view name to appear wherever a relation name is allowed, the
clerkcan write:
insertinto faculty
values (’30765’, ’Green’, ’Music’);
Thisinsertionmustberepresentedbyaninsertionintotherelationinstructor,since
instructoristheactualrelationfromwhichthedatabasesystemconstructstheview
faculty. However, to insert a tuple into instructor,wemusthavesomevaluefor
salary. Therearetwo reasonable approachesto dealingwiththisinsertion:
• Rejecttheinsertion,and returnanerrormessagetothe user.
• Inserta tuple(’30765’, ’Green’, ’Music’, null) into the instructor relation.
4.2 Views 125
Another problem with modi?cation of the database through views occurs
withaviewsuchas:
create view instructor info as
select ID, name, building
from instructor, department
where instructor.dept name= department.dept name;
ThisviewliststheID,name,andbuilding-nameofeachinstructorintheuniversity.
Considerthe following insertionthrough thisview:
insertinto instructor info
values (’69987’, ’White’, ’Taylor’);
SupposethereisnoinstructorwithID69987,andnodepartmentintheTaylor
building. Then the only possible method of inserting tuples into the instructor
and department relations is to insert (’69987’, ’White’, null, null)intoinstructor
and (null,’Taylor’,null)intodepartment. Then, we obtain the relations shown in
Figure4.7. However, this update doesnot have the desiredeffect,since the view
relation instructor info still does not include the tuple (’69987’, ’White’, ’Taylor’).
Thus, there is no way to update the relations instructor and department by using
nulls togetthedesiredupdateon instructor info.
Becauseofproblemssuchasthese,modi?cationsaregenerallynotpermit-
tedon viewrelations,exceptinlimitedcases. Differentdatabasesystemsspecify
different conditions under which they permit updates on view relations; see the
database system manuals for details.The general problem of database modi?ca-
tion through views has been the subject of substantial research, and the biblio-
graphicnotesprovidepointerstosomeof thisresearch.
In general, an SQL view is said to be updatable (that is, inserts, updates or
deletescanbeappliedontheview)ifthefollowingconditionsareallsatis?edby
thequeryde?ning theview:
• Thefromclause has only one databaserelation.
• The select clause contains only attribute names of the relation, and does not
haveany expressions,aggregates,ordistinctspeci?cation.
• Any attribute not listed in the select clause can be set to null;thatis,itdoes
not have anotnullconstraint and isnot part ofa primarykey.
• Thequerydoesnothaveagroupbyor havingclause.
Under these constraints, the update, insert,anddelete operations would be
allowedonthe following view:
createview history instructorsas
select *
from instructor
where dept name= ’History’;
126 Chapter 4 Intermediate SQL
ID name dept name salary
10101 Srinivasan Comp.Sci. 65000
12121 Wu Finance 90000
15151 Mozart Music 40000
22222 Einstein Physics 95000
32343 ElSaid History 60000
33456 Gold Physics 87000
45565 Katz Comp.Sci. 75000
58583 Cali?eri History 62000
76543 Singh Finance 80000
76766 Crick Biology 72000
83821 Brandt Comp.Sci. 92000
98345 Kim Elec.Eng. 80000
69987 White null null
instructor
dept name building budget
Biology Watson 90000
Comp.Sci. Taylor 100000
ElectricalEng. Taylor 85000
Finance Painter 120000
History Painter 50000
Music Packard 80000
Physics Watson 70000
null Painter null
department
Figure 4.7 Relations instructor and department after insertion of tuples.
Evenwiththeconditionsonupdatability,thefollowingproblemstillremains.
Suppose that a user tries to insert the tuple (’25566’, ’Brown’, ’Biology’, 100000)
into the history instructors view. This tuple can be inserted into the instructor
relation, but it would not appear in the history instructors view since it does not
satisfythe selectionimposedby theview.
By default, SQL would allow the above update to proceed. However, views
canbede?nedwithawithcheckoptionclauseattheendoftheviewde?nition;
then, if a tuple inserted into the view does not satisfy the view’s where clause
condition, the insertionis rejectedby thedatabase system.Updatesaresimilarly
rejectedifthenewvaluedoesnot satisfythewhere clauseconditions.
SQL:1999 has a more complex set of rules about when inserts, updates, and
deletes can be executed on a view, that allows updates through a larger class of
views;however,the rulesaretoocomplextobe discussedhere.
4.3 Transactions 127
4.3 Transactions
Atransactionconsistsofasequenceofqueryand/orupdatestatements.The SQL
standard speci?es that a transaction begins implicitly when an SQL statement is
executed.Oneofthefollowing SQL statementsmustendthe transaction:
• Commitwork commits the current transaction; that is, it makes the updates
performed by the transaction become permanent in the database. After the
transactionis committed,anewtransactionisautomatically started.
• Rollback work causes the current transaction to be rolled back; that is, it
undoes all the updates performed by the SQL statements in the transaction.
Thus, the database state is restored to what it was before the ?rst statement
of thetransaction was executed.
Thekeywordworkis optionalinboth thestatements.
Transaction rollback is useful if some error condition is detected during ex-
ecution of a transaction. Commit is similar, in a sense, to saving changes to a
document that is being edited, while rollback is similar to quitting the edit ses-
sion without saving changes. Once a transaction has executed commit work,its
effectscan no longer be undone by rollback work.The database systemguaran-
teesthatintheeventofsomefailure,suchasanerrorinoneoftheSQLstatements,
a power outage, or a system crash, a transaction’s effects will be rolled back if it
has not yet executed commit work. In the case of power outage or other system
crash, therollback occurs whenthe systemrestarts.
Forinstance,considerabankingapplication,whereweneedtotransfermoney
fromonebankaccount toanotherinthesamebank.Todoso,weneedtoupdate
two account balances, subtracting the amount transferred from one, and adding
it to the other. If the system crashes after subtracting the amount from the ?rst
account, but before adding it to the second account, the bank balances would be
inconsistent. A similar problem would occur, if the second account is credited
beforesubtractingtheamountfromthe?rstaccount,andthesystemcrashesjust
aftercreditingtheamount.
As another example, consider our running example of a university applica-
tion. We assume that the attribute tot cred of each tuple in the student relation
is kept up-to-date by modifying it whenever the student successfully completes
a course. To do so, whenever the takes relation is updated to record successful
completionofacoursebyastudent(byassigninganappropriategrade)thecorre-
sponding studenttuplemustalsobeupdated.Iftheapplicationperformingthese
two updates crashes after one update is performed, but before the second one is
performed,the datainthedatabase wouldbe inconsistent.
By either committing the actions of a transaction after all its steps are com-
pleted, or rolling back all its actions in case the transaction could not complete
all its actions successfully, the database provides an abstraction of a transaction
as being atomic, that is, indivisible. Either all the effects of the transaction are
re?ectedinthe database,ornone are(afterrollback).
128 Chapter 4 Intermediate SQL
Applying the notion of transactions to the above applications, the update
statementsshouldbeexecutedasasingletransaction.Anerrorwhileatransaction
executesoneofitsstatementswouldresultinundoingoftheeffectsoftheearlier
statementsofthetransaction,sothatthedatabaseisnotleftinapartiallyupdated
state.
If a program terminates without executing either of these commands, the
updatesareeithercommittedorrolledback.Thestandarddoesnotspecifywhich
of thetwo happens, andthe choice isimplementationdependent.
Inmany SQLimplementations,bydefaulteach SQLstatementistakentobea
transaction on its own, and gets committed as soon as it is executed. Automatic
commitofindividualSQLstatementsmustbeturnedoffifatransactionconsisting
of multiple SQL statements needs to be executed. How to turn off automatic
commitdependsonthespeci?cSQLimplementation,althoughthereisastandard
way of doing this using application program interfaces such as JDBC or ODBC,
which we studylater,inSections5.1.1and 5.1.2,respectively.
Abetteralternative,whichispartofthe SQL:1999standard(butsupportedby
only some SQL implementations currently), is to allow multiple SQL statements
to be enclosed between the keywords begin atomic...end. All the statements
betweenthe keywordsthenform a singletransaction.
We study further properties of transactions in Chapter 14; issues in imple-
menting transactions in a single database are addressed in Chapters 15 and 16,
while Chapter 19 addresses issues in implementing transactions across multiple
databases, to deal with problems such as transfer of money across accounts in
differentbanks, which havedifferentdatabases.
4.4 Integrity Constraints
Integrity constraints ensure that changes made to the database by authorized
usersdonotresultinalossofdataconsistency.Thus,integrityconstraintsguard
against accidental damageto thedatabase.
Examplesofintegrityconstraints are:
• An instructorname cannot be null.
• No two instructorscan havethesameinstructor ID.
• Everydepartment name in the course relationmust have a matching depart-
mentname inthe department relation.
• Thebudgetof adepartmentmustbe greaterthan $0.00.
In general, an integrity constraint can be an arbitrary predicate pertaining
to the database. However, arbitrary predicates may be costly to test. Thus, most
databasesystemsallowonetospecifyintegrityconstraintsthatcanbetestedwith
minimaloverhead.
WehavealreadyseensomeformsofintegrityconstraintsinSection3.2.2.We
study some more forms of integrity constraints in this section. In Chapter 8, we
4.4 Integrity Constraints 129
study another form of integrity constraint, called functional dependencies,that
isusedprimarilyintheprocess ofschemadesign.
Integrity constraints are usually identi?ed as part of the database schema
design process, and declared as part of the create table command used to create
relations.However,integrityconstraintscanalsobeaddedtoanexistingrelation
by using the command alter table table-name add constraint,whereconstraint
can be any constraint on the relation. When such a command is executed, the
system ?rst ensures that the relation satis?es the speci?ed constraint. If it does,
theconstraint isaddedto therelation;ifnot, thecommand is rejected.
4.4.1 Constraints on a Single Relation
WedescribedinSection3.2howtode?netablesusingthecreatetablecommand.
The create table command may also include integrity-constraint statements. In
addition to the primary-key constraint, there are a number of other ones that
can be included in the create table command. The allowed integrity constraints
include
• notnull
• unique
• check(<predicate>)
Wecovereach ofthesetypesof constraints inthefollowing sections.
4.4.2 Not Null Constraint
As we discussed in Chapter 3, the null value is a member of all domains, and
as a result is a legal value for every attribute in SQL by default. For certain
attributes, however, null values may be inappropriate. Consider a tuple in the
student relation where name is null. Such a tuple gives student information for
an unknown student; thus, it does not contain useful information. Similarly, we
would not want the departmentbudget to be null. Incases such as this, we wish
toforbidnullvalues,andwecandosobyrestrictingthedomainoftheattributes
nameand budget toexcludenullvalues,by declaringitas follows:
name varchar(20)notnull
budgetnumeric(12,2)notnull
Thenotnullspeci?cationprohibits theinsertionof anullvaluefor theattribute.
Any database modi?cation that would cause a null to be inserted in an attribute
declaredtobe notnullgeneratesanerrordiagnostic.
Therearemany situationswherewewant toavoidnullvalues.Inparticular,
SQL prohibits null values in the primary key of a relation schema. Thus, in our
universityexample,inthedepartmentrelation,iftheattributedept nameisdeclared
130 Chapter 4 Intermediate SQL
astheprimarykeyfor department,itcannottakeanullvalue.Asaresultitwould
not needtobe declaredexplicitlytobe notnull.
4.4.3 Unique Constraint
SQL alsosupportsanintegrityconstraint:
unique(A
j
1
, A
j
2
,...,A
j
m
)
The unique speci?cation says that attributes A
j
1
, A
j
2
,...,A
j
m
form a candidate
key; that is, no two tuples in the relation can be equal on all the listed attributes.
However, candidate key attributes are permitted to be null unless they have
explicitly been declared to be not null. Recall that a null value does not equal
any other value. (The treatment of nulls here is the same as that of the unique
construct de?nedinSection3.8.4.)
4.4.4 The check Clause
When applied to a relation declaration, the clause check(P) speci?es a predicate
Pthat must besatis?edby everytupleinarelation.
A common use of the check clause is to ensure that attribute values satisfy
speci?ed conditions, in effect creating a powerful type system. For instance, a
clause check (budget>0)inthecreate table command for relation department
would ensurethat thevalueofbudgetisnonnegative.
Asanother example,considerthe following:
createtable section
(course id varchar(8),
sec id varchar(8),
semester varchar(6),
year numeric(4,0),
building varchar(15),
room number varchar(7),
time slot id varchar(4),
primarykey(course id, sec id, semester, year),
check (semesterin(’Fall’, ’Winter’, ’Spring’,’Summer’)));
Here,weusethecheckclausetosimulateanenumeratedtype,byspecifyingthat
semester must be one of ’Fall’, ’Winter’, ’Spring’, or ’Summer’. Thus, the check
clause permits attribute domains to be restricted in powerful ways that most
programming-language typesystemsdonot permit.
The predicate in the check clause can, according to the SQL standard, be an
arbitrary predicate that can include a subquery. However, currently none of the
widelyuseddatabase productsallowsthe predicatetocontain asubquery.
4.4 Integrity Constraints 131
4.4.5 Referential Integrity
Often, we wish to ensure that a value that appears in one relation for a given set
of attributes also appears for a certain set of attributes in another relation. This
conditioniscalledreferentialintegrity.
Foreignkeyscanbespeci?edaspartoftheSQLcreatetablestatementbyusing
theforeignkeyclause,aswesawearlierinSection3.2.2.Weillustrateforeign-key
declarations by using the SQL DDL de?nition of part of our university database,
shown in Figure 4.8. The de?nition of the course table has a declaration “foreign
key(dept name)referencesdepartment”.Thisforeign-keydeclarationspeci?esthat
foreachcoursetuple,thedepartmentnamespeci?edinthetuplemustexistinthe
departmentrelation.Withoutthisconstraint,itispossibleforacoursetospecifya
nonexistent departmentname.
More generally, let r
1
and r
2
be relations whose set of attributes are R
1
and
R
2
, respectively, with primary keys K
1
and K
2
. We say that a subset  of R
2
is a
foreignkeyreferencing K
1
inrelationr
1
ifitisrequiredthat,foreverytuple t
2
in
r
2
,theremustbeatuplet
1
inr
1
such that t
1
.K
1
= t
2
.  .
Requirements of this form are called referential-integrity constraints,or
subset dependencies. The latter term arises because the preceding referential-
integrity constraint can be stated as a requirement that the set of values on  in
r
2
must be a subset of the values on K
1
in r
1
. Note that, for a referential-integrity
constraint to make sense,  and K
1
must be compatible sets of attributes; that is,
either  mustbeequalto K
1
,ortheymustcontainthesamenumberofattributes,
and the types of corresponding attributes must be compatible (we assume here
that  and K
1
areordered).Unlikeforeign-keyconstraints,ingeneralareferential
integrityconstraintdoesnotrequire K
1
tobeaprimarykeyofr
1
;asaresult,more
than onetuplein r
1
can havethe samevaluefor attributes K
1
.
By default, in SQL a foreign key references the primary-key attributes of the
referenced table. SQLalsosupportsaversionofthereferencesclausewherealist
of attributes of the referenced relation can be speci?ed explicitly. The speci?ed
list of attributesmust, however, be declaredas a candidate key of the referenced
relation, using either a primary key constraint, or a unique constraint. A more
general form of a referential-integrity constraint, where the referenced columns
neednotbeacandidatekey,cannotbedirectlyspeci?edinSQL.TheSQLstandard
speci?es other constructs that can be used to implement such constraints; they
aredescribedinSection4.4.7.
Wecanusethefollowingshortformaspartofanattributede?nitiontodeclare
that theattributeforms aforeignkey:
dept namevarchar(20) references department
Whenareferential-integrityconstraintisviolated,thenormalprocedureisto
reject the action that caused the violation (that is, the transaction performing the
update action is rolled back). However, a foreign key clause can specify that if
a delete or update action on the referenced relation violates the constraint, then,
132 Chapter 4 Intermediate SQL
createtable classroom
(building varchar(15),
room number varchar(7),
capacity numeric(4,0),
primarykey(building, room number))
createtable department
(dept name varchar(20),
building varchar(15),
budget numeric(12,2) check (budget>0),
primarykey(dept name))
createtable course
(course id varchar(8),
title varchar(50),
dept name varchar(20),
credits numeric(2,0)check(credits>0),
primarykey(course id),
foreignkey(dept name)references department)
createtable instructor
(ID varchar(5),
name varchar(20), notnull
dept name varchar(20),
salary numeric(8,2),check (salary >29000),
primarykey(ID),
foreignkey(dept name)references department)
createtable section
(course id varchar(8),
sec id varchar(8),
semester varchar(6), check (semesterin
(’Fall’, ’Winter’, ’Spring’,’Summer’),
year numeric(4,0),check (year >1759 and year <2100)
building varchar(15),
room number varchar(7),
time slot id varchar(4),
primarykey(course id, sec id, semester, year),
foreignkey(course id)references course,
foreignkey(building, room number)references classroom)
Figure 4.8 SQL data de?nition for part of the university database.
instead of rejecting the action, the system must take steps to change the tuple in
the referencing relation to restore the constraint. Consider this de?nition of an
integrityconstraint onthe relation course:
4.4 Integrity Constraints 133
createtable course
(...
foreignkey (dept name)references department
on deletecascade
on updatecascade,
...);
Because of the clause on delete cascade associated with the foreign-key dec-
laration, if a delete of a tuple in department results in this referential-integrity
constraintbeingviolated,thesystemdoesnotrejectthedelete.Instead,thedelete
“cascades” to the course relation, deleting the tuple that refers to the department
that was deleted. Similarly, the system does not reject an update to a ?eld refer-
enced by the constraint if it violates the constraint; instead, the system updates
the ?eld dept name in the referencing tuples in course to the new value as well.
SQL also allows the foreign key clause to specify actions other than cascade,if
theconstraintisviolated:Thereferencing?eld(here, dept name)canbesettonull
(by usingsetnullinplaceofcascade), orto thedefaultvalueforthedomain(by
using setdefault).
If there is a chain of foreign-key dependencies across multiple relations, a
deletion or update at one end of the chain can propagate across the entire chain.
An interesting case where the foreign key constraint on a relation references
the same relation appears in Practice Exercises 4.9. If a cascading update or
deletecausesaconstraintviolationthatcannotbehandledbyafurthercascading
operation, the system aborts the transaction. As a result, all the changes caused
by thetransaction and itscascading actions areundone.
Null values complicate the semantics of referential-integrity constraints in
SQL. Attributes of foreign keys are allowed to be null, provided that they have
not otherwise been declared to be not null. If all the columns of a foreign key
are nonnull in a given tuple, the usual de?nition of foreign-key constraints is
used for that tuple. If any of the foreign-key columns is null, the tuple is de?ned
automatically tosatisfythe constraint.
This de?nition may not always be the right choice, so SQL also provides
constructs that allow you to change the behavior with null values; we do not
discussthe constructs here.
4.4.6 Integrity Constraint Violation During a Transaction
Transactions may consist of several steps, and integrity constraints may be vio-
lated temporarily after one step, but a later step may remove the violation. For
instance, suppose we have a relation person with primary key name, and an at-
tribute spouse, and suppose that spouse is a foreign key on person.Thatis,the
constraintsaysthatthe spouseattributemustcontainanamethatispresentinthe
persontable.Supposewewishtonotethefactthat JohnandMaryaremarriedto
eachotherbyinsertingtwotuples,oneforJohnandoneforMary,intheabovere-
lation,withthespouseattributessettoMaryandJohn,respectively.Theinsertion
of the ?rst tuple would violatethe foreign-key constraint, regardlessof which of
134 Chapter 4 Intermediate SQL
the two tuples is inserted ?rst. After the second tuple is inserted the foreign-key
constraint wouldhold again.
Tohandlesuchsituations,the SQLstandardallowsaclauseinitiallydeferred
to be added to a constraint speci?cation; the constraint would then be checked
at the end of a transaction, and not at intermediate steps. A constraint can alter-
natively be speci?ed as deferrable, which means it is checked immediately by
default,butcanbedeferredwhendesired.Forconstraintsdeclaredasdeferrable,
executingastatementset constraints constraint-list deferred as part of a transac-
tion causes the checking of the speci?ed constraints to be deferred to the end of
that transaction.
However,youshouldbeawarethatthedefaultbehavioristocheckconstraints
immediately,and many database implementations do not support deferredcon-
straint checking.
We can work around the problem in the above example in another way, if
the spouse attribute can be set to null: We set the spouse attributes to null when
inserting the tuples for John and Mary, and we update them later.However, this
techniquerequiresmoreprogrammingeffort,anddoesnotworkiftheattributes
cannot be setto null.
4.4.7 Complex Check Conditions and Assertions
The SQL standard supports additional constructs for specifying integrity con-
straints that are described in this section. However, you should be aware that
theseconstructs arenot currentlysupportedby mostdatabasesystems.
As de?ned by the SQL standard, the predicate in the check clause can be
an arbitrary predicate, which can include a subquery. If a database implemen-
tation supports subqueries in the check clause, we could specify the following
referential-integrityconstraint onthe relation section:
check(time slot id in(select time slot id from time slot))
Thecheckconditionveri?esthatthetime slot idineachtupleinthesectionrelation
isactuallytheidenti?erofatimeslotinthe time slotrelation.Thus,thecondition
has to be checked not only when a tuple is inserted or modi?ed in section,but
also when the relation time slot changes (in this case, when a tuple is deleted or
modi?edinrelation time slot).
Anothernaturalconstraintonouruniversityschemawouldbetorequirethat
every section has at least one instructor teaching the section. In an attempt to
enforce this, we may try to declare that the attributes (course id, sec id, semester,
year)ofthesection relation form a foreign key referencing the corresponding
attributes of the teaches relation. Unfortunately, these attributes do not form a
candidate key of the relation teaches. A check constraint similar to that for the
time slot attribute can be usedto enforce this constraint, if check constraints with
subqueriesweresupportedby adatabasesystem.
Complex check conditions can be useful when we want to ensure integrity
of data, but may be costly to test. For example, the predicate in the check clause
4.4 Integrity Constraints 135
createassertion credits earned constraintcheck
(notexists (select ID
from student
where tot cred<>(select sum(credits)
from takes naturaljoin course
where student.ID= takes.ID
and grade isnotnulland grade<> ’F’ )
Figure 4.9 An assertion example.
would not only have to be evaluated when a modi?cation is made to the section
relation, but may have to be checked if a modi?cation is made to the time slot
relationbecause that relationisreferencedinthesubquery.
Anassertion is a predicateexpressinga conditionthat we wishthe database
always to satisfy. Domain constraints and referential-integrity constraints are
special forms of assertions. We have paid substantial attention to these forms of
assertions because they are easily tested and apply to a wide range of database
applications. However, there are many constraints that we cannot express by
using only thesespecialforms. Two examplesof suchconstraints are:
• For each tuple in the student relation, the value of the attribute tot cred must
equal the sum of credits of courses that the student has completed success-
fully.
• An instructor cannot teach in two different classrooms in a semester in the
sametimeslot.
1
Anassertionin SQL takesthe form:
create assertion<assertion-name>check<predicate>;
In Figure 4.9, we show how the ?rst example of constraints can be written
in SQL.SinceSQL does not provide a “for all X, P(X)” construct (where P is a
predicate),weareforcedtoimplementtheconstraintbyanequivalentconstruct,
“not exists Xsuch that not P(X)”,that can beexpressedin SQL.
Weleavethe speci?cationof thesecond constraint as anexercise.
When an assertion is created, the system tests it for validity. If the assertion
is valid, then any future modi?cation to the database is allowed only if it does
not cause that assertion to be violated. This testing may introduce a signi?cant
amount of overhead if complex assertions have been made. Hence, assertions
should be used with great care. The high overhead of testing and maintaining
assertionshasledsomesystemdeveloperstoomitsupportforgeneralassertions,
or toprovidespecializedformsofassertionthat areeasiertotest.
1
We assume that lectures are not displayed remotely in a second classroom! An alternative constraint that speci?es
that “an instructor cannot teach two courses in a given semester in the same time slot” may not hold since courses are
sometimescross-listed; that is, the same course isgiventwo identi?ers and titles.
136 Chapter 4 Intermediate SQL
Currently, none of the widely used database systems supports either sub-
queriesinthecheckclausepredicate,orthecreateassertionconstruct.However,
equivalentfunctionalitycanbeimplementedusingtriggers,whicharedescribed
in Section 5.3, if they are supported by the database system. Section 5.3 also de-
scribeshowthereferentialintegrityconstraintontime slot idcanbeimplemented
using triggers.
4.5 SQL Data Types and Schemas
InChapter3,we coveredanumberofbuilt-indatatypessupportedin SQL,such
asintegertypes,realtypes,andcharactertypes.Thereareadditionalbuilt-indata
typessupportedbySQL,whichwedescribebelow.Wealsodescribehowtocreate
basicuser-de?nedtypesin SQL.
4.5.1 Date and Time Types in SQL
InadditiontothebasicdatatypesweintroducedinSection3.2,the SQLstandard
supportsseveraldatatypesrelatingtodatesand times:
• date: A calendar date containing a (four-digit) year, month, and day of the
month.
• time: The time of day, in hours, minutes, and seconds. A variant, time(p),
canbeusedtospecifythenumberoffractionaldigitsforseconds(thedefault
being0).Itisalsopossibletostoretime-zoneinformationalongwiththetime
by specifyingtimewithtimezone.
• timestamp:Acombinationofdateandtime.Avariant,timestamp(p),canbe
used to specify the number of fractional digits for seconds (the default here
being 6).Time-zoneinformation isalsostoredifwith timezoneis speci?ed.
Dateand timevaluescan bespeci?edlikethis:
date ’2001-04-25’
time ’09:30:00’
timestamp ’2001-04-25 10:29:01.45’
Dates must be speci?ed in the format year followed by month followed by day,
as shown. The seconds ?eld of time or timestamp can have a fractional part, as
inthetimestampabove.
We can use an expression of the form cast e as t to convert a character string
(or string valued expression) e to the type t,wheret is one of date, time,or
timestamp. The string must be in the appropriate format as illustrated at the
beginning of this paragraph. When required, time-zone information is inferred
from thesystemsettings.
Toextractindividual?eldsofadateortimevalue d,wecanuseextract(?eld
fromd),where?eldcanbeoneofyear,month,day,hour,minute,orsecond.Time-
zone informationcan be extractedusing timezone hourand timezone minute.
4.5 SQLData Types and Schemas 137
SQL de?nes several functions to get the current date and time. For example,
current datereturnsthecurrentdate,current timereturnsthecurrenttime(with
time zone), and localtime returns the current local time (without time zone).
Timestamps(dateplustime)arereturnedbycurrent timestamp(withtimezone)
and localtimestamp (local dateand timewithout timezone).
SQL allows comparison operations on all the types listed here, and it allows
both arithmetic and comparison operations on the various numeric types. SQL
also provides a data type called interval, and it allows computations based on
dates and times and on intervals. For example, if x and y are of type date,then
x ? y is an interval whose value is the number of days from date x to date y.
Similarly,addingorsubtracting anintervaltoadateortimegivesback adateor
time,respectively.
4.5.2 Default Values
SQL allows a default value to be speci?ed for an attribute as illustrated by the
following createtablestatement:
create table student
(ID varchar(5),
name varchar(20)notnull,
dept name varchar(20),
tot cred numeric(3,0)default0,
primarykey (ID));
The default value of the tot cred attribute is declared to be 0. As a result, when a
tuple is inserted into the student relation, if no value is provided for the tot cred
attribute, its value is set to 0. The following insert statement illustrates how an
insertioncan omitthe valuefor the tot credattribute.
insertinto student(ID, name, dept name)
values (’12789’, ’Newman’, ’Comp. Sci.’);
4.5.3 Index Creation
Manyqueriesreferenceonlyasmallproportionoftherecordsina?le.Forexam-
ple, a querylike “Find all instructors in the Physics department”or “Find the tot
credvalueofthestudentwith ID22201” references only a fraction of the student
records. It is inef?cient for the system to read every record and to check ID ?eld
for the ID “32556,” orthe building ?eldfor the value “Physics”.
An index on an attribute of a relation is a data structure that allows the
databasesystemto?ndthosetuplesintherelationthathaveaspeci?edvaluefor
that attribute ef?ciently, without scanning through all the tuples of the relation.
Forexample,ifwecreateinindexonattributeIDofrelation student,thedatabase
system can ?nd the record with any speci?ed ID value, such as 22201, or 44553,
directly, without reading all the tuples of the studentrelation.Anindexcanalso
138 Chapter 4 Intermediate SQL
becreatedonalistofattributes,forexampleonattributes name,anddept nameof
student.
Westudylater,inChapter11,howindicesareactuallyimplemented,includ-
ing aparticularlywidelyusedkindofindexcalledaB
+
-treeindex.
Although the SQL language does not formally de?ne any syntax for creating
indices,manydatabasessupportindexcreationusingthesyntaxillustratedbelow.
createindex studentID indexon student(ID);
The above statement creates an index named studentID index on the attribute ID
of therelation student.
When a user submits an SQL query that can bene?t from using an index, the
SQL query processor automatically uses the index. For example, given an SQL
querythatselectsthe studenttuplewith ID22201,the SQLqueryprocessorwould
use the index studentID index de?ned above to ?nd the required tuple without
readingthewhole relation.
4.5.4 Large-Object Types
Many current-generation database applications need to store attributes that can
be large (of the order of many kilobytes), such as a photograph, or very large
(of the order of many megabytes or even gigabytes), such as a high-resolution
medical image or video clip. SQL therefore provides large-object data types for
characterdata(clob)andbinarydata(blob).Theletters “lob”inthesedatatypes
stand for “LargeOBject.”For example,we maydeclareattributes
book review clob(10KB)
image blob(10MB)
movie blob(2GB)
Forresulttuplescontaininglargeobjects(multiplemegabytestogigabytes),it
isinef?cientorimpracticaltoretrieveanentirelargeobjectintomemory.Instead,
an application would usually use an SQL query to retrievea “locator” for a large
objectandthenusethelocatortomanipulatetheobjectfromthehostlanguagein
whichtheapplicationitselfiswritten.Forinstance,theJDBCapplicationprogram
interface(describedinSection5.1.1)permitsalocatortobefetchedinsteadofthe
entire large object; the locator can then be used to fetch the large object in small
pieces, rather than all at once, much like reading data from an operating system
?leusing a read function call.
4.5.5 User-De?ned Types
SQL supports two forms of user-de?ned data types. The ?rst form, which we
coverhere,iscalleddistincttypes. The other form, calledstructureddatatypes,
allows the creation of complex data types with nested record structures, arrays,
4.5 SQLData Types and Schemas 139
andmultisets.Wedonotcoverstructureddatatypesinthischapter,butdescribe
themlater,inChapter 22.
It is possible for several attributes to have the same data type. For example,
the name attributes for student name and instructor name might have the same
domain:thesetofallpersonnames.However,thedomainsofbudgetanddept name
certainlyoughttobedistinct.Itisperhapslessclearwhether nameand dept name
shouldhavethesamedomain.Attheimplementationlevel,bothinstructornames
and department names are character strings. However, we would normally not
considerthequery“Findallinstructorswhohavethesamenameasadepartment”
tobeameaningfulquery.Thus,ifweviewthedatabaseattheconceptual,rather
than thephysical,level, name and dept nameshould havedistinctdomains.
More importantly, at a practical level, assigning an instructor’s name to a
departmentnameisprobablyaprogrammingerror;similarly,comparingamon-
etary value expressed in dollars directly with a monetary value expressed in
pounds is also almost surely a programming error. A good type system should
be able to detect such assignments or comparisons. To support such checks, SQL
providesthenotionofdistincttypes.
The create type clause can be used to de?ne new types. For example, the
statements:
create type Dollarsas numeric(12,2)?nal;
create type Poundsasnumeric(12,2)?nal;
de?ne the user-de?ned types Dollars and Pounds to be decimal numbers with a
total of 12 digits, two of which are placed after the decimal point. (The keyword
?nalisn’treallymeaningfulinthiscontextbutisrequiredbytheSQL:1999standard
forreasonswewon’tgetintohere;someimplementationsallowthe?nalkeyword
to be omitted.) The newly created types can then be used, for example, as types
of attributesof relations.For example,we can declarethe department tableas:
createtable department
(dept name varchar(20),
building varchar(15),
budget Dollars);
An attempt to assign a value of type Dollars to a variable of type Pounds results
in a compile-time error, although both are of the same numeric type. Such an
assignment is likely to be due to a programmer error, where the programmer
forgot about the differences in currency. Declaring different types for different
currencieshelpscatch such errors.
As a result of strong type checking, the expression (department.budget+20)
wouldnotbeacceptedsincetheattributeandtheintegerconstant20havediffer-
ent types. Values of one type can be cast (that is, converted) to another domain,
as illustratedbelow:
cast (department.budgetto numeric(12,2))
140 Chapter 4 Intermediate SQL
We could do addition on the numeric type, but to save the result back to an
attributeoftype Dollarswewouldhavetouseanothercastexpressiontoconvert
the typeback to Dollars.
SQL provides drop type and alter type clauses to drop or modify types that
havebeencreatedearlier.
Even before user-de?ned types were added to SQL (in SQL:1999), SQL had a
similar but subtly different notion of domain (introduced in SQL-92), which can
add integrity constraints to an underlying type. For example, we could de?ne a
domain DDollars as follows.
createdomain DDollars asnumeric(12,2)notnull;
The domain DDollars can be used as an attribute type, just as we used the type
Dollars. However, there are two signi?cant differences between types and do-
mains:
1. Domainscanhaveconstraints,suchasnotnull,speci?edonthem,andcan
havedefaultvaluesde?nedforvariablesofthedomaintype,whereasuser-
de?ned types cannot have constraints or default values speci?ed on them.
User-de?nedtypes are designedto be usednot just for specifying attribute
types,butalsoinproceduralextensionsto SQLwhereitmaynotbepossible
to enforceconstraints.
2. Domains are not strongly typed. As a result, values of one domain type
canbeassignedtovaluesofanotherdomaintypeaslongastheunderlying
typesarecompatible.
When applied to a domain, the check clause permits the schema designer to
specify a predicate that must be satis?ed by any attribute declared to be from
this domain. For instance, a check clause can ensure that an instructor’s salary
domainallows onlyvaluesgreaterthan aspeci?edvalue:
create domain YearlySalary numeric(8,2)
constraint salary value testcheck(value>=29000.00);
The domain YearlySalary has a constraint that ensures that the YearlySalary is
greater than or equal to $29,000.00. The clause constraint salary value test is op-
tional,andisusedtogivethenamesalary value testtotheconstraint.Thenameis
usedby thesystemtoindicatethe constraint that anupdateviolated.
As another example, a domain can be restricted to contain only a speci?ed
setof valuesby using the inclause:
createdomain degree levelvarchar(10)
constraint degree level test
check(valuein(’Bachelors’, ’Masters’,or ’Doctorate’));
4.5 SQLData Types and Schemas 141
SUPPORT FOR TYPES AND DOMAINS IN DATABASE IMPLEMENTATIONS
Althoughthecreatetypeandcreatedomainconstructsdescribedinthissection
are part of the SQL standard, the forms of these constructs described here are
not fully supported by most database implementations. PostgreSQL supports
thecreatedomainconstruct,butitscreatetype constructhasadifferentsyntax
andinterpretation.
IBM DB2 supports a version of the create type that uses the syntax create
distinct type, but does not support create domain.MicrosoftSQL Server im-
plements a version of create type construct that supports domain constraints,
similar to the SQL createdomain construct.
Oracle does not support either construct as described here. However, SQL
also de?nes a morecomplexobject-orientedtype system, which we study later
in Chapter 22. Oracle, IBM DB2, PostgreSQL,andSQL Server all support object-
orientedtypesystemsusing differentformsofthecreatetype construct.
4.5.6 Create Table Extensions
Applications often require creation of tables that have the same schema as an
existingtable. SQL providesacreatetablelikeextensiontosupportthistask:
create table temp instructor like instructor;
Theabovestatementcreatesanewtabletemp instructorthathasthesameschema
as instructor.
When writing a complex query, it is often useful to store the result of a
queryasanewtable;thetableisusuallytemporary.Twostatementsarerequired,
one to create the table (with appropriate columns) and the second to insert the
queryresultintothetable. SQL:2003providesasimplertechniquetocreateatable
containing the results of a query. For example the following statement creates a
table t1 containing the resultsof aquery.
create table t1as
(select *
from instructor
where dept name= ’Music’)
with data;
By default,the names and datatypesof the columns areinferredfrom the query
result.Namescanbeexplicitlygiventothecolumnsbylistingthecolumnnames
aftertherelationname.
As de?ned by the SQL:2003 standard, if the with data clause is omitted, the
table is created but not populated with data. However many implementations
populate the table with data by default even if the with data clause is omitted.
142 Chapter 4 Intermediate SQL
Note that several implementations support the functionality of create table...
like and create table ... as using different syntax; see the respective system
manuals for furtherdetails.
The above create table ...as statement closely resembles the create view
statement and both are de?ned by using queries. The main difference is that the
contents of the table are set when the table is created, whereas the contents of a
viewalways re?ectthe currentqueryresult.
4.5.7 Schemas, Catalogs, and Environments
To understand the motivation for schemas and catalogs, consider how ?les are
named in a ?le system. Early ?le systems were ?at; that is, all ?les were stored
in a single directory. Current ?le systems, of course, have a directory (or, syn-
onymously, folder) structure, with ?les stored within subdirectories. To name
a ?le uniquely, we must specify the full path name of the ?le, for example,
/users/avi/db-book/chapter3.tex.
Like early ?le systems, early database systems also had a single name space
for all relations. Users had to coordinate to make sure they did not try to use
thesamenamefordifferentrelations.Contemporarydatabasesystemsprovidea
three-levelhierarchyfornamingrelations.Thetoplevelofthehierarchyconsists
ofcatalogs,eachofwhichcancontainschemas. SQLobjectssuchasrelationsand
views are contained within a schema. (Some database implementations use the
term “database”inplaceof thetermcatalog.)
In order to perform any actions on a database, a user (or a program) must
?rst connect to the database. The user must provide the user name and usually,
a password for verifying the identity of the user. Each user has a default catalog
and schema, and the combination is unique to the user. When a user connects to
a database system, the default catalog and schema are set up for the connection;
this corresponds to the current directory being set to the user’s home directory
when theuserlogsintoanoperating system.
Toidentifyarelationuniquely,athree-partname maybe used,for example,
catalog5.univ schema.course
We may omit the catalog component, in which case the catalog part of the name
is considered to be the default catalog for the connection. Thus if catalog5 is
the default catalog, we can use univ schema.course to identify the same relation
uniquely.
If a user wishes to access a relation that exists in a different schema than the
defaultschemaforthatuser,thenameoftheschemamustbespeci?ed.However,
if a relation is in the default schema for a particular user, then even the schema
namemaybeomitted.Thuswecanusejustcourseifthedefaultcatalogiscatalog5
and thedefaultschema is univ schema.
With multiple catalogs and schemas available, different applications and
different users can work independently without worrying about name clashes.
Moreover, multiple versions of an application—one a production version, other
testversions—can runon thesamedatabase system.
4.6 Authorization 143
The default catalog and schema are part of an SQL environment that is set
up for each connection. The environment additionally contains the user identi-
?er (also referred to as the authorization identi?er). All the usual SQL statements,
including the DDLand DMLstatements,operateinthe context ofaschema.
Wecancreateanddropschemasbymeansofcreateschemaanddropschema
statements. In most database systems, schemas are also created automatically
when user accounts are created, with the schema name set to the user account
name. The schema is created in either a default catalog, or a catalog speci?ed in
creatingtheuseraccount.Thenewlycreatedschemabecomesthedefaultschema
for theuseraccount.
Creationanddroppingofcatalogsisimplementationdependentandnotpart
of the SQL standard.
4.6 Authorization
We may assign a user several forms of authorizations on parts of the database.
Authorizations ondatainclude:
• Authorizationtoreaddata.
• Authorizationtoinsertnewdata.
• Authorizationtoupdatedata.
• Authorizationtodeletedata.
Each of these typesof authorizations is called a privilege. We may authorize the
user all, none, or a combination of these types of privileges on speci?ed parts of
adatabase,suchas arelationoraview.
When a user submits a query or an update, the SQL implementation ?rst
checks if the query or update is authorized, based on the authorizations that the
userhas beengranted.Ifthequeryorupdateisnot authorized,itisrejected.
In addition to authorizations on data, users may also be granted authoriza-
tions on the database schema, allowing them, for example, to create, modify, or
drop relations. A user who has some form of authorization may be allowed to
pass on (grant) this authorization to other users, or to withdraw (revoke) an au-
thorization that was granted earlier. In this section, we see how each of these
authorizations canbe speci?edin SQL.
Theultimateformofauthorityisthatgiventothedatabaseadministrator.The
databaseadministratormayauthorizenewusers,restructurethedatabase,andso
on.Thisformofauthorizationisanalogoustothatofasuperuser,administrator,
or operatorforan operatingsystem.
4.6.1 Granting and Revoking of Privileges
The SQL standard includes the privileges select, insert, update,anddelete.The
privilege all privileges can be used as a short form for all the allowable privi-
144 Chapter 4 Intermediate SQL
leges. A user who creates a new relation is given all privileges on that relation
automatically.
The SQL data-de?nition language includes commands to grant and revoke
privileges.Thegrantstatementisusedtoconferauthorization.Thebasicformof
this statementis:
grant<privilegelist>
on<relationname orviewname>
to<user/rolelist>;
The privilege list allows the granting of several privileges in one command. The
notion ofrolesiscoveredlater,inSection4.6.2.
Theselectauthorizationonarelationisrequiredtoreadtuplesintherelation.
The following grant statement grants database users Amit and Satoshi select
authorization onthe department relation:
grantselect on departmentto Amit, Satoshi;
This allowsthose userstorunqueriesonthe department relation.
The update authorization on a relation allows a user to update any tuple
in the relation. The update authorization may be given either on all attributes
of the relation or on only some. If update authorization is included in a grant
statement, the list of attributes on which update authorization is to be granted
optionally appears in parentheses immediately after the update keyword. If the
list of attributes is omitted, the update privilege will be granted on all attributes
of therelation.
This grant statement gives users Amit and Satoshi update authorization on
the budgetattributeof the department relation:
grantupdate(budget)on departmentto Amit, Satoshi;
The insert authorization on a relation allows a user to insert tuples into the
relation. The insert privilege may also specify a list of attributes; any inserts to
the relation must specify only these attributes, and the system either gives each
oftheremainingattributesdefaultvalues(ifadefaultisde?nedfortheattribute)
or setsthemto null.
The delete authorization on a relation allows a user to delete tuples from a
relation.
The user name public refers to all current and future users of the system.
Thus,privilegesgrantedtopublicareimplicitlygrantedtoallcurrentandfuture
users.
By default, a user/role that is granted a privilege is not authorized to grant
thatprivilegetoanotheruser/role.SQLallowsaprivilegegranttospecifythatthe
recipientmayfurthergranttheprivilegetoanotheruser.Wedescribethisfeature
inmoredetailinSection4.6.5.
4.6 Authorization 145
Itisworthnotingthatthe SQLauthorizationmechanismgrantsprivilegeson
an entire relation, or on speci?ed attributes of a relation. However, it does not
permitauthorizations onspeci?ctuplesof arelation.
To revoke an authorization, we use the revoke statement. It takes a form
almostidenticaltothat ofgrant:
revoke<privilegelist>
on<relationnameor viewname>
from<user/rolelist>;
Thus, torevokethe privilegesthat we grantedpreviously,wewrite
revokeselect on departmentfrom Amit, Satoshi;
revokeupdate(budget)on departmentfrom Amit, Satoshi;
Revocationofprivilegesismorecomplexiftheuserfromwhomtheprivilege
is revoked has granted the privilege to another user. We return to this issue in
Section4.6.5.
4.6.2 Roles
Consider the real-world roles of various people in a university. Each instructor
musthavethesametypesofauthorizationsonthesamesetofrelations.Whenever
a new instructor is appointed, she will have to be given all these authorizations
individually.
Abetterapproachwouldbetospecifytheauthorizationsthateveryinstructor
istobegiven,andtoidentifyseparatelywhichdatabaseusersareinstructors.The
systemcanusethesetwopiecesofinformationtodeterminetheauthorizationsof
eachinstructor.Whenanewinstructorishired,auseridenti?ermustbeallocated
to him, and he must be identi?ed as an instructor. Individual permissions given
to instructorsneednot bespeci?edagain.
The notion of roles captures this concept. A set of roles is created in the
database. Authorizations can be granted to roles, in exactly the same fashion as
they are granted to individual users. Each database user is granted a set of roles
(which may beempty)that sheis authorizedtoperform.
Inouruniversitydatabase,examplesofrolescouldincludeinstructor,teaching
assistant, student, dean,anddepartment chair.
A less preferable alternative would be to create an instructor userid, and
permiteach instructor to connect to the database using the instructor userid.The
problem with this approach is that it would not be possible to identify exactly
whichinstructorcarriedoutadatabaseupdate,leadingtosecurityrisks.Theuse
of roles has the bene?t of requiring users to connect to the database with their
own userid.
Anyauthorizationthatcanbegrantedtoausercanbegrantedtoarole.Roles
aregrantedtousersjust asauthorizations are.
146 Chapter 4 Intermediate SQL
Rolescan be createdin SQL as follows:
createrole instructor;
Roles can then be granted privileges just as the users can, as illustrated in this
statement:
grantselect on takes
to instructor;
Rolescanbegrantedtousers,aswellastootherroles,asthesestatementsshow:
grant dean toAmit;
createrole dean;
grant instructor to dean;
grant dean toSatoshi;
Thus theprivilegesof auseror aroleconsist of:
• Allprivilegesdirectlygrantedtotheuser/role.
• Allprivilegesgrantedtorolesthat havebeengrantedtotheuser/role.
Notethattherecanbeachainofroles;forexample,therole teaching assistant
maybegrantedtoall instructors.Inturntheroleinstructorisgrantedtoall deans.
Thus, the dean role inherits all privileges granted to the roles instructor and to
teaching assistant inadditiontoprivilegesgranteddirectlyto dean.
When a user logs in to the database system, the actions executedby the user
during that session have all the privileges granted directly to the user, as well
as all privileges granted to roles that are granted (directly or indirectly via other
roles) tothat user.Thus, if a userAmit has beengranted the role dean,userAmit
holdsallprivilegesgranteddirectlytoAmit,aswellasprivilegesgrantedtodean,
plusprivilegesgrantedtoinstructor,and teaching assistantif,asabove,thoseroles
were granted (directly or indirectly) to the role dean.
It is worth noting that the concept of role-based authorization is not speci?c
to SQL, and role-based authorization is used for access control in a wide variety
of sharedapplications.
4.6.3 Authorization on Views
In our university example, consider a staff member who needs to know the
salaries of all faculty in a particular department, say the Geology department.
Thisstaffmemberisnotauthorizedtoseeinformationregardingfacultyinother
departments.Thus,thestaffmembermustbedenieddirectaccesstotheinstructor
relation.But,ifheistohaveaccesstotheinformationfortheGeologydepartment,
he might be granted access to a view that we shall call geo instructor, consisting
4.6 Authorization 147
of only those instructor tuples pertaining to the Geology department. This view
canbede?nedinSQL as follows:
create view geo instructor as
(select *
from instructor
where dept name =’Geology’);
Supposethat the staffmemberissuesthefollowing SQL query:
select *
from geo instructor;
Clearly, the staff member is authorized to see the result of this query. However,
when the query processor translates it into a query on the actual relations in
the database, it produces a query on instructor. Thus, the system must check
authorization ontheclerk’s querybeforeitbegins queryprocessing.
A user who creates a view does not necessarily receive all privileges on that
view.Shereceivesonlythoseprivilegesthatprovidenoadditionalauthorization
beyondthosethatshealreadyhad.Forexample,auserwhocreatesaviewcannot
be given update authorization on a view without having update authorization
on the relations used to de?ne the view. If a user creates a view on which no
authorization can be granted, the system will deny the view creation request.
In our geo instructor view example, the creator of the view must have select
authorization onthe instructor relation.
As we will see later, in Section 5.2, SQL supports the creation of functions
and procedures, which may in turn contain queries and updates. The execute
privilege can be granted on a function or procedure, enabling a user to execute
the function/procedure. By default, just like views, functions and procedures
havealltheprivilegesthatthecreatorofthefunctionorprocedurehad.Ineffect,
the function or procedure runs as if it were invoked by the user who created the
function.
Although this behavior is appropriate in many situations, it is not always
appropriate. Starting with SQL:2003, if the function de?nition has an extra clause
sql security invoker, then it is executed under the privileges of the user who
invokes the function, rather than the privileges of the de?ner of the function.
This allows the creation of libraries of functions that can run under the same
authorization asthe invoker.
4.6.4 Authorizations on Schema
The SQLstandardspeci?esaprimitiveauthorizationmechanismforthedatabase
schema: Only the owner of the schema can carry out any modi?cation to the
schema, such as creating or deleting relations, adding or dropping attributes of
relations,and addingor droppingindices.
148 Chapter 4 Intermediate SQL
However, SQL includes a references privilege that permits a user to declare
foreign keys when creating relations. The SQL references privilege is granted on
speci?c attributes in a manner like that for the update privilege. The following
grant statement allows user Mariano to create relations that reference the key
branch nameof the branchrelationas aforeignkey:
grantreferences(dept name)on departmenttoMariano;
Initially, it may appear that there is no reason ever to prevent users from
creating foreign keys referencing another relation. However, recall that foreign-
keyconstraintsrestrictdeletionandupdateoperationsonthereferencedrelation.
Suppose Mariano creates a foreign key in a relation r referencing the dept name
attributeofthedepartmentrelationandtheninsertsatupleintor pertainingtothe
Geology department. It is no longer possible to delete the Geology department
fromthedepartmentrelationwithoutalsomodifyingrelationr.Thus,thede?nition
ofaforeignkeybyMarianorestrictsfutureactivitybyotherusers;therefore,there
isaneedforthe referencesprivilege.
Continuingtousetheexampleofthe departmentrelation, the referencespriv-
ilege on department is also required to create a check constraint on a relation r if
the constraint has a subquery referencing department. This is reasonable for the
same reason as the one we gave for foreign-key constraints; a check constraint
that referencesarelationlimitspotentialupdatestothat relation.
4.6.5 Transfer of Privileges
A user who has been granted some form of authorization may be allowed to
passonthisauthorizationtootherusers.Bydefault,auser/rolethatisgranteda
privilegeisnotauthorizedtograntthatprivilegetoanotheruser/role.Ifwewish
to grant a privilege and to allow the recipient to pass the privilege on to other
users,weappendthewithgrantoptionclausetotheappropriategrantcommand.
Forexample,ifwewishtoallowAmittheselectprivilegeondepartmentandallow
Amittograntthis privilegetoothers,we write:
grantselect on departmentto Amitwithgrantoption;
The creator of an object (relation/view/role) holds all privileges on the object,
including theprivilegetogrant privilegestoothers.
Consider, as an example, the granting of update authorization on the teaches
relation of the university database. Assume that, initially, the database adminis-
tratorgrantsupdateauthorizationon teachestousersU
1
, U
2
,andU
3
,whomayin
turnpassonthisauthorizationtootherusers.Thepassingofaspeci?cauthoriza-
tionfromoneusertoanothercanberepresentedbyanauthorizationgraph.The
nodesof this grapharetheusers.
Consider the graph for update authorization on teaches. The graph includes
anedgeU
i
?U
j
ifuserU
i
grantsupdateauthorizationon teachestoU
j
.Theroot
of the graph is the database administrator. In the sample graph in Figure 4.10,
4.6 Authorization 149
U
3
DBA
U
1
U
5
U
2
U
4
Figure 4.10 Authorization-grant graph (U
1
,U
2
,...,U
5
are users and DBA refers to the
database administrator).
observe that user U
5
is granted authorization by both U
1
and U
2
; U
4
is granted
authorization by only U
1
.
A user has an authorizationifandonlyifthere is a path from the root of the
authorizationgraph(thenoderepresentingthedatabaseadministrator)downto
thenode representingthe user.
4.6.6 Revoking of Privileges
Suppose that the database administrator decides to revoke the authorization of
userU
1
.SinceU
4
hasauthorizationfromU
1
,thatauthorizationshouldberevoked
as well. However, U
5
was granted authorization by both U
1
and U
2
.Sincethe
database administrator did not revoke update authorization on teaches from U
2
,
U
5
retainsupdateauthorizationonteaches.IfU
2
eventuallyrevokesauthorization
from U
5
,thenU
5
losesthe authorization.
A pair of devious users might attempt to defeat the rules for revocation
of authorization by granting authorization to each other. For example, if U
2
is
initially granted an authorization by the database administrator, and U
2
further
grants it to U
3
. Suppose U
3
now grants the privilege back to U
2
.Ifthedatabase
administrator revokes authorization from U
2
, it might appear that U
2
retains
authorization through U
3
. However, note that once the administrator revokes
authorization from U
2
, there is no path in the authorization graph from the root
to either U
2
or to U
3
.Thus,SQL ensures that the authorization is revoked from
both theusers.
As we just saw, revocation of a privilege from a user/role may cause other
users/rolesalsotolosethatprivilege.Thisbehavioriscalled cascading revocation.
Inmostdatabasesystems,cascadingisthedefaultbehavior.However,therevoke
statementmayspecifyrestrict inordertopreventcascading revocation:
revokeselect on departmentfrom Amit,Satoshirestrict;
150 Chapter 4 Intermediate SQL
In this case, the system returns an error if there are any cascading revocations,
and doesnot carry outthe revokeaction.
Thekeywordcascadecanbeusedinsteadofrestricttoindicatethatrevocation
should cascade; however, it can be omitted, as we have done in the preceding
examples,since itisthedefaultbehavior.
The following revoke statement revokes only the grant option, rather than
the actual select privilege:
revokegrantoptionforselect on departmentfromAmit;
Note that some database implementations do not support the above syntax; in-
stead, the privilege itself can be revoked, and then granted again without the
grant option.
Cascading revocation is inappropriate in many situations. Suppose Satoshi
has the role of dean,grantsinstructor to Amit, and later the role dean is revoked
from Satoshi (perhaps because Satoshi leaves the university); Amit continues to
be employedon thefaculty,and shouldretainthe instructor role.
To deal with the above situation, SQL permits a privilege to be granted by a
role rather than by a user. SQL has a notion of the current role associated with
a session. By default, the current role associated with a session is null (except
in some special cases). The current role associated with a session can be set by
executing set role role name. The speci?ed role must have been granted to the
user,elsethe set rolestatementfails.
To grant a privilegewith the grantor set to the current role associated with a
session,we canaddtheclause:
grantedbycurrent role
tothegrant statement,providedthe currentroleisnot null.
Suppose the granting of the role instructor (or other privileges) to Amit is
done using the granted by current role clause, with the current role set to dean),
instead of the grantor being the user Satoshi. Then, revoking of roles/privileges
(includingtheroledean)fromSatoshiwillnotresultinrevokingofprivilegesthat
hadthegrantorsettotheroledean,evenifSatoshiwastheuserwhoexecutedthe
grant; thus, Amit would retain the instructor role even after Satoshi’s privileges
arerevoked.
4.7 Summary
• SQLsupportsseveraltypesofjoinsincludinginnerandouterjoinsandseveral
typesofjoinconditions.
• View relations can be de?ned as relations containing the result of queries.
Viewsareusefulforhidingunneededinformation,andforcollectingtogether
informationfrom morethanone relationinto a singleview.
ReviewTerms 151
• Transactions are a sequence of queries and updates that together carry out
a task. Transactions can be committed, or rolled back; when a transaction
is rolled back, the effects of all updates performed by the transaction are
undone.
• Integrityconstraintsensurethatchangesmadetothedatabasebyauthorized
usersdonot resultina lossof data consistency.
• Referential-integrityconstraints ensurethat avaluethatappearsinone rela-
tion for a given set of attributes also appears for a certain set of attributes in
another relation.
• Domain constraints specify the setof possible valuesthat may be associated
with an attribute. Such constraints may also prohibit the use of null values
for particularattributes.
• Assertions are declarative expressions that state predicates that we require
always tobe true.
• The SQL data-de?nition language provides support for de?ning built-in do-
maintypessuch asdate and time,aswellasuser-de?neddomaintypes.
• SQL authorizationmechanismsallowonetodifferentiateamong theusersof
the database as far as the type of access they are permitted on various data
valuesinthe database.
• A user who has been granted some form of authority may be allowed to
pass on this authority to other users. However, we must be careful about
how authorization can be passed among users if we are to ensure that such
authorizationcan be revokedat somefuturetime.
• Roleshelptoassignasetofprivilegestoauseraccordingtotherolethatthe
userplaysintheorganization.
Review Terms
• Jointypes
?
Inner and outerjoin
?
Left,right and fullouterjoin
?
Natural,using, andon
• Viewde?nition
• Materializedviews
• View update
• Transactions
?
Commitwork
?
Rollback work
?
Atomictransaction
• Integrityconstraints
• Domainconstraints
• Uniqueconstraint
• Check clause
• Referentialintegrity
?
Cascadingdeletes
?
Cascadingupdates
152 Chapter 4 Intermediate SQL
• Assertions
• Dateand timetypes
• Defaultvalues
• Indices
• Largeobjects
• User-de?nedtypes
• Domains
• Catalogs
• Schemas
• Authorization
• Privileges
?
select
?
insert
?
update
?
allprivileges
?
Granting of privileges
?
Revokingofprivileges
?
Privilegetogrant privileges
?
Grant option
• Roles
• Authorizationon views
• Executeauthorization
• Invokerprivileges
• Row-levelauthorization
Practice Exercises
4.1 Writethefollowing queriesin SQL:
a. Displayalistofallinstructors,showingtheir ID,name,andthenum-
berofsectionsthattheyhavetaught.Makesuretoshowthenumber
ofsectionsas0forinstructorswhohavenottaughtanysection.Your
queryshoulduseanouterjoin,andshouldnotusescalarsubqueries.
b. Write the same query as above, but using a scalar subquery, without
outerjoin.
c. Display the list of all course sections offered in Spring 2010, along
withthenamesoftheinstructorsteachingthesection.Ifasectionhas
morethanoneinstructor,itshouldappearasmanytimesintheresult
as it has instructors. If it does not have any instructor, it should still
appearinthe resultwiththe instructorname setto “—”.
d. Displaythelistofalldepartments,withthetotalnumberofinstructors
in each department, without using scalar subqueries. Make sure to
correctlyhandledepartmentswithno instructors.
4.2 Outer join expressions can be computed in SQL without using the SQL
outerjoinoperation.Toillustratethisfact,showhowtorewriteeachofthe
following SQL querieswithout using the outerjoinexpression.
a. select* from studentnaturalleftouterjoin takes
b. select* from studentnaturalfullouterjoin takes
PracticeExercises 153
4.3 Suppose we have three relations r(A, B), s(B, C), and t(B, D), with all
attributesdeclaredasnotnull.Considertheexpressions
• r naturalleftouterjoin (s naturalleftouterjoin t),and
• (r naturalleftouterjoin s)naturalleftouterjoin t
a. Give instances of relations r, s and t such that in the result of the
second expression, attribute C has a null value but attribute D has a
non-null value.
b. Istheabovepattern,with C nulland Dnotnullpossibleintheresult
ofthe ?rst expression?Explainwhy orwhy not.
4.4 Testing SQL queries: To test if a query speci?ed in English has been cor-
rectly written in SQL,theSQL query is typically executed on multiple test
databases,andahumanchecksiftheSQLqueryresultoneachtestdatabase
matches theintentionof thespeci?cationinEnglish.
a. InSection3.3.3wesawanexampleofanerroneous SQLquerywhich
wasintendedto?ndwhichcourseshadbeentaughtbyeachinstruc-
tor; the query computed the natural join of instructor, teaches,and
course,andasaresultunintentionallyequatedthe dept nameattribute
of instructorand course.Giveanexampleofadatasetthatwouldhelp
catchthis particularerror.
b. Whencreatingtestdatabases,itisimportanttocreatetuplesinrefer-
encedrelationsthatdonothaveanymatchingtupleinthereferencing
relation, for each foreign key. Explain why, using an example query
ontheuniversitydatabase.
c. Whencreatingtestdatabases,itisimportanttocreatetupleswithnull
values for foreign key attributes, provided the attribute is nullable
(SQL allows foreign key attributes to take on null values, as long as
they are not part of the primary key, and have not been declared as
not null). Explain why, using an example query on the university
database.
Hint:usethe queriesfrom Exercise4.1.
4.5 Show how to de?ne the view student grades (ID, GPA) giving the grade-
point average of each student, based on the query in Exercise 3.2; recall
that we used a relation grade points(grade, points) to get the numeric points
associated with a letter grade. Make sure your view de?nition correctly
handlesthe case of nullvaluesfor the grade attributeof the takes relation.
4.6 Complete the SQL DDL de?nition of the university database of Figure 4.8
toincludethe relations student, takes, advisor,andprereq.
154 Chapter 4 Intermediate SQL
employee (employee name, street, city)
works (employee name, company name, salary)
company (company name, city)
manages (employee name, manager name)
Figure 4.11 Employee database for Figure 4.7 and 4.12.
4.7 ConsidertherelationaldatabaseofFigure4.11.Givean SQL DDLde?nition
ofthisdatabase.Identifyreferential-integrityconstraintsthatshouldhold,
and includetheminthe DDLde?nition.
4.8 AsdiscussedinSection4.4.7,weexpecttheconstraint“aninstructorcannot
teach sections in two different classrooms in a semester in the same time
slot”tohold.
a. Write an SQL query that returns all (instructor, section) combinations
that violatethisconstraint.
b. Write an SQL assertion to enforce this constraint (as discussed in
Section 4.4.7, current generation database systems do not support
suchassertions,although theyarepartof the SQL standard).
4.9 SQLallowsaforeign-keydependencytorefertothesamerelation,asinthe
following example:
createtable manager
(employee name varchar(20) notnull
manager name varchar(20) notnull,
primarykey employee name,
foreignkey(manager name)references manager
ondeletecascade )
Here, employee name is a key to the table manager, meaning that each em-
ployeehasatmostonemanager.Theforeign-keyclauserequiresthatevery
manageralsobeanemployee.Explainexactlywhathappenswhenatuple
intherelation manager isdeleted.
4.10 SQL provides an n-ary operation called coalesce, which is de?ned as
follows: coalesce(A
1
, A
2
,...,A
n
) returns the ?rst nonnull A
i
in the list
A
1
, A
2
,...,A
n
,andreturnsnull ifallof A
1
, A
2
,...,A
n
are null.
Let a and b be relations with the schemas A(name, address, title), and
B(name, address, salary), respectively. Show how to express a natural full
outer join b using the full outer-join operation with an on condition and
the coalesce operation. Make sure that the result relationdoes not contain
twocopiesoftheattributesnameandaddress,andthatthesolutioniscorrect
evenifsometuplesina andbhavenullvaluesforattributesnameoraddress.
Exercises 155
salaried worker (name, of?ce, phone, salary)
hourly worker (name, hourly wage)
address(name,street,city )
Figure 4.12 Employee database for Exercise 4.16.
4.11 Some researchers have proposed the concept of marked nulls. A marked
null ?
i
is equal to itself, but if i null j,then?
i
null=?
j
. One application of
markednullsistoallowcertainupdatesthroughviews.Considertheview
instructor info (Section 4.2). Show how you can use marked nulls to allow
the insertion of the tuple (99999, “Johnson”, “Music”)throughinstructor
info.
Exercises
4.12 ForthedatabaseofFigure4.11,writeaqueryto?ndthoseemployeeswith
nomanager.Notethatanemployeemaysimplyhavenomanagerlistedor
may have a null manager. Write your query using an outer join and then
writeitagainusing noouterjoinatall.
4.13 Underwhat circumstances would thequery
select *
from studentnaturalfullouterjoin takes naturalfullouterjoin course
includetupleswithnull valuesforthe title attribute?
4.14 Show how to de?ne a view tot credits (year, num credits), giving the total
number ofcreditstakenby studentsineachyear.
4.15 Show how to express the coalesce operation from Exercise 4.10 using the
case operation.
4.16 Referential-integrity constraints as de?ned in this chapter involve exactly
two relations. Consider a database that includes the relations shown in
Figure4.12.Supposethatwewishtorequirethateverynamethatappearsin
addressappearsineithersalaried workerorhourly worker,butnotnecessarily
inboth.
a. Proposea syntaxfor expressingsuch constraints.
b. Discuss the actions that the system must take to enforce a constraint
ofthis form.
4.17 Explain why, when a manager, say Satoshi, grants an authorization, the
grantshouldbedonebythemanagerrole,ratherthanbytheuserSatoshi.
156 Chapter 4 Intermediate SQL
4.18 Supposeuser A,whohasallauthorizationsonarelationr,grantsselecton
relation r to public with grant option. Suppose user B then grants select
onr to A.Doesthiscauseacycleintheauthorizationgraph?Explainwhy.
4.19 Databasesystemsthatstoreeachrelationinaseparateoperating-system?le
may use the operating system’s authorization scheme, instead of de?ning
aspecialscheme themselves.Discussanadvantageand adisadvantageof
suchan approach.
Bibliographical Notes
Seethe bibliographicnotesof Chapter3 for SQL reference material.
The rules used by SQL to determine the updatability of a view, and how
updates are re?ected on the underlying database relations, are de?ned by the
SQL:1999 standard,andaresummarizedinMeltonand Simon[2001].
CHAPTER
5
AdvancedSQL
InChapters3and4,weprovideddetailedcoverageofthebasicstructureof SQL.
Inthischapter,wecoversomeofthemoreadvancedfeaturesof SQL.
1
Weaddress
the issue of how to access SQL from a general-purpose programming language,
which is very important for building applications that use a database to store
and retrieve data. We describe how procedural code can be executed within the
database, either by extending the SQL language to support procedural actions,
or by allowing functions de?ned in procedural languages to be executed within
the database. We describe triggers, which can be used to specify actions that
are to be carried out automatically on certain events such as insertion, deletion,
or update of tuples in a speci?ed relation. We discuss recursive queries and
advanced aggregation features supported by SQL. Finally, we describe online
analytic processing (OLAP) systems, which support interactive analysis of very
largedatasets.
5.1 AccessingSQLFromaProgrammingLanguage
SQL provides a powerful declarative query language. Writing queries in SQL is
usually much easier than coding the same queries in a general-purpose pro-
gramming language. However, a database programmer must have access to a
general-purposeprogramminglanguage for atleasttwo reasons:
1. Not all queriescan be expressedin SQL,sinceSQL doesnot providethe full
expressivepowerofageneral-purposelanguage.Thatis,thereexistqueries
thatcanbeexpressedinalanguagesuchasC,Java,orCobolthatcannotbe
expressed in SQL. To write such queries, we can embed SQL within a more
powerfullanguage.
1
Noteregardingchapter andsectionsequencing: Database design—Chapters7and 8—canbestudiedindependently
of the material in this chapter. It is quite possible to study database design ?rst, and study this chapter later. However,
forcourseswithaprogrammingemphasis,a richervarietyoflaboratory exercisesispossibleafter studyingSection5.1,
and we recommendthat it be coveredbefore database designfor such courses.
157
158 Chapter 5 Advanced SQL
2. Nondeclarative actions—such as printing a report, interacting with a user,
or sending the results of a query to a graphical user interface—cannot
be done from within SQL. Applications usually have several components,
and querying or updating data is only one component; other components
are written in general-purpose programming languages. For an integrated
application, there must be a means to combine SQL with a general-purpose
programming language.
Therearetwoapproachestoaccessing SQLfromageneral-purposeprogram-
ming language:
• Dynamic SQL:Ageneral-purposeprogramcanconnecttoandcommunicate
with a database server using a collection of functions (for procedural lan-
guages)ormethods(forobject-orientedlanguages).Dynamic SQLallowsthe
program to construct an SQL query as a character string at runtime, submit
the query, and then retrieve the result into program variables a tuple at a
time. The dynamic SQL component of SQL allows programs to construct and
submit SQL queriesat runtime.
Inthischapter,welookattwostandardsforconnectingtoan SQLdatabase
andperformingqueriesandupdates.One, JDBC(Section5.1.1),isanapplica-
tionprograminterfacefortheJavalanguage.Theother, ODBC(Section5.1.2),
isanapplicationprograminterfaceoriginallydevelopedfortheClanguage,
and subsequently extended to other languages such as C++, C#, and Visual
Basic.
• Embedded SQL: Like dynamic SQL, embedded SQL provides a means by
which a program can interact with a database server. However, under em-
beddedSQL,theSQLstatementsareidenti?edatcompiletimeusingaprepro-
cessor. The preprocessor submits the SQL statements to the database system
for precompilation and optimization; then it replaces the SQL statements in
the application program with appropriate code and function calls before in-
vokingtheprogramming-languagecompiler.Section5.1.3coversembedded
SQL.
A major challenge in mixing SQL with a general-purpose language is the
mismatchinthewaystheselanguagesmanipulatedata.In SQL,theprimarytype
ofdataistherelation. SQLstatementsoperateonrelationsandreturnrelationsas
a result. Programming languages normally operate on a variable at a time, and
those variables correspond roughly to the value of an attribute in a tuple in a
relation. Thus, integrating these two types of languages into a single application
requires providing a mechanism to return the result of a query in a manner that
the programcan handle.
5.1.1 JDBC
The JDBC standard de?nes an application program interface (API) that Java
programs can use to connect to database servers. (The word JDBC was originally
5.1 Accessing SQL Froma Programming Language 159
public static void JDBCexample(String userid, String passwd)
{
try
{
Class.forName ("oracle.jdbc.driver.OracleDriver");
Connection conn = DriverManager.getConnection(
"jdbc:oracle:thin:@db.yale.edu:1521:univdb",
userid, passwd);
Statementstmt = conn.createStatement();
try {
stmt.executeUpdate(
"insert into instructor values(’77987’, ’Kim’, ’Physics’, 98000)");
} catch (SQLException sqle)
{
System.out.println("Could not insert tuple. " + sqle);
}
ResultSet rset = stmt.executeQuery(
"select dept name,avg (salary) "+
" from instructor "+
" group by dept name");
while (rset.next()) {
System.out.println(rset.getString("deptname")+""+
rset.getFloat(2));
}
stmt.close();
conn.close();
}
catch (Exception sqle)
{
System.out.println("Exception : " + sqle);
}
}
Figure5.1 An example of JDBC code.
an abbreviation for Java Database Connectivity, but the full form is no longer
used.)
Figure 5.1 shows an example Java program that uses the JDBC interface. It
illustrateshowconnectionsareopened,howstatementsareexecutedandresults
processed, and how connections are closed. We discuss this example in detail
in this section. The Java program must import java.sql.*, which contains the
interfacede?nitions forthe functionality providedby JDBC.
5.1.1.1 ConnectingtotheDatabase
The?rststepinaccessingadatabasefromaJavaprogramistoopenaconnection
tothedatabase.Thisstepisrequiredtoselectwhichdatabasetouse,forexample,
160 Chapter 5 Advanced SQL
aninstanceofOraclerunningonyourmachine,oraPostgreSQLdatabaserunning
onanothermachine.OnlyafteropeningaconnectioncanaJavaprogramexecute
SQL statements.
A connection is opened using the getConnection method of the Driver-
Managerclass (withinjava.sql).This methodtakesthreeparameters.
2
• The ?rst parameter to the getConnection call is a string that speci?es the
URL,ormachinename,wheretheserverruns(inourexample,db.yale.edu),
along with possibly some other information such as the protocol to be used
tocommunicatewiththedatabase(inourexample,jdbc:oracle:thin:;we
shall shortly see why this is required), the port number the database system
usesforcommunication (inour example,2000), and the speci?cdatabase on
theservertobeused(inourexample,univdb).Notethat JDBCspeci?esonly
theAPI,notthecommunicationprotocol.AJDBCdrivermaysupportmultiple
protocols,andwemustspecifyonesupportedbyboththedatabaseandthe
driver.The protocoldetailsarevendorspeci?c.
• ThesecondparametertogetConnectionisadatabaseuseridenti?er,which
isa string.
• The third parameter is a password, which is also a string. (Note that the
needtospecifyapasswordwithinthe JDBCcodepresentsasecurityriskifan
unauthorized personaccessesyour Java code.)
Inourexampleinthe?gure,wehavecreatedaConnectionobjectwhosehandle
isconn.
Each database product that supports JDBC (all the major database vendors
do) provides a JDBC driver that must be dynamically loaded in order to access
the database from Java. In fact, loading the driver must be done ?rst, before
connecting to the database.
This is done by invoking Class.forName with one argument specifying a
concrete class implementing the java.sql.Driver interface, in the ?rst line of
the program in Figure 5.1. This interface provides for the translation of product-
independent JDBC calls into the product-speci?c calls needed by the speci?c
database management system being used. The example in the ?gure shows the
Oracle driver,oracle.jdbc.driver.OracleDriver.
3
The driver is available in
a .jar ?le at vendor Web sites and should be placed within the classpath so
that theJava compilercan access it.
The actual protocol used to exchange information with the database de-
pends on the driver that is used, and is not de?ned by the JDBC standard. Some
2
There are multiple versions of thegetConnection method, which differ in the parameters that they accept. We
presentthe most commonlyused version.
3
The equivalent driver names for other products are as follows: IBM DB2: com.ibm.db2.jdbc.app.DB2Driver; Mi-
crosoft SQL Server: com.microsoft.sqlserver.jdbc.SQLServerDriver; PostgreSQL: org.postgresql.Driver; and MySQL:
com.mysql.jdbc.Driver. Sun also offers a “bridge driver” that convertsJDBC calls toODBC. This should be used only
for vendorsthat supportODBC but notJDBC.
5.1 Accessing SQL Froma Programming Language 161
drivers support more than one protocol, and a suitable protocol must be cho-
sen depending on what protocol the database that you are connecting to sup-
ports. In our example, when opening a connection with the database, the string
jdbc:oracle:thin:speci?esaparticular protocolsupportedby Oracle.
5.1.1.2 ShippingSQLStatementstotheDatabaseSystem
Onceadatabaseconnectionisopen,theprogramcanuseittosendSQLstatements
to the database system for execution. This is done via an instance of the class
Statement.AStatement object is not the SQL statement itself, but rather an
objectthatallowstheJavaprogramtoinvokemethodsthatshipanSQLstatement
givenasanargumentforexecutionbythedatabasesystem.Ourexamplecreates
aStatementhandle (stmt)on the connectionconn.
To execute a statement, we invoke either the executeQuery method or the
executeUpdate method, depending on whether the SQL statement is a query
(and, thus, returns a result set) or nonquery statement such as update, insert,
delete, create table, etc. In our example, stmt.executeUpdate executes an up-
datestatementthatinsertsintothe instructorrelation.Itreturnsanintegergiving
thenumberoftuplesinserted,updated,ordeleted.ForDDLstatements,thereturn
valueiszero.Thetry { ... } catch { ... } constructpermitsustocatchany
exceptions (error conditions) that arise when JDBC calls are made, and print an
appropriatemessagetothe user.
5.1.1.3 RetrievingtheResultofaQuery
Theexampleprogramexecutesaquerybyusingstmt.executeQuery.Itretrieves
the set of tuples inthe resultinto aResultSetobjectrset and fetches them one
tuple at a time. The next method on the result set tests whether or not there
remains at least one unfetched tuple in the result set and if so, fetches it. The
return value of the next method is a Boolean indicating whether it fetched a
tuple. Attributes from the fetched tuple are retrieved using various methods
whose names begin with get.ThemethodgetString can retrieve any of the
basic SQL data types (converting the value to a Java String object), but more
restrictive methods such as getFloatcanbeusedaswell.Theargumenttothe
various get methods can either be an attribute name speci?ed as a string, or an
integerindicatingthepositionofthedesiredattributewithinthetuple.Figure5.1
shows two ways of retrieving the values of attributes in a tuple: using the name
oftheattribute(dept name)andusingthepositionoftheattribute(2,todenotethe
second attribute).
ThestatementandconnectionarebothclosedattheendoftheJavaprogram.
Note that it is important to close the connection because there is a limit imposed
on the number of connections to the database; unclosed connections may cause
that limit to be exceeded. If this happens, the application cannot open any more
connections to the database.
162 Chapter 5 Advanced SQL
PreparedStatementpStmt = conn.prepareStatement(
"insert into instructor values(?,?,?,?)");
pStmt.setString(1, "88877");
pStmt.setString(2, "Perry");
pStmt.setString(3, "Finance");
pStmt.setInt(4, 125000);
pStmt.executeUpdate();
pStmt.setString(1, "88878");
pStmt.executeUpdate();
Figure5.2 Prepared statements in JDBC code.
5.1.1.4 PreparedStatements
We can create a prepared statement in which some values are replaced by “?”,
therebyspecifyingthatactualvalueswillbeprovidedlater.Thedatabasesystem
compiles the query when it is prepared. Each time the query is executed (with
new values to replace the “?”s), the database system can reuse the previously
compiled form of the query and apply the new values. The code fragment in
Figure 5.2 shows how prepared statements can be used.
The prepareStatement method of the Connection class submits an SQL
statement for compilation. It returns an object of class PreparedStatement.At
this point, no SQL statement has been executed. TheexecuteQuery and execu-
teUpdate methods ofPreparedStatement class do that. But before they can be
invoked, we must use methods of class PreparedStatement that assign values
forthe“?”parameters.ThesetStringmethodandothersimilarmethodssuchas
setIntforotherbasicSQLtypesallowustospecifythevaluesfortheparameters.
The?rstargumentspeci?esthe “?”parameterforwhichweareassigningavalue
(the ?rst parameter is 1, unlike most other Java constructs, which start with 0).
The secondargument speci?esthe valueto be assigned.
In the example in the ?gure, we prepare an insert statement, set the “?” pa-
rameters, and then invoke executeUpdate. The ?nal two lines of our example
show that parameter assignments remain unchanged until we speci?cally reas-
sign them. Thus, the ?nal statement, which invokes executeUpdate, inserts the
tuple(“88878”, “Perry”, “Finance”, 125000).
Prepared statements allow for more ef?cient execution in cases where the
same query can be compiled once and then run multiple times with different
parameter values. However, there is an even more signi?cant advantage to pre-
paredstatementsthatmakesthemthepreferredmethodofexecutingSQLqueries
whenever a user-entered value is used, even if the query is to be run only once.
Suppose that we read in a user-entered value and then use Java string manipu-
lationtoconstructthe SQL statement.Iftheuserenterscertainspecialcharacters,
suchasasinglequote,theresulting SQL statementmaybesyntacticallyincorrect
unlesswe takeextraordinarycareincheckingthe input.ThesetStringmethod
doesthisforusautomaticallyandinsertstheneededescapecharacterstoensure
syntactic correctness.
5.1 Accessing SQL Froma Programming Language 163
Inourexample,supposethatthevaluesforthevariables ID, name, dept name,
and salaryhavebeenenteredbyauser,andacorrespondingrowistobeinserted
into the instructor relation. Suppose that, instead of using a prepared statement,
a query is constructed by concatenating the strings using the following Java
expression:
"insert into instructor values(’ " + ID+"’,’"+name+"’,"+
"’+deptname + " ’, " ’ balance + ")"
and the query is executed directly using theexecuteQuery method of aState-
ment object. Now, if the user typed a single quote in the ID or name ?elds, the
querystringwouldhaveasyntaxerror.Itisquitepossiblethataninstructorname
may havea quotationmark initsname (forexample, “O’Henry”).
While the above example might be considered an annoyance, the situation
canbemuchworse.AtechniquecalledSQL injection can be used by malicious
hackers to stealdata ordamagethedatabase.
SupposeaJavaprograminputsastringname and constructs the query:
"select * from instructor where name = ’" + name + "’"
Ifthe user, insteadof enteringa name, enters:
X’ or ’Y’ = ’Y
thentheresultingstatementbecomes:
"select * from instructor where name = ’" + "X’ or ’Y’ = ’Y" + "’"
which is:
select * from instructor where name = ’X’ or ’Y’ = ’Y’
In the resulting query, the where clause is always true and the entire instructor
relation is returned. More clever malicious users could arrange to output even
more data. Use of a prepared statement would prevent this problem because
the input string would have escape characters inserted, so the resulting query
becomes:
"select * from instructor where name = ’X\’or\’Y\’=\’Y’
whichisharmlessandreturnstheemptyrelation.
Oldersystemsallowmultiplestatementsto be executedina singlecall,with
statements separated by a semicolon. This feature is being eliminated because
the SQL injection technique was used by malicious hackers to insert whole SQL
statements. Because these statements run with the privilegesof the owner of the
164 Chapter 5 Advanced SQL
Javaprogram,devastating SQL statementssuchasdroptablecouldbeexecuted.
Developersof SQL applicationsneedto bewary of such potentialsecurityholes.
5.1.1.5 CallableStatements
JDBCalsoprovidesaCallableStatementinterfacethatallowsinvocationof SQL
stored procedures and functions (described later, in Section 5.2). These play the
same role for functions and procedures asprepareStatementdoesforqueries.
CallableStatement cStmt1 = conn.prepareCall("{? = call some function(?)}");
CallableStatement cStmt2 = conn.prepareCall("{call some procedure(?,?)}");
The datatypesof function returnvaluesand out parametersof proceduresmust
beregisteredusingthemethodregisterOutParameter(),andcanberetrieved
using get methods similar to those for result sets. See a JDBC manual for more
details.
5.1.1.6 MetadataFeatures
Aswenotedearlier,aJavaapplicationprogramdoesnotincludedeclarationsfor
datastoredinthedatabase.ThosedeclarationsarepartoftheSQLDDLstatements.
Therefore,aJavaprogramthatusesJDBCmusteitherhaveassumptionsaboutthe
database schema hard-coded into the program or determine that information
directly from the database system at runtime. The latter approach is usually
preferable,since itmakesthe applicationprogrammore robusttochanges inthe
database schema.
Recall that when we submit a query using the executeQuery method, the
result of the query is contained in a ResultSet object. The interface ResultSet
has a method, getMetaData(), that returns a ResultSetMetaData object that
containsmetadataabouttheresultset.ResultSetMetaData,inturn,hasmethods
to ?nd metadata information, such as the number of columns in the result, the
nameofaspeci?edcolumn,orthetypeofaspeci?edcolumn.Inthisway,wecan
executeaqueryevenifwe haveno ideaofthe schema of theresult.
TheJavacodesegmentbelowusesJDBC to print out the names and types of
all columns of a result set. The variablers in the code below is assumed to refer
to aResultSetinstance obtained by executingaquery.
ResultSetMetaDatarsmd = rs.getMetaData();
for(int i = 1; i <= rsmd.getColumnCount(); i++) {
System.out.println(rsmd.getColumnName(i));
System.out.println(rsmd.getColumnTypeName(i));
}
The getColumnCount method returns the arity (number of attributes) of the
resultrelation.Thatallowsustoiteratethrougheachattribute(notethatwestart
5.1 Accessing SQL Froma Programming Language 165
DatabaseMetaDatadbmd = conn.getMetaData();
ResultSet rs = dbmd.getColumns(null, "univdb", "department", "%");
// Arguments to getColumns: Catalog, Schema-pattern,Table-pattern,
// and Column-Pattern
// Returns: One row for each column; row has a number of attributes
// such as COLUMN NAME, TYPE NAME
while( rs.next()) {
System.out.println(rs.getString("COLUMN NAME"),
rs.getString("TYPE NAME");
}
Figure5.3 Finding column information in JDBC usingDatabaseMetaData.
at1,asisconventionalin JDBC).Foreachattribute,weretrieveitsnameanddata
typeusingthe methodsgetColumnNameandgetColumnTypeName,respectively.
TheDatabaseMetaDatainterfaceprovidesawayto?nd metadataaboutthe
database. The interface Connection has a method getMetaData that returns a
DatabaseMetaData object. The DatabaseMetaData interface in turn has a very
large number of methods to get metadata about the database and the database
systemtowhich the applicationis connected.
For example, there are methods that return the product name and version
number of the database system. Other methods allow the application to query
thedatabase systemabout itssupportedfeatures.
Still other methods return information about the database itself. The code
in Figure 5.3 illustrates how to ?nd information about columns (attributes) of
relationsinadatabase.Thevariableconnisassumedtobeahandleforanalready
opened database connection. The method getColumns takes four arguments: a
catalog name (null signi?es that the catalog name is to be ignored), a schema
name pattern, a table name pattern, and a column name pattern. The schema
name, table name, and column name patterns can be used to specify a name or
a pattern. Patterns can use the SQL string matching special characters “%” and
“ ”; for instance, the pattern “%” matches all names. Only columns of tables of
schemas satisfying the speci?ed name or pattern are retrieved. Each row in the
result set contains information about one column. The rows have a number of
columns such as the name of the catalog, schema, table and column, the type of
the column, and so on.
Examples of other methods provided by DatabaseMetaData that provide
information about the database include those for retrieval of metadata about
relations (getTables()), foreign-key references (getCrossReference()),au-
thorizations, database limits such as maximum number of connections, and so
on.
The metadata interfacescan be usedfor a varietyof tasks.For example,they
can be used to write a database browser that allows a user to ?nd the tables in
a database, examine their schema, examine rows in a table, apply selections to
seedesiredrows,andsoon.Themetadatainformationcanbeusedtomakecode
166 Chapter 5 Advanced SQL
usedforthesetasksgeneric;forexample,codetodisplaytherowsinarelationcan
be written in such a way that it would work on all possible relations regardless
of their schema. Similarly, it is possible to write code that takes a query string,
executes the query, and prints out the results as a formatted table; the code can
work regardlessof the actual querysubmitted.
5.1.1.7 OtherFeatures
JDBC provides a number of other features, such as updatable result sets.Itcan
create an updatable result set from a query that performs a selection and/or a
projectiononadatabaserelation.Anupdatetoatupleintheresultsetthenresults
in an update tothe correspondingtuple ofthe database relation.
RecallfromSection4.3thatatransactionallowsmultipleactionstobetreated
asasingleatomicunitwhichcanbecommittedorrolledback.
Bydefault,eachSQLstatementistreatedasaseparatetransactionthatiscom-
mitted automatically. The method setAutoCommit() in the JDBC Connection
interfaceallowsthisbehaviortobeturnedonoroff.Thus,ifconnisanopencon-
nection,conn.setAutoCommit(false)turnsoffautomaticcommit.Transactions
mustthenbecommittedorrolledbackexplicitlyusingeitherconn.commit()or
conn.rollback().conn.setAutoCommit(true)turnson automatic commit.
JDBC provides interfaces to deal with large objects without requiring an en-
tire large object to be created in memory. To fetch large objects, the ResultSet
interface provides methods getBlob() and getClob() that are similar to the
getString() method, but return objects of type Blob and Clob, respectively.
These objects do not store the entire large object, but instead store “locators” for
thelargeobjects,thatis,logicalpointerstotheactuallargeobjectinthedatabase.
Fetching data from these objects is very much like fetching data from a ?le or
an input stream, and can be performed using methods such as getBytes and
getSubString.
Conversely, to store large objects in the database, the PreparedStatement
class permits a database column whose type is blob to be linked to an input
stream (such as a ?le that has been opened) using the method setBlob(int
parameterIndex, InputStream inputStream).Whenthepreparedstatement
is executed, data are read from the input stream, and written to the blob in the
database. Similarly, a clob column can be set using the setClob method, which
takesas argumentsa parameterindexand acharacter stream.
JDBC includes a row set feature that allows result sets to be collected and
shipped to other applications. Row sets can be scanned both backward and for-
ward and can be modi?ed. Because row sets are not part of the database itself
once they aredownloaded,we donot coverdetailsof theirusehere.
5.1.2 ODBC
The Open Database Connectivity (ODBC) standard de?nes an API that applica-
tionscanusetoopenaconnectionwithadatabase,sendqueriesandupdates,and
get back results. Applications such as graphical user interfaces, statistics pack-
5.1 Accessing SQL Froma Programming Language 167
void ODBCexample()
{
RETCODE error;
HENV env; /* environment */
HDBC conn; /* database connection */
SQLAllocEnv(&env);
SQLAllocConnect(env, &conn);
SQLConnect(conn, "db.yale.edu", SQL NTS,"avi",SQL NTS,
"avipasswd", SQL NTS);
{
char deptname[80];
?oat salary;
int lenOut1,lenOut2;
HSTMT stmt;
char * sqlquery = "select dept name, sum (salary)
from instructor
group by dept name";
SQLAllocStmt(conn, &stmt);
error = SQLExecDirect(stmt, sqlquery, SQL NTS);
if (error == SQL SUCCESS) {
SQLBindCol(stmt, 1, SQL C CHAR, deptname , 80, &lenOut1);
SQLBindCol(stmt, 2, SQL C FLOAT, &salary, 0 , &lenOut2);
while (SQLFetch(stmt) == SQL SUCCESS) {
printf (" %s %g\n", depthname, salary);
}
}
SQLFreeStmt(stmt, SQL DROP);
}
SQLDisconnect(conn);
SQLFreeConnect(conn);
SQLFreeEnv(env);
}
Figure5.4 ODBC code example.
ages, and spreadsheets can make use of the same ODBC API to connect to any
databaseserverthat supports ODBC.
EachdatabasesystemsupportingODBCprovidesalibrarythatmustbelinked
with the client program. When the client program makes an ODBC API call, the
codeinthelibrarycommunicateswiththeservertocarryouttherequestedaction,
and fetch results.
Figure 5.4 shows an example of C code using the ODBC API.The?rststep
in using ODBC to communicate with a server is to set up a connection with
the server. To do so, the program ?rst allocates an SQL environment, then a
168 Chapter 5 Advanced SQL
database connection handle. ODBC de?nes the types HENV, HDBC,andRETCODE.
TheprogramthenopensthedatabaseconnectionbyusingSQLConnect.Thiscall
takes several parameters, including the connection handle, the server to which
to connect, the user identi?er, and the password for the database. The constant
SQL NTSdenotesthat the previousargumentis anull-terminated string.
Once the connection is set up, the program can send SQL commands to the
databasebyusing SQLExecDirect.Clanguagevariablescanbeboundtoattributes
of the query result, so that when a result tuple is fetched using SQLFetch,its
attributevaluesarestoredincorrespondingCvariables.TheSQLBindColfunction
does this task; the second argument identi?es the position of the attribute in the
queryresult,andthethirdargumentindicatesthetypeconversionrequiredfrom
SQLtoC.Thenextargumentgivestheaddressofthevariable.Forvariable-length
types like character arrays, the last two arguments give the maximum length of
the variable and a location where the actual length is to be stored when a tuple
is fetched. A negative value returned for the length ?eld indicates that the value
isnull. For ?xed-length typessuch as integeror ?oat, the maximum length ?eld
is ignored, while a negative value returned for the length ?eld indicates a null
value.
The SQLFetch statement is in a while loop that is executed until SQLFetch
returns a value other than SQL SUCCESS.Oneachfetch,theprogramstoresthe
values in C variables as speci?ed by the calls on SQLBindCol and prints out these
values.
Attheendofthesession,theprogramfreesthestatementhandle,disconnects
from the database, and frees up the connection and SQL environment handles.
Good programming style requires that the result of every function call must be
checked to make sure there are no errors; we have omitted most of these checks
for brevity.
ItispossibletocreateanSQLstatementwithparameters;forexample,consider
the statement insert into department values(?,?,?).Thequestionmarks
are placeholders for values which will be supplied later. The above statement
can be “prepared,” that is, compiled at the database, and repeatedly executed
by providing actual values for the placeholders—in this case, by providing an
departmentname,building,and budgetfor therelation department.
ODBC de?nesfunctions foravarietyof tasks,such as?ndingallthe relations
in the database and ?nding the names and types of columns of a query result or
arelationinthedatabase.
By default, each SQL statement is treated as a separate transaction that is
committed automatically. The SQLSetConnectOption(conn, SQL AUTOCOMMIT, 0)
turns off automatic commit on connection conn, and transactions must then be
committed explicitly by SQLTransact(conn, SQL COMMIT) or rolled back by
SQLTransact(conn, SQL ROLLBACK).
The ODBC standard de?nes conformance levels, which specify subsets of the
functionality de?ned by the standard. An ODBC implementation may provide
only core level features, or it may provide more advanced (level 1 or level 2)
features.Level1requiressupportforfetchinginformationaboutthecatalog,such
as information about what relations are present and the types of their attributes.
5.1 Accessing SQL Froma Programming Language 169
ADO.NET
The ADO.NET API, designed for the Visual Basic .NET and C# languages, pro-
videsfunctionstoaccessdata,whichatahighlevelarenotdissimilartotheJDBC
functions,althoughdetailsdiffer.LikeJDBCandODBC,theADO.NETAPIallows
accesstoresultsof SQL queries,aswellastometadata,butisconsiderablysim-
pler to use than ODBC. A database that supports ODBC can be accessed using
the ADO.NET API,andtheADO.NET calls are translated into ODBC calls. The
ADO.NET API can also be used with some kinds of nonrelational data sources
such as Microsoft’s OLE-DB, XML (covered in Chapter 23), and more recently,
the Entity Framework developed by Microsoft. See the bibliographic notes for
moreinformationon ADO.NET.
Level2requiresfurtherfeatures,suchastheabilitytosendandretrievearraysof
parametervaluesand toretrievemoredetailedcatalog information.
The SQL standard de?nes a call level interface (CLI) that is similar to the
ODBC interface.
5.1.3 Embedded SQL
The SQL standard de?nes embeddings of SQL in a variety of programming lan-
guages, such as C, C++, Cobol, Pascal, Java, PL/I, and Fortran. A language in
which SQL queries are embedded is referred to as a host language, and the SQL
structurespermittedinthe host languageconstitute embedded SQL.
Programs written in the host language can use the embedded SQL syntax to
access and update data stored in a database. An embedded SQL program must
be processed by a special preprocessor prior to compilation. The preprocessor
replacesembedded SQLrequestswithhost-languagedeclarationsandprocedure
calls that allow runtime execution of the database accesses. Then, the resulting
program is compiled by the host-language compiler. This is the main distinction
betweenembedded SQL and JDBC or ODBC.
In JDBC, SQL statementsareinterpretedat runtime(evenif theyareprepared
?rst using the prepared statement feature). When embedded SQL is used, some
SQL-relatederrors(including data-typeerrors)may be caught at compiletime.
Toidentifyembedded SQL requeststothe preprocessor,we usethe EXEC SQL
statement;ithas theform:
EXEC SQL<embedded SQL statement>;
The exact syntax for embedded SQL requests depends on the language in
which SQL is embedded. In some languages, such as Cobol, the semicolon is
replaced with END-EXEC.
170 Chapter 5 Advanced SQL
We place the statement SQL INCLUDE SQLCA in the program to identify the
place where the preprocessor should insert the special variables used for com-
munication betweenthe program and the database system.
Before executing any SQL statements, the program must ?rst connect to the
database. This isdone using:
EXEC SQL connect to serveruser user-nameusing password;
Here, serveridenti?esthe serverto which a connection isto be established.
Variablesofthehostlanguagecanbeusedwithinembedded SQLstatements,
but they must be precededby a colon (:) to distinguishthem from SQL variables.
VariablesusedasabovemustbedeclaredwithinaDECLAREsection,asillustrated
below. The syntax for declaring the variables, however, follows the usual host
language syntax.
EXEC SQL BEGIN DECLARE SECTION;
int credit amount;
EXEC SQL END DECLARE SECTION;
EmbeddedSQLstatementsaresimilarinformtoregularSQLstatements.There
are,however,severalimportant differences,as wenote here.
Towritearelationalquery,weusethedeclarecursorstatement.Theresultof
thequeryisnotyetcomputed.Rather,theprogrammustusetheopenand fetch
commands(discussedlaterinthissection)toobtaintheresulttuples.Asweshall
see,useof a cursor isanalogous toiteratingthrougha resultsetin JDBC.
Consider the university schema. Assume that we have a host-language vari-
able credit amount in our program, declared as we saw earlier, and that we wish
to ?nd the names of all students who have taken more than credit amount credit
hours. Wecan writethisqueryas follows:
EXEC SQL
declare ccursor for
select ID, name
from student
where tot cred>:credit amount;
The variable cintheprecedingexpressioniscalleda cursorforthe query.Weuse
thisvariabletoidentifythequery.Wethenusetheopenstatement,whichcauses
the queryto beevaluated.
Theopen statementfor oursamplequeryisas follows:
EXEC SQL open c;
This statement causes the database system to execute the query and to save the
resultswithinatemporaryrelation.Thequeryusesthevalueofthehost-language
variable(credit amount) at the time the openstatementis executed.
5.1 Accessing SQL Froma Programming Language 171
If the SQL query results in an error, the database system stores an error diag-
nostic inthe SQL communication-area (SQLCA)variables.
Wethenuseaseriesoffetchstatements,eachofwhichcausesthevaluesofone
tuple to be placed in host-language variables. The fetch statement requires one
host-language variable for each attribute of the result relation. For our example
query, we need one variable to hold the ID value and another to hold the name
value. Suppose that those variables are si and sn, respectively, and have been
declaredwithina DECLARE section.Thenthestatement:
EXEC SQL fetch cinto:si,: sn;
produces a tuple of the result relation. The program can then manipulate the
variables si and sn by usingthe featuresof the host programming language.
Asinglefetchrequestreturnsonlyonetuple.Toobtainalltuplesoftheresult,
theprogrammustcontainalooptoiterateoveralltuples.Embedded SQL assists
the programmer in managing this iteration. Although a relation is conceptually
a set, the tuples of the result of a query are in some ?xed physical order. When
the program executes an open statement on a cursor, the cursor is set to point to
the ?rst tuple of the result. Each time it executes a fetch statement, the cursor is
updatedtopointtothenexttupleoftheresult.Whennofurthertuplesremainto
beprocessed,thecharacterarrayvariable SQLSTATEinthe SQLCAissetto’02000’
(meaning“nomoredata”);theexactsyntaxforaccessingthisvariabledependson
thespeci?cdatabasesystemyouuse.Thus,wecanuseawhileloop(orequivalent
loop) toprocesseach tuple of the result.
We must use the closestatementtotellthedatabasesystemtodeletethetem-
porary relation that held the result of the query. For our example, this statement
takesthe form
EXECSQLclose c;
Embedded SQL expressions for database modi?cation (update, insert,and
delete) do not return a result. Thus, they are somewhat simpler to express. A
database-modi?cationrequesttakestheform
EXEC SQL< any validupdate,insert, ordelete>;
Host-language variables, preceded by a colon, may appear in the SQL database-
modi?cationexpression.Ifanerrorconditionarisesintheexecutionofthestate-
ment, a diagnosticissetin the SQLCA.
Database relations can also be updated through cursors. For example, if we
wanttoadd100tothesalaryattributeofeveryinstructorintheMusicdepartment,
we could declarea cursor as follows.
172 Chapter 5 Advanced SQL
SQLJ
The Java embedding of SQL, called SQLJ, provides the same features as other
embedded SQL implementations,butusingadifferentsyntax thatmoreclosely
matches features already present in Java, such as iterators. For example, SQLJ
uses the syntax #sql instead of EXEC SQL, and instead of cursors, uses the Java
iteratorinterfacetofetchqueryresults.Thustheresultofexecutingaqueryisa
Java iterator, and thenext() method of the Java iterator interface can be used to
step through the result tuples, just as the preceding examples use fetch on the
cursor.Theiteratormusthaveattributesdeclared,whosetypesmatchthetypes
of the attributes in the SQL query result. The code snippet below illustrates the
use ofiterators.
#sql iterator deptInfoIter ( String dept name, int avgSal);
deptInfoIter iter = null;
#sql iter = { select dept name, avg(salary)
from instructor
group by dept name };
while (iter.next()) {
String deptName = iter.dept name();
int avgSal = iter.avgSal();
System.out.println(deptName + " " + avgSal);
}
iter.close();
SQLJ is supported by IBM DB2 and Oracle; both provide translators that
convert SQLJ code into JDBC code. The translator can connect to the database
in order to check the syntactic correctness of queries at compile time, and to
ensurethatthe SQL typesofqueryresultsarecompatiblewiththeJavatypesof
variables they are assigned to. As of early 2009, SQLJ is not supported by other
database systems.
We do not describe SQLJ in detail here; see the bibliographic notes for more
information.
EXEC SQL
declare c cursorfor
select *
from instructor
where dept name= ‘Music’
forupdate;
We then iterate through the tuples by performing fetch operations on the cursor
(asillustratedearlier),andafterfetchingeachtupleweexecutethefollowingcode:
5.2 Functions and Procedures 173
EXEC SQL
update instructor
set salary = salary + 100
wherecurrentof c;
Transactionscanbecommittedusing EXEC SQL COMMIT,orrolledbackusing
EXEC SQL ROLLBACK.
QueriesinembeddedSQLarenormallyde?nedwhentheprogramiswritten.
There are rare situations where a query needs to be de?ned at runtime. For
example,anapplicationinterfacemayallowausertospecifyselectionconditions
ononeormoreattributesofarelation,andmayconstructthewhereclauseofan
SQLqueryatruntime,withconditionsononlythoseattributesforwhichtheuser
speci?esselections.Insuchcases,aquerystringcanbeconstructedandprepared
at runtime, using a statement of the form EXEC SQL PREPARE <query-name>
FROM :<variable>,and a cursor canbe openedon thequeryname.
5.2 FunctionsandProcedures
WehavealreadyseenseveralfunctionsthatarebuiltintotheSQLlanguage.Inthis
section,weshowhowdeveloperscanwritetheirownfunctions andprocedures,
storetheminthedatabase,andtheninvokethemfromSQLstatements.Functions
areparticularlyusefulwithspecializeddatatypessuchasimagesandgeometric
objects.Forinstance,aline-segmentdatatypeusedinamapdatabasemayhavean
associatedfunctionthatcheckswhethertwolinesegmentsoverlap,andanimage
datatypemay haveassociatedfunctions tocompare two imagesfor similarity.
Proceduresandfunctionsallow “businesslogic”tobestoredinthedatabase,
andexecutedfrom SQL statements.Forexample,universitiesusuallyhavemany
rules about how many courses a student can take in a given semester, the mini-
mumnumberofcoursesafull-timeinstructormustteachinayear,themaximum
number of majors a student can be enrolled in, and so on. While such business
logic can be encoded as programming-language procedures stored entirely out-
sidethedatabase,de?ningthemasstoredproceduresinthedatabasehasseveral
advantages.Forexample,itallowsmultipleapplicationstoaccesstheprocedures,
and it allows a single point of change in case the business rules change, without
changingotherpartsoftheapplication.Applicationcodecanthencallthestored
procedures,insteadof directlyupdatingdatabase relations.
SQLallowsthede?nitionoffunctions,procedures,andmethods.Thesecanbe
de?ned either by the procedural component of SQL,orbyanexternalprogram-
ming language such as Java, C, or C++. We look at de?nitions in SQL ?rst, and
thenseehow to usede?nitionsinexternallanguagesinSection5.2.3.
Although the syntax we present here is de?ned by the SQL standard, most
databases implementnonstandard versions of this syntax. For example, the pro-
cedural languages supported by Oracle (PL/SQL), Microsoft SQL Server (Trans-
actSQL),andPostgreSQL(PL/pgSQL)alldifferfromthestandardsyntaxwepresent
174 Chapter 5 Advanced SQL
createfunction dept count(dept namevarchar(20))
returnsinteger
begin
declare d countinteger;
select count(*)into d count
from instructor
where instructor.dept name= dept name
return d count;
end
Figure5.5 Function de?ned in SQL.
here.Weillustratesomeofthedifferences,forthecaseofOracle,later(page178).
Seetherespectivesystemmanuals forfurtherdetails.Althoughpartsofthe syn-
tax we present here may not be supported on such systems, the concepts we
describeareapplicableacrossimplementations,althoughwithadifferentsyntax.
5.2.1 DeclaringandInvoking SQLFunctions andProcedures
Suppose that we want a function that, given the name of a department, returns
the count of the number of instructors in that department. We can de?ne the
functionasshowninFigure5.5.
4
Thisfunctioncanbeusedinaquerythatreturns
names and budgetsof all departmentswith more than 12 instructors:
select dept name, budget
from instructor
where dept count(dept name)>12;
The SQL standard supports functions that can return tables as results; such
functionsarecalledtablefunctions.
5
Considerthefunctionde?nedinFigure5.6.
The function returns a table containing all the instructors of a particular depart-
ment. Note that the function’s parameter is referenced by pre?xing it with the
name of the function (instructor of.dept name).
The function can be usedin a queryas follows:
select *
fromtable(instructor of(’Finance’));
Thisqueryreturnsallinstructorsofthe’Finance’department.Intheabovesimple
caseitisstraightforwardtowritethisquerywithoutusingtable-valuedfunctions.
In general, however, table-valued functions can be thought of as parameterized
views thatgeneralize the regularnotion of viewsby allowing parameters.
4
Ifyouareenteringyourownfunctionsorprocedures,youshouldwrite “createorreplace” rather thancreatesothat it
iseasy to modify yourcode(by replacing the function) during debugging.
5
This feature ?rst appeared inSQL:2003.
5.2 Functions and Procedures 175
createfunction instructors of (dept namevarchar(20))
returnstable (
IDvarchar(5),
namevarchar(20),
dept namevarchar(20),
salary numeric(8,2))
returntable
(select ID, name, dept name, salary
from instructor
where instructor.dept name= instructor of.dept name);
Figure5.6 Table function in SQL.
SQLalsosupportsprocedures.Thedept countfunctioncouldinsteadbewritten
as a procedure:
createprocedure dept count proc(in dept namevarchar(20),
out d countinteger)
begin
select count(*)into d count
from instructor
where instructor.dept name= dept count proc.dept name
end
Thekeywordsinandoutindicate,respectively,parametersthatareexpected
to have values assigned to them and parameters whose values are set in the
procedure in order to return results.
Procedures can be invoked either from an SQL procedure or from embedded
SQL by the call statement:
declare d countinteger;
call dept count proc(’Physics’, d count);
Proceduresandfunctionscanbeinvokedfromdynamic SQL,asillustratedbythe
JDBCsyntax in Section5.1.1.4.
SQLpermitsmorethanoneprocedureofthesamename,solongasthenum-
ber of arguments of the procedures with the same name is different. The name,
along with the number of arguments, is used to identify the procedure. SQL also
permits more than one function with the same name, so long as the different
functionswiththesamenameeitherhavedifferentnumbersofarguments,orfor
functions with the same number of arguments, they differ in the type of at least
one argument.
176 Chapter 5 Advanced SQL
5.2.2 Language Constructs forProcedures andFunctions
SQLsupportsconstructsthatgiveitalmostallthepowerofageneral-purposepro-
gramminglanguage.ThepartoftheSQLstandardthatdealswiththeseconstructs
iscalledthePersistentStorageModule(PSM).
Variables are declaredusing adeclare statement and can have any valid SQL
datatype.Assignmentsareperformedusing a set statement.
A compound statement is of the form begin ...end, and it may contain
multiple SQL statements between the begin and the end. Local variables can
be declared within a compound statement, as we have seen in Section 5.2.1.
A compound statement of the form begin atomic ...end ensures that all the
statementscontained withinitareexecutedas asingletransaction.
SQL:1999 supports the while statements and the repeat statements by the
following syntax:
while boolean expressiondo
sequence of statements;
endwhile
repeat
sequence of statements;
until boolean expression
endrepeat
Thereisalso afor loopthat permitsiterationoverallresultsof a query:
declare nintegerdefault0;
for r as
select budgetfrom department
where dept name =‘Music‘
do
set n = n? r.budget
endfor
Theprogramfetchesthequeryresultsonerowatatimeintotheforloopvariable
(r,intheaboveexample).Thestatementleavecanbeusedtoexittheloop,while
iterate starts on the next tuple, from the beginning of the loop, skipping the
remainingstatements.
Theconditionalstatementssupportedby SQL includeif-then-elsestatements
by using thissyntax:
if boolean expression
then statement or compound statement
elseif boolean expression
then statement or compound statement
else statement or compound statement
endif
5.2 Functions and Procedures 177
– – Registersa studentafter ensuringclassroom capacity isnot exceeded
– – Returns0 onsuccess, and -1if capacity isexceeded.
createfunction registerStudent(
in s id varchar(5),
in s courseidvarchar(8),
in s secid varchar(8),
in s semestervarchar (6),
in s year numeric(4,0),
out errorMsgvarchar(100)
returnsinteger
begin
declare currEnrolint;
select count(*)into currEnrol
from takes
where course id = s courseidand sec id = s secid
and semester= s semesterand year = s year;
declare limit int;
select capacity into limit
from classroom naturaljoin section
where course id = s courseidand sec id = s secid
and semester= s semesterand year = s year;
if(currEnrol< limit)
begin
insertinto takes values
(s id, s courseid, s secid, s semester, s year, null);
return(0);
end
– –Otherwise,sectioncapacity limitalreadyreached
set errorMsg=’Enrollmentlimitreachedforcourse ’ || s courseid
||’section’|| s secid;
return(-1);
end;
Figure5.7 Procedure to register a student for a course section.
SQLalsosupportsacasestatementsimilartotheC/C++languagecasestate-
ment(inadditionto caseexpressions,which we saw inChapter3).
Figure 5.7 provides a larger example of the use of procedural constructs in
SQL. The function registerStudent de?ned in the ?gure, registers a student in a
course section, after verifying that the number of students in the section does
notexceedthecapacityoftheroomallocatedtothesection.Thefunctionreturns
an error code, with a value greater than or equal to 0 signifying success, and a
negativevaluesignifyinganerrorcondition,andamessageindicatingthereason
for the failure isreturnedas anoutparameter.
178 Chapter 5 Advanced SQL
NONSTANDARD SYNTAXFOR PROCEDURES AND FUNCTIONS
Although the SQL standard de?nes the syntax for procedures and functions,
most databases do not follow the standard strictly, and there is considerable
variation in the syntax supported. One of the reasons for this situation is that
these databases typically introduced support for procedures and functions be-
fore the syntax was standardized, and they continue to support their original
syntax. Itis notpossible tolist the syntaxsupportedbyeachdatabase here,but
we illustrate a few of the differences in the case of Oracle’s PL/SQL,byshow-
ing below a version of the function from Figure 5.5, as it would be de?ned in
PL/SQL.
createorreplace function dept count(dept namein instructor.dept name%type)
returninteger
as
d count integer;
begin
selectcount(*) into d count
from instructor
where instructor.dept name = dept name;
return d count;
end;
Whilethetwoversionsaresimilarinconcept,thereareanumberofminorsyn-
tacticdifferences,someofwhichareevidentwhencomparingthetwoversions
ofthefunction.Althoughnotshownhere,thesyntaxforcontrol?owinPL/SQL
also hasseveraldifferencesfromthesyntax presentedhere.
Observethat PL/SQL allows a type to be speci?ed as the type of an attribute
of a relation, by adding the suf?x %type. On the other hand, PL/SQL does not
directly support the ability to return a table, although there is an indirect way
of implementing this functionality by creating a table type. The procedural
languages supported by other databases also have a number of syntactic and
semantic differences. See the respective language references for more informa-
tion.
The SQLprocedurallanguagealsosupportsthesignalingofexceptioncondi-
tions,anddeclaringofhandlersthat can handle the exception, asin this code:
declare out of classroom seats condition
declareexithandlerfor out of classroom seats
begin
sequence of statements
end
5.2 Functions and Procedures 179
Thestatementsbetweenthebeginandtheendcanraiseanexceptionbyexecuting
signalout of classroom seats.Thehandlersaysthatiftheconditionarises,theaction
tobetakenistoexittheenclosingbeginendstatement.Alternativeactionswould
be continue, which continues execution from the next statement following the
one that raised the exception. In addition to explicitly de?ned conditions, there
arealsoprede?nedconditionssuchassqlexception,sqlwarning,andnotfound.
5.2.3 ExternalLanguage Routines
Although the procedural extensions to SQL can be very useful, they are unfortu-
nately not supported in a standard way across databases. Even the most basic
features have different syntax or semantics in different database products. As a
result, programmers have to essentially learn a new language for each database
product.Analternativethatisgaininginsupportistode?neproceduresinanim-
perativeprogramminglanguage,butallowthemtobeinvokedfrom SQLqueries
and triggerde?nitions.
SQLallowsustode?nefunctionsinaprogramminglanguagesuchasJava,C#,
C or C++. Functions de?ned in this fashion can be more ef?cient than functions
de?nedinSQL,andcomputationsthatcannotbecarriedoutinSQLcanbeexecuted
by these functions.
Externalproceduresand functions can be speci?edin thisway (note that the
exactsyntax dependsonthe speci?c databasesystemyou use):
createproceduredept count proc(in dept namevarchar(20),
outcount integer)
languageC
external name’/usr/avi/bin/dept count proc’
createfunction dept count (dept namevarchar(20))
returnsinteger
languageC
external name’/usr/avi/bin/dept count’
In general, the external language procedures need to deal with null values in
parameters(both in and out) and returnvalues. They also needto communicate
failure/success status, to deal with exceptions. This information can be commu-
nicated by extra parameters: an sqlstate value to indicate failure/success status,
a parameter to store the return value of the function, and indicator variables for
eachparameter/functionresulttoindicateifthevalueisnull.Othermechanisms
are possible to handle null values, for example by passing pointers instead of
values. The exact mechanisms depend on the database. However, if a function
does not deal with these situations, an extra line parameter style general can be
added to the declaration to indicate that the external procedures/functions take
only the arguments shown and do not handlenull valuesor exceptions.
Functions de?ned in a programming language and compiled outside the
database system may be loaded and executed with the database-system code.
180 Chapter 5 Advanced SQL
However, doing so carries the risk that a bug in the program can corrupt the
databaseinternalstructures,andcanbypasstheaccess-controlfunctionalityofthe
database system. Database systems that are concerned more about ef?cient per-
formancethanaboutsecuritymayexecuteproceduresinsuchafashion.Database
systemsthatareconcernedaboutsecuritymayexecutesuchcodeaspartofasep-
arate process, communicate the parameter values to it, and fetch results back,
via interprocess communication. However, the time overhead of interprocess
communication is quite high; on typical CPU architectures, tens to hundreds of
thousandsofinstructionscanexecuteinthetimetakenforoneinterprocesscom-
munication.
Ifthecodeiswrittenina “safe”language such asJavaorC#,thereisanother
possibility:executingthecodeinasandboxwithinthedatabasequeryexecution
process itself. The sandbox allows the Java or C# code to access its own memory
area, but prevents the code from reading or updating the memory of the query
executionprocess,oraccessing?lesinthe?lesystem.(Creatingasandbox isnot
possible for a language such as C, which allows unrestricted access to memory
through pointers.) Avoiding interprocess communication reduces function call
overheadgreatly.
Several database systems today support external language routines running
in a sandbox within the query execution process. For example, Oracle and IBM
DB2 allow Java functions to run as part of the database process. Microsoft SQL
Server allows procedures compiled into the Common Language Runtime (CLR)
toexecutewithinthedatabaseprocess;suchprocedurescouldhavebeenwritten,
forexample,inC#orVisualBasic. PostgreSQLallowsfunctionsde?nedinseveral
languages, such as Perl,Python, and Tcl.
5.3 Triggers
A trigger is a statement that the system executes automatically as a side effect
of a modi?cation to the database. To design a trigger mechanism, we must meet
two requirements:
1. Specifywhenatriggeristobeexecuted.Thisisbrokenupintoaneventthat
causesthetriggertobecheckedandacondition that must be satis?ed for
triggerexecutionto proceed.
2. Specifythe actions to betakenwhenthetriggerexecutes.
Onceweenteratriggerintothedatabase,thedatabasesystemtakesontherespon-
sibilityofexecutingitwheneverthespeci?edeventoccursandthecorresponding
conditionis satis?ed.
5.3.1 NeedforTriggers
Triggers can be used to implement certain integrity constraints that cannot be
speci?edusingtheconstraint mechanism of SQL.Triggersarealsousefulmecha-
5.3 Triggers 181
nismsforalertinghumansorforstartingcertaintasksautomaticallywhencertain
conditions are met. As an illustration, we could design a trigger that, whenever
a tuple is insertedinto the takes relation, updates the tuple in the student relation
for the student taking the course by adding the number of credits for the course
to the student’s total credits. As another example, suppose a warehouse wishes
to maintain a minimum inventory of each item; when the inventory level of an
item falls below the minimum level, an order can be placed automatically. On
an update of the inventory level of an item, the trigger compares the current
inventorylevelwiththeminimuminventorylevelfortheitem,andifthelevelis
at orbelow the minimum,anew orderiscreated.
Notethattriggersystemscannotusuallyperformupdatesoutsidethedatabase,
and hence, in the inventory replenishment example, we cannot use a trigger to
placeanorderintheexternalworld.Instead,weaddanordertoarelationhold-
ingreorders.Wemustcreateaseparatepermanentlyrunningsystemprocessthat
periodicallyscansthatrelationandplacesorders.Somedatabasesystemsprovide
built-insupportforsendingemailfrom SQLqueriesandtriggers,usingtheabove
approach.
5.3.2 TriggersinSQL
WenowconsiderhowtoimplementtriggersinSQL.Thesyntaxwepresenthereis
de?nedbytheSQLstandard,butmostdatabasesimplementnonstandardversions
of this syntax. Although the syntax we present here may not be supported on
such systems, the concepts we describe are applicable across implementations.
Wediscuss nonstandard triggerimplementationslaterin thissection (page 184).
Figure5.8showshowtriggerscanbeusedtoensurereferentialintegrityonthe
time slot idattributeofthe sectionrelation.The?rsttriggerde?nitioninthe?gure
speci?es that the trigger is initiated after any insert on the relation section and it
ensuresthatthe time slot idvaluebeinginsertedisvalid.An SQLinsertstatement
could insert multiple tuples of the relation, and the for each row clause in the
triggercodewouldthenexplicitlyiterateovereachinsertedrow.Thereferencing
newrowasclausecreatesavariablenrow(calledatransitionvariable)thatstores
thevalueof aninsertedrowaftertheinsertion.
The when statement speci?es a condition. The system executes the rest of
the trigger body only for tuples that satisfy the condition. The begin atomic...
end clause can serve to collect multiple SQL statements into a single compound
statement. In our example,though, there is only one statement, which rolls back
the transaction that causedthe triggerto getexecuted.Thus any transaction that
violates the referential integrity constraint gets rolled back, ensuring the data in
the database satis?esthe constraint.
Itisnotsuf?cienttocheckreferentialintegrityoninsertsalone,wealsoneedto
considerupdatesof section,aswellasdeletesandupdatestothereferencedtable
time slot. The second trigger de?nition in Figure 5.8 considers the case of deletes
to time slot. This trigger checks that the time slot id of the tuple being deleted is
either still present in time slot, or that no tuple in section contains that particular
time slot id value;otherwise,referentialintegritywouldbe violated.
182 Chapter 5 Advanced SQL
create trigger timeslot check1 afterinsert on section
referencingnewrowas nrow
for eachrow
when (nrow.time slot id notin(
select time slot id
from time slot)) /* time slot id not presentin time slot */
begin
rollback
end;
create trigger timeslot check2 afterdelete on timeslot
referencingold rowas orow
for eachrow
when (orow.time slot id notin(
select time slot id
from time slot)/*lasttuplefortime slot id deletedfrom time slot */
and orow.time slot id in (
select time slot id
from section)) /*and time slot id still referenced from section*/
begin
rollback
end;
Figure5.8 Using triggers to maintain referential integrity.
Toensurereferentialintegrity,wewouldalsohavetocreatetriggerstohandle
updatestosectionand time slot;wedescribenexthowtriggerscanbeexecutedon
updates,but leavethe de?nitionof thesetriggersas anexercisetothe reader.
Forupdates,thetriggercanspecifyattributeswhoseupdatecausesthetrigger
to execute; updates to other attributes would not cause it to be executed. For
example, to specify that a trigger executes after an update to the grade attribute
of the takes relation,we write:
after updateof takes on grade
Thereferencingoldrowasclausecanbeusedtocreateavariablestoringthe
old value of an updated or deleted row. The referencing new row as clause can
be usedwithupdatesinadditionto inserts.
Figure5.9showshowatriggercanbeusedtokeepthetot credattributevalue
of studenttuplesup-to-datewhenthe grade attributeisupdatedforatupleinthe
takes relation. The trigger is executed only when the grade attribute is updated
from a value that is either null or ’F’, to a grade that indicates the course is
successfully completed. The update statement is normal SQL syntax except for
the use of the variable nrow.
5.3 Triggers 183
create trigger credits earnedafter updateof takes on (grade)
referencingnewrowas nrow
referencingoldrowas orow
foreach row
when nrow.grade<> ’F’and nrow.grade isnotnull
and(orow.grade = ’F’or orow.grade isnull)
beginatomic
update student
set tot cred= tot cred+
(select credits
from course
where course.course id= nrow.course id)
where student.id = nrow.id;
end;
Figure5.9 Usingatriggertomaintaincredits earned values.
A more realistic implementation of this example trigger would also handle
grade corrections that change a successful completion grade to a fail grade, and
handleinsertionsintothe takesrelation where the gradeindicatessuccessfulcom-
pletion.We leavetheseasan exerciseforthe reader.
As another example of the use of a trigger, the action on delete of a student
tuple could be to check if the student has any entries in the takes relation, and if
so,to deletethem.
Many database systems support a variety of other triggering events, such as
when a user (application) logs on to the database (that is, opens a connection),
thesystemshuts down, orchanges aremadetosystemsettings.
Triggerscanbeactivatedbeforetheevent(insert,delete,orupdate)insteadof
aftertheevent.Triggersthatexecutebeforeaneventcanserveasextraconstraints
thatcanpreventinvalidupdates,inserts,ordeletes.Insteadoflettingtheinvalid
action proceed and cause an error, the trigger might take action to correct the
problem so that the update, insert, or delete becomes valid. For example, if we
attempttoinsertaninstructorintoadepartmentwhosenamedoesnotappearin
thedepartmentrelation,thetriggercouldinsertatupleintothedepartmentrelation
for that department name before the insertion generates a foreign-key violation.
Asanotherexample,supposethevalueofaninsertedgradeisblank,presumably
toindicatethe absence of agrade.We can de?neatriggerthatreplacesthe value
by thenullvalue.Thesetstatementcanbeusedtocarryoutsuchmodi?cations.
Anexampleof sucha triggerappearsinFigure5.10.
Instead of carrying out an action for each affected row, we can carry out a
singleactionfortheentireSQLstatementthatcausedtheinsert,delete,orupdate.
To do so, we use the for each statement clause instead of the for each row
clause.Theclausesreferencingoldtableasorreferencingnewtableascanthen
be used to refer to temporary tables (called transition tables) containing all the
affected rows. Transition tables cannot be used with before triggers, but can be
184 Chapter 5 Advanced SQL
create trigger setnullbeforeupdateon takes
referencingnewrowas nrow
foreach row
when (nrow.grade =’’)
beginatomic
set nrow.grade =null;
end;
Figure5.10 Example of using set to change an inserted value.
usedwithaftertriggers,regardlessofwhethertheyarestatementtriggersorrow
triggers.Asingle SQLstatementcanthenbeusedtocarryoutmultipleactionson
the basisof the transitiontables.
NONSTANDARD TRIGGERSYNTAX
Althoughthetriggersyntaxwedescribehereispartofthe SQLstandard,andis
supported by IBM DB2, most other database systems have nonstandard syntax
forspecifyingtriggers,andmaynotimplementallfeaturesinthe SQLstandard.
Weoutlineafewofthedifferencesbelow;seetherespectivesystemmanualsfor
furtherdetails.
For example, in the Oracle syntax, unlike the SQL standard syntax, the key-
word row does not appear in the referencing statement. The keyword atomic
doesnotappearafterbegin.Thereferencetonrowintheselectstatementnested
in the update statement must begin with a colon (:) to inform the system that
the variable nrow is de?ned externally from the SQL statement. Further, sub-
queriesarenotallowedinthewhenandifclauses.Itispossibletoworkaround
this problem by moving complex predicates from the when clause into a sep-
arate query that saves the result into a local variable, and then reference that
variable in an if clause, and the body of the trigger then moves into the cor-
responding then clause. Further, in Oracle, triggers are not allowed to execute
a transaction rollback directly; however, they can instead use a function called
raise application error to not only roll back the transaction, but also return an
errormessageto theuser/applicationthatperformedtheupdate.
Asanotherexample,inMicrosoft SQL Serverthekeywordonisusedinstead
ofafter.Thereferencingclauseisomitted,andoldandnewrowsarereferenced
by the tuple variables deleted and inserted.Further,thefor each row clause is
omitted, and when is replaced by if.Thebefore speci?cation is not supported,
but aninstead of speci?cation is supported.
In PostgreSQL, triggers do not have a body, but instead invoke a procedure
for each row, which can access variables new and old containing the old and
newvaluesoftherow.Insteadofperformingarollback,thetriggercanraisean
exception,with anassociatederrormessage.
5.3 Triggers 185
create trigger reorder afterupdateof amounton inventory
referencingoldrowas orow,newrowas nrow
foreach row
when nrow.level<= (select level
from minlevel
where minlevel.item = orow.item)
and orow.level> (select level
from minlevel
where minlevel.item = orow.item)
beginatomic
insertinto orders
(select item, amount
from reorder
where reorder.item = orow.item);
end;
Figure5.11 Example of trigger for reordering an item.
Triggers can be disabled or enabled; by default they are enabled when they
arecreated,butcan bedisabledbyusingaltertrigger trigger namedisable(some
databases use alternative syntax such as disable trigger trigger name). A trigger
that has been disabled can be enabled again. A trigger can instead be dropped,
whichremovesitpermanently,byusingthecommanddroptrigger trigger name.
Returningtoourwarehouseinventoryexample,supposewehavethefollow-
ing relations:
• inventory (item, level), which notes the current amount of the item in the
warehouse.
• minlevel (item, level), which notes the minimum amount of the item to be
maintained.
• reorder(item,amount),whichnotestheamountoftheitemtobeorderedwhen
itslevelfalls belowtheminimum.
• orders(item, amount), which notes theamount of the itemto beordered.
Notethat we have beencarefultoplace anorderonlywhen the amount falls
from above the minimum level to below the minimum level. If we check only
that the new value after an update is below the minimum level, we may place
anordererroneouslywhentheitemhasalreadybeenreordered.Wecanthenuse
thetriggershown in Figure5.11 forreorderingthe item.
SQL-based database systems use triggers widely, although before SQL:1999
theywerenotpartoftheSQL standard. Unfortunately, each database system
implementeditsownsyntaxfortriggers,leadingtoincompatibilities.TheSQL:1999
syntax for triggers that we use here is similar, but not identical, to the syntax in
the IBM DB2 and Oracle database systems.
186 Chapter 5 Advanced SQL
5.3.3 WhenNottoUseTriggers
There are many good uses for triggers, such as those we have just seen in Sec-
tion5.3.2,butsomeusesarebesthandledbyalternativetechniques.Forexample,
we could implement the on delete cascade feature of a foreign-key constraint
by using a trigger, instead of using the cascade feature. Not only would this be
more work to implement, but also, it would be much harder for a database user
to understandthesetof constraints implementedinthe database.
Asanotherexample,triggerscanbeusedtomaintainmaterializedviews.For
instance,ifwewishedtosupportveryfastaccesstothetotalnumberofstudents
registeredforeach coursesection,we could dothisby creating arelation
section registration(course id, sec id, semester, year, total students)
de?ned by the query
select course id, sec id, semester, year,count(ID)as total students
from takes
groupby course id, sec id, semester, year;
The value of total students for each course must be maintained up-to-date by
triggers on insert, delete, or update of the takes relation. Such maintenance may
requireinsertion,updateordeletionoftuplesfromsection registration,andtriggers
must be writtenaccordingly.
However, many database systems now support materialized views, which
are automatically maintained by the database system (see Section 4.2.3). As a
result, there is no need to write trigger code for maintaining such materialized
views.
Triggers have been used for maintaining copies, or replicas, of databases. A
collection of triggers on insert, delete, or update can be created on each relation
to record the changes in relations called change or delta relations. A separate
process copies over the changes to the replica of the database. Modern database
systems,however,providebuilt-infacilitiesfordatabasereplication,makingtrig-
gersunnecessaryforreplicationinmostcases.Replicateddatabasesarediscussed
indetailinChapter19.
Another problem with triggers lies in unintended execution of the triggered
actionwhendataareloadedfromabackupcopy,
6
orwhendatabaseupdatesata
sitearereplicatedonabackupsite.Insuchcases,thetriggeredactionhasalready
been executed, and typically should not be executed again. When loading data,
triggers can be disabled explicitly. For backup replica systems that may have to
take over from the primary system, triggers would have to be disabled initially,
andenabledwhenthebackupsitetakesoverprocessingfromtheprimarysystem.
As an alternative, some database systems allow triggers to be speci?ed as not
6
We discuss database backup and recovery fromfailuresindetail in Chapter 16.
5.4 RecursiveQueries 187
course id prereq id
BIO-301 BIO-101
BIO-399 BIO-101
CS-190 CS-101
CS-315 CS-101
CS-319 CS-101
CS-347 CS-101
EE-181 PHY-101
Figure 5.12 The prereq relation.
for replication, which ensures that they are not executed on the backup site
during database replication. Other database systems provide a system variable
that denotes that the database is a replica on which database actions are being
replayed; the trigger body should check this variable and exit if it is true. Both
solutions remove the need for explicit disabling and enabling of triggers.
Triggers should be written with great care, since a trigger error detected at
runtime causes the failure of the action statementthat setoff the trigger.Further-
more, the action of one trigger can set off another trigger. In the worst case, this
could evenlead to an in?nite chain of triggering. Forexample, suppose an insert
trigger on a relation has an action that causes another (new) insert on the same
relation. The insert action then triggers yet another insert action, and so on ad
in?nitum. Some database systems limit the length of such chains of triggers (for
example, to 16 or 32) and consider longer chains of triggering an error. Other
systems ?ag as an error any trigger that attempts to reference the relation whose
modi?cation caused the trigger to execute in the ?rst place.
Triggers can serve a very useful purpose, but they are best avoided when
alternatives exist. Many trigger applications can be substituted by appropriate
use of stored procedures, which we discussed in Section 5.2.
5.4 Recursive Queries **
ConsidertheinstanceoftherelationprereqshowninFigure5.12containinginfor-
mation about the various courses offered at the university and the prerequisite
for each course.
7
Suppose now that we want to ?nd out which courses are a prerequisite
whetherdirectlyorindirectly,foraspeci?ccourse—say,CS-347.Thatis,wewish
to ?nd a course that is a direct prerequisite for CS-347, or is a prerequisite for a
course that is a prerequisitefor CS-347, and so on.
7
This instance of prereq differs from that used earlier for reasons that will become apparent as we use it to explain
recursive queries.
188 Chapter 5 Advanced SQL
Thus, if CS-301 is a prerequisite for CS-347, and CS-201 is a prerequisite for
CS-301,andCS-101isaprerequisiteforCS-201,thenCS-301,CS-201,andCS-101
areallprerequisitesforCS-347.
The transitive closure of the relation prereq is a relation that contains all
pairs (cid, pre)suchthatpre is a direct or indirect prerequisite of cid.Thereare
numerous applications that require computation of similar transitive closures
on hierarchies. For instance, organizations typically consist of several levels of
organizationalunits.Machinesconsistofpartsthatinturnhavesubparts,andso
on; for example, a bicycle may have subparts such as wheels and pedals, which
in turn have subparts such as tires, rims, and spokes. Transitive closure can be
usedonsuch hierarchiesto?nd, for example,all partsinabicycle.
5.4.1 TransitiveClosureUsingIteration
One way to write the above query is to use iteration: First ?nd those courses
that are a directprerequisiteof CS-347, then those courses that are a prerequisite
of all the courses under the ?rst set, and so on. This iterative process continues
until we reach an iteration where no courses are added. Figure 5.13 shows a
function ?ndAllPrereqs(cid) to carry out this task; the function takes the course
id of the course as a parameter (cid), computes the set of all direct and indirect
prerequisitesof that course,and returnsthe set.
The procedure uses three temporary tables:
• c prereq:storesthesetoftuplestobereturned.
• new c prereq:storesthe courses found in the previousiteration.
• temp:usedas temporarystoragewhilesetsof courses aremanipulated.
Note that SQL allowsthe creationoftemporarytablesusingthe command create
temporary table; such tables are available only within the transaction executing
the query, and are dropped when the transaction ?nishes. Moreover, if two in-
stancesof?ndAllPrereqsrunconcurrently,eachgetsitsowncopyofthetemporary
tables; ifthey shared acopy, theirresultcould be incorrect.
The procedure inserts all direct prerequisites of course cid into new c prereq
before the repeat loop. The repeat loop ?rst adds all courses in new c prereq to
c prereq.Next,itcomputesprerequisitesofallthosecoursesinnew c prereq,except
those that have already been found to be prerequisitesof cid, and stores them in
the temporary table temp. Finally, it replaces the contents of new c prereq by the
contents of temp.Therepeat loop terminates when it ?nds no new (indirect)
prerequisites.
Figure 5.14 shows the prerequisites that would be found in each iteration, if
the procedurewerecalledfor thecourse namedCS-347.
We note that the use of the except clause in the function ensures that the
functionworkseveninthe(abnormal)casewherethereisacycleofprerequisites.
Forexample,ifaisaprerequisiteforb,bisaprerequisiteforc,andcisaprerequisite
for a,thereisa cycle.
5.4 RecursiveQueries 189
createfunction ?ndAllPrereqs(cidvarchar(8))
––Findsallcoursesthatareprerequisite(directlyorindirectly)for cid
returnstable(course idvarchar(8))
––Therelationprereq(course id, prereq id) speci?eswhich course is
––directlyaprerequisiteforanothercourse.
begin
createtemporarytable c prereq(course idvarchar(8));
––tablec prereqstoresthesetofcoursestobereturned
createtemporarytable new c prereq(course idvarchar(8));
––tablenew c prereqcontains coursesfound inthe previousiteration
createtemporarytable temp(course idvarchar(8));
––tabletempisusedtostoreintermediateresults
insertinto new c prereq
select prereq id
from prereq
where course id= cid;
repeat
insertinto c prereq
select course id
from new c prereq;
insertinto temp
(select prereq.course id
from new c prereq, prereq
where new c prereq.course id= prereq.prereq id
)
except(
select course id
from c prereq
);
deletefrom new c prereq;
insertinto new c prereq
select*
from temp;
deletefrom temp;
untilnotexists (select*from new c prereq)
endrepeat;
returntable c prereq;
end
Figure 5.13 Finding all prerequisites of a course.
Whilecyclesmaybeunrealisticincourseprerequisites,cyclesarepossiblein
other applications. For instance, suppose we have a relation ?ights(to, from)that
sayswhichcitiescanbereachedfromwhichothercitiesbyadirect?ight.Wecan
190 Chapter 5 Advanced SQL
IterationNumber Tuples in c1
0
1 (CS-301)
2 (CS-301),(CS-201)
3 (CS-301),(CS-201)
4 (CS-301),(CS-201),(CS-101)
5 (CS-301),(CS-201),(CS-101)
Figure5.14 Prerequisites of CS-347 in iterations of function ?ndAllPrereqs.
write code similar to that in the ?ndAllPrereqs function, to ?nd all cities that are
reachable by a sequence of one or more ?ights from a given city. All we have to
do is to replace prereq by ?ight and replace attribute names correspondingly. In
this situation, there can be cycles of reachability, but the function would work
correctlysince itwould eliminatecitiesthat havealreadybeenseen.
5.4.2 RecursioninSQL
It is rather inconvenient to specify transitive closure using iteration. There is an
alternativeapproach, using recursiveviewde?nitions,that is easierto use.
We can use recursion to de?ne the set of courses that are prerequisites of
a particular course, say CS-347, as follows. The courses that are prerequisites
(directlyor indirectly)of CS-347are:
1. Coursesthat areprerequisitesfor CS-347.
2. Courses that are prerequisites for those courses that are prerequisites (di-
rectlyor indirectly)for CS-347.
Notethatcase2isrecursive,sinceitde?nesthesetofcoursesthatareprerequisites
of CS-347 in terms of the set of courses that are prerequisites of CS-347. Other
examples of transitive closure, such as ?nding all subparts (direct or indirect) of
a givenpartcan alsobe de?nedina similarmanner,recursively.
Since the SQL:1999 version, the SQL standard supports a limited form of re-
cursion, using the with recursive clause, where a view (or temporary view) is
expressedintermsofitself.Recursivequeriescanbeused,forexample,toexpress
transitiveclosureconcisely.Recallthatthewithclauseisusedtode?neatempo-
rary view whose de?nition is available only to the query in which it is de?ned.
The additionalkeyword recursive speci?esthat theviewisrecursive.
For example, we can ?nd every pair (cid,pre)suchthatpre is directly or in-
directly a prerequisite for course cid, using the recursive SQL view shown in
Figure5.15.
Any recursive view must be de?ned as the union of two subqueries: a base
query that is nonrecursive and a recursive query that uses the recursive view.
IntheexampleinFigure5.15,thebasequeryistheselectonprereq while the
recursivequerycomputesthejoinof prereq and rec prereq.
5.4 Recursive Queries 191
with recursive c prereq(course id, prereq id) as (
select course id, prereq id
from prereq
union
select prereq.prereq id, c prereq.course id
from prereq, c prereq
where prereq.course id = c prereq.prereq id
)
select ?
from c prereq;
Figure 5.15 Recursive query in SQL.
Themeaningofarecursiveviewisbestunderstoodasfollows.Firstcompute
the base query and add all the resultant tuples to the recursively de?ned view
relation rec prereq (which is initially empty). Next compute the recursive query
using the current contents of the view relation, and add all the resulting tuples
back to the view relation. Keep repeating the above step until no new tuples are
added to the view relation. The resultant view relation instance is called a ?xed
point of the recursive view de?nition. (The term “?xed” refers to the fact that
there is no further change.) The view relation is thus de?ned to contain exactly
the tuples in the ?xed-point instance.
Applyingtheabovelogictoourexample,we?rst?nd alldirectprerequisites
of each course by executing the base query. The recursive query adds one more
level of courses in each iteration, until the maximum depth of the course-prereq
relationship is reached. At this point no new tuples are added to the view, and a
?xed point is reached.
To?ndtheprerequisitesofaspeci?ccourse,suchasCS-347,wecanmodifythe
outerlevelquerybyaddingawhereclause“where rec prereq.course id=‘CS-347‘”.
One way to evaluate the query with the selection is to compute the full contents
of rec prereq using the iterative technique, and then select from this result only
those tuples whose course id is CS-347. However, this would result in computing
(course, prerequisite) pairs for all courses, all of which are irrelevant except for
those for the course CS-347. In fact the database system is not required to use the
above iterative technique to compute the full result of the recursive query and
thenperformtheselection.Itmaygetthesameresultusingothertechniquesthat
may be more ef?cient, such as that used in the function ?ndAllPrereqs which we
sawearlier.Seethebibliographicnotesforreferencestomoreinformationonthis
topic.
Therearesomerestrictionsontherecursivequeryinarecursiveview;speci?-
cally,thequeryshouldbemonotonic,thatis,itsresultonaviewrelationinstance
V
1
should be asupersetof its result on a viewrelationinstance V
2
if V
1
isa super-
set of V
2
. Intuitively, if more tuples are added to the view relation, the recursive
query should return at least the same set of tuples as before, and possibly return
additional tuples.
192 Chapter 5 Advanced SQL
Inparticular,recursivequeriesshouldnotuseanyofthefollowingconstructs,
since theywould make the querynonmonotonic:
• Aggregationontherecursiveview.
• notexists on a subquerythat usesthe recursiveview.
• Setdifference(except) whose right-hand sideusesthe recursiveview.
Forinstance,iftherecursivequerywasoftheformr ?v wherev istherecursive
view,ifweaddatupletov theresultofthequerycanbecomesmaller;thequery
isthereforenot monotonic.
The meaning of recursive views can be de?ned by the iterative procedure as
longastherecursivequeryismonotonic;iftherecursivequeryisnonmonotonic,
themeaningoftheviewishardtode?ne. SQLthereforerequiresthequeriestobe
monotonic. Recursive queries are discussed in more detail in the context of the
Datalog querylanguage, inSectionB.3.6.
SQL also allows creation of recursively de?ned permanent views by using
createrecursiveviewinplaceofwithrecursive.Someimplementationssupport
recursive queriesusing a different syntax; see the respectivesystem manuals for
furtherdetails.
5.5 AdvancedAggregationFeatures**
The aggregation support in SQL, which we have seen earlier, is quite powerful,
andhandlesmostcommontaskswithease.However,therearesometasksthatare
hard toimplementef?cientlywiththe basicaggregationfeatures.Inthissection,
we studyfeaturesthat wereaddedto SQL to handle some such tasks.
5.5.1 Ranking
Findingthepositionofavalueinalargersetisacommonoperation.Forinstance,
wemaywishtoassignstudentsarankinclassbasedontheirgrade-pointaverage
(GPA),withtherank1goingtothestudentwiththehighest GPA,therank2tothe
student with the next highest GPA, and so on. A related type of query is to ?nd
the percentile in which a value in a (multi)set belongs, for example, the bottom
third, middle third, or top third. While such queries can be expressed using the
SQL constructs we have seenso far, they are dif?cult to expressand inef?cient to
evaluate. Programmers may resort to writing the query partly in SQL and partly
inaprogramminglanguage.Westudy SQLsupportfordirectexpressionofthese
typesof querieshere.
In our university example, the takes relation shows the grade each student
earned in each course taken. To illustrate ranking, let us assume we have a view
student grades (ID, GPA)givingthegrade-pointaverageof eachstudent.
8
8
TheSQLstatementtocreatetheviewstudent gradesissomewhatcomplexsincewemustconvertthelettergradesinthe
takes relation to numbers and weightthe grades for each course by the number ofcredits for that course. The de?nition
ofthis viewisthe goal ofExercise4.5.
5.5 AdvancedAggregationFeatures 193
Ranking is done with an order by speci?cation. The following query gives
the rank of each student:
select ID,rank()over (orderby (GPA)desc)ass rank
fromstudent grades;
Note that the order of tuples in the output is not de?ned, so they may not be
sortedby rank.Anextraorderbyclauseisneededtogettheminsortedorder,as
shown below.
select ID,rank ()over (orderby (GPA)desc)ass rank
fromstudent grades
orderbys rank;
Abasicissuewithrankingishowtodealwiththecaseofmultipletuplesthat
are the same on the ordering attribute(s). In our example, this means deciding
what to do if there are two students with the same GPA.Therank function gives
the same rank to all tuplesthat are equalon theorderbyattributes. For instance,
if the highest GPA is shared by two students, both would get rank 1. The next
rank given would be 3, not 2, so if three students get the next highest GPA,they
wouldallgetrank3,andthenextstudent(s)wouldgetrank6,andsoon.Thereis
alsoadense rankfunctionthatdoesnotcreategapsintheordering.Intheabove
example, the tuples with the second highest value all get rank 2, and tuples with
the third highest value get rank 3, and so on.
It is possible to express the above query with the basic SQL aggregation
functions, using the following query:
select ID,(1+(selectcount(*)
fromstudent gradesB
whereB.GPA >A.GPA))ass rank
fromstudent gradesA
orderbys rank;
It should be clear that the rank of a student is merely 1 plus the number of
students with a higher GPA, which is exactly what the above query speci?es.
However, this computation of each student’s rank takes time linear in the size
of the relation, leading to an overall time quadratic in the size of the relation.
On large relations, the above query could take a very long time to execute. In
contrast,thesystem’simplementationoftherankclausecansorttherelationand
compute the rank in much less time.
Ranking can be done within partitions of the data. For instance, suppose we
wish to rank students by department rather than across the entire university.
Assume that a view is de?ned like student grades but including the department
name:dept grades(ID,dept name,GPA).Thefollowingquerythengivestherankof
students within each section:
194 Chapter 5 Advanced SQL
select ID, dept name,
rank() over (partitionby dept nameorderby GPAdesc) as dept rank
from dept grades
orderby dept name, dept rank;
The outer order by clause orders the result tuples by department name, and
within each departmentby the rank.
Multiple rank expressions can be used within a single select statement; thus
we can obtain the overall rank and the rank within the department by using
two rank expressions in the same select clause. When ranking (possibly with
partitioning)occursalongwithagroupbyclause,thegroupbyclauseisapplied
?rst, and partitioning and ranking are done on the results of the group by. Thus
aggregatevaluescanthenbeusedforranking.Wecouldhavewrittenourranking
over the student grades view without using the view, using a single select clause.
We leavedetailsasan exercisefor you.
The ranking functions can be used to ?nd the top n tuples by embedding a
ranking query within an outer-level query; we leave details as an exercise. Note
that the bottom n is simply the same as the top n with a reverse sorting order.
Severaldatabasesystemsprovidenonstandard SQLextensionstospecifydirectly
that only the top n results are required; such extensions do not require the rank
functionandsimplifythejoboftheoptimizer.Forexample,somedatabasesallow
aclauselimit n to be added at the end of an SQL query to specify that only the
?rst n tuples should be output; this clause is used in conjunction with an order
by clause to fetch the top n tuples, as illustrated by the following query, which
retrievestheIDsand GPAs of thetop10 studentsinorderof GPA:
select ID, GPA)
from student grades
orderby GPA
limit 10;
However, the limit clause does not support partitioning, so we cannot get the
topnwithineachpartitionwithoutperformingranking;further,ifmorethanone
studentgetsthesameGPA,itispossiblethatone isincludedin thetop10,while
another is excluded.
Several other functions can be used in place of rank.Forinstance,per-
cent rank of a tuple gives the rank of the tuple as a fraction. If there are n tuples
in the partition
9
and the rank of the tuple is r, then its percent rank is de?ned as
(r ?1)/(n?1)(andas nullifthereisonlyonetupleinthepartition).Thefunction
cume dist,shortforcumulativedistribution,foratupleisde?nedas p/nwhere p
isthenumberoftuplesinthepartitionwithorderingvaluesprecedingorequalto
theorderingvalueofthetupleand nisthenumberoftuplesinthepartition.The
functionrow numbersortstherowsandgiveseachrowauniquenumbercorre-
9
The entire setistreated as a singlepartition ifno explicitpartitionisused.
5.5 Advanced Aggregation Features 195
sponding to its position in the sort order; different rows with the same ordering
value would get different row numbers, in a nondeterministic fashion.
Finally, for a given constant n, the ranking function ntile(n)takesthetuples
in each partition in the speci?ed order and divides them into n buckets with
equal numbers of tuples.
10
For each tuple, ntile(n)thengivesthenumberofthe
bucket in which it is placed, with bucket numbers starting with 1. This function
is particularly useful for constructing histograms based on percentiles. We can
show the quartile into which each student falls based on GPA by the following
query:
select ID, ntile(4) over (order by (GPA desc)) as quartile
from student grades;
The presence of null values can complicate the de?nition of rank, since it is
not clear where they should occur ?rst in the sort order. SQL permits the user to
specify where they should occur by using nulls ?rst or nulls last,forinstance:
select ID, rank () over (order byGPA desc nulls last) as s rank
from student grades;
5.5.2 Windowing
Window queries compute an aggregate function over ranges of tuples. This is
useful, for example, to compute an aggregate of a ?xed range of time; the time
range is called a window. Windows may overlap, in which case a tuple may
contribute to more than one window. This is unlike the partitions we saw earlier,
where a tuple could contribute to only one partition.
An example of the use of windowing is trend analysis. Consider our earlier
sales example. Sales may ?uctuate widely from day to day based on factors
like weather (for example a snowstorm, ?ood, hurricane, or earthquake might
reduce sales for a period of time). However, over a suf?ciently long period of
time, ?uctuations might be less (continuing the example, sales may “make up”
for weather-related downturns). Stock market trend analysis is another example
of the use of the windowing concept. Various “moving averages” are found on
business and investment Web sites.
ItisrelativelyeasytowriteanSQLqueryusingthosefeatureswehavealready
studiedtocomputeanaggregateoveronewindow,forexample,salesovera?xed
3-day period. However, if we want to do this for every 3-day period, the query
becomes cumbersome.
SQL provides a windowing feature to support such queries. Suppose we are
givenaviewtot credits (year,num credits) giving the total number of creditstaken
10
Ifthetotalnumberoftuplesinapartitionisnotdivisiblebyn,thenthenumberoftuplesineachbucketcandifferbyat
most1.Tupleswiththesamevaluefortheorderingattribute may beassignedtodifferentbuckets,nondeterministically,
in order to make the number of tuples in each bucket equal.
196 Chapter 5 Advanced SQL
by students in each year.
11
Note that this relation can contain at most one tuple
for eachyear.Considerthe following query:
select year, avg(num credits)
over(orderby year rows 3preceding)
as avg total credits
from tot credits;
This query computes averages over the 3 preceding tuples in the speci?ed sort
order. Thus, for 2009, if tuples for years 2008 and 2007 are present in the relation
tot credits,witheachyearrepresentedbyonlyonetuple,theresultofthewindow
de?nitionistheaverageofthevaluesforyears2007,2008,and2009.Theaverages
each year would be computed in a similar manner. For the earliest year in the
relation tot credits, the average would be over only that year itself, while for the
next year, the average would be over two years. Note that if the relation tot
creditshasmorethanonetupleforaspeci?cyear,theremaybemultiplepossible
orderingsoftuples,thataresortedbyyear.Inthiscase,thede?nitionofpreceding
tuplesisbasedontheimplementationdependentsort order,and isnot uniquely
de?ned.
Suppose that instead of going back a ?xed number of tuples, we want the
window to consist of all prior years. That means the number of prior years
considered is not ?xed. To get the average total credits over all prior years we
write:
select year,avg(num credits)
over (orderby year rowsunboundedpreceding)
as avg total credits
from tot credits;
It is possible to use the keyword following in place of preceding.Ifwedid
this inour examplethe year value speci?es the beginning of the window instead
oftheend.Similarly,wecanspecifyawindowbeginningbeforethecurrenttuple
and endingafterit:
select year, avg(num credits)
over(orderby year rowsbetween 3precedingand2following)
as avg total credits
from tot credits;
Insteadofaspeci?ccountoftuples,wecanspecifyarangebasedonthevalue
oftheorderbyattribute.Tospecifyarangegoingback4yearsandincludingthe
current year,we write:
11
We leavethe de?nitionofthisviewinterms ofouruniversity exampleas an exercise.
5.6 OLAP 197
select year,avg(num credits)
over (orderby yearrangebetween year-4and year)
as avg total credits
from tot credits;
Be sure to note the use of the keywordrange in the above example. For the year
2010, data for years 2006 to 2010 inclusive would be included regardless of how
many tuples actually exist for that range.
In our example, all tuples pertain to the entire university. Suppose instead,
wehavecreditdatafor eachdepartmentinaview tot credits dept(dept name, year,
num credits) giving the total number of credits students took with the particular
department in the speci?ed year. (Again, we leave writing this view de?nition
as an exercise.) We can write windowing queries that treat each department
separately by partitioning by dept name:
select dept name, year,avg(num credits)
over (partitionby dept name
orderby yearrowsbetween 3precedingandcurrentrow)
as avg total credits
from tot credits dept;
5.6 OLAP**
An online analytical processing (OLAP) system is an interactive system that per-
mitsananalysttoviewdifferentsummariesofmultidimensionaldata.Theword
online indicates that an analyst must be able to request new summaries and get
responses online, within a few seconds, and should not be forced to wait for a
long time to see the result of a query.
There are many OLAP products available, including some that ship with
database products such as Microsoft SQL Server, and Oracle, and other stand-
alonetools.Theinitialversionsofmany OLAPtoolsassumedthatdataismemory
resident. Data analysis on small amounts of data can in fact be performed using
spreadsheet applications, such as Excel. However, OLAP on very large amounts
ofdatarequiresthatdataberesidentinadatabase,andrequiressupportfromthe
databaseforef?cientpreprocessingofdataaswellasforonlinequeryprocessing.
In this section, we study extensions of SQL to support such tasks.
5.6.1 Online Analytical Processing
Consideranapplicationwhereashopwantsto?ndoutwhatkindsofclothesare
popular. Let us suppose that clothes are characterized by their item name, color,
and size, and that we have a relation sales with the schema.
sales (item name, color, clothes size, quantity)
198 Chapter 5 Advanced SQL
item name color clothes size quantity
skirt dark small 2
skirt dark medium 5
skirt dark large 1
skirt pastel small 11
skirt pastel medium 9
skirt pastel large 15
skirt white small 2
skirt white medium 5
skirt white large 3
dress dark small 2
dress dark medium 6
dress dark large 12
dress pastel small 4
dress pastel medium 3
dress pastel large 3
dress white small 2
dress white medium 3
dress white large 0
shirt dark small 2
shirt dark medium 6
shirt dark large 6
shirt pastel small 4
shirt pastel medium 1
shirt pastel large 2
shirt white small 17
shirt white medium 1
shirt white large 10
pants dark small 14
pants dark medium 6
pants dark large 0
pants pastel small 1
pants pastel medium 0
pants pastel large 1
pants white small 3
pants white medium 0
pants white large 2
Figure5.16 An example of sales relation.
Suppose that item name can take on the values (skirt, dress, shirt, pants), color
cantakeonthevalues(dark,pastel,white), clothes size cantakeonvalues(small,
medium,large),and quantityisanintegervaluerepresentingthetotalnumberof
itemsof agiven{item name, color, clothes size }. Aninstance of the sales relationis
shown in Figure 5.16.
5.6 OLAP 199
Statistical analysis often requires grouping on multiple attributes. Given a
relation used for data analysis, we can identify some of its attributes asmeasure
attributes, since they measure some value, and can be aggregated upon. For
instance, the attribute quantity of the sales relation is a measure attribute, since
it measures the number of units sold. Some (or all) of the other attributes of the
relation are identi?ed asdimensionattributes, since they de?ne the dimensions
on which measure attributes, and summaries of measure attributes, are viewed.
In the sales relation, item name, color,andclothes size are dimension attributes. (A
more realistic version of the sales relation would have additional dimensions,
such as time and sales location, and additional measures such as monetary value
of the sale.)
Data that can be modeled as dimension attributes and measure attributes are
calledmultidimensionaldata.
To analyze the multidimensional data, a manager may want to see data laid
out as shown in the table in Figure 5.17. The table shows total quantities for
differentcombinations ofitem name andcolor.Thevalueofclothes size is speci?ed
to be all, indicating that the displayed values are a summary across all values of
clothes size (that is, we want to group the “small”, “medium”,and“large” items
into one single group.
The table in Figure 5.17 is an example of across-tabulation (orcross-tab,for
short), also referred to as a pivot-table. In general, a cross-tab is a table derived
from a relation (say R), where values for one attribute of relation R (say A)form
the row headers and values for another attribute of relation R (say B)formthe
columnheader.Forexample,inFigure5.17,theattributeitem namecorrespondsto
A(with values “dark”, “pastel”,and“white”), and the attribute color corresponds
to to B (with attributes “skirt”, “dress”, “shirt”,and“pants”).
Each cell in the pivot-table can be identi?ed by (a
i
,b
j
), where a
i
is a value
for Aand b
j
avalueforB. The values of the various cells in the pivot-table are
derived from the relation R as follows: If there is at most one tuple in R with any
(a
i
,b
j
) value, the value in the cell is derived from that single tuple (if any); for
instance, it could be the value of one or more other attributes of the tuple. If there
can be multiple tupleswith an (a
i
,b
j
) value,the value in the cell must be derived
skirt
dress
shirt
pants
color
item_name
clothes_size all
dark pastel white total
total
83 51 05 3
20 10 5 35
14 7 28 49
2 0252 7
62 54 48 164
Figure 5.17 Cross tabulation of sales by item name and color.
200 Chapter 5 Advanced SQL
82 01 42 06 2
35 10 7 2 54
10 8 28 5 48
53 35 49 27 164
34
21
77
4
9
42
16
18
45
all
large
medium
small
skirt dress shirt pants all
clothes_size
25311 1
4761 22 9
28572 2
dark
pastel
white
all
item_name
color
Figure5.18 Three-dimensional data cube.
by aggregation on the tuples with that value. In our example, the aggregation
used is the sum of the values for attribute quantity, across all values for clothes
size,asindicatedby“clothes size:all”abovethecross-tabinFigure5.17.Thus,the
valueforcell(skirt,pastel)is35,sincethereare3tuplesinthesalestablethatmeet
that criteria,with values11, 9, and 15.
Inourexample,thecross-tabalsohasanextracolumnandanextrarowstoring
the totals of the cells in the row/column. Most cross-tabs have such summary
rows and columns.
Thegeneralizationofacross-tab,which istwo-dimensional,to ndimensions
can be visualized as an n-dimensional cube, called the data cube.Figure5.18
shows a data cube on the sales relation. The data cube has three dimensions,
item name, color,andclothes size,andthemeasureattributeis quantity.Eachcellis
identi?edbyvaluesforthesethreedimensions.Eachcellinthedatacubecontains
avalue,justasinacross-tab.InFigure5.18,thevaluecontainedinacellisshown
on one of the faces of the cell; other faces of the cell are shown blank if they
are visible. All cells contain values, even if they are not visible. The value for a
dimension may be all, in which case the cell contains a summary over all values
of that dimension, asin the case of cross-tabs.
The number of different ways in which the tuples can be grouped for aggre-
gation can be large.Intheexampleof Figure5.18,thereare3colors,4 items,and
3 sizes resulting in a cube size of 3 ×4 ×3 = 36. Including the summary values,
weobtaina4 ×5×4cube,whosesizeis80.Infact,foratablewithndimensions,
aggregation can be performed with grouping on each of the 2
n
subsets of the n
dimensions.
12
12
Grouping onthe setofall ndimensions isuseful only ifthe table may have duplicates.
5.6 OLAP 201
With an OLAP system, a data analyst can look at different cross-tabs on the
same databy interactivelyselectingthe attributesinthe cross-tab. Eachcross-tab
is a two-dimensional view on a multidimensional data cube. For instance, the
analyst may select a cross-tab on item name and clothes size or a cross-tab on color
and clothes size. The operation of changing the dimensions used in a cross-tab is
calledpivoting.
OLAP systems allow an analyst to see a cross-tab on item name and color for a
?xed value of clothes size, for example, large, instead of the sum across all sizes.
Such an operation is referred to as slicing, since it can be thought of as viewing
a slice of the data cube. The operation is sometimes called dicing,particularly
when values for multiple dimensions are ?xed.
When a cross-tab is used to view a multidimensional cube, the values of
dimensionattributes that arenot part ofthe cross-tabare shown above the cross-
tab. The value of such an attribute can beall, as shown in Figure 5.17, indicating
that data in the cross-tab are a summary over all values for the attribute. Slic-
ing/dicing simply consists of selectingspeci?c valuesfor these attributes, which
are then displayed on top of the cross-tab.
OLAP systems permit users to view data at any desired level of granularity.
The operation of moving from ?ner-granularity data to a coarser granularity (by
means of aggregation) is called a rollup. In our example, starting from the data
cubeonthesalestable,wegotourexamplecross-tabbyrollingupontheattribute
clothes size.Theoppositeoperation—thatofmovingfromcoarser-granularitydata
to ?ner-granularity data—is called a drill down. Clearly, ?ner-granularity data
cannot be generatedfrom coarse-granularity data;theymust be generatedeither
from the original data, or from even ?ner-granularity summary data.
Analysts may wish to view a dimension at different levels of detail. For
instance, an attribute of type datetime contains a date and a time of day. Using
time precise to a second (or less) may not be meaningful: An analyst who is
interestedin rough time of day may look at only the hour value.An analyst who
is interested in sales by day of the week may map the date to a day of the week
and look only at that. Another analyst may be interested in aggregates over a
month, or a quarter, or for an entire year.
Thedifferentlevelsofdetailforanattributecanbeorganizedintoahierarchy.
Figure 5.19a shows a hierarchy on the datetime attribute. As another example,
Figure5.19bshowsahierarchyonlocation,withthecitybeingatthebottomofthe
hierarchy,stateaboveit,countryatthenextlevel,andregionbeingthetoplevel.In
our earlier example,clothes can be grouped by category (for instance, menswear
or womenswear); category would then lie above item name in our hierarchy on
clothes. At the level of actual values, skirts and dresses would fall under the
womenswear category and pants and shirts under the menswear category.
Ananalystmaybeinterestedinviewingsalesofclothesdividedasmenswear
and womenswear, and not interested in individual values. After viewing the
aggregatesat the level of womenswear and menswear, an analyst may drill down
the hierarchy to look at individual values. An analyst looking at the detailed level
may drill up the hierarchy and look at coarser-level aggregates. Both levels can be
displayed on the same cross-tab, as in Figure 5.20.
202 Chapter 5 Advanced SQL
Hour of day Date
DateTime
Day of week Month
Quarter
Year
State
Country
Region
City
(a) Time Hierarchy
(b) Location Hierarchy
Figure5.19 Hierarchies on dimensions.
5.6.2 Cross-TabandRelational Tables
A cross-tab is different from relational tables usually stored in databases, since
the number of columns in the cross-tab depends on the actual data. A change
in the data values may result in adding more columns, which is not desirable
for data storage. However, a cross-tab view is desirable for display to users. It is
straightforward to represent a cross-tab without summary values in a relational
formwitha?xednumberofcolumns.Across-tabwithsummaryrows/columns
canberepresentedbyintroducingaspecialvaluealltorepresentsubtotals,asin
Figure 5.21. The SQL standard actually uses the null value in place of all,butto
avoid confusion with regularnullvalues, we shall continue to useall.
womenswear
category         item_name               color
clothes_size: all
dark   pastel  white      total
total
skirt                8           8          10      53
dress             20         20           5       35
subtotal        28         28         15                 88
menswear
pants            14         14         28       49
shirt              20         20           5       27
subtotal        34         34         33                 76
                      62         62          48               164
Figure5.20 Cross tabulation of sales with hierarchy on item name.
5.6 OLAP 203
item name color clothes size quantity
skirt dark all 8
skirt pastel all 35
skirt white all 10
skirt all all 53
dress dark all 20
dress pastel all 10
dress white all 5
dress all all 35
shirt dark all 14
shirt pastel all 7
shirt white all 28
shirt all all 49
pants dark all 20
pants pastel all 2
pants white all 5
pants all all 27
all dark all 62
all pastel all 54
all white all 48
all all all 164
Figure 5.21 Relational representation of the data in Figure 5.17.
Considerthetuples(skirt,all,all,53)and(dress,all,all,35).Wehaveobtained
these tuples by eliminating individual tuples with different values for color and
clothes size, and by replacingthe value of quantity by an aggregate—namely, the
sum of the quantities. The value all can be thought of as representing the set of
all values for an attribute. Tuples with the value all for the color and clothes size
dimensionscanbeobtainedbyanaggregationonthe salesrelationwithagroup
by on the column item name. Similarly, a group by on color, clothes size can be
used to get the tuples with the value all for item name,andagroup by with no
attributes(whichcansimplybeomittedin SQL)canbeusedtogetthetuplewith
valueallfor item name, color,andclothes size.
Hierarchies can also be represented by relations. For example, the fact that
skirtsanddressesfallunderthewomenswearcategory,andthepantsandshirts
under the menswear category can be represented by a relation itemcategory(item
name,category).Thisrelationcanbejoinedwiththesalesrelation,togetarelation
thatincludesthecategoryforeachitem.Aggregationonthisjoinedrelationallows
ustogeta cross-tabwithhierarchy.Asanotherexample,ahierarchyoncitycan
berepresentedbyasinglerelation city hierarchy(ID, city, state, country, region),or
bymultiplerelations,eachmappingvaluesinonelevelofthehierarchytovalues
atthenextlevel.Weassumeherethatcitieshaveuniqueidenti?ers,storedinthe
attribute ID,toavoidconfusing betweentwocitieswiththesamename,e.g.,the
Spring?eldinMissouriandtheSpring?eldinIllinois.
204 Chapter 5 Advanced SQL
OLAP IMPLEMENTATION
The earliest OLAP systems used multidimensional arrays in memory to store
data cubes, and are referred to as multidimensional OLAP (MOLAP) systems.
Later,OLAPfacilitieswereintegratedintorelationalsystems,withdatastoredin
a relational database. Such systems are referredto asrelational OLAP (ROLAP)
systems. Hybrid systems, which store some summaries in memory and store
the base data and other summaries in a relational database, are called hybrid
OLAP (HOLAP) systems.
Many OLAP systems are implemented as client-server systems. The server
containstherelationaldatabaseaswellasanyMOLAPdatacubes.Clientsystems
obtain views ofthedata by communicatingwith theserver.
Ana¨ ?vewayofcomputingtheentiredatacube(allgroupings)onarelationis
touse anystandard algorithmforcomputingaggregateoperations,onegroup-
ing at a time. The na¨ ?ve algorithm would require a large number of scans of
the relation. A simple optimization is to compute an aggregation on, say, (item
name, color) from an aggregation (item name, color, clothes size), instead of from
the original relation.
ForthestandardSQLaggregatefunctions,wecancomputeanaggregatewith
groupingonasetofattributes Afromanaggregatewithgroupingonasetofat-
tributes B if A ? B;youcandosoasanexercise(seeExercise5.24),butnotethat
to compute avg, we additionally need the count value. (For some nonstandard
aggregatefunctions,suchasmedian,aggregatescannotbecomputedasabove;
theoptimizationdescribedheredoesnotapplytosuch non-decomposableaggre-
gate functions.) The amount of data read drops signi?cantly by computing an
aggregatefromanotheraggregate,insteadoffromtheoriginalrelation.Further
improvements are possible; for instance, multiple groupings can be computed
ona single scanof thedata.
EarlyOLAPimplementationsprecomputedandstoredentiredatacubes,that
is, groupingsonallsubsets ofthedimensionattributes.Precomputationallows
OLAP queries to be answered within a few seconds, even on datasets that may
contain millions of tuples adding up to gigabytes of data. However, there are
2
n
groupingswith ndimensionattributes;hierarchiesonattributesincreasethe
numberfurther.Asaresult,theentiredatacubeisoftenlargerthantheoriginal
relation that formed the data cube and in many cases it is not feasible to store
theentiredata cube.
Insteadofprecomputingandstoringallpossiblegroupings,itmakessenseto
precomputeandstoresomeofthegroupings,andtocomputeothersondemand.
Insteadofcomputingqueriesfromtheoriginalrelation,whichmaytakeavery
longtime,wecancomputethemfromotherprecomputedqueries.Forinstance,
suppose that a query requires grouping by (item name, color), and this has not
beenprecomputed.Thequeryresultcanbecomputedfromsummariesby(item
name, color, clothes size), if that has been precomputed. See the bibliographical
notesforreferencesonhowtoselectagoodsetofgroupingsforprecomputation,
given limits onthestorageavailable forprecomputedresults.
5.6 OLAP 205
5.6.3 OLAP inSQL
Several SQL implementations, such as Microsoft SQL Server, and Oracle, support
apivotclausein SQL,whichallowscreationofcross-tabs.Giventhe salesrelation
fromFigure5.16,thequery:
select *
from sales
pivot(
sum(quantity)
for colorin(’dark’,’pastel’,’white’)
)
orderby item name;
returns the cross-tab shown in Figure 5.22. Note that the for clause within the
pivot clause speci?es what values from the attribute color should appears as
attribute names in the pivot result. The attribute color itself is eliminated from
theresult,althoughallotherattributesareretained,exceptthatthevaluesforthe
newlycreatedattributesarespeci?edtocomefromtheattribute quantity.Incase
more than one tuple contributes values to a given cell, the aggregate operation
withinthepivotclausespeci?eshowthevaluesshouldbecombined.Intheabove
example,the quantityvaluesaresummedup.
Note that the pivot clause by itself does not compute the subtotals we saw
in the pivottable from Figure 5.17. However, we can ?rst generate the relational
representationshowninFigure5.21,asoutlinedshortly,andthenapplythepivot
clauseonthatrepresentationtogetanequivalentresult.Inthiscase,thevalueall
mustalsobelistedintheforclause,andtheorderbyclauseneedstobemodi?ed
toorderallattheend.
item name clothes size dark pastel white
skirt small 2 11 2
skirt medium 5 9 5
skirt large 1 15 3
dress small 2 4 2
dress medium 6 3 3
dress large 12 3 0
shirt small 2 4 17
shirt medium 6 1 1
shirt large 6 2 10
pants small 14 1 3
pants medium 6 0 0
pants large 0 1 2
Figure5.22 Result of SQL pivot operation on the sales relation of Figure 5.16.
206 Chapter 5 Advanced SQL
item name quantity
skirt 53
dress 35
shirt 49
pants 27
Figure5.23 Query result.
The data in a data cube cannot be generatedby a single SQL query, using the
basic group by constructs, since aggregates are computed for several different
groupings of the dimension attributes. For this reason, SQL includes functions to
form thegrouping neededfor OLAP.Wediscussthesebelow.
SQL supports generalizations of the group by construct to perform the cube
and rollup operations. The cube and rollup constructs in the group by clause
allow multiple group by queries to be run in a single query with the result
returnedasasinglerelationinastylesimilartothatoftherelationofFigure5.21.
Consideragain ourretailshop exampleand therelation:
sales (item name, color, clothes size, quantity)
We can ?nd the number of items sold in each item name by writing a simple
groupby query:
select item name,sum(quantity)
from sales
groupby item name;
TheresultofthisqueryisshowninFigure5.23.Notethatthisrepresentsthesame
dataasthelastcolumnofFigure5.17(orequivalently,the?rstrowinthecubeof
Figure5.18).
Similarly, we can ?nd the number of items sold in each color, etc. By using
multiple attributes in the group by clause, we can ?nd how many items were
sold with a certain set of properties. For example, we can ?nd a breakdown of
salesby item-name and color by writing:
select item name, color, sum(quantity)
from sales
groupby item name, color;
The result of this query is shown in Figure 5.24. Note that this represents the
samedataasisshowninthethe?rst4rowsand?rst4columnsofFigure5.17(or
equivalently,the ?rst 4 rows and columns in the cube of Figure5.18).
If,however,wewanttogeneratetheentiredatacubeusingthisapproach,we
would haveto writeaseparatequeryfor each of thefollowing setsof attributes:
5.6 OLAP 207
item name color quantity
skirt dark 8
skirt pastel 35
skirt white 10
dress dark 20
dress pastel 10
dress white 5
shirt dark 14
shirt pastel 7
shirt white 28
pants dark 20
pants pastel 2
pants white 5
Figure 5.24 Query result.
{ (item name, color, clothes size), (item name, color), (item name, clothes size),
(color, clothes size), (item name), (color), (clothes size), ()}
where() denotesan emptygroupbylist.
Thecube construct allows us to accomplish this in one query:
select item name, color, clothes size,sum(quantity)
from sales
groupbycube(item name, color, clothes size);
The above query producesa relationwhose schema is:
(item name, color, clothes size,sum(quantity))
So that the result of this query is indeed a relation, tuples in the result con-
tain null as the value of those attributes not present in a particular grouping.
For example, tuples produced by grouping on clothes size have a schema (clothes
size,sum(quantity)).Theyareconvertedtotupleson(item name,color, clothes size,
sum(quantity)) by inserting null for item name and color.
Datacuberelationsareoftenverylarge.Thecubequeryabove,with3possible
colors,4possibleitemnames,and3sizes,has80tuples.TherelationofFigure5.21
isgeneratedusinggroupingby item nameand color.Italsousesallinplaceofnull
soastobemorereadabletotheaverageuser.Togeneratethatrelationin SQL,we
arrange tosubstituteall for null.Thequery:
select item name, color,sum(quantity)
from sales
groupbycube(item name, color);
208 Chapter 5 Advanced SQL
THEDECODE FUNCTION
Thedecodefunctionallowssubstitutionofvaluesinanattributeofatuple.The
generalformofdecode is:
decode (value, match-1, replacement-1, match-2, replacement-2,...,
match-N, replacement-N, default-replacement);
Itcomparesvalueagainstthematchvaluesandifamatchisfound,itreplacesthe
attributevaluewiththecorrespondingreplacementvalue.Ifnomatchsucceeds,
thentheattributevalueis replacedwith thedefault replacementvalue.
Thedecodefunctiondoesnotworkaswemightlikefornullvaluesbecause,
as we saw in Section 3.6, predicates on nulls evaluate to unknown,which
ultimately becomes false.T odealwiththis,weapplythegrouping function,
whichreturns1ifitsargumentisanullvaluegeneratedbyacubeorrollupand
0 otherwise. Then the relation in Figure 5.21, with occurrences of all replaced
by null, canbecomputedby thequery:
select decode(grouping(item name), 1, ’all’, item name)as item name
decode(grouping(color), 1, ’all’, color)as color
sum(quantity) as quantity
from sales
group by cube(item name, color);
generatestherelationofFigure5.21withnulls.Thesubstitutionofallisachieved
using the SQL decodeand groupingfunctions. Thedecodefunction isconceptu-
allysimplebut itssyntaxis somewhat hardto read.Seeblue box fordetails.
The rollup construct is the same as the cube construct except that rollup
generates fewer group by queries. We saw that group by cube (item name, color,
clothes size) generated all 8 ways of forming a group by query using some (or all
or none) of the attributes. In:
select item name, color, clothes size, sum(quantity)
from sales
groupbyrollup(item name, color, clothes size);
groupbyrollup(item name, color, clothes size) generatesonly4 groupings:
{ (item name, color, clothes size), (item name, color), (item name), () }
Noticethattheorderoftheattributesintherollupmakesadifference;the?nal
attribute (clothes size, in our example) appears in only one grouping, the penul-
timate (second last) attribute in 2 groupings, and so on, with the ?rst attribute
appearingin allgroupsbut one (the emptygrouping).
5.7 Summary 209
Why might we want the speci?c groupings that are used in rollup?These
groups are of frequent practical interest for hierarchies (as in Figure 5.19, for
example). For the location hierarchy (Region, Country, State, City), we may want
togroupbyRegiontogetsalesbyregion.Thenwemaywantto“drilldown”tothe
level of countries within each region, which means we would group by Region,
Country. Drilling down further, we may wish to group byRegion,Country,State
andthenby Region, Country, State, City.Therollupconstructallowsustospecify
this sequenceof drillingdown forfurther detail.
Multiple rollupsandcubes can be used in a single group by clause. For
instance, the following query:
select item name, color, clothes size, sum(quantity)
from sales
groupbyrollup(item name),rollup(color, clothes size);
generatesthe groupings:
{ (item name, color, clothes size), (item name, color), (item name),
(color, clothes size), (color), () }
Tounderstandwhy,observethatrollup(item name)generatestwogroupings,
{(item name), ()},androllup(color, clothes size) generates three groupings, {(color,
clothes size),(color),()}.TheCartesianproductofthetwogivesusthesixgroupings
shown.
Neither the rollup nor the cube clause gives complete control on the group-
ings that are generated. For instance, we cannot use them to specify that we
wantonlygroupings{(color, clothes size),(clothes size, item name)}.Suchrestricted
groupingscanbegeneratedbyusingthegroupingconstructinthehavingclause;
we leavethedetailsas anexerciseforyou.
5.7 Summary
• SQLqueriescanbeinvokedfromhostlanguages,viaembeddedanddynamic
SQL.TheODBC and JDBC standards de?ne application program interfaces
to access SQL databases from C and Java language programs. Increasingly,
programmersusethese APIs toaccess databases.
• Functions and procedures can be de?ned using SQLprocedural extensions
that allow iterationand conditional (if-then-else)statements.
• Triggers de?ne actions to be executed automatically when certain events
occur and corresponding conditions are satis?ed. Triggers have many uses,
such as implementing business rules, audit logging, and even carrying out
actionsoutsidethedatabasesystem.Althoughtriggerswerenotaddedtothe
210 Chapter 5 Advanced SQL
SQL standard until SQL:1999, most database systems have long implemented
triggers.
• Some queries, such as transitive closure, can be expressed either by using
iterationorbyusingrecursiveSQLqueries.Recursioncanbeexpressedusing
eitherrecursiveviewsor recursivewith clause de?nitions.
• SQL supports several advanced aggregation features, including ranking and
windowingqueriesthatsimplifytheexpressionofsomeaggregatesandallow
moreef?cientevaluation.
• Online analytical processing (OLAP)toolshelpanalystsviewdatasumma-
rized in different ways, so that they can gain insight into the functioning of
an organization.
?
OLAP tools work on multidimensional data, characterized by dimension
attributesand measureattributes.
?
Thedatacubeconsistsofmultidimensionaldatasummarizedindifferent
ways. Precomputing thedata cube helpsspeedupquerieson summaries
of data.
?
Cross-tab displays permit users to view two dimensions of multidimen-
sional dataat a time,along with summariesof thedata.
?
Drilldown,rollup,slicing,anddicingareamongtheoperationsthatusers
performwith OLAP tools.
• SQL, starting with the SQL:1999 standard, provides a variety of operators for
data analysis, including cube and rollup operations. Some systems support
apivotclause, which allows easycreationof cross-tabs.
ReviewTerms
• JDBC
• ODBC
• Preparedstatements
• Accessingmetadata
• SQL injection
• Embedded SQL
• Cursors
• Updatablecursors
• Dynamic SQL
• SQL functions
• Storedprocedures
• Procedural constructs
• Externallanguage routines
• Trigger
• Beforeandaftertriggers
• Transition variablesand tables
• Recursivequeries
• Monotonic queries
• Ranking functions
?
Rank
?
Dense rank
?
Partitionby
• Windowing
PracticeExercises 211
• Online analytical processing
(OLAP)
• Multidimensionaldata
?
Measureattributes
?
Dimensionattributes
?
Pivoting
?
Datacube
?
Slicingand dicing
?
Rollupand drilldown
• Cross-tabulation
PracticeExercises
5.1 Describe the circumstances in which you would choose to use embedded
SQL rather than SQL alone or only a general-purpose programming lan-
guage.
5.2 WriteaJavafunctionusing JDBCmetadatafeaturesthattakesaResultSet
as an input parameter, and prints out the result in tabular form, with
appropriatenames ascolumn headings.
5.3 Write a Java function using JDBC metadata features that prints a list of all
relationsinthedatabase,displayingforeachrelationthenamesandtypes
of itsattributes.
5.4 Show how to enforce the constraint “an instructor cannot teach in two
different classrooms in a semester in the same time slot.” using a trigger
(remember that the constraint can be violated by changes to the teaches
relationaswellas to the section relation).
5.5 Write triggers to enforce the referential integrity constraint from section to
time slot,onupdatestosection,andtime slot. Note that the ones we wrote
inFigure5.8 donot covertheupdateoperation.
5.6 To maintain the tot cred attribute of the student relation, carry out the fol-
lowing:
a. Modifythetriggeronupdatesof takes,tohandleallupdatesthatcan
affectthe valueof tot cred.
b. Writeatriggertohandle insertstothe takes relation.
c. Underwhatassumptionsisitreasonablenottocreatetriggersonthe
course relation?
5.7 ConsiderthebankdatabaseofFigure5.25.Letusde?neaviewbranch cust
asfollows:
create view branch custas
select branch name, customer name
from depositor, account
where depositor.account number= account.account number
212 Chapter 5 Advanced SQL
branch(branch name, branch city, assets)
customer(customer name, customer street, cust omer city)
loan (loan number, branch name, amount)
borrower(customer name, loan number)
account (account number, branch name, balance )
depositor (customer name, account number)
Figure5.25 Banking database for Exercises 5.7, 5.8, and 5.28 .
Suppose that the view is materialized; that is, the view is computed and
stored. Write triggers to maintain the view, that is, to keep it up-to-date on
insertions to and deletions from depositor or account. Do not bother about
updates.
5.8 Consider the bank database of Figure 5.25. Write an SQL trigger to carry
out the following action: On delete of an account, for each owner of the
account, check if the owner has any remaining accounts, and if she does
not, deleteherfrom the depositor relation.
5.9 Show how to express group by cube(a,b,c,d)usingrollup;youranswer
should have only onegroupbyclause.
5.10 Givena relation S(student,subject,marks),writeaqueryto?ndthetopn
studentsby totalmarks, by using ranking.
5.11 ConsiderthesalesrelationfromSection5.6.Writean SQLquerytocompute
the cube operation on the relation, giving the relation in Figure 5.21. Do
not use the cubeconstruct.
Exercises
5.12 Considerthe following relationsfora company database:
• emp(ename, dname, salary)
• mgr(ename, mname)
and the Java code in Figure 5.26, which uses the JDBC API.Assumethat
the userid, password, machine name, etc. are all okay. Describe in concise
English what the Java program does. (That is, produce an English sen-
tence like “It ?nds the manager of the toy department,” not a line-by-line
descriptionof what eachJavastatementdoes.)
5.13 Suppose you were asked to de?ne a classMetaDisplay in Java, containing
amethodstatic void printTable(String r); the method takesarelationname
r as input, executes the query “select * from r”, and prints the result out
innicetabularformat,withtheattributenamesdisplayedintheheaderof
the table.
Exercises 213
import java.sql.*;
public class Mystery {
public static void main(String[] args) {
try {
Connection con=null;
Class.forName("oracle.jdbc.driver.OracleDriver");
con=DriverManager.getConnection(
"jdbc:oracle:thin:star/X@//edgar.cse.lehigh.edu:1521/XE");
Statement s=con.createStatement();
String q;
String empName = "dog";
boolean more;
ResultSet result;
do {
q = "select mname from mgr where ename = ’" + empName + "’";
result = s.executeQuery(q);
more = result.next();
if (more) {
empName = result.getString("mname");
System.out.println (empName);
}
} while (more);
s.close();
con.close();
} catch(Exceptione){e.printStackTrace();}}}
Figure5.26 Java code for Exercise 5.12.
a. What do you need to know about relation r to be able to print the
resultin thespeci?edtabular format.
b. What JDBCmethods(s)can getyou therequiredinformation?
c. Write the methodprintTable(String r) using the JDBC API.
5.14 Repeat Exercise 5.13 using ODBC, de?ning void printTable(char *r) as a
function insteadof a method.
5.15 Consideranemployeedatabasewithtwo relations
employee (employee name, street, city)
works (employee name, company name, salary)
where the primary keys are underlined. Write a query to ?nd companies
whoseemployeesearnahighersalary,onaverage,thantheaveragesalary
at “First BankCorporation”.
214 Chapter 5 Advanced SQL
a. Using SQL functions asappropriate.
b. Without using SQL functions.
5.16 Rewrite the query in Section 5.2.1 that returns the name and budget of all
departments with more than 12 instructors, using the with clause instead
of using afunction call.
5.17 ComparetheuseofembeddedSQLwiththeuseinSQLoffunctionsde?ned
in a general-purpose programming language. Under what circumstances
would you use each ofthese features?
5.18 ModifytherecursivequeryinFigure5.15 to de?nearelation
prereq depth(course id, prereq id, depth)
where the attribute depth indicates how many levels of intermediate pre-
requisitesaretherebetweenthecourseandtheprerequisite.Directprereq-
uisiteshave adepthof 0.
5.19 Considerthe relationalschema
part(part id, name, cost)
subpart(part id, subpart id, count)
Atuple(p
1
, p
2
,3) in the subpart relation denotes that the part with part-id
p
2
is a directsubpart of the part with part-id p
1
,andp
1
has 3 copies of p
2
.
Note that p
2
may itself have further subparts. Write a recursive SQL query
that outputsthe namesof allsubparts of the partwith part-id “P-100”.
5.20 Consider again the relational schema from Exercise 5.19. Write a JDBC
function using non-recursive SQL to ?nd the total cost of part “P-100”,
including the costs of all its subparts. Be sure to take into account the
fact that a part may have multiple occurrences of a subpart. You may use
recursionin Java ifyou wish.
5.21 Suppose there are two relations r and s, such that the foreign key B of r
referencestheprimarykey Aofs.Describehowthetriggermechanismcan
beusedtoimplementtheondeletecascadeoption,whenatupleisdeleted
from s.
5.22 The execution of a trigger can cause another action to be triggered. Most
database systems place a limit on how deep the nesting can be. Explain
why they mightplace such a limit.
5.23 Considertherelation,r,showninFigure5.27.Givetheresultofthefollow-
ing query:
Exercises 215
building room number time slot id course id sec id
Gar?eld 359 A BIO-101 1
Gar?eld 359 B BIO-101 2
Saucon 651 A CS-101 2
Saucon 550 C CS-319 1
Painter 705 D MU-199 1
Painter 403 D FIN-201 1
Figure5.27 The relation r for Exercise 5.23.
select building, room number, time slot id, count(*)
fromr
groupbyrollup(building, room number, time slot id)
5.24 For each of the SQL aggregate functions sum, count, min,andmax,show
howtocomputetheaggregatevalueonamultiset S
1
?S
2
,giventheaggre-
gatevalueson multisets S
1
and S
2
.
Onthebasisoftheabove,giveexpressionstocomputeaggregatevalues
withgroupingon asubset Softheattributesofa relationr(A, B,C, D, E),
givenaggregatevaluesforgroupingonattributes T ? S,forthefollowing
aggregatefunctions:
a. sum,count,min, and max
b. avg
c. Standarddeviation
5.25 In Section 5.5.1, we used the student grades view of Exercise 4.5 to write
a query to ?nd the rank of each student based on grade-point average.
Modifythatquerytoshowonlythetop10students(thatis,thosestudents
whose rank is1 through 10).
5.26 Give an example of a pair of groupings that cannot be expressed by using
asinglegroupbyclause withcube androllup.
5.27 Given relation s(a,b,c), show how to use the extended SQL features to
generateahistogramofc versusa,dividinga into20equal-sizedpartitions
(thatis,whereeachpartitioncontains5percentofthetuplesins,sortedby
a).
5.28 Consider the bank database of Figure 5.25 and the balance attribute of the
account relation. Write an SQL query to compute a histogram of balance
values,dividingtherange0tothemaximumaccountbalancepresent,into
threeequalranges.
216 Chapter 5 Advanced SQL
Tools
Most database vendors provide OLAP tools as part of their database systems, or
as add-on applications. These include OLAP tools from Microsoft Corp., Oracle
Express,andInformixMetacube.Toolsmaybeintegratedwithalarger“business
intelligence”productsuchasIBMCognos.Manycompaniesalsoprovideanalysis
tools for speci?c applications, such as customer relationship management (for
example,OracleSiebel CRM).
BibliographicalNotes
SeethebibliographicnotesofChapter3forreferencestoSQLstandardsandbooks
on SQL.
An excellent source for more (and up-to-date) information on JDBC,andon
Java ingeneral,is java.sun.com/docs/books/tutorial.Referencesto books on Java (in-
cludingJDBC)arealsoavailableatthisURL.TheODBCAPIisdescribedinMicrosoft
[1997]andSanders[1998].MeltonandEisenberg[2000]providesaguideto SQLJ,
JDBC, and related technologies. More information on ODBC, ADO,andADO.NET
can be found on msdn.microsoft.com/data.
In the context of functions and procedures in SQL, many database products
support features beyond those speci?ed in the standards, and do not support
manyofthefeaturesofthestandard.Moreinformationonthesefeaturesmaybe
found in the SQL usermanuals of the respectiveproducts.
Theoriginal SQLproposalsforassertionsandtriggersarediscussedinAstra-
hanetal.[1976],Chamberlinetal.[1976],andChamberlinetal.[1981].Meltonand
Simon [2001], Melton [2002], and Eisenberg and Melton [1999] provide textbook
coverageof SQL:1999, the versionofthe SQL standardthat?rst includedtriggers.
Recursivequeryprocessingwas?rststudiedindetailinthecontextofaquery
language called Datalog, which was based on mathematical logic and followed
thesyntaxofthelogicprogramminglanguageProlog.RamakrishnanandUllman
[1995] provides a survey of results in this area, including techniques to optimize
queriesthat selecta subset oftuplesfroma recursivelyde?nedview.
Gray et al. [1995] and Gray et al. [1997] describe the data-cube operator.
Ef?cient algorithms for computing data cubes are described by Agarwal et al.
[1996], Harinarayan etal.[1996], and Ross and Srivastava[1997]. Descriptionsof
extended aggregation support in SQL:1999canbefoundintheproductmanuals
of database systemssuch as Oracle and IBM DB2.
Therehasbeenasubstantialamountofresearchontheef?cientprocessingof
“top-k” queries that return only the top-k-ranked results. A survey of that work
appearsinIlyasetal.[2008].
CHAPTER
6
Formal Relational Query
Languages
In Chapters 2 through 5 we introduced the relational model and covered SQL in
greatdetail.Inthischapterwepresenttheformalmodeluponwhich SQLaswell
asotherrelationalquerylanguagesarebased.
Wecoverthreeformallanguages.Westartbypresentingtherelationalalgebra,
whichformsthebasisofthewidelyused SQLquerylanguage.Wethencoverthe
tuplerelationalcalculusandthedomainrelationalcalculus,whicharedeclarative
querylanguagesbasedonmathematicallogic.
6.1 The Relational Algebra
The relational algebra is a procedural query language. It consists of a set of op-
erations that take one or two relations as input and produce a new relation as
theirresult.Thefundamentaloperationsintherelationalalgebraareselect,project,
union, set difference, Cartesian product,and rename.Inadditiontothefundamental
operations, there are several other operations—namely, set intersection, natural
join,andassignment.Weshallde?netheseoperationsintermsofthefundamental
operations.
6.1.1 Fundamental Operations
The select, project, and rename operations are called unary operations, because
they operate on one relation. The other three operations operate on pairs of
relationsandare,therefore,calledbinaryoperations.
6.1.1.1 TheSelectOperation
The select operation selects tuples that satisfy a given predicate. We use the
lowercase Greek letter sigma (  ) to denote selection. The predicateappears as a
subscriptto  .Theargumentrelationisinparenthesesafterthe  .Thus,toselect
217
218 Chapter6 FormalRelationalQueryLanguages
ID name dept name salary
10101 Srinivasan Comp.Sci. 65000
12121 Wu Finance 90000
15151 Mozart Music 40000
22222 Einstein Physics 95000
32343 ElSaid History 60000
33456 Gold Physics 87000
45565 Katz Comp.Sci. 75000
58583 Cali?eri History 62000
76543 Singh Finance 80000
76766 Crick Biology 72000
83821 Brandt Comp.Sci. 92000
98345 Kim Elec.Eng. 80000
Figure 6.1 The instructor relation.
those tuples of the instructor relation where the instructor is in the “Physics”
department,wewrite:
  dept name =“Physics”
(instructor)
IftheinstructorrelationisasshowninFigure6.1,thentherelationthatresults
fromtheprecedingqueryisasshowninFigure6.2.
Wecan?ndallinstructorswithsalarygreaterthan$90,000bywriting:
  salary>90000
(instructor)
Ingeneral,weallowcomparisonsusing =, null, <, ?, >,and?intheselection
predicate.Furthermore,wecancombineseveralpredicatesintoalargerpredicate
byusingtheconnectives and(?), or(?),and not(¬).Thus,to?ndtheinstructors
inPhysicswithasalarygreaterthan$90,000,wewrite:
  dept name =“Physics”?salary>90000
(instructor)
Theselectionpredicatemayincludecomparisonsbetweentwoattributes.To
illustrate, consider the relation department. To ?nd all departments whose name
isthesameastheirbuildingname,wecanwrite:
  dept name =building
(department)
ID name dept name salary
22222 Einstein Physics 95000
33456 Gold Physics 87000
Figure 6.2 Result of   dept name =“Physics”
(instructor).
6.1 TheRelationalAlgebra 219
SQLVERSUSRELATIONALALGEBRA
Thetermselectinrelationalalgebrahasadifferentmeaningthantheoneusedin
SQL,whichisanunfortunatehistoricalfact.Inrelationalalgebra,thetermselect
corresponds to what we refer to in SQL as where. We emphasize the different
interpretationsheretominimizepotentialconfusion.
6.1.1.2 TheProjectOperation
Suppose we want to list all instructors’ ID, name,andsalary,butdonotcare
about the dept name.Theproject operation allows us to produce this relation.
The project operation is a unary operation that returns its argument relation,
with certain attributes left out. Since a relation is a set, any duplicate rows are
eliminated. Projection is denoted by the uppercase Greek letter pi (null). We list
those attributes that we wish to appear in the result as a subscript to null.The
argument relationfollows inparentheses.Wewrite the queryto produce such a
listas:
null
ID, name, salary
(instructor)
Figure6.3showstherelationthatresultsfromthisquery.
6.1.1.3 CompositionofRelationalOperations
The fact that the result of a relational operation is itself a relation is important.
Consider the more complicated query “Find the name of all instructors in the
Physicsdepartment.” We write:
ID name salary
10101 Srinivasan 65000
12121 Wu 90000
15151 Mozart 40000
22222 Einstein 95000
32343 ElSaid 60000
33456 Gold 87000
45565 Katz 75000
58583 Cali?eri 62000
76543 Singh 80000
76766 Crick 72000
83821 Brandt 92000
98345 Kim 80000
Figure 6.3 Result of null
ID, name, salary
(instructor).
220 Chapter6 FormalRelationalQueryLanguages
null
name
(  dept name =“Physics”
(instructor))
Notice that, instead of giving the name of a relation as the argument of the
projectionoperation,wegiveanexpressionthatevaluatestoarelation.
Ingeneral,sincetheresultofarelational-algebraoperationisofthesametype
(relation) as its inputs, relational-algebra operations can be composed together
into a relational-algebra expression. Composing relational-algebra operations
into relational-algebra expressions is just like composing arithmetic operations
(suchas+,?,?,and÷)intoarithmeticexpressions.Westudytheformalde?nition
ofrelational-algebraexpressionsinSection6.1.2.
6.1.1.4 TheUnionOperation
Consideraqueryto?ndthesetofallcoursestaughtintheFall2009semester,the
Spring2010semester,orboth.Theinformationiscontainedinthesectionrelation
(Figure 6.4). To ?nd the set of all courses taught in the Fall 2009 semester, we
write:
null
course id
(  semester =“Fall”?year=2009
(section))
To?ndthesetofallcoursestaughtintheSpring2010semester,wewrite:
null
course id
(  semester =“Spring” ?year=2010
(section))
Toanswerthequery,weneedtheunionofthesetwosets;thatis,weneedall
section IDs that appearin eitheror both of the two relations.We ?nd thesedata
course id sec id semester year building room number time slot id
BIO-101 1 Summer 2009 Painter 514 B
BIO-301 1 Summer 2010 Painter 514 A
CS-101 1 Fall 2009 Packard 101 H
CS-101 1 Spring 2010 Packard 101 F
CS-190 1 Spring 2009 Taylor 3128 E
CS-190 2 Spring 2009 Taylor 3128 A
CS-315 1 Spring 2010 Watson 120 D
CS-319 1 Spring 2010 Watson 100 B
CS-319 2 Spring 2010 Taylor 3128 C
CS-347 1 Fall 2009 Taylor 3128 A
EE-181 1 Spring 2009 Taylor 3128 C
FIN-201 1 Spring 2010 Packard 101 B
HIS-351 1 Spring 2010 Painter 514 C
MU-199 1 Spring 2010 Packard 101 D
PHY-101 1 Fall 2009 Watson 100 A
Figure 6.4 The section relation.
6.1 TheRelationalAlgebra 221
course id
CS-101
CS-315
CS-319
CS-347
FIN-201
HIS-351
MU-199
PHY-101
Figure 6.5 Courses offered in either Fall 2009, Spring 2010 or both semesters.
by thebinary operationunion, denoted,as insettheory,by ?.Sotheexpression
neededis:
null
course id
(  semester =“Fall” ?year=2009
(section)) ?
null
course id
(  semester =“Spring” ?year=2010
(section))
TheresultrelationforthisqueryappearsinFigure6.5.Noticethatthereare8
tuplesintheresult,eventhoughthereare3distinctcoursesofferedintheFall2009
semesterand6distinctcoursesofferedintheSpring2010semester.Sincerelations
are sets, duplicate values such as CS-101, which is offered in both semesters,are
replacedbyasingleoccurrence.
Observe that, in our example, we took the union of two sets, both of which
consisted of course id values. In general, we must ensure that unions are taken
between compatible relations. For example, it would not make sense to take the
union of the instructor relation and the student relation. Although both relations
have four attributes,they differon the salary and tot cred domains. The union of
these two attributes would not make sense in most situations. Therefore, for a
unionoperationr ?stobevalid,werequirethattwoconditionshold:
1. The relations rand s mustbe of thesamearity.That is,theymust havethe
samenumberofattributes.
2. The domains of the ith attribute of r and the ith attribute of s must be the
same,foralli.
Notethat rand scanbeeitherdatabaserelationsortemporaryrelationsthatare
theresultofrelational-algebraexpressions.
6.1.1.5 TheSet-DifferenceOperation
The set-difference operation, denoted by ?, allows us to ?nd tuples that are
in one relation but are not in another. The expression r ? s produces a relation
containingthosetuplesin rbutnotins.
222 Chapter6 FormalRelationalQueryLanguages
course id
CS-347
PHY-101
Figure 6.6 Courses offered in the Fall 2009 semester but not in Spring 2010 semester.
Wecan?ndallthecoursestaughtintheFall2009semesterbutnotinSpring
2010semesterbywriting:
null
course id
(  semester =“Fall”?year=2009
(section)) -
null
course id
(  semester =“Spring” ?year=2010
(section))
TheresultrelationforthisqueryappearsinFigure6.6.
As with the union operation, we must ensure that set differences are taken
between compatible relations.Therefore,for aset-differenceoperationr ?s to be
valid, we require that the relations r and s be of the same arity, and that the
domainsoftheithattributeof randthe ithattributeof sbethesame,foralli.
6.1.1.6 TheCartesian-ProductOperation
The Cartesian-product operation, denoted by a cross (×), allows us to combine
information from any two relations. We write the Cartesian product of relations
r
1
andr
2
asr
1
×r
2
.
Recall that a relation is by de?nition a subset of a Cartesian product of a set
ofdomains.Fromthatde?nition,weshouldalreadyhaveanintuitionaboutthe
de?nitionoftheCartesian-productoperation.However,sincethesameattribute
name may appear in both r
1
and r
2
, we need to devise a naming schema to
distinguish between these attributes. We do so here by attaching to an attribute
the name of the relation from which the attribute originally came. For example,
therelationschemaforr = instructor ×teachesis:
(instructor.ID,instructor.name,instructor.dept name,instructor.salary
teaches.ID,teaches.course id, teaches.sec id,teaches.semester,teaches.year)
With this schema, we can distinguish instructor.ID from teaches.ID. For those at-
tributes that appear in only one of the two schemas, we shall usually drop the
relation-namepre?x.Thissimpli?cationdoesnotleadtoanyambiguity.Wecan
thenwritetherelationschemafor ras:
(instructor.ID, name,dept name,salary
teaches.ID,course id, sec id,semester,year)
This naming convention requires that the relations that are the arguments of
the Cartesian-product operation have distinct names. This requirement causes
problems in some cases, such as when the Cartesian product of a relation with
itselfisdesired.Asimilarproblemarisesifweusetheresultofarelational-algebra
expressioninaCartesianproduct,sinceweshallneedanamefortherelationso
6.1 TheRelationalAlgebra 223
ID course id sec id semester year
10101 CS-101 1 Fall 2009
10101 CS-315 1 Spring 2010
10101 CS-347 1 Fall 2009
12121 FIN-201 1 Spring 2010
15151 MU-199 1 Spring 2010
22222 PHY-101 1 Fall 2009
32343 HIS-351 1 Spring 2010
45565 CS-101 1 Spring 2010
45565 CS-319 1 Spring 2010
76766 BIO-101 1 Summer 2009
76766 BIO-301 1 Summer 2010
83821 CS-190 1 Spring 2009
83821 CS-190 2 Spring 2009
83821 CS-319 2 Spring 2010
98345 EE-181 1 Spring 2009
Figure 6.7 The teaches relation.
thatwecanrefertotherelation’sattributes.InSection6.1.1.7,weseehowtoavoid
theseproblemsbyusingtherenameoperation.
Nowthatweknowtherelationschemaforr =instructor ×teaches,whattuples
appear in r? As you may suspect, we construct a tuple of r out of each possible
pairoftuples:onefromtheinstructorrelation(Figure6.1)andonefromtheteaches
relation (Figure 6.7). Thus, r is a large relation, as you can see from Figure 6.8,
whichincludesonlyaportionofthetuplesthatmakeup r.
1
Assume that we have n
1
tuples in instructor and n
2
tuples in teaches. Then,
therearen
1
?n
2
waysofchoosingapairoftuples—onetuplefromeachrelation;
sotherearen
1
?n
2
tuplesinr.Inparticular,notethatforsometuplestinr,itmay
bethatt[instructor.ID] null t[teaches.ID].
In general, if we have relations r
1
(R
1
)andr
2
(R
2
), then r
1
× r
2
is a relation
whose schema isthe concatenation of R
1
and R
2
.RelationRcontains alltuples t
forwhichthereisatuplet
1
inr
1
andatuplet
2
inr
2
forwhich t[R
1
] = t
1
[R
1
]and
t[R
2
] = t
2
[R
2
].
Suppose that we want to ?nd the names of all instructors in the Physics
department together with the course id of all courses they taught. We need the
information inboth the instructor relation and the teaches relationto do so. If we
write:
  dept name =“Physics”
(instructor × teaches)
thentheresultistherelationinFigure6.9.
1
Notethatwerenamed instructor.IDas inst.IDtoreducethewidthofthetablesinFigures6.8and6.9.
224 Chapter6 FormalRelationalQueryLanguages
inst.ID name dept name salary teaches.ID course id sec id semester year
10101 Srinivasan Physics 95000 10101 CS-101 1 Fall 2009
10101 Srinivasan Physics 95000 10101 CS-315 1 Spring 2010
10101 Srinivasan Physics 95000 10101 CS-347 1 Fall 2009
10101 Srinivasan Physics 95000 10101 FIN-201 1 Spring 2010
10101 Srinivasan Physics 95000 15151 MU-199 1 Spring 2010
10101 Srinivasan Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
12121 Wu Physics 95000 10101 CS-101 1 Fall 2009
12121 Wu Physics 95000 10101 CS-315 1 Spring 2010
12121 Wu Physics 95000 10101 CS-347 1 Fall 2009
12121 Wu Physics 95000 10101 FIN-201 1 Spring 2010
12121 Wu Physics 95000 15151 MU-199 1 Spring 2010
12121 Wu Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
15151 Mozart Physics 95000 10101 CS-101 1 Fall 2009
15151 Mozart Physics 95000 10101 CS-315 1 Spring 2010
15151 Mozart Physics 95000 10101 CS-347 1 Fall 2009
15151 Mozart Physics 95000 10101 FIN-201 1 Spring 2010
15151 Mozart Physics 95000 15151 MU-199 1 Spring 2010
15151 Mozart Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
22222 Einstein Physics 95000 10101 CS-101 1 Fall 2009
22222 Einstein Physics 95000 10101 CS-315 1 Spring 2010
22222 Einstein Physics 95000 10101 CS-347 1 Fall 2009
22222 Einstein Physics 95000 10101 FIN-201 1 Spring 2010
22222 Einstein Physics 95000 15151 MU-199 1 Spring 2010
22222 Einstein Physics 95000 22222 PHY-101 1 Fall 2009
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
Figure 6.8 Result of instructor × teaches.
WehavearelationthatpertainsonlytoinstructorsinthePhysicsdepartment.
However,the course idcolumnmaycontaininformationaboutcoursesthatwere
not taught by the corresponding instructor. (If you do not see why that is true,
recall that the Cartesian product takes all possible pairings of one tuple from
instructorwithonetupleof teaches.)
SincetheCartesian-productoperationassociateseverytupleofinstructorwith
everytupleofteaches,weknowthatifaaninstructorisinthePhysicsdepartment,
and has taught a course (as recorded in the teaches relation), then there is some
6.1 TheRelationalAlgebra 225
inst.ID name dept name salary teaches.ID course id sec id semester year
22222 Einstein Physics 95000 10101 CS-437 1 Fall 2009
22222 Einstein Physics 95000 10101 CS-315 1 Spring 2010
22222 Einstein Physics 95000 12121 FIN-201 1 Spring 2010
22222 Einstein Physics 95000 15151 MU-199 1 Spring 2010
22222 Einstein Physics 95000 22222 PHY-101 1 Fall 2009
22222 Einstein Physics 95000 32343 HIS-351 1 Spring 2010
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
33456 Gold Physics 87000 10101 CS-437 1 Fall 2009
33456 Gold Physics 87000 10101 CS-315 1 Spring 2010
33456 Gold Physics 87000 12121 FIN-201 1 Spring 2010
33456 Gold Physics 87000 15151 MU-199 1 Spring 2010
33456 Gold Physics 87000 22222 PHY-101 1 Fall 2009
33456 Gold Physics 87000 32343 HIS-351 1 Spring 2010
... ... ... ... ... ... ... ... ...
... ... ... ... ... ... ... ... ...
Figure 6.9 Result of   dept name =“Physics”
(instructor × teaches).
tuplein  dept name =“Physics”
(instructor ×teaches)thatcontainshisname,andwhich
satis?esinstructor.ID=teaches.ID.So,ifwewrite:
  instructor.ID =teaches.ID
(  dept name =“Physics”
(instructor × teaches))
we get only those tuples of instructor × teaches that pertain to instructors in
Physicsandthecoursesthattheytaught.
Finally,sinceweonlywantthenamesofallinstructorsinthePhysicsdepart-
menttogetherwiththe course idofallcoursestheytaught,wedoaprojection:
null
name, course id
(  instructor.ID =teaches.ID
(  dept name =“Physics”
(instructor × teaches)))
The result of this expression, shown in Figure 6.10, is the correct answer to our
query. Observe that although instructor Gold is in the Physics department, he
doesnotteachanycourse(asrecordedintheteachesrelation),andthereforedoes
notappearintheresult.
name course id
Einstein PHY-101
Figure 6.10 Result of
null
name, course id
(  instructor.ID =teaches.ID
(  dept name =“Physics”
(instructor × teaches))).
226 Chapter6 FormalRelationalQueryLanguages
Note that there is often more than one way to write a query in relational
algebra.Considerthefollowingquery:
null
name, course id
(  instructor.ID =teaches.ID
((  dept name =“Physics”
(instructor)) × teaches))
Notethesubtledifferencebetweenthetwoqueries:inthequeryabove,theselec-
tion that restricts dept name to Physics is applied to instructor, and the Cartesian
product is appliedsubsequently; in contrast, the Cartesianproduct was applied
beforetheselectionintheearlierquery.However,thetwoqueriesareequivalent;
thatis,theygivethesameresultonanydatabase.
6.1.1.7 TheRenameOperation
Unlike relations in the database, the results of relational-algebra expressions do
not have a name that we can use to refer to them. It is useful to be able to give
themnames;therenameoperator,denotedbythelowercaseGreekletterrho(  ),
letsusdothis.Givenarelational-algebraexpression E,theexpression
  x
(E)
returnstheresultofexpressionEunderthename x.
Arelationr by itself is considered a (trivial) relational-algebra expression.
Thus, we can also apply the rename operation to a relation r to get the same
relationundera new name.
Asecondformoftherenameoperationisasfollows:Assumethatarelational-
algebraexpression E hasarityn.Then,theexpression
  x(A
1
,A
2
,...,A
n
)
(E)
returns the result of expression E under the name x, and with the attributes
renamed to A
1
, A
2
,...,A
n
.
To illustrate renaming a relation, we consider the query “Find the highest
salaryintheuniversity.”Ourstrategyisto(1)compute?rstatemporaryrelation
consisting of those salaries that are not the largest and (2) take the set difference
betweentherelationnull
salary
(instructor)andthetemporaryrelationjustcomputed,
toobtaintheresult.
1. Step1:Tocompute thetemporaryrelation,weneedtocomparethe values
of all salaries. We do this comparison by computing the Cartesianproduct
instructor × instructor andformingaselectiontocomparethevalueofany
two salaries appearing in one tuple. First, we need to devise a mechanism
to distinguish between the two salary attributes. We shall use the rename
operation to rename one reference to the instructor relation; thus we can
referencetherelationtwicewithoutambiguity.
6.1 TheRelationalAlgebra 227
salary
65000
90000
40000
60000
87000
75000
62000
72000
80000
92000
Figure 6.11 Result of the subexpression
null
instructor.salary
(  instructor.salary < d.salary
(instructor ×   d
(instructor))).
We can now write the temporary relation that consists of the salaries that
arenotthelargest:
null
instructor.salary
(  instructor.salary < d.salary
(instructor ×   d
(instructor)))
This expression gives those salaries in the instructor relation for which a
larger salary appears somewhere in the instructor relation (renamed as d).
Theresultcontainsallsalariesexceptthelargestone.Figure6.11showsthis
relation.
2. Step2: The queryto ?nd the largestsalary inthe universitycan be written
as:
null
salary
(instructor) ?
null
instructor.salary
(  instructor.salary < d.salary
(instructor ×   d
(instructor)))
Figure6.12showstheresultofthisquery.
The rename operation is not strictly required, since it is possible to use a
positionalnotationforattributes.Wecannameattributesofarelationimplicitlyby
usingapositionalnotation,where$1,$2,...refertothe?rstattribute,thesecond
attribute, and so on. The positional notation also applies to results of relational-
algebra operations. The following relational-algebra expression illustrates the
salary
95000
Figure 6.12 Highest salary in the university.
228 Chapter6 FormalRelationalQueryLanguages
useofpositionalnotationtowritetheexpressionwesawearlier,whichcomputes
salariesthatarenotthelargest:
null
$4
(  $4 < $8
(instructor × instructor))
Note that the Cartesian product concatenates the attributes of the two relations.
Thus, for the result of the Cartesianproduct (instructor × instructor), $4 refersto
the salary attribute from the ?rst occurrence of instructor, while $8 refers to the
salaryattributefromthesecondoccurrenceofinstructor.Apositionalnotationcan
alsobeusedtorefertorelationnames,ifabinaryoperationneedstodistinguish
between its two operand relations. For example, $R1 could refer to the ?rst
operandrelation,and$R2couldrefertothesecondoperandrelationofaCartesian
product.However,thepositionalnotationisinconvenientfor humans, sincethe
position of the attribute is a number, rather than an easy-to-remember attribute
name.Hence,wedonotusethepositionalnotationinthistextbook.
6.1.2 Formal De?nition of the Relational Algebra
TheoperationsinSection6.1.1allowustogiveacompletede?nitionofanexpres-
sionintherelationalalgebra.Abasicexpressionintherelationalalgebraconsists
ofeitheroneofthefollowing:
  Arelationinthedatabase
  Aconstantrelation
Aconstantrelationiswrittenbylistingitstupleswithin{},forexample
{(22222,Einstein,Physics,95000),(76543,Singh,Finance,80000) }.
A general expression in the relational algebra is constructed out of smaller
subexpressions. Let E
1
and E
2
be relational-algebra expressions. Then, the fol-
lowingareallrelational-algebraexpressions:
  E
1
? E
2
  E
1
? E
2
  E
1
× E
2
    P
(E
1
),wherePisapredicateonattributesin E
1
  null
S
(E
1
),whereSisalistconsistingofsomeoftheattributesin E
1
    x
(E
1
),where xisthenewnamefortheresultof E
1
6.1.3 Additional Relational-Algebra Operations
Thefundamentaloperationsoftherelationalalgebraaresuf?cienttoexpressany
relational-algebraquery.However,ifwerestrictourselvestojustthefundamental
operations,certaincommonqueriesarelengthytoexpress.Therefore,wede?ne
additional operations that do not add any power to the algebra, but simplify
6.1 TheRelationalAlgebra 229
course id
CS-101
Figure 6.13 Courses offered in both the Fall 2009 and Spring 2010 semesters.
commonqueries.Foreachnewoperation,wegiveanequivalentexpressionthat
usesonlythefundamentaloperations.
6.1.3.1 TheSet-IntersectionOperation
The ?rst additionalrelational-algebra operationthat we shall de?ne issetinter-
section(?).Supposethatwewishto?ndthesetofallcoursestaughtinboththe
Fall2009andtheSpring2010semesters.Usingsetintersection,wecanwrite
null
course id
(  semester =“Fall”?year=2009
(section)) ?
null
course id
(  semester =“Spring” ?year=2010
(section))
TheresultrelationforthisqueryappearsinFigure6.13.
Note that we can rewrite any relational-algebra expression that uses set in-
tersection by replacing the intersection operation with a pair of set-difference
operationsas:
r ? s = r ? (r ? s)
Thus,setintersectionisnotafundamentaloperationanddoesnotaddanypower
totherelationalalgebra.Itissimplymoreconvenienttowriter ? s thantowrite
r ? (r ? s).
6.1.3.2 TheNatural-JoinOperation
It is often desirable to simplify certain queries that require a Cartesian product.
Usually, a query that involves a Cartesian product includes a selection opera-
tion on the result of the Cartesian product. The selection operation most often
requires that all attributes that are common to the relations that are involved in
theCartesianproductbeequated.
In our example query from Section 6.1.1.6 that combined information from
the instructor and teaches tables, the matching condition required instructor.ID to
beequalto teaches.ID.Thesearetheonlyattributesinthetworelationsthathave
thesamename.
The natural joinisabinaryoperationthatallowsustocombinecertainselec-
tionsandaCartesianproductintooneoperation.Itisdenotedbythejoinsymbol
  . The natural-join operation forms a Cartesian product of its two arguments,
performsaselectionforcingequalityonthoseattributesthatappearinbothrela-
tionschemas,and?nallyremovesduplicateattributes.Returningtotheexample
of the relations instructor and teaches,computinginstructor natural join teaches
considersonlythosepairsoftupleswhereboththetuplefrom instructorandthe
230 Chapter6 FormalRelationalQueryLanguages
ID name dept name salary course id sec id semester year
10101 Srinivasan Comp.Sci. 65000 CS-101 1 Fall 2009
10101 Srinivasan Comp.Sci. 65000 CS-315 1 Spring 2010
10101 Srinivasan Comp.Sci. 65000 CS-347 1 Fall 2009
12121 Wu Finance 90000 FIN-201 1 Spring 2010
15151 Mozart Music 40000 MU-199 1 Spring 2010
22222 Einstein Physics 95000 PHY-101 1 Fall 2009
32343 ElSaid History 60000 HIS-351 1 Spring 2010
45565 Katz Comp.Sci. 75000 CS-101 1 Spring 2010
45565 Katz Comp.Sci. 75000 CS-319 1 Spring 2010
76766 Crick Biology 72000 BIO-101 1 Summer 2009
76766 Crick Biology 72000 BIO-301 1 Summer 2010
83821 Brandt Comp.Sci. 92000 CS-190 1 Spring 2009
83821 Brandt Comp.Sci. 92000 CS-190 2 Spring 2009
83821 Brandt Comp.Sci. 92000 CS-319 2 Spring 2010
98345 Kim Elec.Eng. 80000 EE-181 1 Spring 2009
Figure 6.14 The natural join of the instructor relation with the teaches relation.
tuple from teaches have the same value on the common attribute ID.Theresult
relation,showninFigure6.14,hasonly13tuples,theonesthatgiveinformation
aboutaninstructorandacoursethatthatinstructoractuallyteaches.Noticethat
we do not repeat those attributes that appear in the schemas of both relations;
rather they appear only once. Notice also the order in which the attributes are
listed:?rsttheattributescommontotheschemasofbothrelations,secondthose
attributes unique to the schema of the ?rst relation, and ?nally, those attributes
uniquetotheschemaofthesecondrelation.
Although the de?nition of natural join is complicated, the operation is easy
to apply. As an illustration, consider again the example “Find the names of all
instructorstogetherwiththecourse idofallcoursestheytaught.”Weexpressthis
querybyusingthenaturaljoinasfollows:
null
name, course id
(instructor   teaches)
Sincetheschemasfor instructorand teacheshavetheattribute IDincommon,
thenatural-joinoperationconsidersonlypairsoftuplesthathavethesamevalue
on ID.Itcombineseachsuchpairoftuplesintoasingletupleontheunionofthe
two schemas; that is,(ID, name, dept name, salary, course id).Afterperformingthe
projection,weobtaintherelationinFigure6.15.
ConsidertworelationschemasRandS—whichare,ofcourse,listsofattribute
names.Ifweconsidertheschemastobesets,ratherthanlists,wecandenotethose
attributenamesthatappearinbothRandSby R ? S,anddenotethoseattribute
namesthatappearinR,inS,orinbothbyR ? S.Similarly,thoseattributenames
that appear in R but not S are denoted by R ? S,whereasS ? R denotes those
6.1 TheRelationalAlgebra 231
name course id
Srinivasan CS-101
Srinivasan CS-315
Srinivasan CS-347
Wu FIN-201
Mozart MU-199
Einstein PHY-101
ElSaid HIS-351
Katz CS-101
Katz CS-319
Crick BIO-101
Crick BIO-301
Brandt CS-190
Brandt CS-319
Kim EE-181
Figure 6.15 Result of null
name, course id
(instructor   teaches).
attribute names that appear in S but not in R. Note that the union, intersection,
anddifferenceoperationshereareonsetsofattributes,ratherthanonrelations.
We are now ready for a formal de?nition of the natural join. Consider two
relationsr(R)ands(S).Thenaturaljoinofrands,denotedbyr   s,isarelation
onschema R ? Sformallyde?nedasfollows:
r   s = null
R ? S
(  r.A
1
=s.A
1
?r.A
2
=s.A
2
?...?r.A
n
=s.A
n
(r × s))
where R ? S={A
1
, A
2
,...,A
n
}.
Please note that if r(R)ands(S) are relations without any attributes in com-
mon,thatis, R ? S=?,thenr   s = r × s.
Letusconsideronemoreexampleoftheuseofnaturaljoin,towritethequery
“FindthenamesofallinstructorsintheComp.Sci.departmenttogetherwiththe
coursetitlesofallthecoursesthattheinstructorsteach.”
null
name,title
(  dept name =“Comp.Sci.”
(instructor   teaches   course))
TheresultrelationforthisqueryappearsinFigure6.16.
Noticethatwewroteinstructor  teaches  coursewithoutinsertingparenthe-
sestospecifytheorderinwhichthenatural-joinoperationsonthethreerelations
shouldbeexecuted.Intheprecedingcase,therearetwopossibilities:
(instructor   teaches)   course
instructor   (teaches   course)
Wedidnotspecifywhichexpressionweintended,becausethetwoareequivalent.
Thatis,thenaturaljoinisassociative.
232 Chapter6 FormalRelationalQueryLanguages
name title
Brandt GameDesign
Brandt ImageProcessing
Katz ImageProcessing
Katz Intro.toComputerScience
Srinivasan Intro.toComputerScience
Srinivasan Robotics
Srinivasan DatabaseSystemConcepts
Figure 6.16 Result of
null
name,title
(  dept name =“Comp. Sci.”
(instructor   teaches   course)).
Thethetajoinoperationisavariantofthenatural-joinoperationthatallowsus
tocombineaselectionandaCartesianproductintoasingleoperation.Consider
relationsr(R)ands(S),andlet  beapredicateonattributesintheschema R ?S.
Thethetajoinoperationr     s isde?nedasfollows:
r     s =     (r × s)
6.1.3.3 TheAssignmentOperation
Itisconvenientattimestowritearelational-algebraexpressionbyassigningparts
of it to temporary relation variables. The assignment operation, denoted by ?,
works like assignment in a programming language. To illustrate this operation,
considerthede?nitionofthenatural-joinoperation.Wecouldwriter   s as:
temp1 ? R × S
temp2 ?   r.A
1
=s.A
1
?r.A
2
=s.A
2
?...?r.A
n
=s.A
n
(temp1)
result = null
R ? S
(temp2)
The evaluationof anassignment doesnot resultinany relationbeing displayed
to the user. Rather, the result of the expression to the right of the ? is assigned
totherelationvariableontheleftofthe ?.Thisrelationvariablemaybeusedin
subsequentexpressions.
Withtheassignmentoperation,aquerycanbewrittenasasequentialprogram
consisting of a series of assignments followed by an expression whose value is
displayed as the result of the query. For relational-algebra queries, assignment
mustalwaysbemadetoatemporaryrelationvariable.Assignmentstopermanent
relationsconstituteadatabasemodi?cation.Notethattheassignmentoperation
doesnotprovideanyadditionalpowertothealgebra.Itis,however,aconvenient
waytoexpresscomplexqueries.
6.1.3.4 OuterjoinOperations
Theouter-joinoperationisanextensionofthejoinoperationtodealwithmissing
information.Supposethatthereissomeinstructorwhoteachesnocourses.Then
6.1 TheRelationalAlgebra 233
ID name dept name salary course id sec id semester year
10101 Srinivasan Comp.Sci. 65000 CS-101 1 Fall 2009
10101 Srinivasan Comp.Sci. 65000 CS-315 1 Spring 2010
10101 Srinivasan Comp.Sci. 65000 CS-347 1 Fall 2009
12121 Wu Finance 90000 FIN-201 1 Spring 2010
15151 Mozart Music 40000 MU-199 1 Spring 2010
22222 Einstein Physics 95000 PHY-101 1 Fall 2009
32343 ElSaid History 60000 HIS-351 1 Spring 2010
33456 Gold Physics 87000 null null null null
45565 Katz Comp.Sci. 75000 CS-101 1 Spring 2010
45565 Katz Comp.Sci. 75000 CS-319 1 Spring 2010
58583 Cali?eri History 62000 null null null null
76543 Singh Finance 80000 null null null null
76766 Crick Biology 72000 BIO-101 1 Summer 2009
76766 Crick Biology 72000 BIO-301 1 Summer 2010
83821 Brandt Comp.Sci. 92000 CS-190 1 Spring 2009
83821 Brandt Comp.Sci. 92000 CS-190 2 Spring 2009
83821 Brandt Comp.Sci. 92000 CS-319 2 Spring 2010
98345 Kim Elec.Eng. 80000 EE-181 1 Spring 2009
Figure 6.17 Result of instructor   teaches.
thetupleintheinstructorrelation(Figure6.1)forthatparticularinstructorwould
notsatisfytheconditionofanaturaljoinwiththeteachesrelation(Figure6.7)and
thatinstructor’sdatawouldnotappearintheresultofthenaturaljoin,shownin
Figure6.14.For example,instructors Cali?eri,Gold,andSinghdonot appearin
theresultofthenaturaljoin,sincetheydonotteachanycourse.
Moregenerally,sometuplesineitherorbothoftherelationsbeingjoinedmay
be “lost” in this way. The outerjoin operation works in a manner similar to the
natural join operation we have already studied, but preserves those tuples that
wouldbelostinanjoinbycreatingtuplesintheresultcontainingnullvalues.
We can use the outer-join operation to avoid this loss of information. There
are actually three forms of the operation: left outer join, denoted   ; right outer
join, denoted   ;andfull outer join, denoted   . All three forms of outer join
computethe join,and addextratuplestothe resultof thejoin. Forexample,the
results of the expression instructor   teaches and teaches   instructor appear
inFigures6.17and 6.18,respectively.
Theleftouterjoin(  )takesalltuplesintheleftrelationthatdidnotmatch
withanytupleintherightrelation,padsthetupleswithnullvaluesforallother
attributesfromtherightrelation,andaddsthemtotheresultofthenaturaljoin.
InFigure6.17,tuple(58583,Cali?eri,History,62000,null,null,null,null),issuch
a tuple. All information from the left relation is present in the result of the left
outerjoin.
234 Chapter6 FormalRelationalQueryLanguages
ID course id sec id semester year name dept name salary
10101 CS-101 1 Fall 2009 Srinivasan Comp.Sci. 65000
10101 CS-315 1 Spring 2010 Srinivasan Comp.Sci. 65000
10101 CS-347 1 Fall 2009 Srinivasan Comp.Sci. 65000
12121 FIN-201 1 Spring 2010 Wu Finance 90000
15151 MU-199 1 Spring 2010 Mozart Music 40000
22222 PHY-101 1 Fall 2009 Einstein Physics 95000
32343 HIS-351 1 Spring 2010 ElSaid History 60000
33456 null null null null Gold Physics 87000
45565 CS-101 1 Spring 2010 Katz Comp.Sci. 75000
45565 CS-319 1 Spring 2010 Katz Comp.Sci. 75000
58583 null null null null Cali?eri History 62000
76543 null null null null Singh Finance 80000
76766 BIO-101 1 Summer 2009 Crick Biology 72000
76766 BIO-301 1 Summer 2010 Crick Biology 72000
83821 CS-190 1 Spring 2009 Brandt Comp.Sci. 92000
83821 CS-190 2 Spring 2009 Brandt Comp.Sci. 92000
83821 CS-319 2 Spring 2010 Brandt Comp.Sci. 92000
98345 EE-181 1 Spring 2009 Kim Elec.Eng. 80000
Figure 6.18 Result of teaches   instructor.
Therightouterjoin (  ) is symmetricwith the left outer join: It pads tuples
fromtherightrelationthatdidnotmatchanyfromtheleftrelationwithnullsand
addsthemtotheresultofthenaturaljoin.InFigure6.18,tuple(58583, null, null,
null,null,Cali?eri,History,62000),issuchatuple.Thus,allinformationfromthe
rightrelationispresentintheresultoftherightouterjoin.
The full outer join(  ) does both the left and right outer join operations,
padding tuples from the left relation that did not match any from the right
relation, as well as tuples from the right relation that did not match any from
theleftrelation,andaddingthemtotheresultofthejoin.
Notethatingoingfromourleft-outer-joinexampletoourright-outer-joinex-
ample,wechosetoswaptheorderoftheoperands.Thusbothexamplespreserve
tuples from the instructor relation, and thus contain the same information. In
ourexamplerelations,teachestuplesalwayshavematchinginstructortuples,and
thus teaches   instructor would give the same result as teaches   instructor.
If there were tuples in teaches without matching tuples in instructor, such tu-
ples would appear padded with nulls in teaches   instructor as well as in
teaches   instructor.Furtherexamplesofouterjoins(expressedin SQLsyntax)
maybefoundinSection4.1.2.
Since outer-join operations may generate results containing null values, we
need to specify how the different relational-algebra operations deal with null
values.Section3.6dealtwiththisissueinthecontextof SQL.Thesameconcepts
applyforthecaseofrelationalalgebra,andweomitdetails.
6.1 TheRelationalAlgebra 235
It is interesting to note that the outer-join operations can be expressed by
thebasicrelational-algebraoperations.Forinstance,theleftouterjoinoperation,
r   s,canbewrittenas:
(r   s) ?(r ? null
R
(r   s))×{(null,...,null)}
wheretheconstantrelation {(null,...,null)}isontheschema S ? R.
6.1.4 Extended Relational-Algebra Operations
We now describe relational-algebra operations that provide the ability to write
queries that cannot be expressed using the basic relational-algebra operations.
Theseoperationsarecalledextendedrelational-algebraoperations.
6.1.4.1 GeneralizedProjection
The ?rst operation is the generalized-projection operation, which extends the
projection operation by allowing operations such as arithmetic and string func-
tions to be used in the projection list. The generalized-projection operation has
theform:
null
F
1
,F
2
,...,F
n
(E)
where E is any relational-algebra expression, and each of F
1
,F
2
,...,F
n
is an
arithmetic expression involving constants and attributes in the schema of E.As
a base case, the expressionmay be simply an attribute or a constant. In general,
an expression can use arithmetic operations such as +, ?, ?,and÷ on numeric
valuedattributes,numericconstants,andonexpressionsthatgenerateanumeric
result. Generalized projection also permits operations on other data types, such
asconcatenationofstrings.
Forexample,theexpression:
null
ID,name,dept name,salary÷12
(instructor)
givesthe ID,name,dept name,andthemonthlysalaryofeachinstructor.
6.1.4.2 Aggregation
The second extended relational-algebra operation is the aggregate operation G,
which permits the use of aggregate functions such as min or average, on sets of
values.
Aggregatefunctionstakeacollectionofvaluesandreturnasinglevalueasa
result.For example,the aggregatefunctionsum takes a collection of values and
returnsthesumofthevalues.Thus,thefunctionsumappliedonthecollection:
{1,1,3,4,4,11}
236 Chapter6 FormalRelationalQueryLanguages
returnsthevalue24.Theaggregatefunctionavgreturnstheaverageofthevalues.
When applied to the preceding collection, it returns the value 4. The aggregate
functioncountreturnsthenumberof theelementsinthecollection,and returns
6 on the preceding collection. Other common aggregate functions include min
andmax, which return the minimum and maximum values in a collection; they
return1and11,respectively,ontheprecedingcollection.
The collections on which aggregate functions operate can have multiple oc-
currences of a value; the order in which the values appear is not relevant. Such
collectionsarecalledmultisets.Setsareaspecialcaseofmultisetswherethereis
onlyonecopyofeachelement.
To illustrate the concept of aggregation, we shall use the instructor relation.
Suppose that we want to ?nd out the sum of salaries of all instructors; the
relational-algebraexpressionforthisqueryis:
G
sum(salary)
(instructor)
The symbol G is the letter G in calligraphic font; read it as “calligraphic G.” The
relational-algebra operation G signi?es that aggregation is to be applied, and
its subscript speci?es the aggregate operation to be applied. The result of the
expressionaboveisarelationwithasingleattribute,containingasinglerowwith
anumericalvaluecorrespondingtothesumofthesalariesofallinstructors.
There are cases where we must eliminate multiple occurrences of a value
before computing an aggregate function. If we do want to eliminate duplicates,
we use the same function names as before, with the addition of the hyphenated
string “distinct”appendedtotheendofthefunctionname(forexample,count-
distinct). An example arises in the query “Find the total number of instructors
whoteachacourseintheSpring2010semester.”Inthiscase,aninstructorcounts
onlyonce,regardlessofthenumberofcoursesectionsthattheinstructorteaches.
The required information is contained in the relation teaches, and we write this
queryasfollows:
G
count?distinct(ID)
(  semester=“Spring”?year =2010
(teaches))
The aggregate function count-distinct ensures that even if an instructor teaches
morethanonecourse,sheiscountedonlyonceintheresult.
Therearecircumstanceswherewewouldliketoapplytheaggregatefunction
not to a single set of tuples, but instead to a group of sets of tuples. As an
illustration,considerthequery“Findtheaveragesalaryineachdepartment.”We
writethisqueryasfollows:
dept name
G
average(salary)
(instructor)
Figure 6.19 shows the tuples in the instructor relation grouped by the dept
nameattribute.Thisisthe?rststepincomputingthequeryresult.Thespeci?ed
aggregate is computed for each group, and the result of the query is shown in
Figure6.20.
6.1 TheRelationalAlgebra 237
ID name dept name salary
76766 Crick Biology 72000
45565 Katz Comp.Sci. 75000
10101 Srinivasan Comp.Sci. 65000
83821 Brandt Comp.Sci. 92000
98345 Kim Elec.Eng. 80000
12121 Wu Finance 90000
76543 Singh Finance 80000
32343 ElSaid History 60000
58583 Cali?eri History 62000
15151 Mozart Music 40000
33456 Gold Physics 87000
22222 Einstein Physics 95000
Figure 6.19 Tuples of the instructor relation, grouped by the dept name attribute
Incontrast,considerthequery“Findtheaveragesalaryofallinstructors.”We
writethisqueryasfollows:
G
average(salary)
(instructor)
In this case the attribute dept name has been omitted from the left side of the G
operator,sotheentirerelationistreatedasasinglegroup.
ThegeneralformoftheaggregationoperationG isasfollows:
G
1
,G
2
,...,G
n
G
F
1
(A
1
), F
2
(A
2
),..., F
m
(A
m
)
(E)
where E is any relational-algebra expression; G
1
,G
2
,...,G
n
constitute a list of
attributes on which to group; each F
i
is an aggregate function; and each A
i
is
an attribute name. The meaning of the operation is as follows: The tuples in the
resultofexpression E arepartitionedintogroupsinsuchawaythat:
dept name salary
Biology 72000
Comp.Sci. 77333
Elec.Eng. 80000
Finance 85000
History 61000
Music 40000
Physics 91000
Figure 6.20 The result relation for the query “Find the average salary in each department”.
238 Chapter6 FormalRelationalQueryLanguages
MULTISETRELATIONALALGEBRA
Unlike therelationalalgebra, SQL allows multiple copiesofatuple inaninput
relationaswellasinaqueryresult.TheSQLstandardde?neshowmanycopies
ofeachtuplearethereintheoutputofaquery,whichdependsinturnonhow
manycopiesoftuplesarepresentintheinputrelations.
To model this behavior of SQL, a version of relational algebra, called the
multiset relational algebra, is de?ned to work on multisets, that is, sets that
may contain duplicates. The basic operations in the multiset relational algebra
arede?nedasfollows:
1. Ifthereare c
1
copiesoftuple t
1
in r
1
,andt
1
satis?es selection    ,thenthereare
c
1
copiesoft
1
in    (r
1
).
2. For each copy of tuple t
1
in r
1
, there is a copy of tuple null
A
(t
1
)innull
A
(r
1
), where
null
A
(t
1
)denotestheprojectionofthesingletuplet
1
.
3. Iftherearec
1
copiesoftuplet
1
inr
1
andc
2
copiesoftuplet
2
inr
2
,therearec
1
?c
2
copiesofthetuplet
1
.t
2
inr
1
×r
2
.
Forexample,supposethatrelationsr
1
withschema(A,B)andr
2
withschema
(C)arethefollowingmultisets:
r
1
={ (1,a),(2,a)} r
2
={ (2),(3),(3)}
Then null
B
(r
1
)wouldbe{(a),(a)},whereasnull
B
(r
1
) ×r
2
wouldbe:
{(a,2),(a,2),(a,3),(a,3),(a,3),(a,3)}
Multisetunion,intersectionandsetdifferencecanalsobede?nedinasimilar
way, following the corresponding de?nitions in SQL,whichwesawinSec-
tion3.5.Thereisnochangeinthede?nitionoftheaggregationoperation.
1. Alltuplesinagrouphavethesamevaluesfor G
1
,G
2
,...,G
n
.
2. Tuplesindifferentgroupshavedifferentvaluesfor G
1
,G
2
,...,G
n
.
Thus, the groups can be identi?ed by the values of attributes G
1
,G
2
,...,G
n
.
Foreachgroup(g
1
,g
2
,...,g
n
),theresulthasatuple(g
1
,g
2
,...,g
n
,a
1
,a
2
,...,a
m
)
where, for each i, a
i
istheresultofapplyingtheaggregatefunctionF
i
on the
multisetofvaluesforattribute A
i
inthegroup.
Asaspecialcaseoftheaggregateoperation,thelistofattributesG
1
,G
2
,...,G
n
can be empty, in which case there is a single group containing all tuples in the
relation.Thiscorrespondstoaggregationwithoutgrouping.
6.2 TheTupleRelationalCalculus 239
SQLANDRELATIONALALGEBRA
FromacomparisonoftherelationalalgebraoperationsandtheSQLoperations,
itshouldbeclearthatthereisacloseconnectionbetweenthetwo.AtypicalSQL
queryhastheform:
select A
1
, A
2
,...,A
n
fromr
1
, r
2
,...,r
m
where P
Each A
i
representsanattribute,andeachr
i
arelation.Pisapredicate.Thequery
isequivalenttothemultisetrelational-algebraexpression:
null
A
1
, A
2
,...,A
n
(  P
(r
1
× r
2
× ··· × r
m
))
Ifthewhereclauseisomitted,thepredicate Pistrue.
More complex SQL queries can also be rewritten in relational algebra. For
example,thequery:
select A
1
, A
2
,sum(A
3
)
fromr
1
, r
2
,...,r
m
where P
groupby A
1
, A
2
isequivalentto:
A
1
, A
2
G
sum(A
3
)
(null
A
1
, A
2
,..., A
n
(  P
(r
1
× r
2
× ··· × r
m
)))
Joinexpressionsinthefromclausecanbewrittenusingequivalentjoinexpres-
sions in relational algebra; we leave the details as an exercise for the reader.
However, subqueries in the where or select clause cannot be rewritten into
relationalalgebrainsuchastraightforwardmanner,sincethereisnorelational-
algebraoperationequivalenttothesubqueryconstruct.Extensionsofrelational
algebrahavebeenproposedforthistask,butarebeyondthescopeofthisbook.
6.2 The Tuple Relational Calculus
Whenwewritearelational-algebraexpression,weprovideasequenceofproce-
dures that generates the answer to our query. The tuple relational calculus, by
contrast,isanonproceduralquerylanguage.Itdescribesthedesiredinformation
withoutgivingaspeci?cprocedureforobtainingthatinformation.
Aqueryinthetuplerelationalcalculusisexpressedas:
{t | P(t)}
240 Chapter6 FormalRelationalQueryLanguages
Thatis,itisthesetofalltuplestsuchthatpredicatePistruefort.Followingour
earliernotation,weuse t[A]todenotethevalueoftupletonattribute A,andwe
uset ?rtodenotethattupletisinrelationr.
Before we give a formal de?nition of the tuple relational calculus, we re-
turntosomeofthequeriesforwhichwewroterelational-algebraexpressionsin
Section6.1.1.
6.2.1 Example Queries
Find the ID, name, dept name, salary for instructors whose salary is greater than
$80,000:
{t |t ? instructor ? t[salary] > 80000}
Suppose that we want only the ID attribute, rather than all attributes of the
instructorrelation.Towritethisqueryinthetuplerelationalcalculus,weneedto
writeanexpressionforarelationontheschema(ID).Weneedthosetupleson(ID)
suchthatthereisatupleininstructorwiththesalaryattribute >80000.Toexpress
this request, we need the construct “there exists” from mathematical logic. The
notation:
?t ? r (Q(t))
means “thereexistsatupletinrelationrsuchthatpredicateQ(t)istrue. ”
Using this notation, we can write the query “Find the instructor ID for each
instructorwithasalarygreaterthan$80,000”as:
{t|?s ? instructor (t[ID] = s[ID]
?s[salary] > 80000)}
In English, we read the preceding expression as “Thesetofalltuples tsuch that
there exists a tuple s in relation instructor for which the values of t and s for
the IDattributeareequal,andthevalueofsforthesalaryattributeisgreaterthan
$80,000.”
Tuple variable t is de?ned on only the ID attribute, since that is the only
attributehavingaconditionspeci?edfor t.Thus,theresultisarelationon(ID).
Consider the query “Find the names of all instructors whose department is
in the Watson building.” This query is slightly more complex than the previous
queries, since it involves two relations: instructor and department.Asweshall
see, however, all it requires is that we have two “there exists” clauses in our
tuple-relational-calculusexpression,connectedbyand(?).Wewritethequeryas
follows:
{t|?s ? instructor (t[name] = s[name]
??u ? department (u[dept name] = s[dept name]
?u[building] = “Watson”))}
6.2 TheTupleRelationalCalculus 241
name
Einstein
Crick
Gold
Figure 6.21 Names of all instructors whose department is in the Watson building.
Tuple variable uisrestrictedtodepartmentsthatarelocatedintheWatsonbuild-
ing, while tuple variable s is restricted to instructors whose dept name matches
thatoftuplevariable u.Figure6.21showstheresultofthisquery.
To?ndthesetofallcoursestaughtintheFall2009semester,theSpring2010
semester, or both, we used the union operation in the relational algebra. In the
tuple relational calculus, we shall need two “there exists” clauses, connected by
or(?):
{t|?s ? section (t[course id] = s[course id])
? s[semester] = “Fall” ? s[year] = 2009)}
??u ? section (u[course id] = t[course id])}
? u[semester] = “Spring” ? u[year] = 2010)}
Thisexpressiongivesusthesetofallcourse idtuplesforwhichatleastoneofthe
followingholds:
  Thecourse idappearsinsometupleofthesectionrelationwithsemester=Fall
and year=2009.
  The course id appears in some tuple of the section relation with semester =
Springand year=2010.
If the same course is offeredin both the Fall 2009 and Spring 2010 semesters,its
course id appears only once in the result, because the mathematical de?nition of
asetdoesnotallowduplicatemembers.Theresultofthisqueryappearedearlier
inFigure6.5.
Ifwenowwant onlythosecourse idvaluesforcoursesthatareofferedinboth
theFall2009andSpring2010semesters,allweneedtodoistochangethe or(?)
to and(?)intheprecedingexpression.
{t|?s ? section (t[course id] = s[course id])
? s[semester] = “Fall” ? s[year] = 2009)}
??u ? section (u[course id] = t[course id])}
? u[semester] = “Spring” ? u[year] = 2010)}
TheresultofthisqueryappearedinFigure6.13.
Nowconsiderthequery“FindallthecoursestaughtintheFall2009semester
butnotinSpring2010semester.”Thetuple-relational-calculusexpressionforthis
242 Chapter6 FormalRelationalQueryLanguages
query is similar to the expressions that we have just seen, except for the use of
the not(¬)symbol:
{t|?s ? section (t[course id] = s[course id])
? s[semester] = “Fall” ? s[year] = 2009)}
?¬?u ? section (u[course id] = t[course id])}
? u[semester] = “Spring” ? u[year] = 2010)}
Thistuple-relational-calculusexpressionusesthe ?s ? section (...)clauseto
requirethataparticularcourse idistaughtintheFall2009semester,anditusesthe
¬?u ? section (...)clausetoeliminatethosecourse idvaluesthatappearinsome
tupleofthe sectionrelationashavingbeentaughtintheSpring2010semester.
The query that we shall consider next uses implication, denoted by ?.The
formula P ? Qmeans “PimpliesQ”;thatis,“ifPistrue,thenQmustbetrue.”
Note that P ? Q is logically equivalent to ¬P ? Q. The use of implication
ratherthannotandoroftensuggestsamoreintuitiveinterpretationofaqueryin
English.
Considerthequerythat“Findallstudentswhohavetakenallcoursesoffered
in the Biology department.” To write this query in the tuple relational calculus,
weintroducethe “forall”construct,denotedby ?.Thenotation:
? t ? r (Q(t))
means “Qistrueforalltuplestinrelationr.”
Wewritetheexpressionforourqueryasfollows:
{t|?r ? student (r[ID] = t[ID]) ?
( ? u ? course(u[dept name] = “Biology” ?
? s ? takes (t[ID] = s[ID]
? s[course id] = u[course id]))}
In English, we interpret this expression as “The set of all students (that is, (ID)
tuplest)suchthat,foralltuplesuinthecourserelation,ifthevalueofuonattribute
dept name is ’Biology’, then there exists a tuple in the takes relation that includes
thestudent IDandthe course id.”
Note that there is a subtlety in the above query: If there is no course offered
in the Biology department, all student IDs satisfy the condition. The ?rst line of
thequeryexpressioniscriticalinthiscase—withoutthecondition
?r ? student (r[ID] = t[ID])
ifthereisnocourseofferedintheBiologydepartment,anyvalueof t (including
valuesthatarenotstudentIDsinthestudentrelation)wouldqualify.
6.2 TheTupleRelationalCalculus 243
6.2.2 Formal De?nition
Wearenowreadyforaformalde?nition.Atuple-relational-calculusexpression
isoftheform:
{t|P(t)}
where P is a formula. Several tuple variables may appear in a formula. A tuple
variableissaidtobea freevariableunlessitisquanti?edbya ?or ?.Thus,in:
t ? instructor ??s ? department(t[dept name] = s[dept name])
tisafreevariable.Tuplevariablesissaidtobeaboundvariable.
Atuple-relational-calculusformulaisbuiltupoutofatoms.Anatomhasone
ofthefollowingforms:
  s ?r,wheresisatuplevariableandrisarelation(wedonotallowuseofthe
/ ?operator).
  s[x] nullu[y], where s and u are tuple variables, x is an attribute on which s
is de?ned, y is an attribute on which u is de?ned, and nullis a comparison
operator(<, ?, =, null, >, ?);werequirethatattributes xand yhavedomains
whosememberscanbecomparedby null .
  s[x] null c,wheresisatuplevariable,xisanattributeonwhich sisde?ned, null
isacomparisonoperator,and cisaconstantinthedomainofattributex.
Webuildupformulaefromatomsbyusingthefollowingrules:
  Anatomisaformula.
  If P
1
isaformula,thensoare ¬P
1
and(P
1
).
  If P
1
and P
2
areformulae,thensoare P
1
? P
2
, P
1
? P
2
,andP
1
? P
2
.
  If P
1
(s)isaformulacontainingafreetuplevariables,andr isarelation,then
?s ? r (P
1
(s)) and ?s ? r (P
1
(s))
arealsoformulae.
As we could for the relational algebra, we can write equivalent expressions
thatarenotidenticalinappearance.Inthetuplerelationalcalculus,theseequiv-
alencesincludethefollowingthreerules:
1. P
1
? P
2
isequivalentto ¬(¬(P
1
) ?¬ (P
2
)).
2. ? t ? r (P
1
(t))isequivalentto¬?t ? r (¬P
1
(t)).
3. P
1
? P
2
isequivalentto ¬(P
1
) ? P
2
.
244 Chapter6 FormalRelationalQueryLanguages
6.2.3 Safety of Expressions
There is one ?nal issue to be addressed. A tuple-relational-calculus expression
maygenerateanin?niterelation.Supposethatwewritetheexpression:
{t |¬(t ? instructor)}
There are in?nitely many tuples that are not in instructor.Mostofthesetuples
contain values that do not evenappear in the database! Clearly,we donot wish
toallowsuchexpressions.
To help us de?ne a restriction of the tuple relational calculus, we introduce
theconceptofthedomainofatuplerelationalformula,P.Intuitively,thedomain
of P, denoted dom(P), is the set of all values referenced by P. They include
valuesmentionedinPitself,aswellasvaluesthatappearinatupleofarelation
mentionedinP.Thus,thedomainofPisthesetofallvaluesthatappearexplicitly
inPorthatappearinoneormorerelationswhosenamesappearinP.Forexample,
dom(t ? instructor ? t[salary] > 80000)isthesetcontaining80000aswellasthe
setof allvaluesappearinginanyattributeofany tupleinthe instructor relation.
Similarly, dom(¬ (t ? instructor)) is also the set of all values appearing in
instructor,sincetherelationinstructorismentionedintheexpression.
We say that an expression {t | P(t)} is safe if all values that appear in the
resultarevaluesfromdom(P).Theexpression {t |¬(t ? instructor)}isnotsafe.
Notethatdom(¬(t ? instructor))isthesetofallvaluesappearingin instructor.
However, it is possible to have a tuple t not in instructor that contains values
that do not appear in instructor. The other examples of tuple-relational-calculus
expressionsthatwehavewritteninthissectionaresafe.
The number of tuples that satisfy an unsafe expression, such as {t |¬ (t ?
instructor)}, could be in?nite, whereas safe expressions are guaranteed to have
?nite results. The class of tuple-relational-calculus expressions that are allowed
isthereforerestrictedtothosethataresafe.
6.2.4 Expressive Power of Languages
Thetuplerelationalcalculusrestrictedtosafeexpressionsisequivalentinexpres-
sivepowertothebasicrelationalalgebra(withtheoperators ?, ?, ×,  ,and  ,but
withouttheextendedrelationaloperationssuchasgeneralizedprojectionandag-
gregation(G)).Thus,foreveryrelational-algebraexpressionusingonlythebasic
operations,thereisanequivalentexpressioninthetuplerelationalcalculus,and
for every tuple-relational-calculus expression, there is an equivalent relational-
algebraexpression.Weshallnotprovethisassertionhere;thebibliographicnotes
containreferencestotheproof.Somepartsoftheproofareincludedintheexer-
cises. We note that the tuple relational calculus does not have any equivalent of
theaggregateoperation,butitcanbeextendedtosupportaggregation.Extending
thetuplerelationalcalculustohandlearithmeticexpressionsisstraightforward.
6.3 TheDomainRelationalCalculus 245
6.3 The Domain Relational Calculus
A second form of relational calculus, called domain relational calculus,uses
domainvariablesthattakeonvaluesfromanattributesdomain,ratherthanvalues
foranentiretuple.Thedomainrelationalcalculus,however,iscloselyrelatedto
thetuplerelationalcalculus.
Domainrelationalcalculus servesasthe theoreticalbasis of thewidelyused
QBElanguage(seeAppendixB.1),justasrelationalalgebraservesasthebasisfor
the SQLlanguage.
6.3.1 Formal De?nition
Anexpressioninthedomainrelationalcalculusisoftheform
{< x
1
, x
2
,...,x
n
> | P(x
1
, x
2
,...,x
n
)}
wherex
1
, x
2
,...,x
n
representdomainvariables.Prepresentsaformulacomposed
ofatoms,aswasthecaseinthetuplerelationalcalculus.Anatominthedomain
relationalcalculushasoneofthefollowingforms:
  < x
1
, x
2
,...,x
n
> ? r,whererisarelationonnattributesand x
1
, x
2
,...,x
n
aredomainvariablesordomainconstants.
  x nully,wherex and y are domain variables and nullis a comparison operator
(<, ?, =, null, >, ?). Werequirethat attributes xand y have domainsthat can
becomparedby null .
  x nullc,wherex is a domain variable, nullis a comparison operator, and c is a
constantinthedomainoftheattributeforwhich xisadomainvariable.
Webuildupformulaefromatomsbyusingthefollowingrules:
  Anatomisaformula.
  If P
1
isaformula,thensoare ¬P
1
and(P
1
).
  If P
1
and P
2
areformulae,thensoare P
1
? P
2
, P
1
? P
2
,andP
1
? P
2
.
  If P
1
(x)isaformulainx,wherexisafreedomainvariable,then
? x (P
1
(x))and ? x (P
1
(x))
arealsoformulae.
Asanotationalshorthand,wewrite?a,b,c(P(a,b,c))for?a(?b(?c(P(a,b,c)))).
246 Chapter6 FormalRelationalQueryLanguages
6.3.2 Example Queries
We now give domain-relational-calculus queries for the examples that we con-
sidered earlier. Note the similarity of these expressions and the corresponding
tuple-relational-calculusexpressions.
  FindtheinstructorID,name,dept name,andsalaryforinstructorswhosesalary
isgreaterthan$80,000:
{< i,n,d,s > | < i,n,d,s > ? instructor ? s > 80000}
  Findallinstructor IDforinstructorswhosesalaryisgreaterthan$80,000:
{< n > |?i,d,s (< i,n,d,s > ? instructor ? s > 80000)}
Althoughthesecondqueryappearssimilartotheonethatwewroteforthetuple
relational calculus, there is an important difference. In the tuple calculus, when
we write ? s for some tuple variable s, we bind it immediately to a relation by
writing ? s ? r. However, when we write ? n in the domain calculus, n refers
not to a tuple, but rather to a domain value. Thus, the domain of variable n is
unconstrained until the subformula < i,n,d,s > ? instructor constrains n to
instructornamesthatappearintheinstructorrelation.
Wenowgiveseveralexamplesofqueriesinthedomainrelationalcalculus.
  FindthenamesofallinstructorsinthePhysicsdepartmenttogetherwiththe
course idofallcoursestheyteach:
{< n,c > |?i,a (< i,c,a,s,y > ? teaches
??d,s (< i,n,d,s > ? instructor ? d = “Physics”))}
  Find the set of all courses taught in the Fall 2009 semester, the Spring 2010
semester,orboth:
{< c > |?s(< c,a,s,y,b,r,t >? section
?s = “Fall” ?y = “2009”
??u(< c,a,s,y,b,r,t >? section
?s = “Spring” ?y = “2010”
  Find all students who have taken all courses offered in the Biology depart-
ment:
{< i > |?n, d,t (< i,n,d,t > ? student) ?
? x,y,z,w(< x,y,z,w>? course ? z = “Biology” ?
?a,b (< a,x,b,r, p,q > ? takes ? < c,a > ? depositor))}
6.3 TheDomainRelationalCalculus 247
Notethataswasthecasefortuple-relational-calculus,ifnocoursesareoffered
intheBiologydepartment,allstudentswouldbeintheresult.
6.3.3 Safety of Expressions
Wenotedthat,inthetuplerelationalcalculus(Section6.2),itispossibletowrite
expressionsthatmaygenerateanin?niterelation.Thatledustode?ne safetyfor
tuple-relational-calculus expressions. A similar situation arises for the domain
relationalcalculus.Anexpressionsuchas
{< i,n,d,s > |¬(< i,n,d,s > ? instructor)}
is unsafe, because it allows valuesin the result that are not inthe domainof the
expression.
Forthedomainrelationalcalculus,wemustbeconcernedalsoabouttheform
offormulaewithin “thereexists”and “forall”clauses.Considertheexpression
{< x > |?y(< x, y >? r) ??z(¬(< x, z >? r) ? P(x,z))}
where P is some formula involving x and z. We can test the ?rst part of the
formula, ? y(< x, y > ? r),byconsideringonlythevaluesinr.However,totest
thesecondpartoftheformula, ?z(¬(< x, z > ? r) ? P(x,z)),wemustconsider
valuesforzthatdonotappearinr.Sinceallrelationsare?nite,anin?nitenumber
ofvaluesdonotappearinr.Thus,itisnotpossible,ingeneral,totestthesecond
part of the formula without considering an in?nite number of potential values
for z. Instead, we add restrictions to prohibit expressions such as the preceding
one.
Inthetuplerelationalcalculus,werestrictedanyexistentiallyquanti?edvari-
abletorangeoveraspeci?crelation.Sincewedidnotdosointhedomaincalculus,
we add rules to the de?nition of safety to deal with cases like our example. We
saythatanexpression
{< x
1
, x
2
,...,x
n
> | P(x
1
, x
2
,...,x
n
)}
issafeifallofthefollowinghold:
1. Allvaluesthatappearintuplesoftheexpressionarevaluesfrom dom(P).
2. Forevery “thereexists”subformulaoftheform ? x (P
1
(x)),thesubformula
istrueifandonlyifthereisavaluexindom(P
1
)suchthatP
1
(x)istrue.
3. Forevery“forall”subformulaoftheform ?x(P
1
(x)),thesubformulaistrue
ifandonlyif P
1
(x)istrueforallvaluesxfromdom(P
1
).
Thepurposeoftheadditionalrulesistoensurethatwecantest“forall”and
“there exists” subformulae without having to test in?nitely many possibilities.
Consider the second rule in the de?nition of safety. For ? x (P
1
(x)) to be true,
248 Chapter6 FormalRelationalQueryLanguages
we need to ?nd only one x for which P
1
(x) is true. In general, there would be
in?nitely many values to test. However, if the expression is safe, we know that
wecanrestrictourattentiontovaluesfromdom(P
1
).Thisrestrictionreducestoa
?nitenumberthetupleswemustconsider.
The situation for subformulae of the form ?x (P
1
(x)) is similar. To assert
that ?x (P
1
(x)) is true, we must, in general, test all possible values, so we must
examinein?nitelymanyvalues.Asbefore,ifweknowthattheexpressionissafe,
itissuf?cientforustotest P
1
(x)forthosevaluestakenfromdom(P
1
).
All the domain-relational-calculus expressions that we have written in the
examplequeriesofthissectionaresafe,exceptfortheexampleunsafequerywe
sawearlier.
6.3.4 Expressive Power of Languages
Whenthedomainrelationalcalculusisrestrictedtosafeexpressions,itisequiv-
alent in expressive power to the tuple relational calculus restricted to safe ex-
pressions. Since we noted earlier that the restricted tuple relational calculus is
equivalenttotherelationalalgebra,allthreeofthefollowingareequivalent:
  The basic relational algebra (without the extended relational-algebra opera-
tions)
  Thetuplerelationalcalculusrestrictedtosafeexpressions
  Thedomainrelationalcalculusrestrictedtosafeexpressions
We note that the domain relational calculus also does not have any equivalent
of the aggregate operation, but it can be extended to support aggregation, and
extendingittohandlearithmeticexpressionsisstraightforward.
6.4 Summary
  The relational algebra de?nes a set of algebraic operations that operate on
tables, and output tables as their results. These operations can be combined
togetexpressionsthatexpressdesiredqueries.Thealgebrade?nesthebasic
operationsusedwithinrelationalquerylanguages.
  Theoperationsinrelationalalgebracanbedividedinto:
?
Basicoperations
?
Additional operations that can be expressed in terms of the basic opera-
tions
?
Extended operations, some of which add further expressive power to
relationalalgebra
  The relational algebra is a terse, formal language that is inappropriate for
casual users of a database system. Commercial database systems, therefore,
PracticeExercises 249
uselanguageswithmore“syntacticsugar.”InChapters3through5,wecover
themostin?uentiallanguage—SQL,whichisbasedonrelationalalgebra.
  The tuple relational calculus and the domain relational calculus are non-
procedurallanguagesthatrepresentthebasicpowerrequiredinarelational
querylanguage.Thebasicrelationalalgebraisaprocedurallanguagethatis
equivalent in power to both forms of the relational calculus when they are
restrictedtosafeexpressions.
  The relational calculi are terse, formal languages that are inappropriate for
casualusersofadatabasesystem.Thesetwoformallanguagesformthebasis
for two more user-friendly languages, QBE and Datalog, that we cover in
AppendixB.
Review Terms
  Relationalalgebra
  Relational-algebraoperations
?
Select  ?
Project null
?
Union ?
?
Setdifference ?
?
Cartesianproduct ×
?
Rename    Additionaloperations
?
Setintersection ?
?
Naturaljoin  ?
Assignmentoperation
?
Outerjoin
null Leftouterjoin   null Rightouterjoin  null Fullouterjoin     Multisets
  Grouping
  Nullvalue
  Tuplerelationalcalculus
  Domainrelationalcalculus
  Safetyofexpressions
  Expressivepoweroflanguages
Practice Exercises
6.1 Write the following queries in relational algebra, using the university
schema.
a. Find the titles of courses in the Comp. Sci. department that have 3
credits.
b. Findthe IDsofallstudentswhoweretaughtbyaninstructornamed
Einstein;makesuretherearenoduplicatesintheresult.
c. Findthehighestsalaryofanyinstructor.
d. Find all instructors earning the highest salary (there may be more
thanonewiththesamesalary).
250 Chapter6 FormalRelationalQueryLanguages
employee(person name,street,city)
works(person name,company name, salary)
company(company name,city)
manages(person name,manager name)
Figure 6.22 Relational database for Exercises 6.2, 6.8, 6.11, 6.13, and 6.15
e. FindtheenrollmentofeachsectionthatwasofferedinAutumn2009.
f. Findthemaximumenrollment,acrossallsections,inAutumn2009.
g. FindthesectionsthathadthemaximumenrollmentinAutumn2009.
6.2 ConsidertherelationaldatabaseofFigure6.22,wheretheprimarykeysare
underlined.Giveanexpressionintherelationalalgebratoexpresseachof
thefollowingqueries:
a. Findthenamesofallemployeeswholiveinthesamecityandonthe
samestreetasdotheirmanagers.
b. Find the names of all employees in this database who do not work
for “FirstBankCorporation”.
c. Findthenamesofallemployeeswhoearnmorethaneveryemployee
of “SmallBankCorporation”.
6.3 Thenaturalouter-joinoperationsextendthenatural-joinoperationsothat
tuplesfromtheparticipatingrelationsarenotlostintheresultofthejoin.
Describehowthetheta-joinoperationcanbeextendedsothattuplesfrom
theleft,right,orbothrelationsarenotlostfromtheresultofathetajoin.
6.4 (Division operation): The division operator of relational algebra, “÷”,is
de?ned as follows. Let r(R)ands(S) be relations, and let S ? R;thatis,
everyattributeofschema Sisalsoinschema R.Thenr ÷ s isarelationon
schema R ? S(that is, on the schema containing all attributes of schema
Rthatarenotinschema S).Atupletisinr ÷ s ifandonlyifbothoftwo
conditionshold:
  tisin null
R?S
(r)
  For every tuple t
s
in s, there is a tuple t
r
in r satisfying both of the
following:
a. t
r
[S] = t
s
[S]
b. t
r
[R ? S] = t
Giventheabovede?nition:
a. Write a relational algebra expression using the division operator to
?nd the IDs of all students who have taken all Comp. Sci. courses.
(Hint: project takes to just ID and course id, and generate the set of
Practice Exercises 251
all Comp. Sci. course ids using a select expression, before doing the
division.)
b. Show how to write the above query in relational algebra, without
using division. (By doing so, you would have shown how to de?ne
the division operation using the other relational algebra operations.)
6.5 Let the following relation schemas be given:
R = (A,B,C)
S = (D,E,F)
Let relations r(R)ands(S) be given. Give an expression in the tuple rela-
tional calculus that is equivalent to each of the following:
a. null A
(r)
b.   B =17
(r)
c. r × s
d. null A,F
(  C =D
(r × s))
6.6 Let R = (A, B, C), and letr
1
andr
2
both be relations on schema R.Give
an expression in the domain relational calculus that is equivalent to each
of the following:
a. null A
(r
1
)
b.   B =17
(r
1
)
c. r
1
? r
2
d. r
1
? r
2
e. r
1
? r
2
f. null A,B
(r
1
)  null B,C
(r
2
)
6.7 Let R = (A,B)andS = (A,C), and let r(R)ands(S)berelations.W rite
expressions in relational algebra for each of the following queries:
a. {< a > |?b (< a,b > ? r ? b = 7)}
b. {< a,b,c > | < a,b > ? r ? < a,c > ? s}
c. {< a > |?c (< a,c > ? s ??b
1
,b
2
(< a,b
1
> ? r ? < c, b
2
> ?
r ? b
1
> b
2
))}
6.8 Considerthe relational database ofFigure 6.22where the primary keysare
underlined. Give an expression in tuple relational calculus for each of the
following queries:
252 Chapter6 FormalRelationalQueryLanguages
a. Find all employees who work directly for “Jones.”
b. Find all cities of residence of all employees who work directly for
“Jones.”
c. Find the name of the manager of the manager of “Jones.”
d. Findthoseemployeeswhoearnmorethanallemployeeslivinginthe
city “Mumbai.”
6.9 Describe how to translate join expressions in SQL to relational algebra.
Exercises
6.10 Write the following queries in relational algebra, using the university
schema.
a. FindthenamesofallstudentswhohavetakenatleastoneComp.Sci.
course.
b. Findthe IDsandnamesofallstudentswhohavenottakenanycourse
offering before Spring 2009.
c. For each department, ?nd the maximum salary of instructors in that
department.You may assume that everydepartmenthas at least one
instructor.
d. Find the lowest, across all departments,of the per-departmentmaxi-
mum salary computed by the preceding query.
6.11 ConsidertherelationaldatabaseofFigure6.22,wheretheprimarykeysare
underlined. Give an expression in the relational algebra to express each of
the following queries:
a. Find the names of all employees who work for “First Bank Corpora-
tion”.
b. Findthenamesandcitiesofresidenceofallemployeeswhoworkfor
“First Bank Corporation”.
c. Find the names, street addresses, and cities of residence of all em-
ployees who work for “First Bank Corporation” and earn more than
$10,000.
d. Findthenamesofallemployeesinthisdatabasewholiveinthesame
city as the company for which they work.
e. Assumethecompaniesmaybelocatedinseveralcities.Findallcom-
panies located in every city in which “Small Bank Corporation” is
located.
Exercises 253
6.12 Using the university example, write relational-algebra queries to ?nd the
coursesectionstaughtbymorethanoneinstructorinthefollowingways:
a. Usinganaggregatefunction.
b. Withoutusinganyaggregatefunctions.
6.13 Consider the relational database of Figure 6.22. Give a relational-algebra
expressionforeachofthefollowingqueries:
a. Findthecompanywiththemostemployees.
b. Findthecompanywiththesmallestpayroll.
c. Findthose companies whose employeesearna higher salary,on av-
erage,thantheaveragesalaryatFirstBankCorporation.
6.14 Considerthefollowingrelationalschemaforalibrary:
member(memb no,name,dob)
books(isbn, title, authors,publisher)
borrowed(memb no,isbn,date)
Writethefollowingqueriesinrelationalalgebra.
a. Findthenamesofmemberswhohaveborrowedanybookpublished
by “McGraw-Hill”.
b. Findthe name of memberswho have borrowed all books published
by “McGraw-Hill”.
c. Findthenameandmembershipnumberofmemberswho havebor-
rowedmorethan?vedifferentbookspublishedby “McGraw-Hill”.
d. Foreachpublisher,?ndthenameandmembershipnumberofmem-
berswhohaveborrowedmorethan?vebooksofthatpublisher.
e. Find the average number of books borrowed per member. Take into
account that if an member does not borrow any books, then that
memberdoesnotappearintheborrowedrelationatall.
6.15 Consider the employeedatabase of Figure 6.22. Give expressions in tuple
relationalcalculusanddomainrelationalcalculusforeachofthefollowing
queries:
a. Findthenamesofallemployeeswho workfor “FirstBankCorpora-
tion”.
b. Findthenamesandcitiesofresidenceofallemployeeswhoworkfor
“FirstBankCorporation”.
c. Find the names, street addresses, and cities of residence of all em-
ployeeswho work for “First Bank Corporation” and earn more than
$10,000.
254 Chapter6 FormalRelationalQueryLanguages
d. Find all employees who live in the same city as that in which the
companyforwhichtheyworkislocated.
e. Find all employeeswho livein the same city and on the same street
astheirmanagers.
f. Findall employeesinthe database who donot work for “First Bank
Corporation”.
g. Find all employees who earn more than every employee of “Small
BankCorporation”.
h. Assumethatthecompaniesmaybelocatedinseveralcities.Findall
companies located in every city in which “Small Bank Corporation”
islocated.
6.16 Let R = (A, B)andS = (A, C), and let r(R)ands(S)berelations.
Write relational-algebra expressions equivalent to the following domain-
relational-calculusexpressions:
a. {< a > |?b (< a,b > ? r ? b = 17)}
b. {< a,b,c > | < a,b > ? r ? < a,c > ? s}
c. {< a > |?b(< a,b > ? r)??c(?d (< d,c > ? s) ? < a,c > ? s)}
d. {< a > |?c (< a,c > ? s ?? b
1
,b
2
(< a,b
1
> ? r ? < c,b
2
>
? r ? b
1
> b
2
))}
6.17 Repeat Exercise 6.16, writing SQL queries instead of relational-algebra ex-
pressions.
6.18 Let R = (A, B)andS = (A, C), and let r(R)ands(S)berelations.
Usingthespecialconstant null,writetuple-relational-calculusexpressions
equivalenttoeachofthefollowing:
a. r   s
b. r   s
c. r   s
6.19 Give a tuple-relational-calculus expression to ?nd the maximum value in
relationr(A).
Bibliographical Notes
The original de?nition of relational algebra is in Codd [1970]. Extensions to the
relationalmodelanddiscussionsofincorporationofnullvaluesintherelational
algebra(the RM/Tmodel),aswellasouterjoins,areinCodd[1979].Codd[1990]
is a compendium of E.F. Codd’s paperson the relational model.Outerjoins are
alsodiscussedinDate[1993b].
BibliographicalNotes 255
The original de?nition of tuple relational calculus is in Codd [1972]. A for-
mal proof of the equivalence of tuple relational calculus and relational algebra
is in Codd [1972]. Several extensions to the relational calculus have been pro-
posed.Klug[1982]andEscobar-Molanoetal.[1993]describeextensionstoscalar
aggregatefunctions.
This page intentionally left blank 
PART
2
DATABASEDESIGN
Databasesystemsaredesignedtomanagelargebodiesofinformation.Theselarge
bodies of information do not exist in isolation. They are part of the operation of
some enterprise whose end product may be information from the database or
may be some device or service for which the database plays only a supporting
role.
The ?rst two chapters of this part focus on the design of database schemas.
The entity-relationship (E-R) model described in Chapter 7 is a high-level data
model. Instead of representing all data in tables, it distinguishes between basic
objects, called entities,andrelationships among these objects. It is often used as a
?rst step in database-schema design.
Relational database design—the design of the relational schema— was cov-
ered informally in earlier chapters. There are, however, principles that can be
used to distinguish good database designs from bad ones. These are formal-
ized by means of several “normal forms” that offer different trade-offs between
the possibility of inconsistencies and the ef?ciency of certain queries. Chapter 8
describes the formal design of relational schemas.
The design of a complete database application environment that meets the
needs of the enterprise being modeled requires attention to a broader set of
issues, many of which are covered in Chapter 9. This chapter ?rst covers the
design of Web-based interfaces to applications. The chapter then describes how
largeapplicationsarearchitectedusingmultiplelayersofabstraction.Finally,the
chapterprovidesadetaileddiscussionofsecurityattheapplicationanddatabase
levels.
257
This page intentionally left blank 
CHAPTER
7
Database Design and the E-R
Model
Up to this point in the text, we have assumed a given database schema and
studiedhowqueriesandupdatesareexpressed.Wenowconsiderhowtodesign
a database schema in the ?rst place. In this chapter, we focus on the entity-
relationship data model (E-R), which provides a means of identifying entities to
berepresentedinthedatabaseandhowthoseentitiesarerelated.Ultimately,the
databasedesignwillbeexpressedintermsofarelationaldatabasedesignandan
associated set of constraints. We show in this chapter how an E-R design can be
transformed into a set of relation schemas and how some of the constraints can
be captured in that design. Then, in Chapter 8, we consider in detail whether a
set of relation schemas is a good or bad database design and study the process
of creating good designs using a broader set of constraints. These two chapters
coverthefundamental concepts ofdatabasedesign.
7.1 Overview of the Design Process
The task of creating a database application is a complex one, involving design
of the database schema, design of the programs that access and update the data,
and design of a security scheme to control access to data. The needs of the users
playacentralroleinthedesignprocess.Inthischapter,wefocusonthedesignof
the database schema, although we brie?y outline some of the other design tasks
laterinthechapter.
The design of a complete database application environment that meets the
needsoftheenterprisebeingmodeledrequiresattentiontoabroad setofissues.
These additional aspects of the expected use of the database in?uence a variety
ofdesignchoicesatthe physical,logical,and viewlevels.
7.1.1 Design Phases
Forsmallapplications,itmaybefeasibleforadatabasedesignerwhounderstands
the application requirements to decide directly on the relations to be created,
259
260 Chapter 7 Database Design and the E-R Model
their attributes, and constraints on the relations. However, such a direct design
processisdif?cultforreal-worldapplications,sincetheyareoftenhighlycomplex.
Often no one person understands the complete data needs of an application.
The database designer must interact with users of the application to understand
the needs of the application, represent them in a high-level fashion that can be
understoodbytheusers,andthentranslatetherequirementsintolowerlevelsof
the design. A high-level data model serves the database designer by providing
a conceptual framework in which to specify, in a systematic fashion, the data
requirements of the database users, and a database structure that ful?lls these
requirements.
• Theinitialphase ofdatabasedesignistocharacterizefully thedataneedsof
the prospective database users. The database designer needs to interact ex-
tensivelywith domain expertsand usersto carryout this task. The outcome
of this phase is a speci?cation of user requirements. While there are tech-
niques for diagrammatically representing user requirements,in this chapter
werestrictourselvestotextualdescriptionsofuserrequirements.
• Next,thedesignerchoosesadatamodeland,byapplyingtheconceptsofthe
chosen data model, translates these requirements into a conceptual schema
ofthedatabase.Theschemadevelopedatthisconceptual-designphasepro-
vides a detailed overview of the enterprise. The entity-relationship model,
which we study in the rest of this chapter, is typically used to represent the
conceptual design. Stated in terms of the entity-relationship model, the con-
ceptualschemaspeci?estheentitiesthatarerepresentedinthedatabase,the
attributesoftheentities,therelationshipsamongtheentities,andconstraints
on the entities and relationships. Typically, the conceptual-design phase re-
sultsinthecreationofanentity-relationshipdiagramthatprovidesagraphic
representationoftheschema.
Thedesignerreviewstheschematocon?rmthatalldatarequirementsare
indeedsatis?edandarenotincon?ictwithoneanother.Shecanalsoexamine
the design to remove any redundant features. Her focus at this point is on
describingthedataandtheirrelationships,ratherthanonspecifyingphysical
storagedetails.
• A fully developed conceptual schema also indicates the functional require-
mentsoftheenterprise.Inaspeci?cationoffunctionalrequirements,users
describe the kinds of operations (or transactions) that will be performed on
thedata.Exampleoperationsincludemodifyingorupdatingdata,searching
forandretrievingspeci?cdata,anddeletingdata.Atthisstageofconceptual
design, the designer can review the schema to ensure it meets functional
requirements.
• Theprocessofmovingfromanabstractdatamodeltotheimplementationof
thedatabase proceedsintwo?nal designphases.
?
Inthe logical-design phase,thedesignermapsthehigh-levelconceptual
schema onto the implementation data model of the database system that
7.1 Overview of the Design Process 261
will be used. The implementation data model is typically the relational
data model, and this step typically consists of mapping the conceptual
schemade?nedusingtheentity-relationshipmodelintoarelationschema.
?
Finally, the designer uses the resulting system-speci?c database schema
in the subsequent physical-design phase, in which the physical features
of the database are speci?ed. These features include the form of ?le or-
ganization and choice of index structures, discussed in Chapters 10 and
11.
The physical schema of a database can be changed relatively easily after an
application has been built. However, changes to the logical schema are usually
harder to carry out, since they may affect a number of queries and updates scat-
tered across application code. It is therefore important to carry out the database
designphasewithcare,beforebuildingthe restofthe databaseapplication.
7.1.2 Design Alternatives
A major part of the database design process is deciding how to represent in the
design the various types of “things” such as people, places, products, and the
like. We use the term entity to refer to any such distinctly identi?able item. In
a university database, examples of entities would include instructors, students,
departments, courses, and course offerings.
1
The various entities are related to
each other in a variety of ways, all of which need to be captured in the database
design.Forexample,astudenttakesacourseoffering,whileaninstructorteaches
acourseoffering;teachesandtakesareexamplesofrelationshipsbetweenentities.
In designing a database schema, we must ensure that we avoid two major
pitfalls:
1. Redundancy: A bad design may repeat information. For example, if we
storethecourseidenti?erandtitleofacoursewitheachcourseoffering,the
title would be stored redundantly (that is, multiple times, unnecessarily)
witheachcourseoffering.Itwouldsuf?cetostoreonlythecourseidenti?er
witheachcourseoffering,andtoassociatethetitlewiththecourseidenti?er
only once, inacourse entity.
Redundancy can also occur in a relational schema. In the university
example we have used so far, we have a relation with section information
and a separate relation with course information. Suppose that instead we
haveasinglerelationwherewerepeatallofthecourseinformation(course
id, title, dept name, credits) once for each section (offering) of the course.
Clearly,informationabout courses wouldthenbe storedredundantly.
Thebiggestproblemwithsuchredundantrepresentationofinformation
is that the copies of a piece of information can become inconsistent if the
1
Acoursemay haveruninmultiplesemesters,aswellasmultipletimesinasemester.Werefertoeachsuchofferingof
acourseasasection.
262 Chapter 7 Database Design and the E-R Model
information is updated without taking precautions to update all copies of
the information. For example, different offerings of a course may have the
same course identi?er, but may have different titles. It would then become
unclear what the correct title of the course is. Ideally, information should
appearinexactlyone place.
2. Incompleteness: A bad design may make certain aspects of the enterprise
dif?cult or impossible to model. For example, suppose that, as in case (1)
above, we only had entitiescorresponding tocourse offering, without hav-
ing an entity corresponding to courses. Equivalently, in terms of relations,
suppose we have a single relation where we repeat all of the course infor-
mation once for each section that the course is offered. It would then be
impossibletorepresentinformationabout anewcourse,unlessthat course
is offered. We might try to make do with the problematic design by stor-
ing null valuesfor the sectioninformation. Such awork-around isnot only
unattractive,butmaybepreventedbyprimary-keyconstraints.
Avoiding bad designs is not enough. There may be a large number of good
designs from which we must choose. As a simple example, consider a customer
whobuysaproduct.Isthesaleofthisproductarelationshipbetweenthecustomer
andtheproduct?Alternatively,isthesaleitselfanentitythatisrelatedbothtothe
customerandtotheproduct?Thischoice,thoughsimple,maymakeanimportant
differenceinwhataspectsoftheenterprisecanbemodeledwell.Consideringthe
needtomakechoicessuchasthisforthelargenumberofentitiesandrelationships
in a real-world enterprise, it is not hard to see that database design can be a
challenging problem. Indeed we shall see that it requires a combination of both
science and “good taste.”
7.2 The Entity-Relationship Model
The entity-relationship (E-R) data model was developed to facilitate database
designbyallowingspeci?cationofan enterpriseschemathatrepresentstheoverall
logical structureofadatabase.
The E-R model is very useful in mapping the meanings and interactions of
real-worldenterprisesontoaconceptualschema.Becauseofthisusefulness,many
database-designtools drawon concepts from the E-R model.The E-R datamodel
employsthreebasic concepts: entitysets,relationship sets,and attributes, which
westudy?rst.TheE-Rmodelalsohasanassociateddiagrammaticrepresentation,
the E-Rdiagram,which westudylaterinthischapter.
7.2.1 Entity Sets
An entity is a “thing” or “object” in the real world that is distinguishable from
all other objects. For example, each person in a university is an entity. An entity
has a set of properties, and the values for some set of properties may uniquely
identify an entity. For instance, a person may have a person id property whose
7.2 The Entity-Relationship Model 263
value uniquely identi?es that person. Thus, the value 677-89-9011 for person id
woulduniquelyidentifyoneparticularpersonintheuniversity.Similarly,courses
can be thought of as entities, and course id uniquely identi?es a course entity in
the university. An entity may be concrete, such as a person or a book, or it may
be abstract,such as acourse,acourse offering,ora?ight reservation.
Anentitysetisasetofentitiesofthesametypethatsharethesameproperties,
or attributes. The set of all people who are instructors at a given university, for
example,canbede?nedastheentitysetinstructor.Similarly,theentitysetstudent
mightrepresentthe setofallstudentsintheuniversity.
In the process of modeling, we often use the term entity set in the abstract,
without referring to a particular set of individual entities. We use the term ex-
tension of the entity set to refer to the actual collection of entities belonging to
theentityset.Thus,thesetofactualinstructorsintheuniversityformstheexten-
sion of the entity set instructor. The above distinction is similar to the difference
betweenarelationand arelationinstance, which we sawinChapter2.
Entitysetsdonotneedtobedisjoint.Forexample,itispossibletode?nethe
entitysetofallpeopleinauniversity(person).Apersonentitymaybeaninstructor
entity,a studententity,both, orneither.
An entity is represented by a set of attributes. Attributes are descriptive
properties possessed by each member of an entity set. The designation of an
attribute for an entity set expresses that the database stores similar information
concerning each entity in the entity set; however, each entity may have its own
valueforeachattribute.Possibleattributesoftheinstructorentitysetare ID,name,
dept name,andsalary. In reallife,therewould be furtherattributes,such as street
number,apartmentnumber,state,postalcode,andcountry,butweomitthemto
keepourexamplessimple.Possibleattributesofthecourseentitysetare course id,
title, dept name,andcredits.
Each entity has a value for each of its attributes. For instance, a particular
instructorentitymayhavethevalue12121for ID,thevalueWuforname,thevalue
Finance for dept name,and thevalue90000for salary.
The ID attribute is used to identify instructors uniquely, since there may
be more than one instructor with the same name. In the United States, many
enterprises?nd it convenient to use the social-security number of a person
2
as an
attribute whose value uniquely identi?es the person. In general the enterprise
would havetocreateand assignaunique identi?erfor eachinstructor.
A database thus includes a collection of entity sets, each of which contains
any number of entities of the same type. Figure 7.1 shows part of a university
databasethatconsistsoftwoentitysets: instructorand student.Tokeepthe?gure
simple,only someoftheattributesofthetwoentitysetsareshown.
A database for a university may include a number of other entity sets. For
example, in addition to keeping track of instructors and students, the university
alsohasinformationaboutcourses,whicharerepresentedbytheentitysetcourse
2
In the United States, the government assigns to each person in the country a unique number, called a social-security
number, toidentifythatpersonuniquely.Eachpersonissupposedtohaveonlyonesocial-securitynumber, andnotwo
peopleare supposedto havethe same social-securitynumber.
264 Chapter 7 Database Design and the E-R Model
instructor
student
22222 Einstein
Katz
Kim
Crick
Srinivasan
Singh
45565
98345
76766
10101
76543
12345
98988
76653
23121
00128
76543
Shankar
Tanaka
Aoi
Chavez
Peltier
Zhang
Brown
44553
Figure 7.1 Entity sets instructor and student.
with attributes course id, title, dept name and credits. In a real setting, a university
databasemaykeepdozensofentitysets.
7.2.2 Relationship Sets
A relationship is an association among several entities. For example, we can
de?nearelationship advisorthatassociatesinstructorKatzwithstudentShankar.
Thisrelationshipspeci?esthat KatzisanadvisortostudentShankar.
A relationship set is a set of relationships of the same type. Formally, it is a
mathematicalrelationonn ? 2(possiblynondistinct)entitysets.If E
1
, E
2
,...,E
n
areentitysets,thenarelationshipset Risasubsetof
{(e
1
,e
2
,...,e
n
) | e
1
? E
1
,e
2
? E
2
,...,e
n
? E
n
}
where (e
1
,e
2
,...,e
n
)isarelationship.
Consider the two entity sets instructor and student in Figure 7.1. We de?ne
the relationship set advisor to denote the association between instructors and
students.Figure7.2depictsthisassociation.
As another example, consider the two entity sets student and section.Wecan
de?ne the relationship set takes to denote the association between a student and
the coursesections inwhich that studentis enrolled.
The association between entity sets is referredto as participation; that is, the
entity sets E
1
, E
2
,...,E
n
participate in relationship set R.Arelationship in-
stance in an E-R schema represents an association between the named entities in
thereal-worldenterprisethatisbeingmodeled.Asanillustration,theindividual
instructorentityKatz,whohasinstructor ID45565,andthestudententityShankar,
whohasstudent ID12345,participateinarelationshipinstanceofadvisor.Thisre-
lationshipinstancerepresentsthatintheuniversity,theinstructorKatzisadvising
studentShankar.
The function that an entity plays in a relationship is called that entity’s role.
Since entity sets participating in a relationship set are generally distinct, roles
7.2 The Entity-Relationship Model 265
instructor
student
76766 Crick
Katz
Srinivasan
Kim
Singh
Einstein
45565
10101
98345
76543
22222
98988
12345
00128
76543
76653
23121
44553
Tanaka
Shankar
Zhang
Brown
Aoi
Chavez
Peltier
Figure 7.2 Relationship set advisor.
are implicit and are not usually speci?ed. However, they are useful when the
meaningofarelationshipneedsclari?cation.Suchisthecasewhentheentitysets
of a relationship set are not distinct; that is, the same entity set participates in a
relationshipsetmorethanonce,indifferentroles.Inthistypeofrelationshipset,
sometimescalledarecursiverelationshipset,explicitrolenamesarenecessaryto
specifyhowanentityparticipatesinarelationshipinstance.Forexample,consider
the entity set course that records information about all the courses offered in the
university. To depict the situation where one course (C2) is a prerequisite for
anothercourse(C1)wehaverelationshipsetprereq that is modeled by ordered
pairsofcourseentities.The?rstcourseofapairtakestheroleofcourseC1,whereas
the second takes the role of prerequisite course C2. In this way, all relationships
of prereqarecharacterized by(C1,C2)pairs;(C2,C1)pairsareexcluded.
A relationship may also have attributes called descriptive attributes.Con-
sider a relationship set advisor with entity sets instructor and student.W ecould
associate the attribute date with that relationship to specify the date when an
instructor became the advisor of a student. The advisor relationship among the
entities corresponding to instructor Katz and student Shankar has the value “10
June 2007” for attribute date, which means that Katz became Shankar’s advisor
on10 June 2007.
Figure7.3shows the relationship set advisor witha descriptiveattribute date.
Pleasenotethat Katzadvisestwo studentswithtwodifferentadvisingdates.
Asamorerealisticexampleofdescriptiveattributesforrelationships,consider
theentitysets studentand section,whichparticipateinarelationshipset takes.We
maywishtostoreadescriptiveattribute gradewiththerelationshiptorecordthe
gradethat astudentgotintheclass.Wemayalsostoreadescriptiveattribute for
credit to record whether a student has taken the course for credit, or is auditing
(or sittinginon)thecourse.
A relationship instance in a given relationship set must be uniquely identi-
?able from its participating entities, without using the descriptive attributes. To
understandthispoint,supposewewanttomodelallthedateswhenaninstructor
266 Chapter 7 Database Design and the E-R Model
instructor
student
76766 Crick
Katz
Srinivasan
Kim
Singh
Einstein
45565
10101
98345
76543
22222
98988
12345
00128
76543
44553
Tanaka
Shankar
Zhang
Brown
Aoi
Chavez
Peltier
3 May 2008
10 June 2007
12 June 2006
6 June 2009
30 June 2007
31 May 2007
4 May 2006
76653
23121
Figure 7.3 date as attribute of the advisor relationship set.
became an advisor of a particular student. The single-valued attribute date can
store a single date only. We cannot represent multiple dates by multiple relation-
ship instances between the same instructor and a student, since the relationship
instanceswouldnotbeuniquelyidenti?ableusingonlytheparticipatingentities.
The right way to handle this case is to create a multivalued attribute date,which
can storeallthedates.
It is possible to have more than one relationship set involving the same
entitysets.Inourexample,the instructorand studententitysetsparticipateinthe
relationship set advisor. Additionally, suppose each student must have another
instructorwhoservesasadepartmentadvisor(undergraduateorgraduate).Then
the instructor and student entity sets may participate in another relationship set,
dept advisor.
The relationship sets advisor and dept advisor provide examples of a binary
relationshipset—thatis,onethatinvolvestwoentitysets.Mostoftherelationship
sets in a database system are binary. Occasionally, however, relationship sets
involvemorethan twoentitysets.
As an example, suppose that we have an entity set project that represents
all the research projects carried out in the university. Consider the entity sets
instructor, student,andproject.Eachprojectcanhavemultipleassociatedstudents
and multiple associated instructors. Furthermore, each student working on a
projectmusthaveanassociatedinstructorwhoguidesthestudentontheproject.
Fornow,weignorethe?rsttworelationships,betweenprojectandinstructor,and
between project and student. Instead, we focus on the information about which
instructor is guiding which student on a particular project. To represent this
information,werelatethethreeentitysetsthroughtherelationshipset proj guide,
whichindicatesthataparticularstudentisguidedbyaparticularinstructorona
particularproject.
Note that a student could have different instructors as guides for different
projects,whichcannotbecapturedbyabinaryrelationshipbetweenstudentsand
instructors.
7.2 The Entity-Relationship Model 267
Thenumberofentitysetsthatparticipateinarelationshipsetisthedegreeof
therelationshipset.Abinaryrelationshipsetisofdegree2;aternaryrelationship
setisofdegree3.
7.2.3 Attributes
For each attribute, there isa set of permittedvalues,called the domain,orvalue
set, of that attribute. The domain of attribute course id might be the set of all text
strings of a certain length. Similarly, the domain of attribute semester might be
stringsfrom theset {Fall,Winter,Spring,Summer}.
Formally, an attribute of an entity set is a function that maps from the entity
setintoadomain.Sinceanentitysetmayhaveseveralattributes,eachentitycan
be described by a set of (attribute, data value) pairs, one pair for each attribute
of the entity set. For example, a particular instructor entity may be described by
the set {(ID, 76766), (name,Crick),( dept name, Biology), (salary, 72000)}, meaning
thattheentitydescribesapersonnamedCrickwhoseinstructor IDis76766,who
is a member of the Biology department with salary of $72,000. We can see, at
this point, an integration of the abstract schema with the actual enterprise being
modeled.Theattributevaluesdescribinganentityconstituteasigni?cantportion
ofthedatastoredinthedatabase.
Anattribute,asused inthe E-R model,can be characterized by the following
attributetypes.
• Simpleandcompositeattributes.Inourexamplesthusfar,theattributeshave
been simple; that is, they have not been divided into subparts. Composite
attributes, on the other hand, can be divided into subparts (that is, other
attributes).Forexample,anattributenamecouldbestructuredasacomposite
attributeconsistingof?rst name,middle initial,andlast name.Usingcomposite
attributesinadesignschemaisagoodchoiceifauserwillwishtorefertoan
entire attribute on some occasions, and to only a component of the attribute
on other occasions. Suppose we were to to add an address to the student
entity-set.Theaddresscanbede?nedasthecompositeattribute addresswith
the attributes street, city, state,andzip code.
3
Composite attributes help us to
grouptogetherrelatedattributes,makingthemodelingcleaner.
Notealsothatacompositeattributemayappearasahierarchy.Inthecom-
posite attribute address, its component attribute street can be further divided
into street number, street name,andapartment number.Figure7.4depictsthese
examplesofcompositeattributesforthe instructor entityset.
• Single-valuedandmultivaluedattributes.Theattributesinourexamplesall
haveasinglevalueforaparticularentity.Forinstance,thestudent IDattribute
for a speci?c student entity refers to only one student ID. Such attributes are
said to be single valued. There may be instances where an attribute has a
set of values for a speci?c entity. Suppose we add to the instructor entity set
3
Weassume theaddress formatusedintheUnitedStates,whichincludesanumericpostal codecalledazipcode.
268 Chapter 7 Database Design and the E-R Model
name address
first_name middle_initial last_name street city state postal_code
street_number street_name apartment_number
composite
attributes
component
attributes
Figure 7.4 Composite attributes instructor name and address.
a phone numberattribute. An instructor may have zero, one, or several phone
numbers, and different instructors may have different numbers of phones.
This type of attribute is said to be multivalued. As another example, we
could add to the instructor entity set an attribute dependent name listing all
the dependents. This attribute would be multivalued, since any particular
instructormay havezero,one,or moredependents.
Todenotethatanattributeismultivalued,weencloseitinbraces,forexample
{phone number}or {dependent name}.
Whereappropriate,upperandlowerboundsmaybeplacedonthenumber
of valuesina multivaluedattribute.For example,a universitymaylimitthe
number of phone numbers recorded for a single instructor to two. Placing
boundsinthiscaseexpressesthatthe phone numberattributeofthe instructor
entitysetmayhave betweenzeroand twovalues.
• Derivedattribute.Thevalueforthistypeofattributecanbederivedfromthe
values of other related attributes or entities. For instance, let us say that the
instructor entity set has an attribute students advised, which represents how
manystudentsaninstructoradvises.Wecanderivethevalueforthisattribute
by counting thenumber of studententitiesassociatedwiththat instructor.
As another example,suppose that the instructor entity set has an attribute
age that indicates the instructor’s age. If the instructor entity set also has an
attribute date of birth, we can calculate age from date of birth and the current
date.Thus, ageisaderivedattribute.Inthiscase,date of birthmaybereferred
to as a base attribute, or a stored attribute. The value of a derived attribute is
notstoredbutiscomputedwhen required.
An attribute takes a null value when an entity does not have a value for it.
The null value may indicate “not applicable”—that is, that the value does not
exist for the entity. For example, one may have no middle name. Null can also
designate that an attribute value is unknown. An unknown value may be either
missing (the value does exist, but we do not have that information) or not known
(we donot know whetheror not thevalueactually exists).
For instance, if the name value for a particular instructor is null, we assume
that the value is missing, since every instructor must have a name. A null value
for the apartment number attribute could mean that the address does not include
7.3 Constraints 269
an apartment number (not applicable), that an apartment number exists but we
do not know what it is (missing), or that we do not know whether or not an
apartmentnumber ispart ofthe instructor’s address(unknown).
7.3 Constraints
An E-Renterpriseschemamayde?necertainconstraintstowhichthecontentsof
adatabase must conform. Inthis section, we examinemapping cardinalitiesand
participationconstraints.
7.3.1 Mapping Cardinalities
Mapping cardinalities, or cardinality ratios, express the number of entities to
which another entitycan beassociatedviaarelationshipset.
Mapping cardinalities are most useful in describing binary relationship sets,
although they can contribute to the description of relationship sets that involve
more than two entity sets. In this section, we shall concentrate on only binary
relationshipsets.
For a binary relationship set R between entity sets A and B, the mapping
cardinalitymustbe one of thefollowing:
• One-to-one.AnentityinA is associated with at most one entity in B,andan
entityin B isassociatedwith at mostone entityin A.(SeeFigure7.5a.)
• One-to-many.AnentityinA is associated with any number (zero or more)
of entities in B.AnentityinB, however, can be associated with at most one
entityin A.(SeeFigure7.5b.)
(b) (a)
a
1
a
2
a
3
a
4
b
1
b
2
b
3
a
2
a
1
a
3
b
1
b
2
b
3
b
4
b
5
AB AB
Figure 7.5 Mapping cardinalities. (a) One-to-one. (b) One-to-many.
270 Chapter 7 Database Design and the E-R Model
a
a
2
a
3
a
5
a
1
a
2
a
4
a
2
a
1
a
3
a
4
b
1
b
2
b
3
ABB A
b
1
b
2
b
3
b
4
(a) (b)
Figure 7.6 Mapping cardinalities. (a) Many-to-one. (b) Many-to-many.
• Many-to-one.AnentityinA is associated with at most one entity in B.An
entity in B, however, can be associated with any number (zero or more) of
entitiesin A.(SeeFigure7.6a.)
• Many-to-many.AnentityinAisassociatedwithanynumber(zeroormore)
of entities in B, and an entity in B is associated with any number (zero or
more)ofentitiesin A.(SeeFigure7.6b.)
The appropriate mapping cardinality for a particular relationship set obviously
dependsonthe real-worldsituationthat therelationshipsetismodeling.
As an illustration, consider the advisor relationship set. If, in a particular
university, a student can be advised by only one instructor, and an instructor
can advise severalstudents, then the relationship set from instructor to student is
one-to-many. If a student can be advised by several instructors (as in the case of
studentsadvisedjointly),the relationshipsetis many-to-many.
7.3.2 Participation Constraints
Theparticipationofanentityset E inarelationshipset Rissaidtobetotalifevery
entityin E participatesinatleastonerelationshipin R.Ifonlysomeentitiesin E
participateinrelationshipsin R,theparticipationofentityset E inrelationship R
issaidtobepartial.InFigure7.5a,theparticipationof B intherelationshipsetis
total while the participation of Ain the relationship set is partial. In Figure 7.5b,
the participationofboth Aand B intherelationshipsetaretotal.
For example, we expect every student entity to be related to at least one
instructor through the advisor relationship. Therefore the participation of student
intherelationshipsetadvisoristotal.Incontrast,aninstructorneednotadviseany
students. Hence, it is possible that only some of the instructor entitiesare related
to the student entity set through the advisor relationship, and the participation of
instructor inthe advisor relationshipsetisthereforepartial.
7.3 Constraints 271
7.3.3 Keys
We must have a way to specify how entities within a given entity set are distin-
guished. Conceptually, individual entities are distinct; from a database perspec-
tive, however, the differences among them must be expressed in terms of their
attributes.
Therefore, the values of the attribute values of an entity must be such that
they can uniquely identify the entity. In other words, no two entities in an entity
setareallowedtohave exactlythesamevaluefor allattributes.
The notion of a key for a relation schema, as de?ned in Section 2.3, applies
directly to entity sets. That is, a key for an entity is a set of attributes that suf?ce
to distinguish entities from each other. The concepts of superkey, candidate key,
andprimarykeyareapplicabletoentitysetsjustastheyareapplicabletorelation
schemas.
Keys also help to identify relationships uniquely, and thus distinguish rela-
tionships from each other. Below, we de?ne the corresponding notions of keys
for relationships.
The primary key of an entity set allows us to distinguish among the various
entitiesoftheset.Weneedasimilarmechanismtodistinguishamongthevarious
relationshipsofarelationshipset.
Let R be a relationship set involving entity sets E
1
, E
2
,...,E
n
.Letprimary-
key(E
i
) denote the set of attributes that forms the primary key for entity set E
i
.
Assume for now that the attribute names of all primary keys are unique. The
composition of the primary key for a relationship set depends on the set of
attributesassociatedwiththerelationshipset R.
If the relationship set R has no attributes associated with it, then the set of
attributes
primary-key(E
1
) ? primary-key(E
2
)?···?primary-key(E
n
)
describesan individualrelationshipinset R.
If the relationship set R has attributes a
1
,a
2
,...,a
m
associated with it, then
thesetofattributes
primary-key(E
1
) ? primary-key(E
2
)?···?primary-key(E
n
) ?{ a
1
,a
2
,...,a
m
}
describesan individualrelationshipinset R.
Inbothoftheabove cases,thesetofattributes
primary-key(E
1
) ? primary-key(E
2
)?···?primary-key(E
n
)
formsasuperkeyforthe relationshipset.
If the attribute names of primary keys are not unique across entity sets, the
attributes are renamed to distinguish them; the name of the entity set combined
with the name of the attribute would form a unique name. If an entity set par-
ticipates more than once in a relationship set (as in the prereq relationship in
272 Chapter 7 Database Design and the E-R Model
Section7.2.2),the rolename is usedinsteadof thename ofthe entityset,toform
auniqueattributename.
The structure of the primary key for the relationship set depends on the
mappingcardinalityoftherelationshipset.Asanillustration,considertheentity
sets instructor and student, and the relationship set advisor, with attribute date,in
Section7.2.2.Supposethattherelationshipsetismany-to-many.Thentheprimary
keyofadvisorconsistsoftheunionoftheprimarykeysofinstructorandstudent.If
the relationship is many-to-one from student to instructor—that is, each student
canhavehaveatmostoneadvisor—thentheprimarykeyofadvisorissimplythe
primary key of student. However, if an instructor can advise only one student—
that is,if the advisor relationship ismany-to-one from instructor to student—then
theprimarykeyof advisor issimplytheprimarykeyof instructor.Forone-to-one
relationshipseithercandidatekeycan be usedas theprimarykey.
Fornonbinaryrelationships,ifnocardinalityconstraintsarepresentthenthe
superkeyformedasdescribedearlierinthissectionistheonlycandidatekey,and
itischosenastheprimarykey.Thechoiceoftheprimarykeyismorecomplicated
ifcardinalityconstraintsarepresent.Sincewehavenotdiscussedhowtospecify
cardinalityconstraintsonnonbinaryrelations,wedonotdiscussthisissuefurther
inthischapter.Weconsidertheissueinmoredetaillater,inSections7.5.5and8.4.
7.4 Removing Redundant Attributes in Entity Sets
When we designa database using the E-R model,we usuallystart byidentifying
thoseentitysetsthatshouldbeincluded.Forexample,intheuniversityorganiza-
tionwehavediscussedthusfar,wedecidedtoincludesuchentitysetsas student,
instructor, etc. Once the entity sets are decidedupon, we must choose the appro-
priate attributes. These attributes are supposed to represent the various values
we want to capture in the database. In the university organization, we decided
thatforthe instructorentityset,wewillincludetheattributes ID, name, dept name,
and salary. We could haveaddedtheattributes: phone number, of?ce number, home
page, etc. The choice of what attributes to include is up to the designer, who has
agoodunderstanding ofthe structureof the enterprise.
Once the entities and their corresponding attributes are chosen, the relation-
shipsetsamongthevariousentitiesareformed.Theserelationshipsetsmayresult
in a situation where attributes in the various entity sets are redundant and need
to be removed from the original entity sets. To illustrate, consider the entity sets
instructor and department:
• Theentitysetinstructorincludestheattributes ID,name,dept name,andsalary,
with IDformingthe primarykey.
• The entityset departmentincludesthe attributes dept name, building,andbud-
get,withdept nameformingthe primarykey.
We model the fact that each instructor has an associated department using a
relationshipset inst dept relating instructorand department.
7.4 Removing Redundant Attributes in Entity Sets 273
Theattribute dept nameappearsinbothentitysets.Sinceitistheprimarykey
for the entity set department, it is redundant in the entity set instructor and needs
toberemoved.
Removing the attribute dept name from the instructor entity set may appear
rather unintuitive, since the relation instructor that we used in the earlier chap-
tershad an attribute dept name. As we shall seelater, when we create a relational
schema from the E-R diagram, the attribute dept name in fact gets added to the
relation instructor, but only if each instructor has at most one associated depart-
ment. If an instructor has more than one associateddepartment,the relationship
betweeninstructorsanddepartmentsisrecordedinaseparaterelation inst dept.
Treatingtheconnectionbetweeninstructorsanddepartmentsuniformlyasa
relationship,ratherthanasanattributeofinstructor,makesthelogicalrelationship
explicit,andhelpsavoidaprematureassumptionthateachinstructorisassociated
withonlyone department.
Similarly, the student entity set is related to the department entity set through
therelationshipsetstudent deptandthusthereisnoneedfora dept nameattribute
in student.
Asanother example,considercourseofferings(sections)alongwiththetime
slotsoftheofferings.Eachtimeslotisidenti?edbyatime slot id,andhasassociated
with ita setofweeklymeetings,each identi?edby adayof the week,starttime,
andendtime.Wedecidetomodelthesetofweeklymeetingtimesasamultivalued
compositeattribute.Supposewemodelentitysetssectionandtime slotasfollows:
• The entity set section includes the attributes course id, sec id, semester, year,
building, room number,andtime slot id,with( course id, sec id, year, semester)
formingtheprimarykey.
• Theentitysettime slotincludestheattributestime slot id,whichistheprimary
key,
4
and amultivaluedcomposite attribute {(day,start time,end time)}.
5
Theseentitiesarerelatedthroughtherelationshipset sec time slot.
The attribute time slot id appears in both entity sets. Since it is the primary
keyfortheentitysettime slot,itisredundantintheentityset sectionandneedsto
be removed.
As a ?nal example, suppose we have an entity set classroom, with attributes
building, room number,andcapacity,withbuilding and room number forming the
primary key. Suppose also that we have a relationship set sec class that relates
section to classroom. Then the attributes {building, room number} are redundant in
theentityset section.
Agoodentity-relationshipdesigndoesnotcontainredundantattributes.For
our university example, we list the entity sets and their attributes below, with
primarykeysunderlined:
4
Weshallseelateronthattheprimarykeyfortherelationcreatedfromtheentityset time slotincludesdayand start time;
however, day and start time do notform part ofthe primary keyofthe entityset time slot.
5
Wecouldoptionallygiveaname, such as meeting, forthe compositeattribute containing day, start time,andend time.
274 Chapter 7 Database Design and the E-R Model
• classroom: withattributes(building, room number, capacity).
• department:withattributes(dept name, building, budget).
• course:withattributes(course id, title, credits).
• instructor:withattributes(ID, name, salary).
• section: withattributes(course id, sec id, semester, year).
• student:withattributes(ID, name, tot cred).
• time slot: withattributes(time slot id, {(day, start time, end time) }).
The relationshipsetsinourdesignarelistedbelow:
• inst dept:relatinginstructorswithdepartments.
• stud dept:relatingstudentswithdepartments.
• teaches: relatinginstructorswithsections.
• takes:relatingstudentswithsections,withadescriptiveattribute grade.
• course dept:relatingcourseswithdepartments.
• sec course:relatingsectionswithcourses.
• sec class: relatingsections withclassrooms.
• sec time slot:relatingsectionswithtimeslots.
• advisor:relatingstudentswithinstructors.
• prereq:relatingcourseswithprerequisitecourses.
You can verify that none of the entity sets has any attribute that is made
redundant by one of the relationship sets. Further, you can verify that all the
information (other than constraints) in the relational schema for our university
database, which we saw earlier in Figure 2.8 in Chapter 2, has been captured by
the above design,but withseveralattributesinthe relationaldesignreplacedby
relationshipsinthe E-R design.
7.5 Entity-Relationship Diagrams
Aswesawbrie?yinSection1.3.3,an E-Rdiagramcan expresstheoveralllogical
structureofadatabasegraphically. E-Rdiagramsaresimpleandclear—qualities
that may wellaccount inlargepart forthe widespreaduseofthe E-Rmodel.
7.5.1 Basic Structure
An E-Rdiagramconsists of the following majorcomponents:
7.5 Entity-Relationship Diagrams 275
instructor
ID
name
salary
student
ID
name
tot_cred
advisor
Figure 7.7 E-R diagram corresponding to instructors and students.
• Rectanglesdividedintotwopartsrepresententitysets.The?rstpart,which
inthistextbookisshadedblue,containsthenameoftheentityset.Thesecond
partcontains thenames ofalltheattributesoftheentityset.
• Diamondsrepresentrelationshipsets.
• Undividedrectanglesrepresenttheattributesofarelationshipset.Attributes
that arepartoftheprimarykeyareunderlined.
• Lineslinkentitysetstorelationshipsets.
• Dashed lineslinkattributesofarelationshipsettothe relationshipset.
• Doublelinesindicatetotalparticipationofanentityinarelationshipset.
• Double diamonds represent identifying relationship sets linked to weak
entitysets(wediscussidentifyingrelationshipsetsandweakentitysetslater,
inSection7.5.6).
Consider the E-R diagram in Figure 7.7, which consists of two entity sets, in-
structorandstudent relatedthroughabinaryrelationshipsetadvisor.Theattributes
associated with instructor are ID, name,andsalary. The attributes associated with
student are ID, name,andtot cred. In Figure 7.7, attributes of an entity set that are
membersoftheprimarykeyareunderlined.
Ifarelationshipsethassomeattributesassociatedwithit,thenweenclosethe
attributesinarectangleandlinktherectanglewithadashedlinetothediamond
representing that relationship set. For example, in Figure 7.8, we have the date
descriptiveattributeattachedtotherelationshipset advisortospecifythedateon
which aninstructor becamethe advisor.
ID
name
salary
ID
name
tot_cred
date
instructor student
advisor
Figure 7.8 E-R diagram with an attribute attached to a relationship set.
276 Chapter 7 Database Design and the E-R Model
instructor student
ID
name
salary
instructor
ID
name
salary
instructor
ID
name
salary
ID
name
tot_cred
student
ID
name
tot_cred
student
ID
name
tot_cred
(a)
(b)
(c)
advisor
advisor
advisor
Figure 7.9 Relationships. (a) One-to-one. (b) One-to-many. (c) Many-to-many.
7.5.2 Mapping Cardinality
Therelationshipset advisor,betweenthe instructorand studententitysetsmaybe
one-to-one, one-to-many, many-to-one, or many-to-many. To distinguish among
thesetypes,wedraweitheradirectedline(?)oranundirectedline(—)between
the relationshipsetand theentitysetinquestion,asfollows:
• One-to-one: We draw a directed line from the relationship set advisor to
bothentitysets instructorand student(seeFigure7.9a).Thisindicatesthatan
instructor may advise at most one student, and a student may have at most
one advisor.
• One-to-many: We draw a directed line from the relationship set advisor to
the entity set instructor and an undirected line to the entity set student (see
Figure7.9b).Thisindicatesthataninstructormayadvisemanystudents,but
astudentmayhaveatmostoneadvisor.
• Many-to-one: We draw an undirected line from the relationship set advisor
to the entity set instructor and a directed line to the entity set student.This
indicates that an instructor may advise at most one student, but a student
mayhavemanyadvisors.
• Many-to-many:Wedrawanundirectedlinefromtherelationshipset advisor
to both entity sets instructor and student (see Figure 7.9c). This indicates that
7.5 Entity-Relationship Diagrams 277
an instructor may advise many students, and a student may have many
advisors.
E-Rdiagramsalsoprovideawaytoindicatemorecomplexconstraintsonthe
number of times each entity participates in relationships in a relationship set. A
line may have an associated minimum and maximum cardinality, shown in the
form l..h,wherel is the minimum and h the maximum cardinality. A minimum
value of 1 indicates total participation of the entity set in the relationship set;
that is, each entity in the entity set occurs in at least one relationship in that
relationshipset.Amaximumvalueof1indicatesthattheentityparticipatesinat
mostone relationship,whileamaximumvalue ?indicatesno limit.
Forexample,considerFigure7.10.Thelinebetween advisorand studenthasa
cardinalityconstraintof1..1,meaningtheminimumandthemaximumcardinal-
ity are both 1. That is, each student must have exactly one advisor. The limit 0..?
on the line between advisor and instructor indicates that an instructor can have
zeroormorestudents.Thus,therelationship advisorisone-to-manyfrom instruc-
tor to student, and further the participation of student in advisor is total, implying
that astudentmust haveanadvisor.
Itiseasytomisinterpretthe0..?ontheleftedgeandthinkthattherelationship
advisorismany-to-onefrominstructortostudent—thisisexactlythereverseofthe
correctinterpretation.
If both edges have a maximum value of 1, the relationship is one-to-one. If
we had speci?ed a cardinality limit of 1..? on the left edge, we would be saying
that eachinstructor mustadviseat leastone student.
The E-R diagram in Figure 7.10 could alternatively have been drawn with
a double line from student to advisor, and an arrow on the line from advisor to
instructor,inplaceofthecardinalityconstraintsshown. Thisalternativediagram
wouldenforceexactlythesameconstraintsastheconstraintsshowninthe?gure.
7.5.3 Complex Attributes
Figure7.11showshowcompositeattributescanberepresentedintheE-Rnotation.
Here, a composite attribute name, with component attributes ?rst name, middle
initial,andlast name replaces the simple attribute name of instructor.Asanother
example, suppose we were to add an address to the instructor entity-set. The
address can be de?ned as the composite attribute address with the attributes
instructor
ID
name
salary
student
ID
name
tot_cred
advisor
1..1 0..
*
Figure 7.10 Cardinality limits on relationship sets.
278 Chapter 7 Database Design and the E-R Model
instructor
ID
name
?rst_name
middle_initial
last_name
address
street
street_number
street_name
apt_number
city
state
zip
{ phone_number }
date_of_birth
age ( )
Figure 7.11 E-R diagram with composite, multivalued, and derived attributes.
street, city, state,andzip code. The attribute street is itself a composite attribute
whose componentattributesare street number, street name,andapartment number.
Figure 7.11 also illustrates a multivalued attribute phone number, denoted by
“{phone number}”,andaderivedattribute age,depictedbya“age() ”.
7.5.4 Roles
We indicate roles in E-R diagrams by labeling the lines that connect diamonds to
rectangles. Figure 7.12 shows the role indicators course id and prereq id between
the courseentitysetandthe prereqrelationshipset.
7.5.5 Nonbinary Relationship Sets
Nonbinaryrelationshipsetscanbespeci?edeasilyinan E-Rdiagram.Figure7.13
consistsofthethreeentitysets instructor, student,andproject,relatedthroughthe
relationshipset proj guide.
course
course_id
title
credits
course_id
prereq_id
prereq
Figure 7.12 E-R diagram with role indicators.
7.5 Entity-Relationship Diagrams 279
instructor
ID
name
salary
student
ID
name
tot_cred
...
project
proj_guide
Figure 7.13 E-R diagram with a ternary relationship.
We can specify some types of many-to-one relationships in the case of non-
binary relationship sets. Suppose a student can have at most one instructor as
a guide on a project. This constraint can be speci?ed by an arrow pointing to
instructor onthe edgefrom proj guide.
We permit at most one arrow out of a relationship set, since an E-R diagram
withtwoormorearrowsoutofanonbinaryrelationshipsetcanbeinterpretedin
twoways.Supposethereisarelationshipset Rbetweenentitysets A
1
, A
2
,...,A
n
,
and the only arrows are on the edges to entity sets A
i+1
, A
i+2
,...,A
n
. Then, the
twopossibleinterpretationsare:
1. A particular combination of entities from A
1
, A
2
,...,A
i
can be associated
with at most one combination of entities from A
i+1
, A
i+2
,...,A
n
.Thus,the
primary key for the relationship R can be constructed by the union of the
primarykeysof A
1
, A
2
,...,A
i
.
2. For each entity set A
k
, i < k ? n, each combination of the entities from the
otherentitysetscanbeassociatedwithatmostoneentityfrom A
k
.Eachset
{A
1
, A
2
,...,A
k?1
, A
k+1
,...,A
n
},fori < k ? n,thenformsacandidatekey.
Each of these interpretations has been used in different books and systems. To
avoid confusion, we permit only one arrow out of a relationship set, in which
case the two interpretations are equivalent. In Chapter 8 (Section 8.4), we study
functional dependencies, which allow eitheroftheseinterpretationstobe speci?ed
inanunambiguous manner.
7.5.6 Weak Entity Sets
Consider a section entity, which is uniquely identi?ed by a course identi?er,
semester,year,andsectionidenti?er.Clearly,sectionentitiesarerelatedtocourse
entities.Supposewecreatearelationshipset sec coursebetweenentitysetssection
and course.
Now, observe that the information in sec course is redundant, since section
already has an attribute course id, which identi?es the course with which the
section is related. One option to deal with this redundancy is to get rid of the
280 Chapter 7 Database Design and the E-R Model
relationshipsec course;however,bydoingsotherelationshipbetweensectionand
course becomesimplicitinan attribute,which isnot desirable.
An alternative way to deal with this redundancy is to not store the attribute
course id in the section entity and to only store the remaining attributes sec id,
year,andsemester.
6
However, the entity set section then does not have enough
attributes to identify a particular section entity uniquely; although each section
entity is distinct, sections for different courses may share the same sec id, year,
and semester. To deal with this problem, we treat the relationship sec course as
a special relationship that provides extra information, in this case the course id,
requiredtoidentify section entitiesuniquely.
Thenotionof weak entity setformalizestheaboveintuition.Anentitysetthat
doesnothavesuf?cientattributestoformaprimarykeyistermedaweakentity
set. Anentitysetthathasaprimarykeyistermedastrongentity set.
For a weak entity set to be meaningful, it must be associated with another
entity set, called the identifying or owner entity set. Every weak entity must
be associated with an identifying entity; that is, the weak entity set is said to be
existencedependentontheidentifyingentityset.Theidentifyingentitysetissaid
toowntheweakentitysetthatitidenti?es.Therelationshipassociatingtheweak
entitysetwiththeidentifyingentitysetiscalledthe identifyingrelationship.
The identifying relationship is many-to-one from the weak entity set to the
identifying entity set, and the participation of the weak entity set in the rela-
tionshipistotal.Theidentifyingrelationshipsetshouldnothaveanydescriptive
attributes,sinceanysuchattributescaninsteadbeassociatedwiththeweakentity
set.
Inourexample,theidentifyingentitysetforsectioniscourse,andtherelation-
ship sec course, which associates section entities with their corresponding course
entities,isthe identifyingrelationship.
Although a weak entity set does not have a primary key, we nevertheless
need a means of distinguishing among all those entities in the weak entity set
that depend on one particular strong entity. The discriminator of a weak entity
set is a set of attributes that allows this distinction to be made. For example, the
discriminator of the weak entity set section consists of the attributes sec id, year,
and semester, since, for each course, this set of attributes uniquely identi?es one
singlesectionforthatcourse.Thediscriminatorofaweakentitysetisalsocalled
the partial keyoftheentityset.
The primary key of a weak entity set is formed by the primary key of the
identifying entity set, plus the weak entity set’s discriminator. In the case of the
entity set section, its primary key is {course id, sec id, year, semester},wherecourse
idistheprimarykeyoftheidentifyingentityset,namelycourse,and{sec id, year,
semester}distinguishes section entitiesfor thesamecourse.
Note that we could have chosen to make sec id globally unique across all
courses offered in the university, in which case the section entity set would have
6
Note that the relational schema we eventually create from the entity set section does have the attribute course id,for
reasons that willbecome clear later, eventhough we have droppedthe attribute course idfrom the entityset section.
7.5 Entity-Relationship Diagrams 281
course
course_id
title
credits
section
sec_id
semester
year
sec_course
Figure 7.14 E-R diagram with a weak entity set.
hadaprimarykey.However,conceptually,a section isstilldependentona course
for itsexistence,which ismadeexplicitbymakingitaweakentityset.
In E-R diagrams, a weak entity set is depicted via a rectangle, like a strong
entityset,but therearetwomaindifferences:
• The discriminator of a weak entity is underlined with a dashed, rather than
asolid,line.
• The relationship set connecting the weak entity set to the identifying strong
entitysetisdepictedbyadoublediamond.
In Figure 7.14, the weak entity set section depends on the strong entity set course
viatherelationshipset sec course.
The?gurealsoillustratestheuseofdoublelinestoindicate total participation;
the participation of the (weak) entity set section in the relationship sec course is
total, meaning that every section must be related via sec course to some course.
Finally,thearrowfrom sec courseto courseindicatesthateachsectionisrelatedto
asinglecourse.
A weak entity set can participate in relationships other than the identifying
relationship. For instance, the section entity could participate in a relationship
with the time slot entity set, identifying the time when a particular class section
meets.A weak entity set may participateas owner inan identifying relationship
withanotherweakentityset.Itisalsopossibletohaveaweakentitysetwithmore
thanoneidentifyingentityset.Aparticularweakentitywouldthenbeidenti?ed
by a combination of entities, one from each identifying entity set. The primary
key of the weak entity set would consist of the union of the primary keys of the
identifyingentitysets,plusthediscriminatorofthe weakentityset.
Insomecases,thedatabasedesignermaychoosetoexpressaweakentityset
asamultivaluedcompositeattributeoftheownerentityset.Inourexample,this
alternativewouldrequirethattheentitysetcoursehaveamultivalued,composite
attribute section. A weak entity set may be more appropriately modeled as an
attribute if it participates in only the identifying relationship, and if it has few
attributes. Conversely, a weak entity set representation more aptly models a
situation where the set participates in relationships other than the identifying
relationship, and where the weak entity set has several attributes. It is clear that
section violates the requirements for being modeled as a multivalued composite
attribute,andismodeledmoreaptlyasaweakentityset.
282 Chapter 7 Database Design and the E-R Model
7.5.7 E-R diagram for the University Enterprise
InFigure7.15,we showan E-R diagramthat correspondstotheuniversityenter-
prisethat we have beenusingthus far inthe text.This E-R diagramisequivalent
to the textual description of the university E-R model that we saw in Section 7.4,
but withseveraladditionalconstraints, and section now being a weakentity.
Inouruniversitydatabase,wehaveaconstraintthateachinstructormusthave
exactlyoneassociateddepartment.Asaresult,thereisadoublelineinFigure7.15
between instructor and inst dept, indicating total participation of instructor in inst
dept;thatis,eachinstructormustbeassociatedwithadepartment.Further,there
is an arrow from inst dept to department, indicating that each instructor can have
at mostone associateddepartment.
time_slot course
student
ID
name
salary
ID
name
tot_cred
course_id
title
credits
time_slot_id
{ day
start_time
end_time
}
course_id prereq_id
advisor
teaches
takes
sec_course
sec_time_slot
grade
prereq
inst_dept stud_dept
instructor
department
dept_name
building
budget
section
sec_id
semester
year
course_dept
sec_class
classroom
building
room_number
capacity
Figure 7.15 E-R diagram for a university enterprise.
7.6 Reduction to Relational Schemas 283
Similarly, entity sets course and student have double lines to relationship sets
course dept and stud dept respectively, as also entity set section to relationship set
sec time slot. The ?rst two relationships, in turn, have an arrow pointing to the
otherrelationship, department,whilethethirdrelationshiphasanarrowpointing
to time slot.
Further, Figure 7.15 shows that the relationship set takes has a descriptive
attribute grade, and that each student has at most one advisor. The ?gure also
shows that section is now a weak entity set, with attributes sec id, semester,and
yearformingthediscriminator;sec courseistheidentifyingrelationshipsetrelating
weakentityset section tothestrongentityset course.
In Section7.6, we shall show how this E-R diagramcan be used to derivethe
variousrelationschemaswe use.
7.6 Reduction to Relational Schemas
We can represent a database that conforms to an E-R database schema by a col-
lectionofrelationschemas.Foreachentitysetandforeachrelationshipsetinthe
database design, there is a unique relation schema to which we assign the name
ofthecorresponding entitysetor relationshipset.
Both the E-R model and the relational database model are abstract, logical
representationsofreal-worldenterprises.Becausethetwomodelsemploysimilar
designprinciples,wecan convertan E-Rdesignintoarelationaldesign.
Inthissection,wedescribehowan E-Rschemacanberepresentedbyrelation
schemas and how constraints arising from the E-R design can be mapped to
constraints on relationschemas.
7.6.1 Representation of Strong Entity Sets with Simple Attributes
Let E be a strong entityset with only simple descriptiveattributes a
1
, a
2
,...,a
n
.
WerepresentthisentitybyaschemacalledEwithndistinctattributes.Eachtuple
inarelationonthisschemacorrespondstoone entityoftheentityset E.
Forschemasderivedfromstrongentitysets,theprimarykeyoftheentityset
servesas the primary key ofthe resulting schema. This follows directlyfrom the
fact that eachtuplecorrespondstoaspeci?centityintheentityset.
As an illustration, consider the entity set student of the E-R diagram in Fig-
ure 7.15. This entity set has three attributes: ID, name, tot cred.Werepresentthis
entitysetbyaschemacalled student withthreeattributes:
student(ID, name, tot cred)
Notethatsincestudent IDistheprimarykeyoftheentityset,itisalsotheprimary
keyoftherelationschema.
Continuingwithourexample,fortheE-RdiagraminFigure7.15,allthestrong
entitysets,excepttime slot,haveonlysimpleattributes.Theschemasderivedfrom
thesestrongentitysetsare:
284 Chapter 7 Database Design and the E-R Model
classroom (building, room number, capacity)
department(dept name, building, budget)
course (course id, title, credits)
instructor (ID, name, salary)
student(ID, name, tot cred)
As you can see, both the instructor and student schemas are different from the
schemaswehaveusedinthepreviouschapters(theydonotcontaintheattribute
dept name).We shallrevisitthisissueshortly.
7.6.2 Representation of Strong Entity Sets with Complex Attributes
Whenastrongentitysethasnonsimpleattributes,thingsareabitmorecomplex.
We handle composite attributes by creating a separate attribute for each of the
component attributes; we do not create a separate attribute for the composite
attribute itself. To illustrate, consider the version of the instructor entity set de-
picted in Figure 7.11. For the composite attribute name, the schema generated
for instructor contains the attributes ?rst name, middle name,andlast name;there
isnoseparateattributeorschemafor name.Similarly,for thecompositeattribute
address,theschemageneratedcontainstheattributesstreet,city,state,andzip code.
Since streetisacompositeattributeitisreplacedby street number,street name,and
apt number.WerevisitthismatterinSection8.2.
Multivalued attributes are treated differently from other attributes. We have
seenthatattributesinanE-Rdiagramgenerallymapdirectlyintoattributesforthe
appropriaterelationschemas.Multivaluedattributes,however,areanexception;
newrelationschemas arecreatedfor theseattributes,as weshall seeshortly.
Derivedattributesarenotexplicitlyrepresentedintherelationaldatamodel.
However,theycanberepresentedas “methods”inotherdatamodelssuchasthe
object-relationaldatamodel,which isdescribedlaterinChapter22.
The relational schema derived from the version of entity set instructor with
complexattributes,without includingthemultivaluedattribute,isthus:
instructor (ID, ?rst name, middle name, last name,
street number, street name, apt number,
city, state, zip code, date of birth)
ForamultivaluedattributeM,wecreatearelationschemaRwithanattribute
A that corresponds to M and attributes corresponding to the primary key of the
entitysetorrelationshipsetofwhich Misanattribute.
As an illustration, consider the E-R diagram in Figure 7.11 that depicts the
entity set instructor, which includes the multivalued attribute phone number.The
primarykeyof instructoris ID.Forthismultivaluedattribute,wecreatearelation
schema
instructor phone(ID, phone number)
7.6 Reduction to Relational Schemas 285
Eachphonenumberofaninstructorisrepresentedasauniquetupleintherelation
onthisschema.Thus, ifwehadaninstructorwith ID22222, andphone numbers
555-1234and555-4321,therelationinstructor phonewouldhavetwotuples(22222,
555-1234) and (22222, 555-4321).
We create a primary key of the relation schema consisting of all attributes of
the schema. In the above example, the primary key consists of both attributes of
therelation instructor phone.
Inaddition,wecreateaforeign-keyconstraint ontherelationschemacreated
from the multivalued attribute, with the attribute generated from the primary
key of the entity set referencing the relation generated from the entity set. In the
above example, the foreign-key constraint on the instructor phone relation would
be thatattribute ID references the instructor relation.
Inthecasethatanentitysetconsistsofonlytwoattributes—asingleprimary-
keyattributeBandasinglemultivaluedattributeM—therelationschemaforthe
entity set would contain only one attribute, namely the primary-key attribute B.
We can drop this relation, while retaining the relation schema with the attribute
Band attribute Athat correspondsto M.
Toillustrate,considertheentitysettime slotdepictedinFigure 7.15.Here,time
slot idistheprimarykeyofthetime slotentitysetandthereisasinglemultivalued
attribute that happens alsoto be composite.The entity set can be representedby
justthe following schemacreatedfrom themultivaluedcompositeattribute:
time slot (time slot id, day, start time, end time)
Althoughnotrepresentedasaconstraintonthe E-Rdiagram,weknowthatthere
cannot be two meetings of a class that start at the same time of the same day-of-
the-week but end at different times; based on this constraint, end time has been
omittedfromtheprimarykeyofthe time slot schema.
Therelationcreatedfromtheentitysetwouldhaveonlyasingleattributetime
slot id; the optimization of dropping this relation has the bene?t of simplifying
theresultantdatabaseschema,althoughithasadrawbackrelatedtoforeignkeys,
which we brie?ydiscussinSection7.6.4.
7.6.3 Representation of Weak Entity Sets
Let A be a weak entity set with attributes a
1
,a
2
,...,a
m
.LetB be the strong
entity set on which A depends. Let the primary key of B consist of attributes
b
1
, b
2
,...,b
n
.W erepresenttheentitysetA by a relation schema called A with
one attributeforeach memberofthe set:
{a
1
,a
2
,...,a
m
} ?{b
1
,b
2
,...,b
n
}
For schemas derived from a weak entity set, the combination of the pri-
mary key of the strong entity set and the discriminator of the weak entity set
serves as the primary key of the schema. In addition to creating a primary key,
we also create a foreign-key constraint on the relation A, specifying that the
286 Chapter 7 Database Design and the E-R Model
attributes b
1
, b
2
,...,b
n
reference the primary key of the relation B. The foreign-
key constraint ensures that for each tuple representing a weak entity, there is a
correspondingtuplerepresentingthecorresponding strongentity.
As an illustration, consider the weak entity set section in the E-R diagram
of Figure 7.15. This entity set has the attributes: sec id, semester,andyear.The
primary key of the course entity set, on which section depends, is course id.Thus,
werepresent section by aschemawiththefollowing attributes:
section (course id, sec id, semester, year)
The primary key consists of the primary key of the entity set course,alongwith
the discriminator of section,whichissec id, semester,andyear.W ealsocreatea
foreign-key constraint on the section schema, with the attribute course id refer-
encing the primary key of the course schema, and the integrity constraint “on
delete cascade”.
7
Because of the “on delete cascade” speci?cation on the foreign
key constraint, if a course entity is deleted, then so are all the associated section
entities.
7.6.4 Representation of Relationship Sets
Let Rbe arelationshipset,let a
1
,a
2
,...,a
m
be thesetofattributesformedby the
unionoftheprimarykeysofeachoftheentitysetsparticipatingin R,andletthe
descriptiveattributes(ifany)of Rbe b
1
,b
2
,...,b
n
.Werepresentthisrelationship
setbyarelationschemacalled Rwithone attributefor eachmemberofthe set:
{a
1
,a
2
,...,a
m
} ?{b
1
,b
2
,...,b
n
}
Wedescribedearlier,inSection7.3.3,howtochooseaprimarykeyforabinary
relationship set. As we saw in that section, taking all the primary-key attributes
fromalltherelatedentitysetsservestoidentifyaparticulartuple,butforone-to-
one,many-to-one,andone-to-manyrelationshipsets,thisturnsouttobealarger
set of attributes than we need in the primary key. The primary key is instead
chosen asfollows:
• For a binary many-to-many relationship, the union of the primary-key at-
tributesfromthe participatingentitysetsbecomestheprimarykey.
• For a binary one-to-one relationship set, the primary key of either entity set
can bechosen asthe primarykey.Thechoice canbe madearbitrarily.
• Forabinarymany-to-oneorone-to-manyrelationshipset,theprimarykeyof
theentitysetonthe “many”sideoftherelationshipsetservesastheprimary
key.
7
The “ondeletecascade” feature offoreignkeyconstraints inSQL isdescribedin Section4.4.5.
7.6 Reduction to Relational Schemas 287
• Forann-aryrelationshipsetwithoutanyarrowsonitsedges,theunionofthe
primarykey-attributesfromtheparticipatingentitysetsbecomestheprimary
key.
• For an n-ary relationship set with an arrow on one of its edges, the primary
keysoftheentitysetsnot onthe “arrow”sideoftherelationshipsetserveas
the primary key for the schema. Recall that we allowed only one arrow out
ofarelationshipset.
We also create foreign-key constraints on the relation schema R as follows:
For each entity set E
i
related to relationship set R, we create a foreign-key con-
straint from relation schema R, with the attributes of R that were derived from
primary-key attributes of E
i
referencing the primary key of the relation schema
representing E
i
.
As an illustration, consider the relationship set advisor in the E-R diagram of
Figure7.15.Thisrelationshipsetinvolvesthe followingtwoentitysets:
• instructor withtheprimarykey ID.
• student withthe primarykey ID.
Sincetherelationshipsethasnoattributes,the advisorschemahastwoattributes,
the primary keys of instructor and student. Since both attributes have the same
name, we rename them i ID and s ID.Sincetheadvisor relationship set is many-
to-one from student to instructor the primary key for the advisor relation schema
is s ID.
We also create two foreign-key constraints on the advisor relation, with at-
tributei IDreferencingtheprimarykeyof instructorandattributes IDreferencing
theprimarykeyof student.
Continuingwithourexample,fortheE-RdiagraminFigure7.15,theschemas
derivedfromarelationshipsetaredepictedinFigure7.16.
Observe that for the case of the relationship set prereq, the role indicators
associatedwiththerelationshipareusedasattributenames,sincebothrolesrefer
tothesamerelation course.
Similartothecaseofadvisor,theprimarykeyforeachoftherelationssec course,
sec time slot, sec class, inst dept, stud dept and course dept consists of the primary
key of only one of the two related entity sets, since each of the corresponding
relationshipsismany-to-one.
Foreignkeysarenotshown inFigure7.16,butforeachoftherelationsinthe
?guretherearetwoforeign-keyconstraints,referencingthetworelationscreated
from the two related entity sets. Thus, for example, sec course has foreign keys
referencing section and classroom, teaches has foreign keys referencing instructor
and section,andtakes has foreignkeysreferencing student and section.
Theoptimizationthatallowedustocreateonlyasinglerelationschemafrom
the entity set time slot, which had a multivalued attribute, prevents the creation
ofaforeignkeyfromtherelationschema sec time slottotherelationcreatedfrom
entity set time slot, since we dropped the relation created from the entity set time
288 Chapter 7 Database Design and the E-R Model
teaches (ID, course id, sec id, semester, year)
takes (ID, course id, sec id, semester, year, grade)
prereq(course id, prereq id)
advisor (s ID, i ID)
sec course (course id, sec id, semester, year)
sec time slot (course id, sec id, semester, year, time slot id)
sec class (course id, sec id, semester, year, building, room number)
inst dept (ID, dept name)
stud dept (ID, dept name)
course dept(course id, dept name)
Figure 7.16 Schemas derived from relationship sets in the E-R diagram in Figure 7.15.
slot.Weretainedtherelationcreatedfromthemultivaluedattribute,andnamed
it time slot, but this relation may potentially have no tuples corresponding to a
time slot id,ormayhavemultipletuplescorrespondingtoa time slot id;thus,time
slot id in sec time slot cannot referencethisrelation.
Theastutereadermaywonderwhywehavenotseentheschemassec course,
sec time slot, sec class, inst dept, stud dept,andcourse dept in the previouschapters.
The reason is that the algorithm we have presented thus far results in some
schemas that can be either eliminated or combined with other schemas. We ex-
plorethisissuenext.
7.6.4.1 RedundancyofSchemas
Arelationshipsetlinkingaweakentitysettothecorrespondingstrongentityset
istreatedspecially.AswenotedinSection7.5.6,theserelationshipsaremany-to-
one and have no descriptive attributes. Furthermore, the primary key of a weak
entity set includes the primary key of the strong entity set. In the E-R diagram
of Figure 7.14, the weak entity set section is dependent on the strong entity set
course via the relationship set sec course.Theprimarykeyofsection is {course id,
sec id, semester, year} and the primary key of course is course id.Sincesec course
has no descriptiveattributes,the sec course schema has attributes course id, sec id,
semester,andyear. The schema for the entity set section includes the attributes
course id, sec id, semester,andyear(amongothers).Every(course id, sec id, semester,
year) combination ina sec course relationwould also be presentinthe relationon
schema section,and viceversa.Thus,the sec courseschemaisredundant.
In general, the schema for the relationship set linking a weak entity set to its
corresponding strong entity set is redundant and does not need to be present in
arelationaldatabasedesignbaseduponan E-R diagram.
7.6.4.2 Combinationof Schemas
Consider a many-to-one relationship set AB from entity set A to entity set B.
Using our relational-schema construction algorithm outlined previously, we get
7.6 Reduction to Relational Schemas 289
three schemas: A, B,andAB. Suppose further that the participation of A in the
relationship is total; that is, every entity a in the entity set B must participate in
therelationshipAB.Thenwecancombinetheschemas Aand ABtoformasingle
schemaconsistingoftheunionofattributesofbothschemas.Theprimarykeyof
the combined schema is the primary key of the entity set into whose schema the
relationshipsetschemawasmerged.
To illustrate, let’s examine the various relations in the E-R diagram of Fig-
ure7.15that satisfytheabove criteria:
• inst dept.The schemas instructor and departmentcorrespond tothe entitysets
A and B, respectively. Thus, the schema inst dept can be combined with the
instructor schema. The resulting instructor schema consists of the attributes
{ID, name, dept name, salary}.
• stud dept. The schemas student and department correspond to the entity sets
A and B, respectively. Thus, the schema stud deptcanbecombinedwiththe
student schema. The resulting student schema consists of the attributes {ID,
name, dept name, tot cred}.
• course dept. The schemas course and department correspond to the entity sets
Aand B,respectively.Thus,theschema course deptcanbecombinedwiththe
course schema. The resulting course schema consists of the attributes {course
id, title, dept name, credits}.
• sec class.TheschemassectionandclassroomcorrespondtotheentitysetsAand
B, respectively. Thus, the schema sec classcanbecombinedwiththesection
schema. The resulting section schema consists of the attributes {course id, sec
id, semester, year, building, room number}.
• sec time slot. The schemas section and time slot correspond to the entity sets
A and B respectively, Thus, the schema sec time slot can be combined with
thesectionschemaobtainedinthepreviousstep.Theresultingsectionschema
consistsoftheattributes{course id,sec id,semester,year,building,room number,
time slot id}.
Inthecaseofone-to-onerelationships,therelationschemafortherelationship
setcan becombined withtheschemas for eitherofthe entitysets.
We can combine schemas even if the participation is partial by using null
values. In the above example, if inst dept were partial, then we would store null
values for the dept name attribute for those instructors who have no associated
department.
Finally,weconsidertheforeign-keyconstraintsthatwouldhaveappearedin
theschemarepresentingtherelationshipset.Therewouldhavebeenforeign-key
constraints referencing each of the entity sets participating in the relationship
set. We drop the constraint referencing the entity set into whose schema the
relationship set schema is merged, and add the other foreign-key constraints to
the combined schema. For example, inst dept has a foreign key constraint of the
attribute dept name referencing the department relation. This foreign constraint is
290 Chapter 7 Database Design and the E-R Model
added to the instructor relation when the schema for inst dept is merged into
instructor.
7.7 Entity-Relationship Design Issues
Thenotionsofanentitysetandarelationshipsetarenotprecise,anditispossible
to de?ne a set of entities and the relationships among them in a number of
different ways. In this section, we examine basic issues in the design of an E-R
databaseschema. Section7.10coversthedesignprocessinfurtherdetail.
7.7.1 Use of Entity Sets versus Attributes
Considertheentityset instructor withtheadditionalattribute phone number(Fig-
ure 7.17a.) It can easily be argued that a phone is an entity in its own right with
attributesphone numberandlocation;thelocationmaybetheof?ceorhomewhere
thephoneislocated,withmobile(cell)phonesperhapsrepresentedbythevalue
“mobile.” If we take this point of view,we donot add the attribute phone number
tothe instructor. Rather,we create:
• A phoneentitysetwithattributes phone numberand location.
• A relationship set inst phone, denoting the association between instructors
and thephonesthattheyhave.
This alternativeisshown inFigure7.17b.
What,then,isthemaindifferencebetweenthesetwode?nitionsofaninstruc-
tor? Treating a phone as an attribute phone number implies that instructors have
precisely one phone number each. Treating a phone as an entity phone permits
instructorstohaveseveralphonenumbers(includingzero)associatedwiththem.
However,wecouldinsteadeasilyde?ne phone numberasamultivaluedattribute
toallowmultiplephonesperinstructor.
The main difference then is that treating a phone as an entity better models
a situation where one may want to keep extra information about a phone, such
asitslocation,oritstype(mobile, IPphone,orplainoldphone),orallwhoshare
instructor
ID
name
salary
phone
phone_number
location
instructor
ID
name
salary
phone_number
(a) (b)
inst_phone
Figure 7.17 Alternatives for adding phone to the instructor entity set.
7.7 Entity-Relationship Design Issues 291
thephone.Thus,treatingphoneasanentityismoregeneralthantreatingitasan
attributeand isappropriatewhen thegeneralitymay beuseful.
In contrast, it would not be appropriate to treat the attribute name (of an
instructor)asanentity;itisdif?culttoarguethatnameisanentityinitsownright
(in contrast to the phone). Thus, it is appropriate to have name as an attribute of
the instructor entityset.
Two natural questions thus arise: What constitutes an attribute, and what
constitutesanentityset?Unfortunately,therearenosimpleanswers.Thedistinc-
tionsmainlydependonthestructureofthereal-worldenterprisebeingmodeled,
and onthesemantics associatedwiththeattributeinquestion.
A common mistake is to use the primary key of an entity set as an attribute
of another entity set, instead of using a relationship. For example, it is incorrect
tomodel the IDof a studentas an attribute of an instructor evenifeach instructor
advisesonly one student.The relationship advisor is the correct way torepresent
theconnectionbetweenstudentsandinstructors,sinceitmakestheirconnection
explicit,ratherthan implicitviaanattribute.
Another related mistake that people sometimes make is to designate the
primary-key attributes of the related entity sets as attributes of the relationship
set. For example, ID (the primary-key attributes of student)andID (the primary
key of instructor) should not appear as attributes of the relationship advisor.This
shouldnotbedonesincetheprimary-keyattributes are already implicit in the
relationshipset.
8
7.7.2 Use of Entity Sets versus Relationship Sets
It is not always clear whether an object is best expressed by an entity set or a
relationship set. In Figure 7.15, we used the takes relationship set to model the
situationwhereastudenttakesa(sectionofa)course.Analternativeistoimagine
that there is a course-registration record for each course that each student takes.
Then,wehaveanentitysettorepresentthecourse-registrationrecord.Letuscall
thatentityset registration.Eachregistrationentityisrelatedtoexactlyonestudent
andtoexactlyonesection,sowehavetworelationshipsets,onetorelatecourse-
registration records to students and one to relate course-registration records to
sections.InFigure7.18,weshowtheentitysetssectionandstudentfromFigure7.15
withthetakesrelationshipsetreplacedbyoneentitysetandtworelationshipsets:
• registration, theentitysetrepresentingcourse-registrationrecords.
• section reg,therelationshipsetrelating registration and course.
• student reg,the relationshipsetrelating registration and student.
Notethatweusedoublelinestoindicatetotalparticipationbyregistrationentities.
8
WhenwecreatearelationschemafromtheE-Rschema,theattributesmayappearinaschemacreatedfromthe advisor
relationshipset, as weshall seelater;however,they shouldnotappear inthe advisorrelationshipset.
292 Chapter 7 Database Design and the E-R Model
registration
...
...
...
section
sec_id
semester
year
student
ID
name
tot_cred
section_reg student_reg
Figure 7.18 Replacement of takes by registration and two relationship sets
Boththe approach of Figure7.15and that ofFigure7.18 accurately represent
the university’s information, but the use of takes is more compact and probably
preferable. However, if the registrar’s of?ce associates other information with a
course-registrationrecord,itmight bebest tomakeitan entityinitsown right.
One possible guideline in determining whether to use an entity set or a
relationshipsetistodesignatearelationshipsettodescribeanactionthatoccurs
between entities. This approach can also be useful in deciding whether certain
attributesmaybemoreappropriatelyexpressedasrelationships.
7.7.3 Binary versus n-ary Relationship Sets
Relationshipsindatabasesareoftenbinary.Somerelationshipsthat appeartobe
nonbinary could actually be better represented by several binary relationships.
For instance, one could create a ternary relationship parent, relating a child to
his/hermotherandfather.However,sucharelationshipcouldalsoberepresented
by two binary relationships, mother and father, relating a child to his/her mother
andfatherseparately.Usingthetworelationships motherand fatherprovidesusa
recordofachild’smother,evenifwearenotawareofthefather’sidentity;anull
value would be required if the ternary relationship parent is used. Using binary
relationshipsetsispreferableinthiscase.
Infact, itisalways possibletoreplaceanonbinary (n-ary,for n> 2)relation-
ship set by a number of distinct binary relationship sets. For simplicity,consider
theabstractternary(n = 3)relationshipset R,relatingentitysets A, B,andC.We
replacetherelationshipset Rbyanentityset E,andcreatethreerelationshipsets
as shown inFigure7.19:
• R
A
,relatingE and A.
• R
B
,relatingE and B.
• R
C
,relatingE and C.
7.7 Entity-Relationship Design Issues 293
BRC
A
C BE
A
R
A
R
B
R
C
(a) (b)
Figure 7.19 Ternary relationship versus three binary relationships.
If the relationship set R had any attributes, these are assigned to entity set E;
further,aspecialidentifyingattributeiscreatedfor E (sinceitmustbepossibleto
distinguishdifferententitiesinanentitysetonthebasisoftheirattributevalues).
For each relationship (a
i
,b
i
,c
i
) in the relationship set R, we create a new entity
e
i
inthe entityset E.Then, in eachof the threenew relationship sets,we insert a
relationshipas follows:
• (e
i
,a
i
)inR
A
.
• (e
i
,b
i
)inR
B
.
• (e
i
,c
i
)inR
C
.
Wecangeneralizethisprocessinastraightforwardmannerto n-aryrelation-
shipsets.Thus,conceptually,wecanrestrictthe E-Rmodeltoincludeonlybinary
relationshipsets.However,thisrestrictionisnotalwaysdesirable.
• An identifying attribute may have to be created for the entity set created to
representtherelationshipset.Thisattribute,alongwiththeextrarelationship
sets required, increases the complexity of the design and (as we shall see in
Section7.6)overallstoragerequirements.
• An n-aryrelationshipsetshows moreclearlythatseveralentitiesparticipate
inasinglerelationship.
• There may not be a way to translate constraints on the ternary relationship
into constraints on the binary relationships. For example, consider a con-
straint that says that R is many-to-one from A, B to C;thatis,eachpairof
entitiesfrom Aand B isassociatedwithatmostone C entity.Thisconstraint
cannot be expressedby using cardinality constraints on the relationship sets
R
A
, R
B
,andR
C
.
Consider the relationship set proj guide in Section 7.2.2, relating instructor,
student,andproject. We cannot directly split proj guide into binary relationships
between instructor and project and between instructor and student.Ifwedidso,
294 Chapter 7 Database Design and the E-R Model
instructor
student
76766 Crick
Katz
Srinivasan
Kim
Singh
Einstein
45565
10101
98345
76543
22222
98988
12345
00128
76543
76653
23121
44553
Tanaka
Shankar
Zhang
Brown
Aoi
Chavez
Peltier
May 2009
June 2007
June 2006
June 2009
June 2007
May 2007
May 2006
Figure 7.20 date as an attribute of the student entity set.
we would be able to record that instructor Katz works on projects A and B with
studentsShankarandZhang;however,wewouldnotbeabletorecordthatKatz
works on project A with student Shankar and works on project B with student
Zhang, butdoesnotworkonproject AwithZhang oronproject BwithShankar.
Therelationshipsetproj guidecanbesplitintobinaryrelationshipsbycreating
anewentitysetasdescribedabove.However,doingsowouldnotbeverynatural.
7.7.4 Placement of Relationship Attributes
The cardinality ratio of a relationship can affect the placement of relationship
attributes.Thus, attributesofone-to-one or one-to-many relationshipsetscan be
associatedwithone oftheparticipatingentitysets,rather thanwiththe relation-
ship set. For instance, let us specifythat advisor is a one-to-many relationship set
such that one instructor may advise several students, but each student can be
advisedbyonlyasingleinstructor.Inthiscase,theattribute date,whichspeci?es
when the instructor became the advisor of a student, could be associated with
the student entity set, as Figure 7.20 depicts. (To keep the ?gure simple, only
some of the attributes of the two entity sets are shown.) Since each student entity
participates in a relationship with at most one instance of instructor,makingthis
attributedesignationhasthesamemeaningaswouldplacingdatewiththeadvisor
relationshipset.Attributesofaone-to-many relationshipsetcan berepositioned
to only the entity set on the “many” side of the relationship. For one-to-one rela-
tionshipsets,ontheotherhand,therelationshipattributecanbeassociatedwith
eitheroneofthe participatingentities.
The design decision of where to place descriptive attributes in such cases
—as a relationship or entity attribute—should re?ect the characteristics of the
enterprisebeingmodeled.The designermaychoose toretain date as an attribute
of advisor to express explicitly that the date refers to the advising relationship
andnotsomeotheraspectofthestudent’suniversitystatus(forexample,dateof
acceptance totheuniversity).
7.8 Extended E-R Features 295
The choice of attribute placement is more clear-cut for many-to-many rela-
tionship sets.Returning to our example,let usspecifythe perhapsmore realistic
casethat advisor isamany-to-many relationshipsetexpressingthataninstructor
may advise one or more students, and that a student may be advised by one
or more instructors. If we are to express the date on which a speci?c instructor
became the advisor of a speci?c student, date must be an attribute of the advisor
relationshipset,ratherthaneitheroneoftheparticipatingentities.Ifdatewerean
attributeofstudent,forinstance,wecouldnotdeterminewhichinstructorbecame
the advisoron that particulardate.When anattributeis determinedby thecom-
bination of participating entity sets, rather than by either entity separately, that
attribute must be associated with the many-to-many relationship set. Figure 7.3
depictsthe placement of date asa relationshipattribute;again, tokeepthe ?gure
simple,only someoftheattributesofthetwoentitysetsareshown.
7.8 Extended E-R Features
Althoughthebasic E-Rconceptscanmodelmostdatabasefeatures,someaspects
of a database may be more aptly expressed by certain extensions to the basic
E-R model. In this section, we discuss the extended E-R features of specializa-
tion,generalization,higher-andlower-levelentitysets,attributeinheritance,and
aggregation.
Tohelpwiththediscussions,we shalluseaslightlymoreelaboratedatabase
schemafortheuniversity.Inparticular,weshallmodelthevariouspeoplewithin
auniversitybyde?ninganentityset person,withattributes ID, name,andaddress.
7.8.1 Specialization
An entity set may include subgroupings of entities that are distinct in some way
fromotherentitiesintheset.Forinstance,asubsetofentitieswithinanentityset
mayhaveattributesthatarenotsharedbyalltheentitiesintheentityset.The E-R
modelprovidesameansforrepresentingthesedistinctiveentitygroupings.
As an example, the entity set person may be further classi?ed as one of the
following:
• employee.
• student.
Each of these person types is described by a set of attributes that includes all
the attributes of entity set person plus possibly additional attributes. For exam-
ple, employee entities may be described further by the attribute salary,whereas
student entities may be described further by the attribute tot cred.Theprocessof
designating subgroupings within an entity set is called specialization.Thespe-
cialization of person allows us to distinguish among person entities according to
whethertheycorrespondtoemployeesorstudents:ingeneral,apersoncouldbe
anemployee,astudent,both, orneither.
296 Chapter 7 Database Design and the E-R Model
As another example, suppose the university divides students into two cate-
gories:graduateandundergraduate.Graduatestudentshaveanof?ceassignedto
them.Undergraduatestudentsareassignedtoaresidentialcollege.Eachofthese
student types is described by a set of attributes that includes all the attributes of
the entityset studentplusadditionalattributes.
Theuniversitycouldcreatetwospecializationsofstudent,namelygraduateand
undergraduate. As we saw earlier, student entities are described by the attributes
ID,name, address,andtot cred.Theentityset graduatewouldhavealltheattributes
of student and an additional attribute of?ce number. The entity set undergraduate
would have all the attributes of student, and an additional attribute residential
college.
We can apply specialization repeatedly to re?ne a design. For instance, uni-
versityemployeesmaybe furtherclassi?edasone ofthe following:
• instructor.
• secretary.
Each of these employee types is described by a set of attributes that includes
all the attributes of entity set employee plus additional attributes. For example,
instructor entities may be described further by the attribute rank while secretary
entitiesaredescribedbytheattributehours per week.Further,secretaryentitiesmay
participateinarelationship secretary forbetweenthe secretaryand employeeentity
sets,which identi?estheemployeeswho areassistedbyasecretary.
Anentitysetmaybe specializedbymorethan one distinguishingfeature.In
ourexample,thedistinguishingfeatureamongemployeeentitiesisthejobtheem-
ployee performs. Another, coexistent, specialization could be based on whether
thepersonisatemporary(limited term)employeeorapermanentemployee,re-
sulting in the entity sets temporary employee and permanent employee.Whenmore
thanonespecializationisformedonanentityset,aparticularentitymaybelong
to multiple specializations. For instance, a given employee may be a temporary
employeewho isasecretary.
IntermsofanE-Rdiagram,specializationisdepictedbyahollowarrow-head
pointingfromthespecializedentitytotheotherentity(seeFigure7.21).Werefer
tothisrelationshipasthe ISArelationship,whichstandsfor “isa”andrepresents,
for example,that aninstructor “isa”employee.
The way we depict specialization in an E-R diagram depends on whether
an entity may belong to multiple specialized entity sets or if it must belong to at
mostonespecializedentityset.Theformercase(multiplesetspermitted)iscalled
overlappingspecialization,whilethelattercase(atmostonepermitted)iscalled
disjointspecialization.Foranoverlappingspecialization(asisthecaseforstudent
and employee as specializations of person), two separate arrows are used. For a
disjoint specialization(as isthe case for instructor and secretaryas specializations
of employee), a single arrow is used. The specialization relationship may also be
referred to as a superclass-subclass relationship. Higher- and lower-level entity
7.8 Extended E-R Features 297
person
ID
name
address
student
instructor
rank
secretary
hours_per_week
employee
salary tot_credits
Figure 7.21 Specialization and generalization.
setsaredepictedasregularentitysets—thatis,asrectanglescontainingthename
oftheentityset.
7.8.2 Generalization
There?nementfromaninitialentitysetintosuccessivelevelsofentitysubgroup-
ingsrepresentsatop-downdesignprocessinwhichdistinctionsaremadeexplicit.
The designprocess may also proceed in a bottom-upmanner, in which multiple
entity sets are synthesized into a higher-level entity set on the basis of common
features.Thedatabase designermayhave?rstidenti?ed:
• instructor entity set with attributes instructor id, instructor name, instructor
salary,andrank.
• secretary entity set with attributes secretary id, secretary name, secretary salary,
and hours per week.
Therearesimilaritiesbetweenthe instructor entitysetandthe secretaryentity
set in the sense that they have several attributes that are conceptually the same
across the two entity sets: namely, the identi?er, name, and salary attributes.
This commonality can be expressed by generalization, which is a containment
relationshipthatexistsbetweenahigher-levelentitysetandoneormorelower-level
entitysets.Inourexample,employeeisthehigher-levelentitysetandinstructorand
secretary are lower-level entity sets. In this case, attributes that are conceptually
the same had different names in the two lower-level entity sets. To create a
generalization, the attributes must be given a common name and represented
with the higher-level entity person.W ecanusetheattributenamesID, name,
address,aswesawinthe exampleinSection7.8.1.
298 Chapter 7 Database Design and the E-R Model
Higher- and lower-level entity sets also may be designated by the terms
superclassandsubclass,respectively.Thepersonentitysetisthesuperclassofthe
employee and studentsubclasses.
For all practical purposes, generalization is a simple inversion of specializa-
tion.Weapplybothprocesses,incombination, inthecourseofdesigningthe E-R
schemaforanenterprise.IntermsoftheE-Rdiagramitself,wedonotdistinguish
between specialization and generalization. New levels of entity representation
are distinguished (specialization) or synthesized (generalization) as the design
schema comes to express fully the database application and the user require-
ments of the database. Differences in the two approaches may be characterized
bytheirstartingpoint and overallgoal.
Specializationstemsfromasingleentityset;itemphasizesdifferencesamong
entities within the set by creating distinct lower-level entity sets. These lower-
level entity sets may have attributes, or may participate in relationships, that do
not apply to all the entities in the higher-level entity set. Indeed, the reason a
designerappliesspecializationistorepresentsuchdistinctivefeatures.If student
and employee have exactly the same attributes as person entities, and participate
in exactly the same relationships as person entities, there would be no need to
specializethe personentityset.
Generalization proceeds from the recognition that a number of entity sets
share some common features (namely, they are described by the same attributes
andparticipateinthesamerelationshipsets).Onthebasisoftheircommonalities,
generalization synthesizes these entity sets into a single, higher-level entity set.
Generalizationisusedtoemphasizethesimilaritiesamonglower-levelentitysets
and to hide the differences; it also permits an economy of representation in that
sharedattributesarenot repeated.
7.8.3 Attribute Inheritance
Acrucialpropertyofthehigher-andlower-levelentitiescreatedbyspecialization
and generalization is attribute inheritance. The attributes of the higher-level
entity sets are said to be inherited by the lower-level entity sets. For example,
student and employee inherit theattributes of person.Thus,studentisdescribedby
its ID,name,andaddressattributes,andadditionallyatot credattribute;employeeis
describedbyitsID,name,andaddressattributes,andadditionallyasalaryattribute.
Attribute inheritance applies through all tiers of lower-level entity sets; thus,
instructorand secretary,whicharesubclassesof employee,inherittheattributes ID,
name,andaddressfrom person,inadditiontoinheriting salaryfrom employee.
Alower-levelentityset(orsubclass)alsoinheritsparticipationintherelation-
shipsetsinwhichitshigher-levelentity(orsuperclass)participates.Likeattribute
inheritance, participation inheritance applies through all tiers of lower-level en-
tity sets. For example, suppose the person entity set participates in a relationship
person deptwithdepartment.Then,thestudent,employee,instructorandsecretaryen-
titysets,whicharesubclassesofthepersonentityset,alsoimplicitlyparticipatein
the person deptrelationshipwith department.Theaboveentitysetscanparticipate
inany relationshipsinwhich the personentitysetparticipates.
7.8 Extended E-R Features 299
Whether a given portion of an E-R model was arrived at by specialization or
generalization,theoutcome isbasically thesame:
• Ahigher-levelentitysetwithattributesandrelationshipsthatapplytoallof
itslower-levelentitysets.
• Lower-level entity sets with distinctive features that apply only within a
particularlower-levelentityset.
Inwhatfollows,althoughweoftenrefertoonlygeneralization,theproperties
that wediscuss belongfully toboth processes.
Figure 7.21 depicts a hierarchy of entity sets. In the ?gure, employee is a
lower-level entity set of person and a higher-level entity set of the instructor and
secretaryentitysets.Inahierarchy,agivenentitysetmaybeinvolvedasalower-
level entity set in only one ISA relationship; that is, entity sets in this diagram
have only single inheritance. If an entity set is a lower-level entity set in more
than one ISA relationship, then the entity set has multiple inheritance,andthe
resultingstructureissaidtobea lattice.
7.8.4 Constraints on Generalizations
To model an enterprise more accurately, the database designer may choose to
place certain constraints on a particular generalization. One type of constraint
involvesdeterminingwhichentitiescanbemembersofagivenlower-levelentity
set.Such membershipmay beone ofthefollowing:
• Condition-de?ned.Incondition-de?nedlower-levelentitysets,membership
isevaluatedonthebasisofwhetherornotanentitysatis?esanexplicitcondi-
tionorpredicate.Forexample,assumethatthehigher-levelentitysetstudent
has the attribute student type.Allstudent entities are evaluated on the de?n-
ingstudent typeattribute.Onlythoseentitiesthatsatisfytheconditionstudent
type= “graduate”areallowedtobelongtothelower-levelentityset graduate
student.Allentitiesthatsatisfythecondition student type= “undergraduate”
are included in undergraduate student. Since all the lower-level entities are
evaluatedonthebasisofthesameattribute(inthiscase,onstudent type),this
typeofgeneralizationissaidtobe attribute-de?ned.
• User-de?ned. User-de?ned lower-level entity sets are not constrained by a
membership condition; rather, the database user assigns entities to a given
entity set. For instance, let us assume that, after 3 months of employment,
university employees are assigned to one of four work teams. We therefore
representtheteamsasfourlower-levelentitysetsofthehigher-levelemployee
entity set. A given employee is not assigned to a speci?c team entity auto-
matically on the basis of an explicit de?ning condition. Instead, the user in
charge of this decision makes the team assignment on an individual basis.
The assignment is implemented by an operation that adds an entity to an
entityset.
300 Chapter 7 Database Design and the E-R Model
A second type of constraint relates to whether or not entities may belong to
more than one lower-level entity set within a single generalization. The lower-
levelentitysetsmaybe oneofthe following:
• Disjoint.Adisjointness constraint requires that an entity belong to no more
thanonelower-levelentityset.Inourexample,studententitycansatisfyonly
oneconditionforthe student typeattribute;anentitycanbeeitheragraduate
studentoran undergraduatestudent,but cannot be both.
• Overlapping.Inoverlapping generalizations, the same entity may belong to
more than one lower-level entity set within a single generalization. For an
illustration, consider the employee work-team example, and assume that
certainemployeesparticipateinmorethanoneworkteam.Agivenemployee
maythereforeappearinmorethanoneoftheteamentitysetsthatarelower-
levelentitysetsof employee. Thus,the generalizationisoverlapping.
In Figure 7.21, we assume a person may be both an employee and a student. We
show this overlapping generalization via separate arrows: one from employee
to person and another from student to person. However, the generalization of
instructor andsecretariesisdisjoint.Weshow thisusing asinglearrow.
A?nalconstraint,thecompletenessconstraintonageneralizationorspecial-
ization,speci?eswhetherornotanentityinthehigher-levelentitysetmustbelong
toatleastoneofthelower-levelentitysetswithinthegeneralization/specialization.
This constraint may beone of thefollowing:
• Totalgeneralizationor specialization.Eachhigher-levelentitymustbelong
toalower-levelentityset.
• Partial generalization or specialization. Some higher-level entities may not
belongtoany lower-levelentityset.
Partial generalizationis the default.We can specifytotal generalizationin an E-R
diagrambyaddingthekeyword“total”inthediagramanddrawingadashedline
from the keyword to the corresponding hollow arrow-head to which it applies
(foratotalgeneralization),ortothesetofhollowarrow-headstowhichitapplies
(for anoverlappinggeneralization).
Thestudentgeneralizationistotal:Allstudententitiesmustbeeithergraduate
orundergraduate.Becausethehigher-levelentitysetarrivedatthroughgeneral-
izationisgenerallycomposedofonlythoseentitiesinthelower-levelentitysets,
the completeness constraint for a generalized higher-level entity set is usually
total. When the generalization is partial, a higher-level entity is not constrained
toappearinalower-levelentityset.Theworkteamentitysetsillustrateapartial
specialization.Sinceemployeesareassignedtoateamonlyafter3monthsonthe
job, some employee entities may not be members of any of the lower-level team
entitysets.
Wemaycharacterizetheteamentitysetsmorefullyasapartial,overlapping
specialization of employee. The generalization of graduate student and undergrad-
7.8 Extended E-R Features 301
uate student into student is a total, disjoint generalization. The completeness and
disjointness constraints, however, do not depend on each other. Constraint pat-
ternsmay alsobe partial-disjointand total-overlapping.
We can see that certain insertion and deletion requirements follow from the
constraints that apply to a given generalization or specialization. For instance,
when atotalcompletenessconstraint isinplace,an entityinsertedintoahigher-
level entity set must also be inserted into at least one of the lower-level entity
sets. With a condition-de?ned constraint, all higher-level entities that satisfy the
condition must be inserted into that lower-level entity set. Finally, an entity that
is deleted from a higher-level entity set also is deleted from all the associated
lower-levelentitysetstowhich itbelongs.
7.8.5 Aggregation
One limitation of the E-R model is that it cannot express relationships among
relationships. To illustrate the need for such a construct, consider the ternary
relationship proj guide, which we saw earlier, between an instructor, student and
project (seeFigure7.13).
Nowsupposethateachinstructorguidingastudentonaprojectisrequiredto
?leamonthlyevaluationreport.Wemodeltheevaluationreportasanentityeval-
uation,withaprimarykeyevaluation id.Onealternativeforrecordingthe(student,
project, instructor) combination to which an evaluation corresponds is to create a
quaternary(4-way)relationshipseteval forbetweeninstructor,student,project,and
evaluation.(Aquaternaryrelationshipisrequired—abinaryrelationshipbetween
studentand evaluation, forexample,wouldnotpermitustorepresentthe(project,
instructor) combination to which an evaluation corresponds.) Using the basic E-R
modelingconstructs,weobtainthe E-RdiagramofFigure7.22.(Wehaveomitted
theattributesofthe entitysets,forsimplicity.)
It appears that the relationship sets proj guide and eval for can be combined
into one single relationship set. Nevertheless, we should not combine them into
asinglerelationship,since some instructor, student, project combinations maynot
haveanassociated evaluation.
There is redundant information in the resultant ?gure, however, since every
instructor, student, project combination in eval for must also be in proj guide.Ifthe
evaluation were a value rather than a entity, we could instead make evaluation
a multivalued composite attribute of the relationship set proj guide. However,
this alternative may not be be an option if an evaluation may also be related
to other entities; for example, each evaluation report may be associated with a
secretary who is responsible for further processing of the evaluation report to
makescholarship payments.
Thebestwaytomodelasituationsuchasthe onejustdescribedistouseag-
gregation.Aggregationisanabstractionthroughwhichrelationshipsaretreated
as higher-level entities. Thus, for our example, we regard the relationship set
proj guide (relating the entity sets instructor, student,andproject) as a higher-level
entity set called proj guide. Such an entity set is treated in the same manner as is
anyotherentityset.Wecanthencreateabinaryrelationshipeval forbetweenproj
302 Chapter7 DatabaseDesignandtheE-RModel
project
evaluation
instructor student
eval_for
proj_guide
Figure 7.22 E-R diagram with redundant relationships.
guide and evaluation to represent which (student, project, instructor) combination
an evaluation is for. Figure 7.23 shows a notation for aggregation commonly used
to represent this situation.
7.8.6 Reduction to Relation Schemas
W ear einapositionnowtodescribehowtheextendedE-R features can be
translated into relation schemas.
7.8.6.1 RepresentationofGeneralization
Therearetwodifferentmethodsofdesigningrelationschemasforan E-Rdiagram
thatincludesgeneralization.AlthoughwerefertothegeneralizationinFigure7.21
in this discussion, we simplify it by including only the ?rst tier of lower-level
entity sets—that is, employee and student. We assume that ID is the primary key
of person.
1. Create a schema for the higher-level entity set. For each lower-level entity
set,createaschemathatincludesanattributeforeachoftheattributesofthat
entity set plus one for each attribute of the primary key of the higher-level
entity set. Thus, for the E-R diagram of Figure 7.21 (ignoring the instructor
and secretary entity sets) we have three schemas:
person (ID, name, street, city)
employee (ID, salary)
student (ID, tot cred)
7.8 Extended E-R Features 303
evaluation
proj_ guide
instructor student
eval_ for
project
Figure 7.23 E-R diagram with aggregation.
The primary-key attributes of the higher-level entity set become primary-
key attributes of the higher-level entity set as well as all lower-level entity
sets.Thesecan be seenunderlinedinthe aboveexample.
In addition, we create foreign-key constraints on the lower-level entity
sets, with their primary-key attributes referencing the primary key of the
relation created from the higher-level entity set. In the above example, the
ID attribute of employee would reference the primary key of person,and
similarlyfor student.
2. Analternativerepresentationispossible,ifthegeneralizationisdisjointand
complete—that is, if no entity is a member of two lower-level entity sets
directlybelowahigher-levelentityset,andifeveryentityinthehigher-level
entitysetisalsoamemberofoneofthelower-levelentitysets.Here,wedo
not create a schema for the higher-level entity set. Instead, for each lower-
level entity set, we create a schema that includes an attribute for each of
theattributesofthatentitysetplusonefor eachattributeofthehigher-level
entityset.Then,for the E-R diagramofFigure7.21,we havetwoschemas:
employee (ID, name, street, city, salary)
student(ID, name, street, city, tot cred)
BoththeseschemashaveID,whichistheprimary-keyattributeofthehigher-
levelentityset person,astheirprimarykey.
304 Chapter 7 Database Design and the E-R Model
Onedrawback ofthesecondmethodliesinde?ningforeign-keyconstraints.
To illustrate the problem, suppose we had a relationship set R involving entity
set person. With the ?rst method, when we create a relation schema R from the
relationship set,we would also de?ne a foreign-key constraint on R, referencing
theschemaperson.Unfortunately,withthesecondmethod,wedonothaveasingle
relation to which a foreign-key constraint on R can refer. To avoid this problem,
we need to create a relation schema person containing at least the primary-key
attributesofthe personentity.
If the second method were used for an overlapping generalization, some
valueswould be storedmultipletimes,unnecessarily.For instance, ifapersonis
bothanemployeeandastudent,valuesfor streetand citywouldbestoredtwice.
If the generalization were disjoint but not complete—that is, if some person
isneitheran employeenor astudent—then anextraschema
person (ID, name, street, city)
wouldberequiredtorepresentsuchpeople.However,theproblemwithforeign-
key constraints mentioned above would remain. As an attempt to work around
the problem, suppose employees and students are additionally represented in
the personrelation.Unfortunately,name,street,and cityinformationwould then
bestoredredundantlyinthe person relation and the studentrelationforstudents,
and similarlyin the person relation and the employee relationfor employees.That
suggestsstoringname,street,andcityinformationonlyinthe personrelationand
removing that information from student and employee. If we do that, the result is
exactlythe ?rstmethodwe presented.
7.8.6.2 Representation ofAggregation
DesigningschemasforanE-Rdiagramcontainingaggregationisstraightforward.
Consider the diagram of Figure 7.23. The schema for the relationship set eval for
between the aggregation of proj guide and the entity set evaluation includes an
attribute for each attribute in the primary keys of the entity set evaluation,and
the relationship set proj guide. It also includes an attribute for any descriptive
attributes, if they exist, of the relationship set eval for. We then transform the
relationship sets and entity sets within the aggregated entity set following the
ruleswehavealreadyde?ned.
Theruleswesawearlierforcreatingprimary-keyandforeign-keyconstraints
onrelationshipsetscanbeappliedtorelationshipsetsinvolvingaggregationsas
well, with the aggregation treated like any other entity set. The primary key of
the aggregation is the primary key of its de?ning relationship set. No separate
relation is required to represent the aggregation; the relation created from the
de?ningrelationshipisusedinstead.
7.9 Alternative Notations for Modeling Data
A diagrammatic representation of the data model of an application is a very
important part of designing a database schema. Creation of a database schema
7.9 AlternativeNotationsforModelingData 305
requires not only data modeling experts, but also domain experts who know
therequirementsoftheapplicationbutmaynotbefamiliarwithdatamodeling.
Anintuitivediagrammaticrepresentationisparticularlyimportantsinceiteases
communicationofinformationbetweenthesegroupsofexperts.
A number of alternative notations for modeling data have been proposed,
ofwhich E-Rdiagramsand UMLclassdiagramsarethemostwidelyused.There
is no universal standard for E-R diagram notation, and different books and E-R
diagram software use different notations. We have chosen a particular notation
E
R
R
R
R
R
role-
name
R
E
A1
A2
A2.1
A2.2
{A3}
A4
E
R
l..h
E
E1
E2 E3
E1
E2 E3
E1
E2 E3
entity set
relationship set
identifying
relationship set
for weak entity set primary key
many-to-many
relationship
many-to-one
relationship
one-to-one
relationship
cardinality
limits
discriminating
anullibute of
weak entity set
total participation
of entity set in
relationship
anullibutes:
simple (A1),
composite (A2) and
multivalued (A3)
derived (A4)
ISA: generalization
or specialization
disjoint
generalization
total (disjoint)
generalization
role indicator
total
A1
E
A1
E
R E
()
Figure 7.24 Symbols used in the E-R notation.
306 Chapter 7 Database Design and the E-R Model
inthissixtheditionofthisbookwhichactuallydiffersfromthenotationweused
inearliereditions,for reasonsthat we explainlaterinthissection.
In the rest of this section, we study some of the alternative E-R diagram
notations,aswellasthe UMLclassdiagramnotation.Toaidincomparisonofour
notation with these alternatives, Figure 7.24 summarizes the set of symbols we
haveusedinour E-Rdiagramnotation.
7.9.1 Alternative E-R Notations
Figure 7.25 indicates some of the alternative E-R notations that are widely used.
One alternative representation of attributes of entities is to show them in ovals
connectedtotheboxrepresentingtheentity;primarykeyattributesareindicated
by underlining them. The above notation is shown at the top of the ?gure. Re-
lationship attributes can be similarly represented,by connecting the ovals to the
diamondrepresentingtherelationship.
participation
in R: total (E1)
and partial (E2)
E1 E2
E2
E1 R
R
R
entity set E with
simple anullibute A1,
composite anullibute A2,
multivalued anullibute A3,
derived anullibute A4,
and primary key A1
many-to-many
relationship
one-to-one
relationship
many-to-one
relationship
R
R
*
*
*
1
1
1
R
E1
E1
E1
E2
E2
E2 E1 E2
generalization
ISA ISA
total
generalization
weak entity set
A1
A2
A3
A2.1 A2.2
A4
E
R
E1 E2
R
E1 E2
Figure 7.25 Alternative E-R notations.
7.9 Alternative Notations for Modeling Data 307
Cardinality constraints on relationships can be indicated in several different
ways, as shown in Figure 7.25. In one alternative, shown on the left side of the
?gure, labels ? and 1 on the edges out of the relationship are used for depicting
many-to-many, one-to-one, and many-to-one relationships. The case of one-to-
many issymmetrictomany-to-one, andis not shown.
Inanotheralternativenotationshownontherightsideofthe?gure,relation-
ship sets are represented by lines between entity sets, without diamonds; only
binary relationships can be modeled thus. Cardinality constraints in such a no-
tation are shown by “crow’s-foot” notation, as in the ?gure. In a relationship R
between E1andE2,crow’sfeetonbothsidesindicatesamany-to-manyrelation-
ship, while crow’s feet on just the E1 side indicates a many-to-one relationship
from E1toE2. Total participation is speci?ed in this notation by a vertical bar.
Notehowever,thatinarelationship Rbetweenentities E1andE2,ifthepartici-
pationof E1inRistotal,the verticalbar isplacedon the oppositeside,adjacent
to entity E2. Similarly, partial participation is indicated by using a circle, again
onthe oppositeside.
The bottom part of Figure 7.25 shows an alternative representationof gener-
alization,using trianglesinsteadofhollowarrow-heads.
Inprioreditionsofthistextuptothe?fthedition,weusedovalstorepresent
attributes, with triangles representing generalization, as shown in Figure 7.25.
Thenotationusingovalsforattributesanddiamondsforrelationshipsiscloseto
the original form of E-R diagrams used by Chen in his paper that introduced the
notionof E-R modeling.That notationis nowreferredtoas Chen’s notation.
TheU.S.NationalInstituteforStandardsandTechnologyde?nedastandard
called IDEF1X in 1993. IDEF1X uses the crow’s-foot notation, with vertical bars on
the relationship edge to denote total participation and hollow circles to denote
partialparticipation,and includesothernotations that we havenot shown.
With the growth in the use of Uni?ed Markup Language (UML), described
laterinSection7.9.2,wehavechosentoupdateourE-Rnotationtomakeitcloserto
theformofUMLclassdiagrams;theconnectionswillbecomeclearinSection7.9.2.
In comparison with our previous notation, our new notation provides a more
compactrepresentationofattributes,andisalsoclosertothenotationsupported
bymany E-Rmodelingtools,inadditiontobeingclosertothe UMLclassdiagram
notation.
There are a variety of tools for constructing E-R diagrams, each of which has
its own notational variants. Some of the tools even provide a choice between
several E-R notation variants. See the references in the bibliographic notes for
moreinformation.
One key difference between entity sets in an E-R diagram and the relation
schemas created from such entities is that attributes in the relational schema
corresponding to E-R relationships, such as the dept name attribute of instructor,
are not shown in the entity set in the E-R diagram. Some data modeling tools
allow users to choose between two views of the same entity, one an entity view
without such attributes,and otherarelationalviewwithsuchattributes.
308 Chapter 7 Database Design and the E-R Model
7.9.2 The Uni?ed Modeling Language UML
Entity-relationshipdiagramshelpmodelthedatarepresentationcomponentofa
softwaresystem.Datarepresentation,however,formsonlyonepartofanoverall
system design. Other components include models of user interactions with the
system, speci?cation of functional modules of the system and their interaction,
etc. The Uni?ed Modeling Language (UML) is a standard developed under the
auspices of the Object Management Group (OMG) for creating speci?cations of
variouscomponents ofasoftware system.Someofthe partsof UMLare:
• Class diagram. A class diagram is similar to an E-R diagram. Later in this
section we illustrate a few features of class diagrams and how they relate to
E-Rdiagrams.
• Use case diagram. Use case diagrams show the interaction between users
and the system, in particular the steps of tasks that users perform (such as
withdrawing money orregisteringfor acourse).
• Activitydiagram.Activitydiagramsdepictthe?owoftasksbetweenvarious
components ofasystem.
• Implementation diagram. Implementationdiagrams show the system com-
ponentsandtheirinterconnections,bothatthesoftwarecomponentleveland
thehardware componentlevel.
Wedonotattempttoprovidedetailedcoverageofthedifferentpartsof UML
here.SeethebibliographicnotesforreferencesonUML.Insteadweillustratesome
featuresofthat part of UML that relatestodatamodelingthrough examples.
Figure 7.26 shows several E-R diagram constructs and their equivalent UML
classdiagramconstructs.Wedescribetheseconstructsbelow. UMLactuallymod-
els objects, whereas E-R models entities. Objects are like entities, and have at-
tributes, but additionally provide a set of functions (called methods) that can be
invoked to compute values on the basis of attributes of the objects, or to update
the object itself.Classdiagramscan depict methodsinadditionto attributes.We
cover objects in Chapter 22. UML does not support composite or multivalued
attributes, and derivedattributes are equivalent to methods that take no param-
eters.Sinceclassessupportencapsulation, UMLallowsattributesandmethodsto
be pre?xed with a “+”, “-”,or“#”, which denote respectivelypublic, private and
protectedaccess.Privateattributescanonlybeusedinmethodsoftheclass,while
protected attributes can be used only in methods of the class and its subclasses;
theseshould befamiliartoanyone who knows Java,C++orC#.
In UMLterminology,relationshipsetsarereferredtoasassociations;weshall
refertothemasrelationshipsetsforconsistency with E-Rterminology.Werepre-
sent binary relationship sets in UML by just drawing a line connecting the entity
sets.Wewritetherelationshipsetnameadjacenttotheline.Wemayalsospecify
the role played by an entity set in a relationship set by writing the role name on
the line, adjacent to the entity set. Alternatively, we may write the relationship
set name in a box, along with attributes of the relationship set, and connect the
7.9 AlternativeNotationsforModelingData 309
–A1
+M1
E
E2 E3
E1
E2 E3
E1
E2 E3
binary
relationship
class with simple anullibutes
and methods (anullibute
pre?xes:  +  =  public,
– =  private, # = protected)
overlapping
generalization
disjoint
generalization
A1
M1
E entity with
anullibutes (simple,
composite,
multivalued, derived)
R
E2 E1
role1 role2
relationship
anullibutes
E2 E1
role1 role2
A1
R
R
cardinality
constraints
E2 E1
R
E2 E1
0.. * 0..1 0..1 0.. *
R
E3
E1
E2
R
E3
E1
E2
n-ary
relationships
E1
E2 E3
overlapping
disjoint
ER Diagram Notation Equivalent in UML
R E2 E1
role1 role2
R E2 E1
role1 role2
A1
() ()
E1
Figure 7.26 Symbols used in the UML class diagram notation.
box by a dotted line to the line depicting the relationship set. This box can then
be treated as an entity set, in the same way as an aggregation in E-R diagrams,
andcanparticipateinrelationshipswithotherentitysets.
SinceUMLversion1.3,UMLsupportsnonbinaryrelationships,usingthesame
diamond notation used in E-R diagrams. Nonbinary relationships could not be
directly represented in earlier versions of UML—they had to be converted to
binary relationships by the technique we have seenearlier in Section 7.7.3. UML
allows the diamond notation to be used even for binary relationships, but most
designersusethelinenotation.
Cardinality constraints are speci?ed in UML in the same way as in E-R dia-
grams,intheforml..h,whereldenotestheminimumandhthemaximumnumber
ofrelationshipsanentitycanparticipatein.However,youshouldbeawarethat
thepositioningoftheconstraintsisexactlythereverseofthepositioningofcon-
straints in E-R diagrams, as shown in Figure 7.26. The constraint 0..? on the E2
310 Chapter 7 Database Design and the E-R Model
side and 0..1ontheE1 side means that each E2 entity can participate in at most
one relationship, whereas each E1 entity can participate in many relationships;
inotherwords,the relationshipismany-to-one from E2toE1.
Singlevaluessuchas1or ?maybewrittenonedges;thesinglevalue1onan
edge is treated as equivalent to 1..1, while ? is equivalent to 0..?. UML supports
generalization;thenotationisbasicallythesameasinour E-Rnotation,including
the representationofdisjointand overlappinggeneralizations.
UMLclassdiagramsincludeseveralothernotationsthatdonotcorrespondto
the E-R notations we have seen.For example,a line betweentwoentitysetswith
asmalldiamondatoneendspeci?esthattheentityonthediamondsidecontains
theotherentity(containmentiscalled “aggregation”in UMLterminology;donot
confusethisuseofaggregationwiththesenseinwhichitisusedintheE-Rmodel).
Forexample,avehicleentitymaycontain anengine entity.
UML class diagrams also provide notations to represent object-oriented lan-
guagefeaturessuchasinterfaces.Seethereferencesinthebibliographicnotesfor
moreinformationon UML class diagrams.
7.10 Other Aspects of Database Design
Our extensive discussion of schema design in this chapter may create the false
impressionthatschemadesignistheonlycomponentofadatabasedesign.There
areindeedseveralotherconsiderationsthatweaddressmorefullyinsubsequent
chapters,and surveybrie?yhere.
7.10.1 Data Constraints and Relational Database Design
We have seen a variety of data constraints that can be expressed using SQL,
including primary-key constraints, foreign-key constraints, check constraints,
assertions, and triggers. Constraints serve several purposes. The most obvious
one is the automation of consistency preservation. By expressing constraints in
the SQL data-de?nitionlanguage,thedesignerisabletoensurethatthedatabase
system itself enforces the constraints. This is more reliable than relying on each
applicationprogramindividuallytoenforceconstraints.Italsoprovidesacentral
locationfor the updateof constraints and the additionof newones.
Afurtheradvantageofstatingconstraintsexplicitlyisthatcertainconstraints
are particularly useful in designing relational database schemas. If we know, for
example,thatasocial-securitynumberuniquelyidenti?esaperson,thenwecan
use a person’s social-security number to link data related to that person even
if these data appear in multiple relations. Contrast that with, for example, eye
color, which is not a unique identi?er. Eye color could not be used to link data
pertaining to a speci?c person across relations because that person’s data could
notbedistinguishedfromdatapertainingtootherpeoplewiththesameeyecolor.
In Section 7.6, we generated a set of relation schemas for a given E-R design
using the constraints speci?ed in the design. In Chapter 8, we formalize this
idea and related ones, and show how they can assist in the design of relational
7.10 Other Aspects of Database Design 311
database schemas. The formal approach to relational database design allows us
to state in a precise manner when a given design is a good one and to transform
poor designs into better ones. We shall see that the process of starting with an
entity-relationship design and generating relation schemas algorithmically from
that designprovidesagoodstart tothe designprocess.
Data constraints are useful as well in determining the physical structure of
data.Itmaybeusefultostoredatathatarecloselyrelatedtoeachotherinphysical
proximityondisksoastogainef?cienciesindiskaccess.Certainindexstructures
work betterwhentheindexison aprimarykey.
Constraint enforcement comes at a potentially high price in performance
each time the database is updated. For each update, the system must check all
of the constraints and either reject updates that fail the constraints or execute
appropriate triggers. The signi?cance of the performance penalty depends not
onlyonthefrequencyofupdatebutalsoonhowthedatabaseisdesigned.Indeed
ef?ciency of the testing of certain types of constraints is an important aspect of
thediscussionofrelationaldatabaseschemadesigninChapter8.
7.10.2 Usage Requirements: Queries, Performance
Database system performance is a critical aspect of most enterprise information
systems.Performancepertainsnotonlytotheef?cientuseofthecomputingand
storage hardware being used, but also to the ef?ciency of people who interact
withthesystemand ofprocessesthat dependupondatabasedata.
Therearetwomainmetricsforperformance:
• Throughput—the number of queries or updates (often referred to as trans-
actions) thatcan be processedonaverageperunitoftime.
• Response time—the amount of time a single transaction takes from start to
?nishineithertheaveragecase ortheworstcase.
Systemsthatprocesslargenumbersoftransactionsinabatchstylefocusonhaving
highthroughput.Systemsthatinteractwithpeopleortime-criticalsystemsoften
focus on response time. These two metrics are not equivalent. High throughput
arisesfromobtaininghighutilizationofsystemcomponents.Doingsomayresult
in certain transactions being delayed until such time that they can be run more
ef?ciently.Those delayedtransactions sufferpoor responsetime.
Mostcommercialdatabasesystemshistoricallyhavefocusedonthroughput;
however,avarietyofapplicationsincludingWeb-basedapplicationsandtelecom-
munication information systems require good response time on average and a
reasonablebound onworst-case responsetime.
Anunderstandingoftypesofqueriesthatareexpectedtobethemostfrequent
helps in the design process. Queries that involve joins require more resources to
evaluate than those that do not. In cases where a join is required, the database
administratormaychoosetocreateanindexthatfacilitatesevaluationofthatjoin.
For queries—whethera joinis involvedor not—indices can be createdto speed
evaluation of selection predicates (SQL where clause) that are likely to appear.
312 Chapter 7 Database Design and the E-R Model
Another aspect of queries that affects the choice of indices is the relative mix of
update and read operations. While an index may speed queries, it also slows
updates,whichareforcedtodoextraworktomaintaintheaccuracyoftheindex.
7.10.3 Authorization Requirements
AuthorizationconstraintsaffectdesignofthedatabaseaswellbecauseSQLallows
access to be granted to users on the basis of components of the logical design
of the database. A relation schema may need to be decomposed into two or
more schemas to facilitate the granting of access rights in SQL. For example, an
employeerecordmayincludedatarelatingtopayroll,jobfunctions,andmedical
bene?ts. Because different administrative units of the enterprise may manage
each of these types of data, some users will need access to payroll data while
being denied access to the job data, medical data, etc. If these data are all in one
relation, the desired division of access, though still feasible through the use of
views,ismorecumbersome.Divisionofdatainthismannerbecomesevenmore
critical when the data are distributed across systems in a computer network, an
issueweconsider inChapter19.
7.10.4 Data Flow, Work?ow
Database applications are often part of a larger enterprise application that in-
teracts not only with the database system but also with various specialized ap-
plications. For example, in a manufacturing company, a computer-aided design
(CAD) system may assist in the design of new products. The CAD system may
extract data from the database via an SQL statement, process the data internally,
perhapsinteractingwithaproductdesigner,andthenupdatethedatabase.Dur-
ing this process, control of the data may pass among several product designers
as well as other people. As another example, consider a travel-expensereport. It
iscreatedbyanemployeereturningfromabusinesstrip(possiblybymeansofa
specialsoftwarepackage)andissubsequentlyroutedtotheemployee’smanager,
perhaps other higher-level managers, and eventually to the accounting depart-
ment for payment (at which point it interacts with the enterprise’s accounting
informationsystems).
Thetermwork?owreferstothecombinationofdataandtasksinvolvedinpro-
cesseslikethoseoftheprecedingexamples.Work?owsinteractwiththedatabase
systemastheymoveamongusersandusersperformtheirtasksonthework?ow.
Inadditiontothedataonwhichwork?owsoperate,thedatabasemaystoredata
about the work?ow itself, including the tasks making up a work?ow and how
theyaretoberoutedamongusers.Work?owsthusspecifyaseriesofqueriesand
updates to the database that may be taken into account as part of the database-
design process. Put in other terms, modeling the enterprise requires us not only
to understand the semantics of the data but also the business processes that use
those data.
7.11 Summary 313
7.10.5 Other Issues in Database Design
Database design is usually not a one-time activity. The needs of an organization
evolve continually, and the data that it needs to store also evolve correspond-
ingly. During the initial database-design phases, or during the development of
anapplication,thedatabasedesignermayrealizethatchangesarerequiredatthe
conceptual, logical, or physical schema levels. Changes in the schema can affect
allaspectsofthedatabaseapplication.Agooddatabasedesignanticipatesfuture
needsofanorganization,andensuresthattheschemarequiresminimalchanges
astheneedsevolve.
It is important to distinguish between fundamental constraints that are ex-
pectedtobepermanentandconstraintsthatareanticipatedtochange.Forexam-
ple,theconstraintthataninstructor-ididentifyauniqueinstructorisfundamen-
tal.Ontheotherhand,auniversitymayhaveapolicythataninstructorcanhave
only one department,which may change atalaterdateifjoint appointmentsare
allowed.Adatabasedesignthatonlyallowsonedepartmentperinstructormight
requiremajorchangesifjointappointmentsareallowed.Suchjointappointments
can be represented by adding an extra relationship, without modifying the in-
structor relation, as long as each instructor has only one primary department
af?liation; a policy change that allows more than one primary af?liation may
require a larger change in the database design. A good design should account
not only for current policies, but should also avoid or minimize changes due to
changes that areanticipated,orhave areasonable chance of happening.
Furthermore, the enterprise that the database is serving likely interacts with
other enterprises and, therefore, multiple databases may need to interact. Con-
versionofdatabetweendifferentschemasisanimportantprobleminreal-world
applications. Various solutions have been proposed for this problem. The XML
data model, which we study in Chapter 23, is widely used for representing data
whenitisexchangedbetweendifferentapplications.
Finally, it is worth noting that database design is a human-oriented activity
in two senses: the end users of the system are people (even if an application
sitsbetweenthedatabaseandtheendusers);andthedatabasedesignerneedsto
interactextensivelywithexpertsintheapplicationdomaintounderstandthedata
requirements of the application. All of the people involved with the data have
needs and preferences that should be taken into account in order for a database
designanddeploymenttosucceedwithinthe enterprise.
7.11 Summary
• Database design mainly involves the design of the database schema. The
entity-relationship(E-R)datamodelisawidelyuseddatamodelfordatabase
design. It provides a convenient graphical representation to view data, rela-
tionships,and constraints.
• The E-R model is intended primarily for the database-design process. It was
developed to facilitate database design by allowing the speci?cation of an
314 Chapter 7 Database Design and the E-R Model
enterprise schema. Such a schema representsthe overall logical structure of
the database. This overall structure can be expressed graphically by an E-R
diagram.
• Anentityisanobjectthatexistsintherealworldandisdistinguishablefrom
otherobjects.Weexpressthedistinctionbyassociatingwitheachentityaset
ofattributesthat describestheobject.
• Arelationshipisanassociationamongseveralentities.Arelationshipsetis
acollectionofrelationshipsofthesametype,andanentitysetisacollection
ofentitiesofthesametype.
• The terms superkey, candidate key,andprimary key apply to entity and
relationshipsetsastheydoforrelationschemas.Identifyingtheprimarykey
of a relationship set requires some care, since it is composed of attributes
fromone or moreoftherelatedentitysets.
• Mappingcardinalitiesexpressthenumberofentitiestowhichanotherentity
can beassociatedviaarelationshipset.
• An entity set that does not have suf?cient attributes to form a primary key
istermeda weak entity set. An entitysetthathas aprimarykeyistermeda
strongentity set.
• The various features of the E-R model offer the database designer numer-
ouschoicesinhowtobestrepresenttheenterprisebeingmodeled.Concepts
and objects may, in certain cases, be represented by entities, relationships,
or attributes. Aspects of the overall structure of the enterprise may be best
describedbyusingweakentitysets,generalization,specialization,or aggre-
gation.Often,thedesignermustweighthemeritsofasimple,compactmodel
versusthose ofamoreprecise,but morecomplex,one.
• A database design speci?ed by an E-R diagram can be represented by a
collection of relation schemas. For each entity set and for each relationship
set in the database, there is a unique relation schema that is assigned the
nameofthecorrespondingentitysetorrelationshipset.Thisformsthebasis
for derivingarelationaldatabasedesignfrom an E-Rdiagram.
• Specialization and generalization de?ne a containment relationship be-
tween a higher-level entity set and one or more lower-level entity sets. Spe-
cializationistheresultoftakingasubsetofahigher-levelentitysettoforma
lower-level entity set. Generalization is the result of taking the union of two
or more disjoint(lower-level)entitysetsto produce a higher-levelentityset.
The attributes of higher-level entity sets are inherited by lower-level entity
sets.
• Aggregationisanabstractioninwhichrelationshipsets(alongwiththeiras-
sociatedentitysets)aretreatedashigher-levelentitysets,andcanparticipate
inrelationships.
• UML is a popular modeling language. UML class diagrams are widely used
for modelingclasses,aswellasfor generalpurposedatamodeling.
PracticeExercises 315
Review Terms
• Entity-relationshipdatamodel
• Entityandentityset
?
Attributes
?
Domain
?
Simpleandcompositeattributes
?
Single-valued and multivalued
attributes
?
Nullvalue
?
Derivedattribute
?
Superkey, candidate key, and
primarykey
• Relationshipand relationshipset
?
Binaryrelationshipset
?
Degreeofrelationshipset
?
Descriptiveattributes
?
Superkey, candidate key, and
primarykey
?
Role
?
Recursiverelationshipset
• E-Rdiagram
• Mappingcardinality:
?
One-to-one relationship
?
One-to-manyrelationship
?
Many-to-one relationship
?
Many-to-many relationship
• Participation
?
Totalparticipation
?
Partialparticipation
• Weak entity sets and strong entity
sets
?
Discriminatorattributes
?
Identifyingrelationship
• Specializationand generalization
?
Superclassand subclass
?
Attributeinheritance
?
Singleand multipleinheritance
?
Condition-de?ned and user-
de?nedmembership
?
Disjointandoverlappinggener-
alization
?
Totaland partialgeneralization
• Aggregation
• UML
• UML class diagram
Practice Exercises
7.1 Construct an E-R diagram for a car insurance company whose customers
own one or more cars each. Each car has associated with it zero to any
number of recorded accidents. Each insurance policy covers one or more
cars, and has one or more premium payments associated with it. Each
paymentisforaparticularperiodoftime,andhasanassociatedduedate,
and thedatewhenthepaymentwasreceived.
7.2 Consideradatabaseusedtorecordthemarksthatstudentsgetindifferent
examsofdifferentcourseofferings(sections).
316 Chapter7 DatabaseDesignandtheE-RModel
a. Construct an E-R diagram that models exams as entities, and uses a
ternaryrelationship,forthedatabase.
b. ConstructanalternativeE-Rdiagramthatusesonlyabinaryrelation-
shipbetweenstudentandsection.Makesurethatonlyonerelationship
exists between a particular student and section pair, yet you can rep-
resentthemarksthatastudentgetsindifferentexams.
7.3 Design an E-R diagram for keeping track of the exploits of your favorite
sportsteam.Youshouldstorethematchesplayed,thescoresineachmatch,
theplayersineachmatch, andindividualplayerstatisticsforeachmatch.
Summarystatisticsshouldbemodeledasderivedattributes.
7.4 ConsideranE-Rdiagraminwhichthesameentitysetappearsseveraltimes,
withitsattributesrepeatedinmorethanone occurrence.Whyisallowing
thisredundancyabadpracticethatoneshouldavoid?
7.5 An E-Rdiagramcanbeviewedasagraph.Whatdothefollowingmeanin
termsofthestructureofanenterpriseschema?
a. Thegraphisdisconnected.
b. Thegraphhasacycle.
7.6 Considertherepresentationofaternaryrelationshipusingbinaryrelation-
shipsasdescribedinSection7.7.3andillustratedinFigure7.27b(attributes
notshown).
BC
A
C BE
A
R
A
R
B
R
C
(a) (b)
(c)
A
B C
R
R
BC
R
AB
R
AC
Figure 7.27 E-R diagram for Practice Exercise 7.6 and Exercise 7.24.
PracticeExercises 317
a. Show a simple instance of E, A, B,C, R
A
, R
B
,andR
C
that cannot
correspondtoany instance of A, B,C,andR.
b. Modify the E-R diagram of Figure 7.27b to introduce constraints that
will guarantee that any instance of E, A, B,C, R
A
, R
B
,andR
C
that
satis?estheconstraintswillcorrespondtoaninstanceof A, B,C,and
R.
c. Modifythetranslationabovetohandletotalparticipationconstraints
ontheternary relationship.
d. The above representation requires that we create a primary-key at-
tribute for E. Show how to treat E as a weak entity set so that a
primary-keyattributeisnot required.
7.7 Aweakentitysetcanalwaysbemadeintoastrongentitysetbyaddingto
itsattributestheprimary-keyattributesofitsidentifyingentityset.Outline
what sort ofredundancy willresultifwedoso.
7.8 Consider a relation such as sec course, generated from a many-to-one rela-
tionship sec course. Do the primary and foreign key constraints created on
therelationenforce themany-to-one cardinalityconstraint? Explainwhy.
7.9 Suppose the advisor relationship were one-to-one. What extra constraints
arerequiredontherelation advisortoensurethattheone-to-onecardinality
constraint isenforced?
7.10 Consider a many-to-one relationship R between entity sets Aand B.Sup-
posetherelationcreatedfrom Riscombinedwiththerelationcreatedfrom
A.InSQL, attributes participating in a foreign key constraint can be null.
Explain how a constraint on total participation of Ain R can be enforced
using notnullconstraints in SQL.
7.11 InSQL,foreignkeyconstraintscanonlyreferencetheprimarykeyattributes
ofthereferencedrelation,orotherattributesdeclaredtobeasuperkeyus-
ing the unique constraint. As a result, total participation constraints on a
many-to-many relationship (or on the “one” side of a one-to-many rela-
tionship)cannotbeenforcedontherelationscreatedfromtherelationship,
using primarykey,foreignkeyand not nullconstraints ontherelations.
a. Explainwhy.
b. Explain how to enforce total participation constraints using com-
plexcheckconstraintsorassertions(seeSection4.4.7).(Unfortunately,
these features are not supported on any widely used database cur-
rently.)
7.12 Figure 7.28 shows a lattice structure of generalization and specialization
(attributes not shown). For entity sets A, B,andC, explain how attributes
318 Chapter 7 Database Design and the E-R Model
XY
ABC
Figure 7.28 E-R diagram for Practice Exercise 7.12.
are inherited from the higher-level entity sets X and Y.Discusshowto
handleacasewhereanattributeof Xhasthesamenameassomeattribute
of Y.
7.13 Temporal changes:AnE-R diagram usually models the state of an enter-
prise at a point in time. Suppose we wish to track temporal changes,thatis,
changes to data over time. For example, Zhang may have been a student
between 1 September 2005 31 May 2009, while Shankar may have had in-
structor Einstein as advisor from 31 May 2008 to 5 December 2008, and
again from 1 June 2009 to 5 January 2010. Similarly, attribute values of an
entityorrelationship,suchas titleand creditsof course, salary,orevenname
of instructor,andtot credof student,can change overtime.
Onewaytomodeltemporalchangesisasfollows.Wede?neanewdata
type called valid time, which is a time-interval, or a set of time-intervals.
We then associate a valid time attribute with each entity and relationship,
recordingthetimeperiodsduringwhichtheentityorrelationshipisvalid.
Theend-timeofanintervalcanbein?nity;forexample,ifShankarbecame
a student on 2 September 2008, and is still a student, we can represent
the end-time of the valid time interval as in?nity for the Shankar entity.
Similarly,wemodelattributesthatcanchangeovertimeasasetofvalues,
eachwithitsown valid time.
a. Draw an E-R diagram with the student and instructor entities, and
the advisor relationship, with the above extensions to track temporal
changes.
b. Convertthe above E-R diagramintoasetofrelations.
Itshouldbeclearthatthesetofrelationsgeneratedaboveisrathercomplex,
leading to dif?culties in tasks such as writing queries in SQL.Analterna-
tive approach, which is used more widely is to ignore temporal changes
whendesigningthe E-Rmodel(inparticular,temporalchangestoattribute
values),andtomodifytherelationsgeneratedfromthe E-Rmodeltotrack
temporalchanges, asdiscussedlaterinSection8.9.
Exercises 319
Exercises
7.14 Explain the distinctions among the terms primary key, candidate key, and
superkey.
7.15 Construct an E-R diagram for a hospital with a set of patients and a set of
medicaldoctors. Associate witheach patient a log ofthe various tests and
examinationsconducted.
7.16 Construct appropriate relation schemas for each of the E-R diagrams in
Practice Exercises7.1to7.3.
7.17 Extend the E-R diagram of Practice Exercise 7.3 to track the same informa-
tionforallteamsinaleague.
7.18 Explainthedifferencebetweenaweakandastrongentityset.
7.19 Wecanconvertanyweakentitysettoastrongentitysetbysimplyadding
appropriateattributes.Why, then,dowehaveweakentitysets?
7.20 ConsidertheE-RdiagraminFigure7.29,whichmodelsanonlinebookstore.
a. Listthe entitysetsand theirprimarykeys.
b. Suppose the bookstore adds Blu-ray discs and downloadable video
toitscollection.Thesameitemmaybepresentinoneorbothformats,
withdifferingprices.Extendthe E-Rdiagramtomodelthisaddition,
ignoring theeffectonshopping baskets.
c. Nowextendthe E-Rdiagram,usinggeneralization,tomodelthecase
where a shopping basket may contain any combination of books,
Blu-raydiscs,or downloadable video.
7.21 Design a database for an automobile company to provide to its dealers to
assist them in maintaining customer records and dealer inventory and to
assistsalesstaffinorderingcars.
Eachvehicleisidenti?edbyavehicleidenti?cationnumber(VIN).Each
individualvehicleisaparticularmodelofaparticularbrandofferedbythe
company (e.g., the XF is a model of the car brand Jaguar of Tata Motors).
Each model can be offered with a variety of options, but an individual
car may have only some (or none) of the available options. The database
needs to store information about models, brands, and options, as well as
informationabout individualdealers,customers,andcars.
Your designshouldincludean E-Rdiagram,asetofrelationalschemas,
andalistofconstraints,includingprimary-keyandforeign-keyconstraints.
7.22 Design a database for a world-wide package deliverycompany (e.g., DHL
orFedEX).Thedatabasemustbeabletokeeptrackofcustomers(whoship
items) and customers (who receive items); some customers may do both.
320 Chapter 7 Database Design and the E-R Model
author
name
address
URL
written_by
published_by
contains
number
number
stocks
book
shopping_basket
basket_id
warehouse
basket_of
ISBN
title
year
price
code
address
phone
publisher
name
address
phone
URL
customer
email
name
address
phone
Figure 7.29 E-R diagram for Exercise 7.20.
Each package must be identi?able and trackable, so the database must
be able to store the location of the package and its history of locations.
Locations includetrucks,planes,airports,and warehouses.
Your designshouldincludean E-Rdiagram,asetofrelationalschemas,
andalistofconstraints,includingprimary-keyandforeign-keyconstraints.
7.23 Design a database for an airline. The database must keep track of cus-
tomersandtheirreservations,?ightsandtheirstatus,seatassignmentson
individual?ights,and thescheduleand routingoffuture?ights.
Your designshouldincludean E-Rdiagram,asetofrelationalschemas,
andalistofconstraints,includingprimary-keyandforeign-keyconstraints.
7.24 In Section 7.7.3, we represented a ternary relationship (repeated in Fig-
ure 7.27a) using binary relationships, as shown in Figure 7.27b. Consider
the alternative shown in Figure 7.27c. Discuss the relative merits of these
twoalternativerepresentationsofaternaryrelationshipbybinaryrelation-
ships.
Bibliographical Notes 321
7.25 ConsidertherelationschemasshowninSection7.6,whichweregenerated
fromtheE-RdiagraminFigure7.15.Foreachschema,specifywhatforeign-
keyconstraints, ifany, shouldbe created.
7.26 Designageneralization–specializationhierarchyfor amotorvehiclesales
company.Thecompanysellsmotorcycles,passengercars,vans,andbuses.
Justify your placement of attributes at each level of the hierarchy. Explain
whytheyshould notbe placedatahigherorlowerlevel.
7.27 Explain the distinction between condition-de?ned and user-de?ned con-
straints. Which of these constraints can the system check automatically?
Explainyouranswer.
7.28 Explainthedistinctionbetweendisjointandoverlappingconstraints.
7.29 Explainthedistinctionbetweentotaland partialconstraints.
Tools
Many database systems provide tools for database design that support E-R dia-
grams.ThesetoolshelpadesignercreateE-Rdiagrams,andtheycanautomatically
create corresponding tables in a database. See bibliographic notes of Chapter 1
forreferencestodatabase-systemvendors’Websites.
There are also several database-independent data modeling tools that sup-
port E-R diagrams and UML class diagrams. The drawing tool Dia, which is
available as freeware, supports E-R diagrams and UML class diagrams. Com-
mercial tools include IBM Rational Rose (www.ibm.com/software/rational), Microsoft
Visio (see www.microsoft.com/of?ce/visio), CA’s ERwin (www.ca.com/us/data-
modeling.aspx), Poseidon for UML (www.gentleware.com), and SmartDraw
(www.smartdraw.com).
Bibliographical Notes
TheE-RdatamodelwasintroducedbyChen[1976].Alogicaldesignmethodology
forrelationaldatabasesusingtheextendedE-RmodelispresentedbyTeoreyetal.
[1986]. The Integration De?nition for Information Modeling (IDEF1X) standard
NIST [1993] released by the United States National Institute of Standards and
Technology(NIST)de?nedstandardsfor E-Rdiagrams.However,avarietyof E-R
notations areinusetoday.
Thalheim [2000] provides a detailed textbook coverage of research in E-R
modeling.BasictextbookdiscussionsareofferedbyBatinietal.[1992]andElmasri
and Navathe[2006]. Davis etal. [1983] providesacollection ofpapers on the E-R
model.
As of 2009, the current UML version was 2.2, with UML version 2.3 near ?nal
adoption.See www.uml.orgfor moreinformationon UML standardsand tools.
This page intentionally left blank 
CHAPTER
8
Relational Database Design
In this chapter, we consider the problem of designing a schema for a relational
database.Manyoftheissuesindoingsoaresimilartodesignissuesweconsidered
in Chapter7using the E-Rmodel.
Ingeneral,thegoalofrelationaldatabasedesignistogenerateasetofrelation
schemasthatallowsustostoreinformationwithoutunnecessaryredundancy,yet
also allows us to retrieve information easily. This is accomplished by designing
schemas that are in an appropriate normal form. To determine whether a relation
schema is in one of the desirable normal forms, we need information about
the real-world enterprise that we are modeling with the database. Some of this
information exists in a well-designed E-R diagram, but additional information
about the enterprisemay be neededas well.
Inthischapter,weintroduceaformalapproachtorelationaldatabasedesign
based on the notion of functional dependencies. We then de?ne normal forms
in terms of functional dependenciesand other types of data dependencies.First,
however, we view the problem of relational design from the standpoint of the
schemas derivedfroma givenentity-relationshipdesign.
8.1 FeaturesofGoodRelationalDesigns
Ourstudyofentity-relationshipdesigninChapter7providesanexcellentstarting
point for creating a relational database design. We saw in Section 7.6 that it
is possible to generate a set of relation schemas directly from the E-R design.
Obviously, the goodness (or badness) of the resulting set of schemas depends
on how good the E-R design was in the ?rst place. Later in this chapter, we shall
studyprecisewaysofassessingthedesirabilityofacollectionofrelationschemas.
However, we can go a long way toward a good design using concepts we have
alreadystudied.
For ease of reference, we repeat the schemas for the university database in
Figure 8.1.
323
324 Chapter 8 Relational Database Design
classroom(building,room number,capacity)
department(dept name,building,budget)
course(course id,title,dept name,credits)
instructor(ID,name,dept name,salary)
section(course id,sec id,semester,year,building,room number,time slot id)
teaches(ID,course id,sec id,semester,year)
student(ID,name,dept name,tot cred)
takes(ID,course id,sec id,semester,year,grade)
advisor(s ID,i ID)
time slot(time slot id,day,start time,end time)
prereq(course id,prereq id)
Figure8.1 Schema for the university database.
8.1.1 DesignAlternative: LargerSchemas
Now, let us explore features of this relational database design as well as some
alternatives.Supposethatinsteadofhavingtheschemasinstructoranddepartment,
we have the schema:
inst dept (ID,name,salary,dept name,building, budget)
This represents the result of a natural join on the relations corresponding to
instructor and department. This seems like a good idea because some queries can
be expressedusing fewerjoins, until we think carefully about the facts about the
universitythat led toour E-R design.
Letusconsidertheinstanceoftheinst deptrelationshowninFigure8.2.Notice
that we have to repeat the department information (“building” and “budget”)
once for each instructor in the department. For example, the information about
theComp.Sci.department(Taylor,100000)isincludedinthetuplesofinstructors
Katz, Srinivasan, and Brandt.
Itisimportantthatallthesetuplesagreeastothebudgetamountsinceother-
wise our database would be inconsistent. In our original design using instructor
anddepartment,westoredtheamountofeachbudgetexactlyonce.Thissuggests
that using inst dept is a bad idea since it stores the budget amounts redundantly
and runs the risk that some user might update the budget amount in one tuple
but not all, and thus createinconsistency.
Evenifwedecidedtolivewiththeredundancyproblem,thereisstillanother
problem with the inst dept schema. Suppose we are creating a new department
in the university. In the alternative design above, we cannot represent directly
theinformationconcerning adepartment(dept name,building,budget)unlessthat
department has at least one instructor at the university. This is because tuples in
theinst depttablerequirevaluesforID,name,andsalary.Thismeansthatwecannot
record information about the newly created department until the ?rst instructor
8.1 Features of Good Relational Designs 325
ID name salary dept name building budget
22222 Einstein 95000 Physics Watson 70000
12121 Wu 90000 Finance Painter 120000
32343 ElSaid 60000 History Painter 50000
45565 Katz 75000 Comp.Sci. Taylor 100000
98345 Kim 80000 Elec.Eng. Taylor 85000
76766 Crick 72000 Biology Watson 90000
10101 Srinivasan 65000 Comp.Sci. Taylor 100000
58583 Cali?eri 62000 History Painter 50000
83821 Brandt 92000 Comp.Sci. Taylor 100000
15151 Mozart 40000 Music Packard 80000
33456 Gold 87000 Physics Watson 70000
76543 Singh 80000 Finance Painter 120000
Figure8.2 The instdept table.
is hired for the new department. In the old design, the schema department can
handlethis,butunderthereviseddesign,wewouldhavetocreateatuplewitha
null value for building and budget. In some cases null values are troublesome, as
we saw in our study of SQL. However, if we decide that this is not a problem to
us inthis case, then we can proceedto usethe reviseddesign.
8.1.2 DesignAlternative: SmallerSchemas
Supposeagainthat,somehow,wehadstartedoutwiththeschemainst dept.How
wouldwerecognizethatitrequiresrepetitionofinformationandshouldbesplit
into the two schemasinstructor anddepartment?
By observing the contents of actual relations on schema inst dept,wecould
note the repetition of information resulting from having to list the building and
budgetonceforeachinstructorassociatedwithadepartment.However,thisisan
unreliable process. A real-world database has a large number of schemas and an
even larger number of attributes. The number of tuples can be in the millions or
higher.Discoveringrepetitionwouldbecostly.Thereisanevenmorefundamental
problemwiththisapproach.Itdoesnotallowustodeterminewhetherthelackof
repetitionisjusta“lucky”specialcaseorwhetheritisamanifestationofageneral
rule.Inourexample,howwouldweknowthatinouruniversityorganization,each
department (identi?ed by its department name) must reside in a single building
and must have a single budget amount? Is the fact that the budget amount for
the Comp. Sci. department appears three times with the same budget amount
just a coincidence? We cannot answer these questions without going back to
the enterprise itself and understanding its rules. In particular, we would need
to discover that the university requires that every department (identi?ed by its
departmentname)musthave only one buildingand one budgetvalue.
Inthecaseofinst dept,ourprocessofcreatinganE-R design successfully
avoided the creation of this schema. However, this fortuitous situation does not
326 Chapter 8 Relational Database Design
always occur. Therefore,we need to allow the database designer to specify rules
suchas“eachspeci?cvaluefordept namecorrespondstoatmostonebudget”even
in cases where dept name is not the primary key for the schema in question. In
otherwords,weneedtowritearulethatsays “iftherewereaschema(dept name,
budget), then dept name is able to serve as the primary key.” This rule is speci?ed
as a functionaldependency
dept name ?budget
Given such a rule, we now have suf?cient information to recognize the problem
oftheinst deptschema. Becausedept namecannot betheprimarykeyforinst dept
(because a department may need several tuples in the relation on schema inst
dept),theamountofabudgetmayhavetoberepeated.
Observations such as these and the rules (functional dependencies in partic-
ular) that result from them allow the database designer to recognize situations
where a schema ought to be split, or decomposed, into two or more schemas. It is
not hard to see that the right way to decomposeinst dept is into schemasinstruc-
tor and department as in the original design. Finding the right decomposition is
muchharderforschemaswithalargenumberofattributesandseveralfunctional
dependencies. To deal with this, we shall rely on a formal methodology that we
developlaterinthis chapter.
Not all decompositions of schemas are helpful. Consider an extreme case
where all we had were schemas consisting of one attribute. No interesting rela-
tionshipsofanykindcouldbeexpressed.Nowconsideralessextremecasewhere
we choose to decomposetheemployeeschema (Section 7.8):
employee (ID,name,street,city,salary)
into the following two schemas:
employee1 (ID,name)
employee2 (name,street,city,salary)
The ?aw in this decomposition arisesfrom the possibilitythat the enterprisehas
two employees with the same name. This is not unlikely in practice, as many
cultures have certain highly popular names. Of course each person would have
a unique employee-id, which is why ID can serve as the primary key. As an
example,let us assume two employees,both named Kim,work at the university
and have the following tuples in the relation on schema employee in the original
design:
(57766, Kim, Main, Perryridge,75000)
(98776, Kim, North, Hampton, 67000)
8.2 Atomic Domains and First Normal Form 327
ID name street city salary
.
.
.
57766
98776
.
.
.
Kim
Kim
Main
North
Perryridge
Hampton
75000
67000
ID name
.
.
.
57766
98776
.
.
.
Kim
Kim
name street city salary
75000
67000
Main
North
Perryridge
Hampton
.
.
.
Kim
Kim
.
.
.
ID name street city salary
employee
.
.
.
57766
57766
98776
98776
.
.
.
75000
67000
75000
67000
Perryridge
Hampton
Perryridge
Hampton
Main
North
Main
North
Kim
Kim
Kim
Kim
natural join
Figure8.3 Loss of information via a bad decomposition.
Figure 8.3 shows these tuples, the resulting tuples using the schemas resulting
from the decomposition, and the result if we attempted to regenerate the origi-
nal tuples using a natural join. As we see in the ?gure, the two original tuples
appear in the result along with two new tuples that incorrectly mix data values
pertaining to the two employees named Kim. Although we have more tuples,
we actually have less information in the following sense. We can indicate that a
certain street,city, and salarypertain to someone named Kim,but we are unable
todistinguishwhichoftheKims.Thus,ourdecompositionisunabletorepresent
certain important facts about the university employees. Clearly, we would like
to avoid such decompositions. We shall refer to such decompositions as being
lossy decompositions, and, conversely, to those that are not as lossless decom-
positions.
8.2 AtomicDomainsandFirstNormalForm
The E-Rmodelallowsentitysetsandrelationshipsetstohaveattributesthathave
somedegreeofsubstructure.Speci?cally,itallowsmultivaluedattributessuchas
328 Chapter 8 Relational Database Design
phone numberinFigure7.11andcompositeattributes(suchasanattributeaddress
with component attributesstreet,city, state,andzip). When we create tables from
E-R designs that contain these typesof attributes, we eliminate this substructure.
For composite attributes, we let each component be an attribute in its own right.
Formultivaluedattributes,wecreateonetupleforeachiteminamultivaluedset.
Intherelationalmodel,weformalizethisideathatattributesdonothaveany
substructure. A domain is atomic if elementsof the domain areconsidered to be
indivisible units. We say that a relation schema R is in ?rst normal form (1NF)if
the domainsof all attributesofR are atomic.
Asetofnamesisanexampleofanonatomicvalue.Forexample,iftheschema
of a relation employee included an attribute children whose domain elements are
setsof names, the schema would not be in?rst normal form.
Composite attributes, such as an attributeaddress with component attributes
street,city,state,andzip also have nonatomic domains.
Integersareassumedtobeatomic,sothesetofintegersisanatomicdomain;
however, the set of all sets of integers is a nonatomic domain. The distinction is
thatwedonotnormallyconsiderintegerstohavesubparts,butweconsidersets
of integers to have subparts—namely, the integers making up the set. But the
important issue is not what the domain itself is, but rather how we use domain
elements in our database. The domain of all integers would be nonatomic if we
consideredeach integerto bean orderedlistof digits.
As a practical illustration of the above point, consider an organization that
assigns employees identi?cation numbers of the following form: The ?rst two
lettersspecifythedepartmentandtheremainingfourdigitsareauniquenumber
within the department for the employee. Examples of such numbers would be
“CS001” and “EE1127”. Such identi?cation numbers can be divided into smaller
units, and are therefore nonatomic. If a relation schema had an attribute whose
domain consists of identi?cation numbers encoded as above, the schema would
not be in ?rst normal form.
When such identi?cation numbers are used, the department of an employee
can be found by writing code that breaks up the structure of an identi?cation
number. Doing so requires extra programming, and information gets encoded
in the application program rather than in the database. Further problems arise
if such identi?cation numbers are used as primary keys: When an employee
changes departments, the employee’s identi?cation number must be changed
everywhereitoccurs,whichcanbeadif?culttask,orthecodethatinterpretsthe
number would givea wrong result.
From the above discussion, it may appear that our use of course identi?ers
suchas“CS-101”,where“CS”indicatestheComputerSciencedepartment,means
thatthedomainofcourseidenti?ersisnotatomic.Suchadomainisnotatomicas
farashumansusingthesystemareconcerned.However,thedatabaseapplication
stilltreatsthedomainasatomic,aslongasitdoesnotattempttosplittheidenti?er
and interpret parts of the identi?er as a department abbreviation. The course
schema stores the department name as a separate attribute, and the database
applicationcanusethisattributevalueto?ndthedepartmentofacourse,instead
8.3 Decomposition Using Functional Dependencies 329
of interpretingparticular characters of the course identi?er. Thus, our university
schema can be consideredto be in ?rst normal form.
Theuseofset-valuedattributescanleadtodesignswithredundantstorageof
data, which in turn can result in inconsistencies. For instance, instead of having
therelationshipbetweeninstructorsandsectionsbeingrepresentedasaseparate
relationteaches,adatabasedesignermaybetemptedtostoreasetofcoursesection
identi?erswitheachinstructorandasetofinstructoridenti?erswitheachsection.
(Theprimarykeysofsectionandinstructorareusedasidenti?ers.)Wheneverdata
pertaining to which instructor teaches which section is changed, the update has
to be performed at two places: in the set of instructors for the section, and the
set of sections for the instructor. Failure to perform both updates can leave the
database in an inconsistent state. Keeping only one of these sets, that either the
set of instructors of a section, or the set of sections of an instructor, would avoid
repeatedinformation;howeverkeepingonlyoneofthesewouldcomplicatesome
queries,and it is unclear which ofthe two toretain.
Sometypesofnonatomicvaluescanbeuseful,althoughtheyshouldbeused
with care. For example, composite-valued attributes are often useful, and set-
valuedattributesarealsousefulinmanycases,whichiswhybotharesupported
in the E-R model. In many domains where entities have a complex structure,
forcing a ?rst normal form representation represents an unnecessary burden on
the application programmer, who has to write code to convert data into atomic
form. There is also the runtime overhead of converting data back and forth from
the atomic form. Support for nonatomic values can thus be very useful in such
domains.Infact,moderndatabasesystemsdosupportmanytypesofnonatomic
values,asweshallseeinChapter22.However,inthischapterwerestrictourselves
to relationsin?rst normal form and, thus, all domainsare atomic.
8.3 DecompositionUsingFunctionalDependencies
InSection8.1,wenotedthatthereisaformalmethodologyforevaluatingwhether
arelationalschemashouldbedecomposed.Thismethodologyisbaseduponthe
concepts of keysand functional dependencies.
In discussing algorithms for relational database design, we shall need to
talk about arbitrary relations and their schema, rather than talking only about
examples. Recalling our introduction to the relational model in Chapter 2, we
summarize our notation here.
• In general, we use Greek letters for sets of attributes (for example,   ). We
use a lowercase Roman letter followed by an uppercase Roman letter in
parentheses to refer to a relation schema (for example, r(R)). We use the
notation r(R) to show that the schema is for relation r,withR denoting the
set of attributes, but at times simplify our notation to use just R when the
relationname doesnot matterto us.
Ofcourse,arelationschemaisasetofattributes,butnotallsetsofattributes
are schemas. When we use a lowercase Greek letter,we are referringto a set
330 Chapter 8 Relational Database Design
of attributes that may or may not be a schema. A Roman letteris used when
we wish toindicate that the setof attributesisde?nitelyaschema.
• Whenasetofattributesisasuperkey,wedenoteitby K.Asuperkeypertains
to a speci?c relation schema, so we use the terminology “K is a superkey of
r(R).”
• We use a lowercase name for relations. In our examples, these names are
intendedtobe realistic(forexample,instructor), whileinour de?nitionsand
algorithms, we use singleletters,liker.
• Arelation,ofcourse,hasaparticularvalueatanygiventime;werefertothat
as an instance and use the term “instance of r”. When it is clear that we are
talkingaboutaninstance,wemayusesimplytherelationname(forexample,
r).
8.3.1 KeysandFunctional Dependencies
A database models a set of entities and relationshipsin the real world. There are
usuallyavarietyofconstraints(rules)onthedataintherealworld.Forexample,
some of the constraints that are expectedtohold ina universitydatabase are:
1. Studentsand instructors are uniquelyidenti?edby their ID.
2. Each studentand instructor has only one name.
3. Each instructor and student is (primarily)associated with only one depart-
ment.
1
4. Eachdepartmenthasonlyonevalueforitsbudget,andonlyoneassociated
building.
Aninstanceofarelationthatsatis?esallsuchreal-worldconstraintsiscalled
a legalinstance ofthe relation;a legalinstance ofadatabase is one whereall the
relationinstances are legal instances.
Some of the most commonly used types of real-world constraints can be
represented formally as keys (superkeys, candidate keys and primary keys), or
as functional dependencies,which we de?ne below.
In Section 2.3, we de?ned the notion of a superkey as a set of one or more
attributes that, taken collectively, allows us to identify uniquely a tuple in the
relation. We restate that de?nition here as follows: Letr(R)bearelationschema.
AsubsetK of Risasuperkeyofr(R)if,inany legalinstance ofr(R), forall pairs
t
1
and t
2
of tuples in the instance of r if t
1
nullt
2
,thent
1
[K] nullt
2
[K]. That is,
no two tuples in any legal instance of relationr(R) may have the same value on
1
An instructor or a student can be associated with more than one department, for example as an adjunct faculty, or
as a minor department. Our simpli?ed university schema models only the primary department associated with each
instructor orstudent. A real university schema wouldcapture secondary associations in otherrelations.
8.3 Decomposition Using Functional Dependencies 331
attribute set K. Clearly, if no two tuples in r have the same value on K,thena
K-value uniquelyidenti?esatuple inr.
Whereas a superkey is a set of attributes that uniquely identi?es an entire
tuple, a functional dependency allows us to express constraints that uniquely
identify the values of certain attributes. Consider a relation schemar(R), and let
  ?R and  ?R.
• Given an instance of r(R), we say that the instance satis?es the functional
dependency  ?  ifforallpairsoftuplest
1
andt
2
inthe instance such that
t
1
[  ] = t
2
[  ], it isalso the case thatt
1
[  ] = t
2
[  ].
• We say that the functional dependency   ?   holds on schema r(R)if,in
everylegalinstance ofr(R) it satis?es the functional dependency.
Usingthefunctional-dependencynotation,wesaythatKisasuperkeyofr(R)if
thefunctional dependencyK ?Rholdsonr(R).Inotherwords,K isasuperkey
if, for every legal instance of r(R), for every pair of tuples t
1
and t
2
from the
instance, whenever t
1
[K] = t
2
[K], it is also the case that t
1
[R] = t
2
[R](thatis,
t
1
= t
2
).
2
Functional dependencies allow us to express constraints that we cannot ex-
presswith superkeys.In Section8.1.2, we consideredthe schema:
inst dept (ID,name,salary,dept name,building, budget)
in which the functional dependency dept name ? budget holds because for each
department(identi?edbydept name)there isa unique budget amount.
Wedenotethefactthatthepairofattributes(ID,dept name)formsasuperkey
forinst dept by writing:
ID,dept name ?name,salary, building,budget
We shall use functional dependenciesin two ways:
1. To test instances of relations to see whether they satisfy a given set F of
functional dependencies.
2. Tospecifyconstraintsonthesetoflegalrelations.Weshallthusconcernour-
selveswithonlythoserelationinstancesthatsatisfyagivensetoffunctional
dependencies.Ifwewishtoconstrainourselvestorelationsonschemar(R)
that satisfya setFof functional dependencies,we saythatF holdsonr(R).
LetusconsidertheinstanceofrelationrofFigure8.4,toseewhichfunctional
dependenciesare satis?ed.ObservethatA ?C is satis?ed.There are two tuples
2
Note that we assume here that relations are sets.SQL deals with multisets, and a primary key declaration inSQL for
asetofattributes K requiresnotonlythatt
1
= t
2
ift
1
[K] = t
2
[K], but also that therebe no duplicatetuples.SQL also
requires that attributes in the set K cannot be assigned anull value.
332 Chapter 8 Relational Database Design
A B C D
a
1
b
1
c
1
d
1
a
1
b
2
c
1
d
2
a
2
b
2
c
2
d
2
a
2
b
3
c
2
d
3
a
3
b
3
c
2
d
4
Figure8.4 Sample instance of relation r.
that have an A value of a
1
. These tuples have the same C value—namely, c
1
.
Similarly, the two tuples with an A value of a
2
have the same C value, c
2
.There
are no other pairs of distinct tuples that have the same A value. The functional
dependency C ? A is not satis?ed, however. To see that it is not, consider the
tuplest
1
=(a
2
,b
3
,c
2
,d
3
)andt
2
=(a
3
,b
3
,c
2
,d
4
).Thesetwotupleshavethesame
C values, c
2
, but they have different A values, a
2
and a
3
, respectively. Thus, we
have found a pairof tuplest
1
andt
2
such thatt
1
[C] = t
2
[C], butt
1
[A] nullt
2
[A].
Some functional dependencies are said to be trivial because they are satis-
?ed by all relations. For example, A ? A is satis?ed by all relations involving
attributeA.Readingthede?nitionoffunctionaldependencyliterally,weseethat,
for all tuples t
1
and t
2
such that t
1
[A] = t
2
[A], it is the case that t
1
[A] = t
2
[A].
Similarly, AB ? Ais satis?ed by all relations involving attribute A. In general,
a functional dependencyof the form  ?  is trivialif  ?  .
It isimportantto realizethat an instance of arelationmaysatisfysome func-
tionaldependenciesthatarenotrequiredtoholdontherelation’sschema.Inthe
instanceoftheclassroomrelationofFigure8.5,weseethatroom number ?capacity
issatis?ed.However,webelievethat,intherealworld,twoclassroomsindiffer-
ent buildings can have the same room number but with different room capacity.
Thus, it is possible, at some time, to have an instance of the classroom relation
in which room number ? capacity is not satis?ed. So, we would not include room
number ?capacityinthesetoffunctionaldependenciesthatholdontheschema
for theclassroom relation.However,we would expectthe functional dependency
building, room number ?capacity to hold on theclassroom schema.
Giventhatasetoffunctionaldependencies F holdsonarelationr(R),itmay
be possible to infer that certain other functional dependenciesmust also hold on
building room number capacity
Packard 101 500
Painter 514 10
Taylor 3128 70
Watson 100 30
Watson 120 50
Figure8.5 An instance of the classroom relation.
8.3 Decomposition Using Functional Dependencies 333
the relation. For example, given a schemar(A,B,C), if functional dependencies
A ? B and B ? C,holdonr, we can infer the functional dependency A ? C
must also hold on r. This is because, given any value of A there can be only
one corresponding value for B, and for that value of B, there can only be one
corresponding value for C. We study later, in Section 8.4.1, how to make such
inferences.
We will use the notation F
+
todenote the closure of the set F,thatis,theset
of all functional dependencies that can be inferred given the set F. Clearly F
+
contains all of the functional dependenciesin F.
8.3.2 Boyce–CoddNormalForm
One of the more desirable normal forms that we can obtain is Boyce–Codd
normal form (BCNF). It eliminates all redundancy that can be discovered based
on functional dependencies, though, as we shall see in Section 8.6, there may be
othertypesofredundancyremaining.ArelationschemaRisinBCNFwithrespect
to a set F of functional dependencies if, for all functional dependencies in F
+
of
the form  ?   ,where  ?R and  ?R, at least one ofthe following holds:
•   ?   is a trivialfunctional dependency(that is,  ?   ).
•   isa superkeyfor schemaR.
A database design is in BCNF if each member of the set of relation schemas that
constitutesthe designis in BCNF.
We havealreadyseeninSection8.1an exampleofarelationalschema thatis
not in BCNF:
inst dept (ID,name,salary,dept name,building, budget)
The functional dependency dept name ? budget holds on inst dept,butdept name
isnotasuperkey(because,adepartmentmayhaveanumberofdifferentinstruc-
tors). In Section 8.1.2, we saw that the decomposition of inst dept into instructor
and department is a better design. The instructor schema is in BCNF.Allofthe
nontrivial functional dependenciesthat hold, such as:
ID ?name,dept name,salary
include IDontheleftsideofthearrow,and IDisasuperkey(actually,inthiscase,
the primary key) for instructor. (In other words, there is no nontrivial functional
dependency with any combination of name,dept name,andsalary,withoutID,on
the side.)Thus,instructor isin BCNF.
Similarly,thedepartmentschemaisin BCNFbecauseallofthenontrivialfunc-
tional dependenciesthat hold, such as:
dept name ?building,budget
334 Chapter 8 Relational Database Design
include dept name on the left side of the arrow, and dept name is a superkey (and
the primarykey)fordepartment.Thus,departmentis in BCNF.
We now state a general rule for decomposing that are not in BCNF.LetR
beaschemathatisnotinBCNF. Then there is at least one nontrivial functional
dependency  ?   suchthat  isnotasuperkeyfor R.WereplaceRinourdesign
with two schemas:
• (  ?  )
• (R ?(  ?  ))
Inthecaseofinst dept above,  = dept name,  = {building, budget},andinst dept
isreplacedby
• (  ?  )=(dept name,building,budget)
• (R ?(  ?  ))= (ID,name,dept name,salary)
In this example, it turns out that   ?   =   .W eneedtostatetheruleaswe
did soas to dealcorrectlywith functional dependenciesthat haveattributesthat
appearonbothsidesofthearrow.Thetechnicalreasonsforthisarecoveredlater
in Section8.5.1.
Whenwedecomposeaschemathatisnotin BCNF,itmaybethatoneormore
of the resulting schemas are not in BCNF. In such cases, further decomposition is
required,theeventualresultof which isa setof BCNF schemas.
8.3.3 BCNFandDependency Preservation
Wehaveseenseveralwaysinwhichtoexpressdatabaseconsistencyconstraints:
primary-key constraints, functional dependencies, check constraints, assertions,
and triggers. Testing these constraints each time the database is updated can be
costly and, therefore, it is useful to design the database in a way that constraints
can be tested ef?ciently. In particular, if testing a functional dependency can be
donebyconsideringjustonerelation,thenthecostoftestingthisconstraintislow.
We shall see that, in some cases, decomposition into BCNF can prevent ef?cient
testingof certainfunctional dependencies.
To illustrate this, suppose that we make a small change to our university
organization. In the design of Figure 7.15, a student may have only one advisor.
This follows from the relationship set advisor being many-to-one from student to
advisor.The“small” change we shall make is that an instructor can be associated
with only a single department and a student may have more than one advisor,
butatmostonefromagivendepartment.
3
One way to implement this change using the E-R design is by replacing the
advisor relationship set with a ternary relationship set, dept advisor, involving
entity sets instructor, student,anddepartment that is many-to-one from the pair
3
Such an arrangement makes sense forstudents with a double major.
8.3 Decomposition Using Functional Dependencies 335
dept_name
building
budget
department
dept_advisor
instructor
ID
name
salary
student
ID
name
tot_cred
Figure8.6 The deptadvisor relationship set.
{student,instructor}todepartmentasshowninFigure8.6.TheE-Rdiagramspeci?es
the constraint that “a student may have more than one advisor, but at most one
correspondingto a givendepartment”.
With this new E-R diagram, the schemas for the instructor, department,and
studentareunchanged. However,theschema derivedfromdept advisor is now:
dept advisor (s ID,i ID,dept name)
Although not speci?ed in the E-R diagram, suppose we have the additional
constraint that “an instructor can act as advisorfor only a single department.”
Then, the following functional dependencieshold ondept advisor:
i ID ?dept name
s ID,dept name ?i ID
The?rstfunctionaldependencyfollowsfromourrequirementthat“aninstructor
canactasanadvisorforonlyonedepartment.”Thesecondfunctionaldependency
followsfromourrequirementthat “astudentmayhaveatmostoneadvisorfora
givendepartment.”
Notice that with this design, we are forced to repeat the department name
once foreachtimeaninstructorparticipatesinadept advisor relationship.Wesee
thatdept advisor is not in BCNF because i ID is not a superkey. Following our rule
for BCNF decomposition,we get:
(s ID,i ID)
(i ID,dept name)
Both the above schemas are BCNF. (In fact, you can verify that any schema with
only twoattributesisin BCNFby de?nition.)Notehowever,that inour BCNF de-
sign,thereisnoschemathatincludesalltheattributesappearinginthefunctional
dependencys ID,dept name ?i ID.
336 Chapter 8 Relational Database Design
Because our design makes it computationally hard to enforce this functional
dependency, we say our design is not dependency preserving.
4
Because depen-
dency preservation is usually considered desirable, we consider another normal
form,weakerthanBCNF,thatwillallowustopreservedependencies.Thatnormal
form iscalled third normal form.
5
8.3.4 ThirdNormalForm
BCNF requires that all nontrivial dependencies be of the form   ?   ,where  is
a superkey. Third normal form (3NF) relaxes this constraint slightly by allowing
certainnontrivialfunctionaldependencieswhoseleftsideisnotasuperkey.Before
we de?ne 3NF, we recall that a candidate key is a minimal superkey—that is, a
superkeyno propersubsetof which is alsoa superkey.
ArelationschemaRisinthirdnormalformwithrespecttoasetFoffunctional
dependenciesif,for allfunctional dependenciesin F
+
of the form  ?   ,where
  ? Rand  ? R, at least one of the following holds:
•   ?   isa trivialfunctional dependency.
•   isa superkeyforR.
• Each attributeA in  ?  is contained ina candidate key forR.
Notethatthethirdconditionabovedoesnotsaythatasinglecandidatekeymust
contain all the attributes in  ?  ; each attribute A in  ?  may be contained in
adifferent candidate key.
The?rsttwoalternativesarethesameasthetwoalternativesinthede?nition
of BCNF. The thirdalternativeofthe 3NFde?nitionseemsratherunintuitive,and
itisnotobviouswhyitisuseful.Itrepresents,insomesense,aminimalrelaxation
of the BCNF conditions that helps ensure that every schema has a dependency-
preserving decomposition into 3NF. Its purpose will become more clear later,
when we studydecompositioninto 3NF.
Observe that any schema that satis?es BCNF also satis?es 3NF, since each of
its functional dependencies would satisfy one of the ?rst two alternatives. BCNF
istherefore amore restrictivenormal form than is 3NF.
The de?nition of 3NF allows certain functional dependencies that are not
allowedin BCNF.Adependency  ?   thatsatis?esonlythe thirdalternativeof
the 3NFde?nition is not allowed in BCNF, but is allowedin 3NF.
6
Now, let us again consider the dept advisor relationship set, which has the
following functional dependencies:
4
Technically, it is possible that a dependency whose attributes do not all appear in any one schema is still implicitly
enforced,becauseofthepresenceofotherdependenciesthatimplyitlogically.Weaddressthatcaselater,inSection8.4.5.
5
Youmayhavenotedthatweskippedsecondnormalform.Itisofhistoricalsigni?canceonlyandisnotusedinpractice.
6
These dependencies are examples of transitive dependencies (see Practice Exercise 8.16). The original de?nition of
3NF was interms oftransitive dependencies.The de?nitionwe use isequivalent but easierto understand.
8.3 Decomposition Using Functional Dependencies 337
i ID ?dept name
s ID,dept name ?i ID
In Section 8.3.3 we argued that the functional dependency “i ID ? dept name”
causedthedept advisorschemanottobein BCNF.Notethathere  =i ID,  =dept
name,and  ?  =dept name.Sincethefunctionaldependencys ID,dept name ?
i ID holds on dept advisor, the attribute dept name is contained in a candidate key
and,therefore,dept advisor isin 3NF.
We have seen the trade-off that must be made between BCNF and 3NF when
there is no dependency-preserving BCNF design. These trade-offs are described
in more detailin Section8.5.4.
8.3.5 HigherNormalForms
Using functional dependencies to decompose schemas may not be suf?cient to
avoid unnecessary repetition of information in certain cases. Consider a slight
variationintheinstructor entity-set de?nition in which we record with each
instructor a set of children’s names and a set of phone numbers. The phone
numbers may be shared by multiple people. Thus, phone number and child name
wouldbemultivaluedattributesand,followingourrulesforgeneratingschemas
froman E-Rdesign,wewouldhavetwoschemas,oneforeachofthemultivalued
attributes,phone numberandchild name:
(ID,child name)
(ID,phone number)
Ifwe were tocombine theseschemas to get
(ID,child name,phone number)
we would ?nd the result to be in BCNF because only nontrivial functional de-
pendencies hold. As a result we might think that such a combination is a good
idea. However, such a combination is a bad idea, as we can see by consider-
ing the example of an instructor with two children and two phone numbers.
For example, let the instructor with ID 99999 have two children named “David”
and “William” and two phone numbers, 512-555-1234 and 512-555-4321. In the
combined schema, we must repeatthe phone numbers once foreach dependent:
(99999, David,512-555-1234)
(99999, David,512-555-4321)
(99999, William, 512-555-1234)
(99999, William, 512-555-4321)
If we did not repeat the phone numbers, and stored only the ?rst and last
tuple,wewouldhaverecordedthedependentnamesandthephonenumbers,but
338 Chapter 8 Relational Database Design
theresultanttupleswouldimplythatDavidcorrespondedto512-555-1234,while
William correspondedto512-555-4321. As we know, this would be incorrect.
Becausenormalformsbasedonfunctional dependenciesarenotsuf?cientto
deal with situations like this, other dependencies and normal forms have been
de?ned.We covertheseinSections 8.6and 8.7.
8.4 Functional-Dependency Theory
We have seenin our examplesthat itisuseful tobe able toreason systematically
about functional dependencies as part of a process of testing schemas for BCNF
or 3NF.
8.4.1 ClosureofaSetofFunctional Dependencies
We shall see that, given a set F of functional dependencies on a schema, we
can prove that certain other functional dependencies also hold on the schema.
We say that such functional dependencies are “logically implied” by F.When
testingfornormalforms,itisnotsuf?cienttoconsiderthegivensetoffunctional
dependencies; rather, we need to consider all functional dependencies that hold
on the schema.
Moreformally,givenarelationalschemar(R),afunctionaldependency f on
RislogicallyimpliedbyasetoffunctionaldependenciesF onr ifeveryinstance
ofr(R) that satis?es F alsosatis?es f.
Suppose we are given a relation schema r(A, B, C, G, H, I)andthesetof
functional dependencies:
A ?B
A ?C
CG ?H
CG ?I
B ?H
The functional dependency:
A ?H
is logically implied. That is, we can show that, whenever a relation satis?es our
givensetoffunctionaldependencies,A?Hmustalsobesatis?edbythatrelation.
Suppose thatt
1
andt
2
are tuplessuch that:
t
1
[A] = t
2
[A]
SincewearegiventhatA ?B,itfollowsfromthede?nitionoffunctionaldepen-
dency that:
t
1
[B] = t
2
[B]
8.4 Functional-Dependency Theory 339
Then, since we are given that B ? H, it follows from the de?nition of functional
dependencythat:
t
1
[H] = t
2
[H]
Therefore, we have shown that, whenever t
1
and t
2
are tuples such that t
1
[A] =
t
2
[A], it must be thatt
1
[H] = t
2
[H]. But that is exactlythe de?nition of A ? H.
Let F be a set of functional dependencies. The closure of F, denoted by F
+
,
is the set of all functional dependencies logically implied by F. Given F,wecan
compute F
+
directly from the formal de?nition of functional dependency. If F
were large, this process would be lengthy and dif?cult. Such a computation of
F
+
requiresargumentsofthetypejustusedtoshowthatA ?H isintheclosure
of our examplesetof dependencies.
Axioms,orrulesofinference,provideasimplertechniqueforreasoningabout
functional dependencies. In the rules that follow, we use Greek letters (  ,   ,   ,
...)forsetsofattributes,anduppercaseRomanlettersfromthebeginningofthe
alphabet forindividualattributes. We use    todenote  ?   .
We can use the following three rules to ?nd logically implied functional
dependencies. By applying these rules repeatedly, we can ?nd all of F
+
, given F.
This collection of rulesiscalledArmstrong’s axioms in honor of the person who
?rst proposedit.
• Re?exivityrule.If  isa setof attributesand  ?   ,then  ?  holds.
• Augmentation rule.If  ?   holds and   is a set of attributes, then
   ?    holds.
• Transitivityrule.If  ?  holds and  ?  holds, then  ?  holds.
Armstrong’s axioms are sound, because they do not generate any incorrect
functional dependencies. They are complete, because, for a given set F of func-
tional dependencies, they allow us to generate all F
+
. The bibliographical notes
providereferencesforproofs of soundnessand completeness.
AlthoughArmstrong’saxiomsarecomplete,itistiresometousethemdirectly
for the computation of F
+
. To simplify matters further, we list additional rules.
It is possible to use Armstrong’s axioms to prove that these rules are sound (see
Practice Exercises8.4and 8.5and Exercise8.26).
• Unionrule.If  ?  holds and  ?  holds, then  ?   holds.
• Decomposition rule.If  ?   holds, then  ?  holds and  ?  holds.
• Pseudotransitivity rule.If  ?   holds and    ?   holds, then    ?   holds.
Letusapplyourrulestotheexampleofschema R = (A, B,C,G, H, I)andthe
set F of functional dependencies {A ? B, A ? C, CG ? H, CG ? I, B ? H}.
Welistseveralmembersof F
+
here:
340 Chapter 8 Relational Database Design
• A ? H.SinceA ? B and B ? H hold, we apply the transitivity rule.
Observe that it was much easier to use Armstrong’s axioms to show that
A ? H holds than it was to argue directly from the de?nitions, as we did
earlierinthissection.
• CG ? HI.SinceCG ? H and CG ? I, the union rule implies that CG ?
HI.
• AG ? I.SinceA ? C andCG ? I,thepseudotransitivityruleimpliesthat
AG ? I holds.
Another way of ?nding that AG ? I holds is as follows: We use the
augmentation rule on A ? C to infer AG ? CG. Applying the transitivity
ruleto this dependencyandCG ? I,weinferAG ? I.
Figure 8.7 shows a procedure that demonstrates formally how to use Arm-
strong’saxiomstocompute F
+
.Inthisprocedure,whenafunctionaldependency
is added to F
+
, it may be already present, and in that case there is no change to
F
+
. We shall see an alternativeway of computing F
+
in Section8.4.2.
The left-hand and right-hand sidesof a functional dependencyare both sub-
sets of R.Sinceasetofsizen has 2
n
subsets, there are a total of 2
n
× 2
n
= 2
2n
possiblefunctional dependencies,wherenisthe number of attributesin R.Each
iterationoftherepeatloopoftheprocedure,exceptthelastiteration,addsatleast
onefunctionaldependencytoF
+
.Thus,theprocedureisguaranteedtoterminate.
8.4.2 ClosureofAttributeSets
We say that an attribute B is functionally determined by   if   ? B.T otest
whetheraset  isasuperkey,wemustdeviseanalgorithmforcomputingtheset
of attributes functionally determined by   . One way of doing this is to compute
F
+
, take all functional dependencies with   as the left-hand side, and take the
unionoftheright-handsidesofallsuchdependencies.However,doingsocanbe
expensive,since F
+
can be large.
An ef?cient algorithm for computing the set of attributes functionally deter-
mined by   is useful not only for testing whether   is a superkey, but also for
severalothertasks, as we shall see laterin this section.
F
+
= F
repeat
for each functional dependency f in F
+
applyre?exivityand augmentation ruleson f
add the resultingfunctional dependenciesto F
+
for each pairof functional dependencies f
1
and f
2
in F
+
if f
1
and f
2
can be combined usingtransitivity
add the resultingfunctional dependencyto F
+
until F
+
does not change any further
Figure8.7 A procedure to compute F
+
.
8.4 Functional-Dependency Theory 341
result :=   ;
repeat
for each functional dependency  ?  inFdo
begin
if  ? result then result := result ?  ;
end
until(result does not change)
Figure8.8 An algorithm to compute   +
, the closure of   under F.
Let   be a set of attributes. We call the set of all attributes functionally de-
termined by   under a set F of functional dependencies the closure of   under
F;wedenoteitby  +
. Figure 8.8 shows an algorithm, written in pseudocode,
to compute   +
.TheinputisasetF of functional dependencies and the set   of
attributes.The outputis storedin the variableresult.
Toillustratehowthealgorithmworks,weshalluseittocompute(AG)
+
with
the functional dependencies de?ned in Section 8.4.1. We start with result = AG.
The?rsttimethatweexecutetherepeatlooptotesteachfunctionaldependency,
we ?nd that:
• A ? B causes us to include B in result. To see this fact, we observe that
A ? B isinF, A ?result (which isAG), soresult:=result ?B.
• A ?C causesresult to becomeABCG.
• CG ?H causesresult tobecomeABCGH.
• CG ?I causesresult to becomeABCGHI.
Thesecondtimethatweexecutetherepeat loop, no new attributes are addedto
result, and the algorithm terminates.
Let us see why the algorithm of Figure 8.8 is correct. The ?rst step is correct,
since  ?  alwaysholds(bythere?exivityrule).Weclaimthat,foranysubset  ofresult,  ?  .Sincewestarttherepeatloopwith  ?resultbeingtrue,wecan
add  toresultonlyif  ?resultand  ?  .Butthenresult ?  bythere?exivity
rule,so  ?  bytransitivity.Anotherapplicationoftransitivityshowsthat  ?
  (using   ?   and   ?   ). The union rule implies that   ? result ?   ,so  functionally determines any new result generated in the repeat loop. Thus, any
attribute returnedby the algorithm isin  +
.
Itiseasytoseethat the algorithm?ndsall of  +
.Ifthereisanattributein  +
that is not yet in result at any point during the execution, then there must be a
functionaldependency  ?  forwhich  ?result,andatleastoneattributein  is not inresult. When the algorithm terminates,all such functional dependencies
have beenprocessed,and the attributes in  addedtoresult;wecanthusbesure
that all attributesin  +
are inresult.
342 Chapter 8 Relational Database Design
It turns out that, in the worst case, this algorithm may take an amount of
timequadraticinthesizeofF.Thereisafaster(althoughslightlymorecomplex)
algorithm that runs in time linear in the size of F; that algorithm is presented as
part of Practice Exercise8.8.
There are severalusesofthe attribute closure algorithm:
• To test if  isasuperkey,wecompute  +
,andcheckif  +
containsallattributes
in R.
• We can check if a functional dependency   ?   holds (or, in other words,
is in F
+
), by checking if   ?   +
. That is, we compute   +
by using attribute
closure, and then check if it contains  . This test is particularly useful, as we
shall seelater in thischapter.
• It gives us an alternative way to compute F
+
:Foreach  ? R, we ?nd the
closure  +
,andforeachS ?   +
,weoutputafunctionaldependency  ? S.
8.4.3 Canonical Cover
Suppose that we have a set of functional dependencies F on a relation schema.
Whenever a user performs an update on the relation, the database system must
ensure that the update does not violate any functional dependencies, that is, all
the functional dependenciesin F are satis?edin the new database state.
The system must roll back the update if it violates any functional dependen-
ciesinthe set F.
Wecanreducetheeffortspentincheckingforviolationsbytestingasimpli?ed
set of functional dependencies that hasthesameclosureasthegivenset.Any
database that satis?es the simpli?ed set of functional dependenciesalso satis?es
theoriginalset,andviceversa,sincethetwosetshavethesameclosure.However,
the simpli?ed set is easier to test. We shall see how the simpli?ed set can be
constructed in a moment. First, we need some de?nitions.
An attribute of a functional dependency is said to be extraneous if we can
remove it without changing the closure of the set of functional dependencies.
The formal de?nition of extraneous attributes is as follows: Consider a set F of
functional dependenciesand the functional dependency  ?  inF.
• Attribute A is extraneous in   if A ?   ,andF logically implies (F ?{  ?
  }) ?{ (  ? A) ?   }.
• AttributeAisextraneousin  if A ?   ,andthesetoffunctionaldependencies
(F ? {  ?  })?{  ? (  ? A)} logicallyimpliesF.
For example, suppose we have the functional dependencies AB ? C and
A ? C in F.Then, B isextraneousin AB ? C.Asanotherexample,supposewe
havethefunctional dependencies AB ?CDand A ? C in F.ThenC would be
extraneousin the right-hand sideof AB ?CD.
Beware of the direction of the implications when using the de?nition of ex-
traneous attributes: If you exchange the left-hand side with the right-hand side,
8.4 Functional-Dependency Theory 343
F
c
= F
repeat
Usetheunion ruleto replaceany dependenciesin F
c
of the form
  1
?   1
and  1
?   2
with  1
?   1
  2
.
Find a functional dependency  ?   in F
c
with an extraneous
attribute eitherin  or in  .
/*Note:the testfor extraneousattributesisdone using F
c
,notF */
Ifan extraneousattribute isfound, deleteitfrom  ?   in F
c
.
until(F
c
does not change)
Figure8.9 Computing canonical cover.
the implication will always hold. That is, (F ?{   ?   }) ?{ (  ? A) ?   } al-
ways logically implies F,andalsoF always logically implies (F ? {  ?   }) ?
{  ? (  ? A)}.
Here is how we can test ef?ciently if an attribute is extraneous. Let R be the
relation schema, and let F be the given set of functional dependencies that hold
on R. Consideran attribute Aina dependency  ?   .
• If A ?   ,tocheckifAisextraneous, considerthe set
F
null
= (F ?{  ?  })?{  ? (  ? A)}
andcheckif  ? Acanbeinferredfrom F
null
.Todoso,compute  +
(theclosure
of  )underF
null
;if  +
includes A,thenAisextraneous in  .
• If A ?   ,tocheckifAis extraneous, let  =   ?{A},andcheckif  ?   can
be inferred from F.T odoso,compute  +
(the closure of   )underF;if  +
includesallattributesin  ,thenAisextraneous in  .
For example, suppose F contains AB ?CD, A ? E,andE ? C.Tocheck
if C is extraneous in AB ? CD, we compute the attribute closure of AB under
F
null
={ AB ? D, A ? E,andE ? C}.TheclosureisABCDE, which includes
CD,soweinferthatC isextraneous.
A canonical cover F
c
for F is a set of dependencies such that F logically
implies all dependencies in F
c
,andF
c
logically implies all dependencies in F.
Furthermore, F
c
musthave the following properties:
• Nofunctional dependencyin F
c
contains an extraneousattribute.
• Each left side of a functional dependency in F
c
is unique. That is, there are
no two dependencies  1
?   1
and  2
?   2
in F
c
such that  1
=   2
.
A canonical cover for a set of functional dependenciesF can be computed as
depictedinFigure8.9.Itisimportanttonotethatwhencheckingifanattributeis
extraneous, the check uses the dependencies in the current value of F
c
,andnot
the dependencies in F. If a functional dependency contains only one attribute
344 Chapter 8 Relational Database Design
in its right-hand side, for example A ? C, and that attribute is found to be
extraneous, we would get a functional dependency with an empty right-hand
side.Suchfunctional dependenciesshould bedeleted.
The canonical cover of F, F
c
, can be shown to have the same closure as F;
hence, testing whether F
c
is satis?ed is equivalent to testing whether F is satis-
?ed. However, F
c
is minimal in a certain sense—it does not contain extraneous
attributes, and it combines functional dependencies with the same left side. It is
cheaperto test F
c
than itisto test F itself.
ConsiderthefollowingsetFoffunctionaldependenciesonschema(A,B,C):
A ?BC
B ?C
A ?B
AB ?C
Let uscompute the canonical coverforF.
• Therearetwofunctionaldependencieswiththesamesetofattributesonthe
leftsideof the arrow:
A ?BC
A ?B
We combine thesefunctional dependenciesintoA ?BC.
• Ais extraneous in AB ? C because F logically implies (F ?{AB ? C}) ?
{B ? C}. This assertion is true because B ? C is already in our set of
functional dependencies.
• C is extraneous inA ?BC,sinceA ?BC is logically implied byA ?B and
B ?C.
Thus, our canonical cover is:
A ?B
B ?C
Given a set F of functional dependencies, it may be that an entire functional
dependencyinthesetisextraneous,inthesensethatdroppingitdoesnotchange
the closure of F. We can show that a canonical cover F
c
of F contains no such
extraneousfunctionaldependency.Supposethat,tothecontrary,thereweresuch
an extraneous functional dependency in F
c
. The right-side attributes of the de-
pendency would then be extraneous, which is not possible by the de?nition of
canonical covers.
A canonical cover might not be unique. For instance, consider the set of
functional dependencies F ={ A ? BC, B ? AC,andC ? AB}. If we apply
8.4 Functional-Dependency Theory 345
the extraneity test to A ? BC, we ?nd that both B and C are extraneous under
F.However,itisincorrecttodeleteboth!Thealgorithmfor?ndingthecanonical
cover picksone of the two, and deletesit. Then,
1. IfC isdeleted,wegettheset F
null
={A ? B,B ? AC,andC ? AB}.Now,B
is not extraneousinthe sideof A ? B under F
null
. Continuingthealgorithm,
we ?nd Aand B are extraneous in the right-hand side of C ? AB,leading
to two canonical covers
F
c
={A ? B, B ? C,C ? A}
F
c
={A ? B, B ? AC,C ? B}.
2. If B is deleted,wegetthe set {A ? C, B ? AC,andC ? AB}.Thiscaseis
symmetricaltothe previouscase,leadingtothe canonical covers
F
c
={A ? C,C ? B,andB ? A}
F
c
={A ? C, B ? C,andC ? AB}.
Asan exercise,can you ?nd one more canonical cover for F?
8.4.4 Lossless Decomposition
Let r(R) be a relation schema, and let F be a set of functional dependencies on
r(R).Let R
1
and R
2
formadecompositionofR. We saythat the decompositionis
alosslessdecompositionifthereisnolossofinformationbyreplacingr(R)with
tworelationschemasr
1
(R
1
)andr
2
(R
2
).Moreprecisely,wesaythedecomposition
islosslessif,foralllegaldatabaseinstances(thatis,databaseinstancesthatsatisfy
the speci?ed functional dependencies and other constraints), relation r contains
the same setof tuplesasthe resultof the following SQL query:
select *
from(select R
1
fromr)
naturaljoin
(select R
2
fromr)
This isstatedmore succinctly in the relational algebraas:
null R
1
(r)  null R
2
(r) =r
In other words, if we project r onto R
1
and R
2
, and compute the natural join
of the projection results, we get back exactly r. A decomposition that is not a
lossless decomposition is called a lossy decomposition.Thetermslossless-join
decomposition and lossy-join decomposition are sometimes used in place of
losslessdecomposition and lossy decomposition.
As an example of a lossy decomposition, recall the decomposition of the
employee schema into:
346 Chapter 8 Relational Database Design
employee1 (ID,name)
employee2 (name,street,city,salary)
thatwesawearlierinSection8.1.2.AswesawinFigure8.3,theresultofemployee1
  employee2 is a superset of the original relationemployee, but the decomposition
is lossy since the join result has lost information about which employee identi-
?ers correspond to which addresses and salaries, in the case where two or more
employeeshavethesame name.
Wecanusefunctionaldependenciestoshowwhencertaindecompositionsare
lossless.LetR, R
1
, R
2
,andFbeasabove. R
1
and R
2
formalosslessdecomposition
ofR ifat least one of the following functional dependenciesisin F
+
:
• R
1
? R
2
? R
1
• R
1
? R
2
? R
2
Inotherwords,if R
1
? R
2
formsasuperkeyofeither R
1
or R
2
,thedecomposition
of Risalosslessdecomposition.Wecanuseattributeclosuretotestef?cientlyfor
superkeys,as wehaveseenearlier.
To illustratethis, considerthe schema
inst dept (ID,name,salary,dept name,building, budget)
that we decomposedinSection 8.1.2into theinstructor anddepartmentschemas:
instructor (ID,name,dept name,salary)
department (dept name,building, budget)
Consider the intersection of these two schemas, which is dept name.Weseethat
because dept name? dept name, building, budget, the lossless-decomposition rule
issatis?ed.
For the general case of decomposition of a schema into multiple schemas at
once,thetestforlosslessdecompositionismorecomplicated.Seethebibliograph-
ical notesfor referenceson the topic.
While the test for binary decomposition is clearly a suf?cient condition for
losslessdecomposition,itisanecessaryconditiononlyifallconstraintsarefunc-
tional dependencies. We shall see other types of constraints later (in particular,
a type of constraint called multivalued dependenciesdiscussedin Section 8.6.1),
thatcanensurethatadecompositionislosslessevenifnofunctionaldependencies
are present.
8.4.5 Dependency Preservation
Using the theory of functional dependencies, it is easier to characterize depen-
dency preservationthan using the ad-hoc approach we took in Section8.3.3.
8.4 Functional-Dependency Theory 347
LetFbeasetoffunctionaldependenciesonaschemaR,andletR
1
, R
2
,...,R
n
be a decomposition of R.Therestriction of F to R
i
is the set F
i
of all functional
dependenciesin F
+
that includeonly attributesof R
i
. Since all functional depen-
denciesinarestrictioninvolveattributesofonlyonerelationschema,itispossible
to testsuch a dependencyfor satisfaction by checking only one relation.
Note that the de?nition of restriction uses all dependencies in F
+
,notjust
thosein F.Forinstance,supposeF ={A ? B,B ? C},andwehaveadecompo-
sition into AC and AB.TherestrictionofF to AC includes A ? C,sinceA ? C
isin F
+
,eventhough itisnot in F.
The set of restrictions F
1
, F
2
,...,F
n
is the set of dependencies that can be
checked ef?ciently. We now must ask whether testing only the restrictions is
suf?cient. Let F
null
= F
1
? F
2
? ··· ? F
n
. F
null
is a set of functional dependencies
on schema R, but, in general, F
null
nullF. However, even if F
null
nullF, it may be that
F
null
= F
+
.Ifthelatteristrue,theneverydependencyinFislogicallyimpliedby
F
null
,and,ifweverifythatF
null
issatis?ed,wehaveveri?edthatFissatis?ed.Wesay
thatadecompositionhavingtheproperty F
null
= F
+
isadependency-preserving
decomposition.
Figure 8.10 shows an algorithm for testing dependency preservation. The
input is a set D = {R
1
, R
2
,...,R
n
} of decomposed relation schemas, and a
set F of functional dependencies. This algorithm is expensive since it requires
computationof F
+
.InsteadofapplyingthealgorithmofFigure8.10,weconsider
two alternatives.
First,notethatifeachmemberofFcanbetestedononeoftherelationsofthe
decomposition,thenthedecompositionisdependencypreserving.Thisisaneasy
waytoshowdependencypreservation;however,itdoesnotalwayswork.There
arecaseswhere,eventhoughthedecompositionisdependencypreserving,there
isadependencyinFthatcannotbetestedinanyonerelationinthedecomposition.
Thus, this alternative test can be used only as a suf?cient condition that is easy
compute F
+
;
for each schema R
i
inD do
begin
F
i
: =the restrictionof F
+
to R
i
;
end
F
null
:=?
for each restriction F
i
do
begin
F
null
= F
null
? F
i
end
compute F
null
;
if (F
null
= F
+
)then return (true)
else return(false);
Figure8.10 Testing for dependency preservation.
348 Chapter 8 Relational Database Design
tocheck; ifitfailswecannot conclude thatthedecompositionisnotdependency
preserving;instead we will have to applythe general test.
Wenowgiveasecondalternativetestfordependencypreservationthatavoids
computing F
+
. We explain the intuition behind the test after presenting the test.
The testappliesthe following procedureto each  ?   in F.
result =  repeat
for each R
i
in the decomposition
t=(result ? R
i
)
+
? R
i
result =result ?t
until(result doesnot change)
The attribute closure here is under the set of functional dependencies F.Ifresult
contains all attributes in   , then the functional dependency   ?   is preserved.
Thedecompositionisdependencypreservingifandonlyiftheprocedureshows
that all the dependenciesin F arepreserved.
Thetwokeyideasbehindtheabovetestareasfollows:
• The ?rst idea is to test each functional dependency  ?   in F to see if it is
preservedin F
null
(where F
null
isasde?nedinFigure8.10).Todoso,wecompute
the closure of   under F
null
; the dependency is preserved exactly when the
closureincludes  .Thedecompositionisdependencypreservingif(andonly
if)all the dependenciesin F arefound to be preserved.
• The second idea is to use a modi?ed form of the attribute-closure algorithm
tocomputeclosureunder F
null
,withoutactually?rstcomputing F
null
.Wewishto
avoidcomputing F
null
sincecomputingitisquiteexpensive.Notethat F
null
isthe
unionof F
i
,whereF
i
istherestrictionofF on R
i
.Thealgorithmcomputesthe
attribute closure of (result ? R
i
)withrespecttoF, intersects the closure with
R
i
, and adds the resultant set of attributes to result; this sequence of steps is
equivalent to computing the closure of result under F
i
. Repeating this step
for eachi insidethe while loopgivesthe closure ofresult under F
null
.
To understand why this modi?ed attribute-closure approach works cor-
rectly ,wenotethatforany  ? R
i
,   ?   +
is a functional dependency in
F
+
,and  ?   +
? R
i
is a functional dependency that is in F
i
,therestriction
of F
+
to R
i
. Conversely, if   ?   were in F
i
,then  would be a subset of
  +
? R
i
.
This test takes polynomial time, instead of the exponential time required to
compute F
+
.
8.5 AlgorithmsforDecomposition
Real-world database schemas are much larger than the examples that ?t in the
pagesofabook.Forthisreason,weneedalgorithmsforthegenerationofdesigns
8.5 Algorithms for Decomposition 349
that are in appropriate normal form. In this section, we present algorithms for
BCNF and 3NF.
8.5.1 BCNFDecomposition
Thede?nitionofBCNFcanbeuseddirectlytotestifarelationisinBCNF.However,
computation of F
+
can be a tedious task. We ?rst describe below simpli?ed
tests for verifying if a relation is in BCNF. If a relation is not in BCNF,itcan
be decomposed to create relations that are in BCNF. Later in this section, we
describe an algorithm to create a lossless decomposition of a relation, such that
the decompositionis in BCNF.
8.5.1.1 Testingfor BCNF
Testingofarelationschema Rtoseeifitsatis?es BCNFcanbe simpli?edinsome
cases:
• To check if a nontrivial dependency   ?   causes a violation of BCNF,
compute  +
(theattributeclosureof  ),andverifythatitincludesallattributes
of R; that is, itisa superkeyof R.
• To check if a relation schema R is in BCNF, it suf?ces to check only the
dependencies in the given set F for violation of BCNF, rather than check all
dependenciesin F
+
.
We can show that if none of the dependencies in F causes a violation of
BCNF, then none of the dependencies in F
+
will cause a violation of BCNF,
either.
Unfortunately,thelatterproceduredoesnotworkwhenarelationisdecomposed.
Thatis,itdoesnotsuf?cetouse F whenwetestarelation R
i
,inadecompositionof
R,forviolationof BCNF.Forexample,considerrelationschema R(A,B,C,D,E),
with functional dependencies F containing A ? B and BC ? D. Suppose
this were decomposed into R
1
(A,B)andR
2
(A,C,D,E). Now, neither of the
dependencies in F contains only attributes from (A,C,D,E)sowemightbe
misled into thinking R
2
satis?es BCNF. In fact, there is a dependency AC ? D
in F
+
(which can be inferred using the pseudotransitivity rule from the two
dependencies in F) that shows that R
2
is not in BCNF. Thus, we may need a
dependency that is in F
+
,butisnotinF, to show that a decomposed relation is
not in BCNF.
An alternative BCNF test is sometimes easier than computing every depen-
dencyin F
+
.Tocheckifarelation R
i
inadecompositionof RisinBCNF,weapply
this test:
• For every subset   of attributes in R
i
,checkthat  +
(the attribute closure
of  under F) eitherincludesno attributeof R
i
?  ,orincludesall attributes
of R
i
.
350 Chapter 8 Relational Database Design
result :={R};
done:= false;
compute F
+
;
while(notdone) do
if (thereis aschema R
i
inresult that is not in BCNF)
then begin
let  ?   be anontrivial functional dependencythat holds
on R
i
such that  ? R
i
isnot in F
+
,and  ?  =?;
result :=(result ? R
i
) ? (R
i
?  ) ? (  ,  );
end
elsedone :=true;
Figure8.11 BCNF decomposition algorithm.
Iftheconditionisviolatedbysomesetofattributes  in R
i
,considerthefollowing
functional dependency,which can be shown to be presentin F
+
:
  ? (  +
?  ) ? R
i
.
The abovedependencyshows that R
i
violates BCNF.
8.5.1.2 BCNF Decomposition Algorithm
We are now able to state a general method to decompose a relation schema so
as to satisfy BCNF. Figure 8.11 shows an algorithm for this task. If R is not in
BCNF,wecandecomposeR into a collection of BCNF schemas R
1
,R
2
,...,R
n
by
the algorithm. The algorithm uses dependencies that demonstrate violation of
BCNF to performthe decomposition.
The decomposition that the algorithm generates is not only in BCNF,butis
also a lossless decomposition. To see why our algorithm generates only lossless
decompositions, we note that, when we replace a schema R
i
with (R
i
?  )and
(  ,  ),the dependency  ?   holds, and (R
i
?  ) ? (  ,  ) =  .
If we did not require   ?   =? , then those attributes in   ?   would not
appearintheschema(R
i
?  )andthedependency  ?   wouldnolongerhold.
It is easy to see that our decomposition of inst dept in Section 8.3.2 would
result from applying the algorithm. The functional dependency dept name ?
building, budget satis?es the   ?  =?condition and would therefore be chosen
to decomposethe schema.
The BCNF decomposition algorithm takes time exponential in the size of the
initialschema,sincethealgorithmforcheckingifarelationinthedecomposition
satis?es BCNF can take exponential time. The bibliographical notes provide ref-
erences to an algorithm that can compute a BCNF decomposition in polynomial
time.However,thealgorithmmay“overnormalize,”thatis,decomposearelation
unnecessarily.
AsalongerexampleoftheuseoftheBCNFdecompositionalgorithm,suppose
we have a database designusingtheclass schema below:
8.5 Algorithms for Decomposition 351
class (course id,title,dept name,credits,sec id,semester,year,building,
room number,capacity, time slot id)
The setof functional dependenciesthat we requireto hold onclass are:
course id ? title,dept name,credits
building, room number ?capacity
course id,sec id,semester,year?building,room number,time slot id
A candidate keyfor thisschema is {course id,sec id,semester,year}.
We can applythe algorithm of Figure8.11 to theclass exampleas follows:
• Thefunctional dependency:
course id ? title,dept name,credits
holds, but course id is not a superkey. Thus, class is not in BCNF.W ereplace
class by:
course(course id,title,dept name,credits)
class-1 (course id,sec id,semester,year,building,room number
capacity, time slot id)
Theonlynontrivialfunctionaldependenciesthatholdoncourseincludecourse
id on the left side of the arrow. Sincecourse id is a keyforcourse,therelation
courseisin BCNF.
• A candidate key for class-1 is {course id, sec id, semester, year}. The functional
dependency:
building,room number ?capacity
holdsonclass-1,but{building,room number}isnot a superkeyforclass-1.We
replace class-1 by:
classroom (building, room number,capacity)
section (course id,sec id,semester,year,
building,room number,time slot id)
classroom andsection are in BCNF.
Thus,thedecompositionofclassresultsinthethreerelationschemascourse,class-
room,andsection,eachofwhichisin BCNF.Thesecorrespondtotheschemasthat
wehaveusedinthis,andprevious,chapters.Youcanverifythatthedecomposi-
tionislosslessanddependencypreserving.
352 Chapter 8 Relational Database Design
let F
c
be a canonical cover forF;
i :=0;
for each functional dependency  ?   in F
c
i :=i+1;
R
i
:=   ;
if none of the schemas R
j
, j = 1,2,...,i contains a candidatekey forR
then
i := i +1;
R
i
:= any candidate keyforR;
/*Optionally,removeredundantrelations*/
repeat
if any schema R
j
is contained in another schema R
k
then
/*Delete R
j
*/
R
j
:= R
i
;
i :=i-1;
untilno more R
j
scanbedeleted
return(R
1
,R
2
,...,R
i
)
Figure8.12 Dependency-preserving, lossless decomposition into 3NF.
8.5.2 3NFDecomposition
Figure 8.12 shows an algorithm for ?nding a dependency-preserving, lossless
decomposition into 3NF. The set of dependencies F
c
used in the algorithm is
a canonical cover for F. Note that the algorithm considers the set of schemas
R
j
, j = 1,2,...,i; initiallyi = 0, and in thiscase the set is empty.
LetusapplythisalgorithmtoourexampleofSection8.3.4,whereweshowed
that:
dept advisor (s ID,i ID,dept name)
isin3NFeventhoughitisnotinBCNF.Thealgorithmusesthefollowingfunctional
dependenciesin F:
f
1
:i ID ?dept name
f
2
:s ID,dept name ?i ID
There are no extraneous attributes in any of the functional dependencies in
F,soF
c
contains f
1
and f
2
. The algorithm then generatesas R
1
the schema, (i ID
dept name), and as R
2
the schema (s ID, dept name,i ID). The algorithm then ?nds
that R
2
contains a candidate key,so no further relationschema is created.
Theresultantsetofschemascancontainredundantschemas,withoneschema
R
k
containing all the attributes of another schema R
j
. For example, R
2
above
contains all the attributes from R
1
. The algorithm deletes all such schemas that
are contained in another schema. Any dependencies that could be tested on an
8.5 Algorithms for Decomposition 353
R
j
that is deleted can also be tested on the corresponding relation R
k
,andthe
decompositionislosslessevenif R
j
isdeleted.
Now let us consider again the class schema of Section 8.5.1.2 and apply the
3NFdecompositionalgorithm.Thesetoffunctionaldependencieswelistedthere
happentobeacanonicalcover.Asaresult,thealgorithmgivesusthesamethree
schemascourse,classroom,andsection.
The above example illustrates an interesting property of the 3NF algorithm.
Sometimes, the result is not only in 3NF,butalsoinBCNF. This suggests an
alternative method of generating a BCNF design. First use the 3NF algorithm.
Then, for any schema in the 3NF design that is not in BCNF,decomposeusing
the BCNF algorithm. If the result is not dependency-preserving,revertto the 3NF
design.
8.5.3 Correctness ofthe3NFAlgorithm
The3NFalgorithmensuresthepreservationofdependenciesbyexplicitlybuilding
aschemaforeachdependencyinacanonicalcover.Itensuresthatthedecomposi-
tionisalosslessdecompositionbyguaranteeingthatatleastoneschemacontains
acandidatekeyfortheschemabeingdecomposed.PracticeExercise8.14provides
someinsightintotheproofthatthissuf?cestoguaranteealosslessdecomposition.
This algorithm is also called the 3NF synthesis algorithm, since it takes a
set of dependencies and adds one schema at a time, instead of decomposing
the initial schema repeatedly. The result is not uniquely de?ned, since a set of
functionaldependenciescanhavemorethanonecanonicalcover,and,further,in
somecases,theresultofthealgorithmdependsontheorderinwhichitconsiders
the dependencies in F
c
. The algorithm may decompose a relation even if it is
alreadyin 3NF; however, the decomposition isstillguaranteed to be in 3NF.
Ifa relation R
i
is inthe decompositiongeneratedby the synthesisalgorithm,
then R
i
isin3NF.Recallthatwhenwetestfor3NF itsuf?cestoconsiderfunctional
dependencies whose right-hand side is a single attribute. Therefore, to see that
R
i
is in 3NF you must convince yourself that any functional dependency  ? B
that holds on R
i
satis?es the de?nition of 3NF. Assume that the dependency that
generated R
i
inthesynthesisalgorithmis  ?   .Now,B mustbein  or  ,since
B is in R
i
and  ?   generated R
i
.Letus considerthe threepossiblecases:
• B is in both   and   . In this case, the dependency   ?   would not have
beenin F
c
since B would be extraneous in  . Thus, this case cannot hold.
• B isin  but not  . Considertwo cases:
?
  isa superkey.The second condition of 3NFis satis?ed.
?
  is not a superkey. Then   must contain some attribute not in   .Now ,
since  ? B is in F
+
,itmustbederivablefromF
c
by using the attribute
closure algorithm on   . The derivation could not have used   ?   —
if it had been used,   must be contained in the attribute closure of   ,
which is not possible, since we assumed  is not a superkey. Now, using
  ? (  ?{B})and  ? B,wecanderive  ? B (since   ?    ,and 
354 Chapter 8 Relational Database Design
cannot contain B because   ? B is nontrivial). This would imply that B
is extraneous in the right-hand side of  ?   , which is not possible since
  ?   is in the canonical cover F
c
.Thus,ifB is in   ,then  must be a
superkey,and the second condition of 3NFmust be satis?ed.
• B is in  but not  .
Since   is a candidate key, the third alternative in the de?nition of 3NF is
satis?ed.
Interestingly, the algorithm we described for decomposition into 3NF can be
implementedinpolynomialtime,eventhoughtestingagivenrelationtoseeifit
satis?es 3NF is NP-hard (which means that it is very unlikely that a polynomial-
timealgorithmwilleverbe inventedfor thistask).
8.5.4 Comparison ofBCNFand3NF
Ofthetwonormalformsforrelationaldatabaseschemas, 3NFandBCNF thereare
advantagesto3NFinthatweknowthatitisalwayspossibletoobtaina3NFdesign
without sacri?cing losslessnessor dependencypreservation.Nevertheless,there
are disadvantages to 3NF: We may have to use null values to represent some of
thepossiblemeaningfulrelationshipsamongdataitems,andthereistheproblem
of repetitionof information.
Our goalsof database designwith functional dependenciesare:
1. BCNF.
2. Losslessness.
3. Dependencypreservation.
Since it is not always possible to satisfy all three, we may be forced to choose
between BCNF anddependencypreservationwith 3NF.
It is worth noting that SQL does not provide a way of specifying functional
dependencies, except for the special case of declaring superkeys by using the
primarykeyoruniqueconstraints.Itispossible,althoughalittlecomplicated,to
write assertions that enforce a functional dependency (see Practice Exercise 8.9);
unfortunately,currentlynodatabasesystemsupportsthecomplexassertionsthat
are required to enforce a functional dependency, and the assertions would be
expensiveto test.Thus evenif we had a dependency-preservingdecomposition,
ifweusestandard SQLwecantestef?cientlyonlythosefunctionaldependencies
whose left-hand sideis a key.
Although testing functional dependencies may involve a join if the decom-
position is not dependency preserving, we could in principle reduce the cost by
using materialized views, which many database systems support, provided the
database systemsupports primarykeyconstraints on materializedviews. Given
a BCNF decomposition that is not dependency preserving, we consider each de-
pendencyina canonical cover F
c
that isnotpreservedin the decomposition.For
each such dependency   ?   , we de?ne a materialized view that computes a
8.6 Decomposition Using Multivalued Dependencies 355
joinofallrelationsinthedecomposition,andprojectstheresulton    .Thefunc-
tionaldependencycanbetestedeasilyonthematerializedview,usingoneofthe
constraints unique(  )orprimarykey (  ).
Onthe negativeside,thereisa spaceandtimeoverheadduetothematerial-
ized view, but on the positive side, the application programmer need not worry
about writing code to keep redundant data consistent on updates; it is the job of
the database systemto maintain the materializedview,that is, keepit up to date
whenthedatabaseisupdated.(Laterinthebook,inSection13.5,weoutlinehow
a database systemcan performmaterializedviewmaintenance ef?ciently.)
Unfortunately, most current database systems do not support constraints on
materialized views. Although the Oracle database does support constraints on
materialized views, by default it performs view maintenance when the view is
accessed, not when the underlying relation is updated;
7
as a result a constraint
violation may get detected well after the update has been performed, which
makesthe detectionuseless.
Thus, in case we are not able to get a dependency-preserving BCNF decom-
position, it is generally preferable to opt for BCNF, since checking functional
dependenciesother than primarykeyconstraints isdif?cultin SQL.
8.6 DecompositionUsingMultivaluedDependencies
Some relation schemas, even though they are in BCNF, do not seem to be suf?-
cientlynormalized,inthesensethattheystillsufferfromtheproblemofrepetition
of information. Consider a variation of the university organization where an in-
structormay be associated with multipledepartments.
inst (ID,dept name,name,street,city)
The astute reader will recognize this schema as a non-BCNF schema because of
thefunctional dependency
ID ? name,street,city
and because ID isnot a keyforinst.
Further assume that an instructor may have several addresses (say, a winter
home and a summer home). Then, we no longer wish to enforce the functional
dependency “ID?street,city”, though, of course, we still want to enforce “ID ?
name” (that is, the university is not dealing with instructors who operate under
multiple aliases!). Following the BCNF decomposition algorithm, we obtain two
schemas:
7
At least as of Oracle version 10g.
356 Chapter 8 Relational Database Design
r
1
(ID,name)
r
2
(ID,dept name,street,city)
Both of these are in BCNF (recall that an instructor can be associated with multi-
ple departments and a department may have several instructors, and therefore,
neither “ID ?dept name” nor “dept name ? ID” hold).
Despiter
2
beinginBCNF,thereisredundancy.Werepeattheaddressinforma-
tion of each residence of an instructor once for each department with which the
instructor is associated. We could solve this problem by decomposingr
2
further
into:
r
21
(dept name, ID)
r
22
(ID,street,city)
but there is no constraint that leadsus todo this.
To deal with this problem, we must de?ne a new form of constraint, called
a multivalued dependency. As we did for functional dependencies, we shall use
multivalued dependencies to de?ne a normal form for relation schemas. This
normal form, calledfourthnormalform(4NF), is more restrictive than BCNF.We
shall see that every 4NF schema is also in BCNF but there are BCNF schemas that
are not in 4NF.
8.6.1 Multivalued Dependencies
Functionaldependenciesruleoutcertaintuplesfrombeinginarelation.IfA ? B,
then we cannot have two tuples with the same A value but different B values.
Multivalued dependencies, on the other hand, do not rule out the existence of
certaintuples.Instead,theyrequirethatothertuplesofacertainformbepresentin
therelation.Forthisreason,functionaldependenciessometimesarereferredtoas
equality-generating dependencies, and multivalued dependencies are referred
to astuple-generatingdependencies.
Let r(R)bearelationschemaandlet  ? R and   ? R.Themultivalued
dependency
  ? ?   holdsonRif,inany legalinstance ofrelationr(R), forall pairsoftuplest
1
andt
2
inr such thatt
1
[  ] = t
2
[  ], thereexisttuplest
3
andt
4
inr such that
t
1
[  ] = t
2
[  ] = t
3
[  ] = t
4
[  ]
t
3
[  ] = t
1
[  ]
t
3
[R ?  ] = t
2
[R ?  ]
t
4
[  ] = t
2
[  ]
t
4
[R ?  ] = t
1
[R ?  ]
8.6 Decomposition Using Multivalued Dependencies 357
?? ? ? – R –
t
1
t
2
t
3
t
4
a
1 
...a
i
a
1 
...a
i
a
1 
...a
i
a
1 
...a
i
a
i + 1 
...a
j
b
i + 1 
...b
j
a
i + 1 
...a
j
b
i + 1 
...b
j
a
j + 1 
...a
n
b
j + 1 
...b
n
b
j + 1 
...b
n
a
j + 1 
...a
n
Figure8.13 Tabular representation of   ? ?   .
Thisde?nitionislesscomplicatedthanitappearstobe.Figure8.13givesatabular
picture of t
1
, t
2
, t
3
,andt
4
. Intuitively, the multivalued dependency   ? ?   says
thattherelationshipbetween  and  isindependentoftherelationshipbetween
  and R?  .Ifthemultivalueddependency  ? ?   issatis?edbyallrelationson
schema R,then  ? ?   is a trivial multivalued dependency on schema R.Thus,
  ? ?   istrivialif  ?   or  ?  = R.
Toillustratethedifferencebetweenfunctionalandmultivalueddependencies,
weconsidertheschemar
2
again,andanexamplerelationonthatschemashown
inFigure8.14.Wemustrepeatthedepartmentnameonceforeachaddressthatan
instructor has, and we must repeat the address for each department with which
an instructor is associated. This repetition is unnecessary, since the relationship
betweenaninstructorandhisaddressisindependentoftherelationshipbetween
thatinstructorandadepartment.IfaninstructorwithID22222isassociatedwith
thePhysicsdepartment,wewantthatdepartmenttobeassociatedwithallofthat
instructor’s addresses. Thus, the relation of Figure 8.15 is illegal. To make this
relation legal, we need to add the tuples (Physics, 22222, Main, Manchester) and
(Math, 22222, North, Rye)to the relationof Figure 8.15.
Comparingtheprecedingexamplewithourde?nitionofmultivalueddepen-
dency, we see that we want the multivalueddependency:
ID ? ? street,city
tohold.(ThemultivalueddependencyID ? ? dept namewilldoaswell.Weshall
soon see that they are equivalent.)
As with functional dependencies,we shall use multivalued dependencies in
two ways:
1. To test relations to determine whether they are legal under a given set of
functional and multivalueddependencies
ID dept name street city
22222 Physics North Rye
22222 Physics Main Manchester
12121 Finance Lake Horseneck
Figure8.14 An example of redundancy in a relation on a BCNF schema.
358 Chapter 8 Relational Database Design
ID dept name street city
22222 Physics North Rye
22222 Math Main Manchester
Figure8.15 An illegal r
2
relation.
2. To specify constraints on the set of legal relations; we shall thus concern
ourselveswithonly those relationsthat satisfya givensetof functional and
multivalueddependencies
Note that, if a relation r fails to satisfy a given multivalued dependency, we can
construct a relation r
null
that does satisfy the multivalued dependency by adding
tuplestor.
Let D denote a set of functional and multivalued dependencies. The closure
D
+
of D is the set of all functional and multivalued dependencies logically im-
plied by D. As we did for functional dependencies, we can compute D
+
from
D, using the formal de?nitions of functional dependencies and multivalued de-
pendencies. We can manage with such reasoning for very simple multivalued
dependencies. Luckily, multivalued dependencies that occur in practice appear
to be quite simple. For complex dependencies, it is better to reason about sets of
dependenciesby usinga systemof inference rules.
Fromthede?nitionofmultivalueddependency,we can derivethe following
rules for  ,  ? R:
• If  ?   ,then  ? ?   . In other words, everyfunctional dependency is also
a multivalueddependency.
• If  ? ?   ,then  ? ? R ?  ?  Appendix C.1.1 outlines a system of inference rules for multivalued dependen-
cies.
8.6.2 Fourth NormalForm
Consideragain ourexample ofthe BCNF schema:
r
2
(ID,dept name,street,city)
in which the multivalued dependency “ID ? ? street, city” holds. We saw in
the opening paragraphs of Section 8.6 that, although this schema is in BCNF,the
design is not ideal, since we must repeat an instructor’s address information for
eachdepartment.Weshallseethatwecanusethegivenmultivalueddependency
toimprovethedatabasedesign,bydecomposingthisschemaintoafourthnormal
formdecomposition.
Arelationschemar(R)isinfourthnormalform(4NF)withrespecttoasetD
offunctionalandmultivalueddependenciesif,forallmultivalueddependencies
8.6 Decomposition Using Multivalued Dependencies 359
in D
+
of the form  ? ?  ,where  ? R and   ? R, at least one of the following
holds:
•   ? ?  isa trivialmultivalueddependency.
•   isa superkeyforR.
A database design is in 4NF if each member of the set of relation schemas that
constitutesthe designis in 4NF.
Note that the de?nition of 4NF differsfrom the de?nitionof BCNF inonly the
use of multivalued dependencies. Every 4NF schema is in BCNF.Toseethisfact,
we note that, ifa schemar(R)isnotinBCNF, then thereisa nontrivial functional
dependency  ?   holdingonR,where  isnotasuperkey.Since  ?  implies
  ? ?  ,r(R) cannot be in 4NF.
Letr(R)bearelationschema,andletr
1
(R
1
),r
2
(R
2
),...,r
n
(R
n
)beadecompo-
sition ofr(R). To check if each relation schemar
i
in the decomposition is in 4NF,
we need to ?nd what multivalued dependencies hold on eachr
i
.Recallthat,for
asetF of functional dependencies, the restriction F
i
of F to R
i
is all functional
dependencies in F
+
that include only attributes of R
i
. Now consider a set D of
both functional and multivalued dependencies. The restriction of D to R
i
is the
set D
i
consisting of:
1. Allfunctional dependenciesin D
+
that include onlyattributesof R
i
.
2. Allmultivalueddependenciesof theform:
  ? ?  ? R
i
where  ? R
i
and  ? ?  is in D
+
.
8.6.3 4NFDecomposition
The analogy between 4NF and BCNF applies to the algorithm for decomposing
aschemainto4NF.Figure8.16showsthe4NF decomposition algorithm. It is
identical to the BCNF decomposition algorithm of Figure 8.11, except that it uses
multivalueddependenciesand usestherestrictionof D
+
to R
i
.
If we apply the algorithm of Figure 8.16 to (ID, dept name, street, city), we
?nd that ID? ?dept nameisanontrivialmultivalueddependency,and IDisnot a
superkeyfortheschema.Followingthealgorithm,wereplaceitbytwoschemas:
r
21
(ID,dept name)
r
22
(ID,street,city)
Thispairofschemas,whichisin4NF,eliminatestheredundancyweencountered
earlier.
As was the case when we were dealing solelywith functional dependencies,
weareinterestedindecompositionsthatarelosslessandthatpreservedependen-
360 Chapter 8 Relational Database Design
result :={R};
done :=false;
compute D
+
;Givenschema R
i
,letD
i
denotethe restrictionof D
+
to R
i
while(notdone)do
if (there isa schema R
i
inresult that is not in 4NF w.r.t. D
i
)
then begin
let  ? ?  be anontrivial multivalueddependencythat holds
on R
i
such that  ? R
i
is not in D
i
,and  ?   =? ;
result :=(result ? R
i
) ? (R
i
?   ) ? (  ,   );
end
elsedone :=true;
Figure8.16 4NF decomposition algorithm.
cies. The following fact about multivalueddependenciesand losslessnessshows
that the algorithm of Figure8.16 generatesonly losslessdecompositions:
• Letr(R)bearelationschema,andletDbeasetoffunctionalandmultivalued
dependencies on R.Letr
1
(R
1
)andr
2
(R
2
) form a decomposition of R.This
decomposition is lossless of R if and only if at least one of the following
multivalueddependenciesisin D
+
:
R
1
? R
2
? ? R
1
R
1
? R
2
? ? R
2
Recall that we stated in Section 8.4.4 that, if R
1
? R
2
? R
1
or R
1
? R
2
? R
2
,then
r
1
(R
1
)andr
2
(R
2
) are a lossless decomposition r(R) . The preceding fact about
multivalueddependenciesisamoregeneralstatementaboutlosslessness.Itsays
that, for every lossless decomposition ofr(R) into two schemasr
1
(R
1
)andr
2
(R
2
),
one of thetwo dependencies R
1
? R
2
? ? R
1
or R
1
? R
2
? ? R
2
musthold.
Theissueofdependencypreservationwhenwedecomposearelationschema
becomes more complicated in the presence of multivalued dependencies. Ap-
pendixC.1.2pursuesthis topic.
8.7 MoreNormalForms
The fourth normal form is by no means the “ultimate” normal form. As we
saw earlier, multivalued dependencies help us understand and eliminate some
forms of repetition of information that cannot be understood in terms of func-
tionaldependencies.Therearetypesofconstraintscalledjoindependenciesthat
generalize multivalued dependencies, and lead to another normal form called
project-joinnormalform(PJNF)(PJNFiscalled?fthnormalforminsomebooks).
Thereisaclassofevenmoregeneralconstraintsthatleadstoanormalformcalled
domain-keynormalform(DKNF).
8.8 Database-Design Process 361
Apractical problemwiththe useofthesegeneralizedconstraints isthat they
are not only hard to reason with, but there is also no set of sound and complete
inference rules for reasoning about the constraints. Hence PJNF and DKNF are
used quiterarely.AppendixC providesmore detailsabout these normal forms.
Conspicuous by its absence from our discussion of normal forms is second
normal form (2NF).W ehavenotdiscussedit,becauseitisofhistoricalinterest
only.Wesimplyde?neit,andletyouexperimentwithitinPracticeExercise8.17.
8.8 Database-DesignProcess
So far we have looked at detailedissues about normal forms and normalization.
In this section, we study how normalization ?ts into the overall database-design
process.
Earlier in the chapter, starting in Section 8.3, we assumed that a relation
schema r(R) is given, and proceeded to normalize it. There are several ways in
which we could have come up with the schemar(R):
1. r(R) could have been generated in converting an E-R diagram to a set of
relationschemas.
2. r(R) could have been a single relation schema containing all attributes that
are of interest. The normalization process then breaks up r(R) into smaller
schemas.
3. r(R)couldhavebeentheresultofanad-hocdesignofrelationsthatwethen
testto verifythat itsatis?esa desirednormal form.
Intherestofthissection,weexaminetheimplicationsoftheseapproaches.Wealso
examinesomepracticalissuesindatabasedesign,includingdenormalizationfor
performanceandexamplesofbaddesignthatarenotdetectedbynormalization.
8.8.1 E-RModelandNormalization
When we de?ne an E-R diagram carefully, identifying all entities correctly, the
relation schemas generated from the E-R diagram should not need much further
normalization.However,therecanbefunctionaldependenciesbetweenattributes
ofanentity.Forinstance,supposeaninstructorentitysethadattributesdept name
and dept address, and there is a functional dependency dept name ? dept address.
We would then needto normalize the relationgeneratedfrominstructor.
Most examples of such dependencies arise out of poor E-R diagram design.
In the above example, if we had designed the E-R diagram correctly, we would
have created adepartment entity set with attribute dept address and a relationship
setbetweeninstructoranddepartment.Similarly,arelationshipsetinvolvingmore
thantwoentitysetsmayresultinaschemathatmaynotbeinadesirablenormal
form. Since most relationship sets are binary, such cases are relatively rare. (In
362 Chapter 8 Relational Database Design
fact,some E-R-diagramvariantsactuallymakeitdif?cultorimpossibletospecify
nonbinary relationship sets.)
Functionaldependenciescanhelpusdetectpoor E-Rdesign.Ifthegenerated
relationschemasarenotindesirednormalform,theproblemcanbe?xedintheE-
Rdiagram.Thatis,normalizationcanbedoneformallyaspartofdatamodeling.
Alternatively, normalization can be left to the designer’s intuition during E-R
modeling, and can be done formally on the relation schemas generated from the
E-R model.
A careful reader will have noted that in order for us to illustrate a need for
multivalueddependenciesandfourthnormalform,wehadtobeginwithschemas
thatwere notderivedfromour E-Rdesign.Indeed,theprocessof creatingan E-R
design tends to generate 4NF designs. If a multivalued dependency holds and is
not implied by the corresponding functional dependency, it usually arises from
one of the following sources:
• A many-to-many relationship set.
• A multivaluedattribute ofan entityset.
Foramany-to-manyrelationshipseteachrelatedentitysethasitsownschemaand
thereisanadditionalschemafortherelationshipset.Foramultivaluedattribute,
aseparateschemaiscreatedconsistingofthatattributeandtheprimarykeyofthe
entityset (as inthe case of thephone numberattributeof the entitysetinstructor).
The universal-relation approach to relational database design starts with an
assumption that there is one single relation schema containing all attributes of
interest.This singleschema de?nes how usersand applicationsinteract with the
database.
8.8.2 NamingofAttributes andRelationships
A desirable feature of a database design is the unique-role assumption,which
means that each attribute name has a unique meaning in the database. This
prevents us from using the same attribute to mean different things in different
schemas. For example, we might otherwise consider using the attribute number
for phone number intheinstructor schema and for roomnumber in theclassroom
schema. The join of a relation on schema instructor with one on classroom is
meaningless.Whileusersandapplicationdeveloperscanworkcarefullytoensure
use of the right number in each circumstance, having a different attribute name
for phone number and forroom number servestoreduce usererrors.
While it is a good idea to keep names for incompatible attributes distinct, if
attributes of different relations have the same meaning, it may be a good idea to
use the same attribute name. For this reason we used the same attribute name
“name” for both the instructor and the student entity sets. If this was not the
case(thatis,weuseddifferentnamingconventionsfortheinstructorandstudent
names),thenifwewishedtogeneralizetheseentitysetsbycreatingapersonentity
set, we would have to rename the attribute. Thus, even if we did not currently
8.8 Database-Design Process 363
have a generalization of student and instructor, if we foresee such a possibility it
isbest touse the same name inboth entitysets(and relations).
Althoughtechnically,theorderofattributenamesinaschemadoesnotmatter,
it is convention to list primary-key attributes ?rst. This makes reading default
output (asfrom select *)easier.
Inlargedatabaseschemas,relationshipsets(andschemasderivedtherefrom)
are often named via a concatenation of the names of related entity sets, perhaps
with an intervening hyphen or underscore. We have used a few such names, for
example inst sec and student sec. We used the names teaches and takes instead of
using thelongerconcatenated names.This wasacceptable sinceitisnot hard for
youtorememberthe associatedentitysetsforafewrelationshipsets.Wecannot
always create relationship-set names by simple concatenation; for example, a
manager or works-for relationship between employees would not make much
senseifitwerecalledemployeeemployee!Similarly,iftherearemultiplerelationship
setspossiblebetweenapairofentitysets,therelationship-setnamesmustinclude
extrapartsto identifythe relationshipset.
Differentorganizationshavedifferentconventionsfornamingentitysets.For
example,wemaycallanentitysetofstudentsstudentorstudents.Wehavechosen
to use the singular form in our database designs. Using either singular or plural
isacceptable, as long as the convention is usedconsistently across all entitysets.
As schemas grow larger, with increasing numbers of relationship sets, using
consistentnamingofattributes,relationships,andentitiesmakeslifemucheasier
for the database designerand application programmers.
8.8.3 Denormalization forPerformance
Occasionallydatabasedesignerschooseaschemathathasredundantinformation;
that is, it is not normalized. They use the redundancy to improve performance
for speci?c applications. The penalty paid for not using a normalized schema is
the extra work (in terms of coding time and execution time) to keep redundant
dataconsistent.
Forinstance,supposeallcourseprerequisiteshavetobedisplayedalongwith
acourseinformation,everytimeacourseisaccessed.Inournormalizedschema,
this requiresa join ofcourse withprereq.
Onealternativetocomputingthejoinonthe?yistostorearelationcontaining
all the attributes of course and prereq. This makes displaying the “full” course
information faster. However, the information for a course is repeated for every
courseprerequisite,andallcopiesmustbeupdatedbytheapplication,whenever
a course prerequisite is added or dropped. The process of taking a normalized
schema and making it nonnormalized is called denormalization, and designers
use itto tune performance of systemsto supporttime-critical operations.
A better alternative, supported by many database systems today, is to use
the normalized schema, and additionally store the join of course and prereq as a
materializedview.(Recallthatamaterializedviewisaviewwhoseresultisstored
in the database and brought up to date when the relations used in the view are
updated.) Like denormalization, using materialized views does have space and
364 Chapter 8 Relational Database Design
timeoverhead;however,ithastheadvantagethatkeepingtheviewuptodateis
the job ofthe database system,not the applicationprogrammer.
8.8.4 Other DesignIssues
There are some aspects of database design that are not addressed by normal-
ization, and can thus lead to bad database design. Data pertaining to time or to
ranges of time have several such issues. We give examples here; obviously, such
designsshould be avoided.
Consider a university database, where we want to store the total number of
instructors in each department in different years. A relation total inst(dept name,
year, size) could be used to store the desired information. The only functional
dependencyon thisrelationisdept name,year?size,andtherelationisinBCNF.
An alternative design is to use multiple relations, each storing the size infor-
mation for a different year. Let us say the years of interest are 2007, 2008, and
2009;wewouldthenhaverelationsoftheformtotal inst 2007,total inst 2008,total
inst 2009, all of which are on the schema (dept name, size). The only functional
dependency here on each relation would be dept name ? size,sotheserelations
are also in BCNF.
However, this alternative design is clearly a bad idea—we would have to
create a new relation every year, and we would also have to write new queries
every year, to take each new relation into account. Queries would also be more
complicated since theymay have torefertomany relations.
Yet another way of representing the same data is to have a single relation
dept year(dept name, total inst 2007, total inst 2008, total inst 2009). Here the only
functional dependenciesarefromdept nametotheotherattributes,andagainthe
relation is in BCNF. This design is also a bad idea since it has problems similar
to the previous design—namely we would have to modify the relation schema
andwritenewquerieseveryyear.Querieswouldalsobemorecomplicated,since
they mayhave to referto many attributes.
Representations such as those in the dept year relation, with one column for
each value of an attribute, are called crosstabs; they are widely used in spread-
sheets and reports and in data analysis tools. While such representations are
useful for display to users, for the reasons just given, they are not desirable in a
database design. SQL includes features to convert data from a normal relational
representationto a crosstab, for display,aswe discussedin Section5.6.1.
8.9 ModelingTemporalData
Suppose we retain data in our university organization showing not only the
address of each instructor, but also all former addresses of which the university
is aware. We may then ask queries such as “Find all instructors who lived in
Princeton in 1981.” In this case, we may have multiple addresses for instructors.
Eachaddresshasanassociatedstartandenddate,indicatingwhentheinstructor
wasresidentatthataddress.Aspecialvaluefortheenddate,e.g.,null,oravalue
8.9 Modeling TemporalData 365
wellintothefuturesuchas9999-12-31,canbeusedtoindicatethattheinstructor
isstill residentatthat address.
Ingeneral,temporaldataaredatathathaveanassociatedtimeintervalduring
whichtheyarevalid.
8
We use the term snapshotofdatatomeanthevalueofthe
dataat aparticular point in time.Thus asnapshot ofcoursedatagivesthevalues
of all attributes, such as title and department, of all courses at a particular point
intime.
Modelingtemporaldataisachallengingproblemforseveralreasons.Forex-
ample,supposewehaveaninstructorentitysetwithwhichwewishtoassociatea
time-varyingaddress.Toaddtemporalinformationtoanaddress,wewouldthen
havetocreateamultivaluedattribute,eachofwhosevaluesisacompositevalue
containing an address and a time interval. In addition to time-varying attribute
values, entities may themselves have an associated valid time. For example, a
student entity may have a valid time from the date the student entered the uni-
versity to the date the student graduated (or left the university). Relationships
too may have associated valid times. For example, the prereq relationship may
record when a course became a prerequisite for another course. We would thus
have to add valid time intervals to attribute values, entity sets, and relationship
sets. Adding such detail to an E-R diagram makes it very dif?cult to create and
to comprehend. There have been several proposals to extend the E-R notation to
specifyinasimplemannerthatanattributevalueorrelationshipistimevarying,
but there are no accepted standards.
When we track data values across time, functional dependencies that we
assumed tohold, such as:
ID ?street,city
maynolongerhold. Thefollowingconstraint (expressedinEnglish)would hold
instead: “Aninstructor IDhasonlyonestreetandcityvalueforanygiventimet.”
Functional dependencies that hold at a particular point in time are called
temporalfunctionaldependencies.Formally,atemporalfunctionaldependency
X
  ? Y holds on a relation schema r(R) if, for all legal instances of r(R), all
snapshots ofr satisfy the functional dependency X ? Y.
We could extend the theory of relational database design to take temporal
functional dependencies into account. However, reasoning with regular func-
tional dependencies is dif?cult enough already, and few designers are prepared
to dealwith temporal functional dependencies.
In practice, database designers fall back to simpler approaches to design-
ing temporal databases. One commonly used approach is to design the entire
database(including E-Rdesignandrelationaldesign)ignoringtemporalchanges
(equivalently,takingonlyasnapshot intoconsideration).Afterthis,thedesigner
8
Thereareothermodelsoftemporaldatathatdistinguishbetweenvalidtimeandtransactiontime,thelatterrecording
whena fact was recordedinthe database. We ignoresuch detailsfor simplicity.
366 Chapter 8 Relational Database Design
studiesthevariousrelationsanddecideswhichrelationsrequiretemporalvaria-
tion to be tracked.
Thenextstepistoaddvalidtimeinformationtoeachsuchrelation,byadding
start and end time as attributes. For example, consider the course relation. The
titleofthecoursemaychangeovertime,whichcanbehandledbyaddingavalid
time range; the resultant schema would be
course(course id,title,dept name,start,end)
Aninstanceofthisrelationmighthavetworecords(CS-101,“IntroductiontoPro-
gramming”,1985-01-01,2000-12-31)and(CS-101,“IntroductiontoC”,2001-01-01,
9999-12-31).Iftherelationisupdatedbychangingthecoursetitleto“Introduction
to Java,”the time “9999-12-31”wouldbeupdatedtothetimeuntilwhichtheold
value (“Introduction to C”)isvalid,and anew tuple would be addedcontaining
the new title(“Introduction to Java”), with an appropriatestarttime.
If another relation had a foreign key referencing a temporal relation, the
database designer has to decide if the reference is to the current version of the
data or to the data as of a particular point in time. For example, we may extend
thedepartmentrelationtotrackchangesinthebuildingorbudgetofadepartment
across time, but a reference from the instructor or student relation may not care
aboutthehistoryofthebuildingorbudget,butmayinsteadimplicitlyrefertothe
temporally current record for the corresponding dept name. On the other hand, a
record in a student’s transcript should refer to the course title at the time when
the student took the course. In this latter case, the referencing relation must also
recordtimeinformation,toidentifyaparticularrecordfromthecourserelation.In
ourexample,theyearandsemesterwhenthecoursewastakencanbemappedtoa
representativetime/datevaluesuchasmidnightofthestartdateofthesemester;
the resulting time/date value is used to identify a particular record from the
temporal versionofthecourse relation, from which the title isretrieved.
The original primary key for a temporal relation would no longer uniquely
identify a tuple. To resolve this problem, we could add the start and end time
attributesto the primarykey.However,some problemsremain:
• Itispossibletostoredatawithoverlappingintervals,whichtheprimary-key
constraint would not catch. If the system supports a nativevalid time type,it
can detectand preventsuch overlappingtimeintervals.
• To specify a foreign key referencing such a relation, the referencing tuples
would have to include the start- and end-time attributes as part of their
foreign key, and the valuesmust match that in the referencedtuple.Further,
if the referenced tuple is updated (and the end time which was in the future
isupdated),the update mustpropagate to all the referencingtuples.
If the system supports temporal data in a better fashion, we can allow
the referencing tuple to specify a point in time, rather than a range, and rely
on the systemto ensure that thereisa tuple in the referencedrelationwhose
validtimeintervalcontainsthepointintime.Forexample,atranscriptrecord
8.10 Summary 367
may specify acourse id and a time (say the start date of a semester),which is
enough to identifythe correct record in thecourse relation.
As a common special case, if all references to temporal data refer to only the
current data, a simpler solution is to not add time information to the relation,
butinsteadcreateacorrespondinghistoryrelationthathastemporalinformation,
for past values. For example, in our bank database, we could use the design we
have created, ignoring temporal changes, to store only the current information.
All historical information is moved to historical relations. Thus, the instructor
relation may store only the current address, while a relation instructor history
maycontainalltheattributesofinstructor,withadditionalstart timeandend time
attributes.
Although we have not providedany formalway to dealwith temporaldata,
the issues we have discussed and the examples we have provided should help
youindesigningadatabasethatrecordstemporaldata.Furtherissuesinhandling
temporal data, includingtemporal queries,are coveredlater, in Section25.2.
8.10 Summary
• We showed pitfalls in database design, and how to systematically design
a database schema that avoids the pitfalls. The pitfalls included repeated
information and inabilityto representsome information.
• We showed the development of a relational database design from an E-R
design, when schemas may be combined safely, and when a schema should
be decomposed.All validdecompositionsmustbe lossless.
• We describedthe assumptionsof atomic domains and ?rst normal form.
• Weintroducedtheconceptoffunctionaldependencies,andusedittopresent
twonormal forms,Boyce–Codd normal form(BCNF) and thirdnormal form
(3NF).
• Ifthedecompositionisdependencypreserving,givenadatabaseupdate,all
functionaldependenciescanbe veri?ablefromindividualrelations,without
computing a joinof relationsin the decomposition.
• We showed how to reason with functional dependencies. We placed special
emphasis on what dependencies are logically implied by a set of dependen-
cies.Wealsode?nedthenotionofacanonicalcover,whichisaminimalsetof
functionaldependenciesequivalenttoagivensetoffunctionaldependencies.
• We outlined an algorithm for decomposing relations into BCNF.Thereare
relationsforwhich there isno dependency-preserving BCNF decomposition.
• We used the canonical covers to decompose a relation into 3NF,whichis
a small relaxation of the BCNF condition. Relations in 3NF may have some
redundancy, but there is always a dependency-preserving decomposition
into 3NF.
368 Chapter 8 Relational Database Design
• We presented the notion of multivalued dependencies, which specify con-
straints that cannot be speci?ed with functional dependencies alone. We de-
?ned fourth normal form (4NF) with multivalued dependencies. Appendix
C.1.1givesdetailson reasoning about multivalueddependencies.
• Other normal forms, such as PJNF and DKNF, eliminate more subtle forms
of redundancy. However, these are hard to work with and are rarely used.
AppendixC givesdetailson thesenormal forms.
• In reviewing the issues in this chapter, note that the reason we could de?ne
rigorous approaches to relational database design is that the relational data
model rests on a ?rm mathematical foundation. That is one of the primary
advantagesoftherelationalmodelcomparedwiththeotherdatamodelsthat
we have studied.
ReviewTerms
• E-R modeland normalization
• Decomposition
• Functional dependencies
• Losslessdecomposition
• Atomicdomains
• Firstnormal form(1NF)
• Legalrelations
• Superkey
• Rsatis?esF
• Fholds on R
• Boyce–Codd normal form
(BCNF)
• Dependencypreservation
• Third normal form(3NF)
• Trivialfunctional dependencies
• Closureofasetoffunctional
dependencies
• Armstrong’s axioms
• Closure of attribute sets
• Restriction ofFto R
i
• Canonical cover
• Extraneous attributes
• BCNF decompositionalgorithm
• 3NF decompositionalgorithm
• Multivalueddependencies
• Fourth normal form (4NF)
• Restriction of a multivalued
dependency
• Project-join normal form (PJNF)
• Domain-key normal form (DKNF)
• Universalrelation
• Unique-roleassumption
• Denormalization
PracticeExercises
8.1 Supposethat we decomposethe schemar(A, B,C, D, E)into
r
1
(A, B,C)
r
2
(A, D, E)
PracticeExercises 369
Show that this decomposition is a lossless decomposition if the following
setFof functional dependenciesholds:
A ? BC
CD ? E
B ? D
E ? A
8.2 Listallfunctionaldependenciessat is?ed bythe relationof Figure8.17.
8.3 Explainhowfunctionaldependenciescanbeusedtoindicatethefollowing:
• A one-to-one relationship set exists between entity sets student and
instructor.
• A many-to-one relationship set exists between entity sets student and
instructor.
8.4 Use Armstrong’s axioms to prove the soundness of the union rule. (Hint:
Use the augmentation rule to show that, if   ?   ,then  ?    . Apply
the augmentation rule again, using  ?  , and then apply the transitivity
rule.)
8.5 Use Armstrong’s axioms to prove the soundness of the pseudotransitivity
rule.
8.6 Compute the closure of the following set F of functional dependencies for
relation schemar (A, B, C, D, E).
A ?BC
CD ?E
B ?D
E ?A
Listthe candidate keysforR.
8.7 Using the functional dependencies of Practice Exercise 8.6, compute the
canonical cover F
c
.
A B C
a
1
b
1
c
1
a
1
b
1
c
2
a
2
b
1
c
1
a
2
b
1
c
3
Figure8.17 Relation of Practice Exercise 8.2.
370 Chapter 8 Relational Database Design
8.8 Considerthe algorithm in Figure 8.18 to compute  +
. Show that this algo-
rithm is more ef?cient than the one presented in Figure 8.8 (Section 8.4.2)
and that it computes  +
correctly.
8.9 Given the database schema R(a,b,c), and a relation r on the schema R,
writean SQLquerytotestwhetherthefunctionaldependencyb ? c holds
on relation r.AlsowriteanSQL assertion that enforces the functional de-
pendency; assume that no null values are present. (Although part of the
SQL standard, such assertions are not supported by any database imple-
mentation currently.)
8.10 Our discussion of lossless-join decomposition implicitly assumed that at-
tributes on the left-hand side of a functional dependency cannot take on
null values. What could go wrong on decomposition, if this property is
violated?
8.11 In the BCNF decomposition algorithm, suppose you use a functional de-
pendency   ?   to decompose a relation schema r(  ,  ,  )intor
1
(  ,  )
andr
2
(  ,  ).
a. What primary and foreign-key constraint do you expect to hold on
the decomposedrelations?
b. Give an example of an inconsistency that can arise due to an erro-
neous update, if the foreign-key constraint were not enforced on the
decomposedrelationsabove.
c. When a relation is decomposed into 3NF using the algorithm in Sec-
tion 8.5.2, what primary and foreign key dependencies would you
expectwill hold on the decomposedschema?
8.12 Let R
1
, R
2
,...,R
n
beadecompositionofschemaU.Letu(U)bearelation,
and letr
i
= null R
I
(u). Show that
u ? r
1
  r
2
  ···   r
n
8.13 Show that the decomposition in Practice Exercise8.1 is not a dependency-
preservingdecomposition.
8.14 Show that it is possible to ensure that a dependency-preserving decom-
position into 3NF is a lossless decomposition by guaranteeing that at least
one schema contains a candidate key for the schema being decomposed.
(Hint: Show that the join of all the projections onto the schemas of the
decompositioncannot have more tuplesthan the original relation.)
8.15 Give an example of a relation schema R
null
and set F
null
of functional depen-
denciessuchthatthereareatleastthreedistinctlosslessdecompositionsof
R
null
into BCNF.
PracticeExercises 371
result :=? ;
/*fdcount isan array whoseithelementcontains thenumber
of attributeson the leftside oftheithFD that are
not yet known to be in  +
*/
fori := 1to|F| do
begin
let  ?   denotetheithFD;
fdcount [i]: =|   |;
end
/*appears isan arraywith one entryfor each attribute. The
entryfor attributeA isa listof integers.Each integer
i on the list indicatesthatAappearson the leftside
of theithFD */
for each attributeAdo
begin
appears [A]: = NIL;
fori := 1to|F| do
begin
let  ?   denote theithFD;
if A ?   then addi toappears [A];
end
end
addin(  );
return(result);
procedureaddin (  );
for each attributeAin  do
begin
if A nullresult then
begin
result := result ?{A};
for each elementi ofappears[A] do
begin
fdcount [i]: =fdcount [i] ? 1;
iffdcount [i]: = 0 then
begin
let  ?   denote theithFD;
addin(  );
end
end
end
end
Figure8.18 An algorithm to compute   +
.
372 Chapter 8 Relational Database Design
8.16 Letaprimeattribute be one thatappearsinat leastone candidatekey.Let
  and   be sets of attributes such that   ?   holds, but   ?   does not
hold.LetAbeanattributethatisnotin  ,isnotin  ,andforwhich  ? A
holds. We say that A is transitively dependent on   . We can restate our
de?nitionof 3NFasfollows:ArelationschemaRisin 3NFwithrespecttoaset
F of functional dependencies if there areno nonprime attributes A in R for which
A is transitively dependent on a key for R. Show that this new de?nition is
equivalenttothe original one.
8.17 Afunctionaldependency  ?   iscalledapartialdependencyifthereis
apropersubset  of  such that  ?   . Wesaythat  ispartiallydependent
on  .ArelationschemaR isinsecond normalform (2NF)if each attribute
AinR meetsone of the following criteria:
• Itappearsina candidate key.
• Itisnot partiallydependenton acandidate key.
Show that every 3NF schema is in 2NF.( Hint: Show that every partial
dependencyisa transitivedependency.)
8.18 Give an example of a relation schema R and a set of dependencies such
thatR isin BCNF but isnot in 4NF.
Exercises
8.19 Givealossless-joindecompositioninto BCNFofschemaRofPracticeExer-
cise 8.1.
8.20 Give a lossless-join, dependency-preserving decomposition into 3NF of
schemaR of Practice Exercise8.1.
8.21 Normalize the following schema, with givenconstraints, to 4NF.
books(accessionno, isbn,title,author,publisher)
users(userid,name,deptid,deptname)
accessionno ?isbn
isbn ?title
isbn ?publisher
isbn ? ?author
userid ?name
userid ?deptid
deptid ?deptname
8.22 Explain what is meant by repetition of information and inability to represent
information. Explain why each of these properties may indicate a bad rela-
tional database design.
Exercises 373
8.23 Why are certain functional dependencies called trivial functional depen-
dencies?
8.24 Use the de?nition of functional dependency to argue that each of Arm-
strong’s axioms(re?exivity,augmentation, and transitivity)issound.
8.25 Considerthefollowing proposedruleforfunctional dependencies:If  ?
  and   ?   ,then  ?   . Prove that this rule is not sound by showing a
relation r that satis?es  ?   and  ?   ,butdoesnotsatisfy  ?   .
8.26 UseArmstrong’saxiomstoprovethesoundnessofthedecompositionrule.
8.27 Using the functional dependenciesof Practice Exercise8.6, compute B
+
.
8.28 Show that the following decomposition of the schema R of Practice Exer-
cise 8.1is not a losslessdecomposition:
(A, B,C)
(C, D, E)
Hint: Givean exampleof arelationr on schemaR such that
null A,B,C
(r)   null C,D,E
(r) nullr
8.29 Consider the following set F of functional dependencies on the relation
schemar(A, B, C, D, E,F):
A ?BCD
BC ?DE
B ?D
D ?A
a. Compute B
+
.
b. Prove(using Armstrong’s axioms)that AF isa superkey.
c. Computeacanonicalcoverfortheabovesetoffunctionaldependen-
cies F; giveeach stepofyour derivationwith an explanation.
d. Givea 3NFdecompositionofr based on the canonical cover.
e. Give a BCNF decomposition of r using the original set of functional
dependencies.
f. Can you get the same BCNF decomposition of r as above, using the
canonical cover?
8.30 List the three design goals for relational databases, and explain why each
isdesirable.
374 Chapter 8 Relational Database Design
8.31 In designing a relational database, why might we choose a non-BCNF de-
sign?
8.32 Given the three goals of relational database design, is there any reason to
design a database schema that is in 2NF, but is in no higher-order normal
form? (SeePractice Exercise8.17 forthe de?nitionof 2NF.)
8.33 Given a relational schema r(A,B,C,D), does A ? ? BC logically imply
A ? ? B and A ? ? C? If yesproveit,elsegivea counter example.
8.34 Explainwhy 4NF isa normal formmore desirablethan BCNF.
BibliographicalNotes
The?rstdiscussionofrelationaldatabasedesigntheoryappearedinanearlypa-
perbyCodd[1970].Inthatpaper,Coddalsointroducedfunctionaldependencies
and ?rst, second, and third normal forms.
Armstrong’saxiomswereintroducedinArmstrong[1974].Signi?cantdevel-
opmentofrelationaldatabasetheoryoccurredinthelate1970s. Theseresultsare
collected in several texts on database theory including Maier [1983], Atzeni and
Antonellis[1993], and Abiteboul et al. [1995].
BCNF was introduced in Codd [1972]. Biskup et al. [1979] give the algorithm
we used to ?nd a lossless dependency-preserving decomposition into 3NF.Fun-
damental results on the lossless decomposition property appear in Aho et al.
[1979a].
Beeri et al. [1977] gives a set of axioms for multivalued dependencies, and
provesthattheauthors’axiomsaresoundandcomplete.Thenotionsof4NF,PJNF,
and DKNFarefromFagin[1977], Fagin[1979], andFagin[1981], respectively.See
the bibliographical notes of Appendix C for further references to literature on
normalization.
Jensen et al. [1994] presents a glossary of temporal-database concepts. A
survey of extensions to E-R modeling to handle temporal data is presented by
GregersenandJensen[1999].Tanseletal.[1993]coverstemporaldatabasetheory,
design, and implementation. Jensen et al. [1996] describes extensions of depen-
dency theory to temporaldata.
CHAPTER
9
Application Design and
Development
Practically all use of databases occurs from within application programs. Corre-
spondingly,almostalluserinteractionwithdatabasesisindirect,viaapplication
programs. Not surprisingly, therefore, database systems have long supported
tools such as form and GUI builders, which help in rapid development of appli-
cations that interface with users. In recent years, the Web has become the most
widelyuseduserinterfacetodatabases.
In this chapter, we study tools and technologies that are used to build appli-
cations, focussingon interactiveapplicationsthatusedatabasestostoredata.
After an introduction to application programs and user interfaces in Sec-
tion9.1,wefocusondevelopingapplicationswithWeb-basedinterfaces.Westart
withanoverviewofWebtechnologiesinSection9.2,anddiscusstheJavaServlets
technology,whichiswidelyusedforbuildingWebapplications,inSection9.3.A
short overview of Web application architectures in presentedSection 9.4. In Sec-
tion9.5,wediscusstoolsforrapidapplicationdevelopment,whileinSection9.6
wecoverperformanceissuesinbuildinglargeWebapplications.InSection9.7,we
discuss issues in application security. We conclude the chapter with Section 9.8,
which coversencryptionand itsuseinapplications.
9.1 ApplicationProgramsandUserInterfaces
Although many people interact with databases, very few people use a query
language to interact with a database system directly. The most common way
in which users interact with databases is through an application program that
provides a user interface at the front end, and interfaces with a database at the
back end. Such applications take input from users, typically through a forms-
basedinterface,andeitherenterdataintoadatabaseorextractinformationfrom
a database based on the user input, and generate output, which is displayed to
theuser.
As an example of an application, consider a university registration system.
Likeothersuchapplications,theregistrationsystem?rstrequiresyoutoidentify
375
376 Chapter 9 Application DesignandDevelopment
(a) Mainframe Era (b) Personal Computer Era (c) Web era
Web Application Server
Database
Internet
Web browsers
Terminals
Mainframe Computer
Propietary Network or
dial up phone lines
Local Area Network
Desktop PCs
Database
Application
Program
Application
Program
Figure9.1 Application architectures in different eras
and authenticate yourself, typically by a user name and password. The applica-
tion then uses your identity to extract information, such as your name and the
coursesforwhich youhaveregistered,fromthedatabaseanddisplaystheinfor-
mation. The application provides a number of interfaces that let you register for
courses and query a variety of other information such as course and instructor
information. Organizations use such applications to automate a variety of tasks,
suchassales,purchases,accountingandpayroll,human-resourcesmanagement,
and inventorymanagement,among many others.
Applicationprogramsmaybeusedevenwhenitisnotapparentthattheyare
being used. For example, a news site may provide a page that is transparently
customizedtoindividualusers,eveniftheuserdoesnotexplicitly?llanyforms
when interacting with the site. To do so, it actually runs an application program
that generates a customized page for each user; customization can, for example,
be basedonthehistoryofarticlesbrowsedbytheuser.
A typical application program includes a front-end component, which deals
with the user interface, a back-end component, which communicates with a
database, and a middle layer, which contains “business logic,” that is, code that
executesspeci?crequestsforinformationorupdates,enforcingrulesofbusiness
such as what actions should be carried out to execute a given task, or who can
carryoutwhattask.
Applicationarchitectureshaveevolvedovertime,asillustratedinFigure9.1.
Applications such as airline reservations have been around since the 1960s. In
theearlydaysofcomputerapplications,applicationsranonalarge“mainframe”
computer, and users interacted with the application through terminals, some of
whichevensupportedforms.
With the widespread use of personal computers, many organizations used a
different architecture for internal applications, with applications running on the
user’scomputer, and accessinga central database.This architecture,oftencalled
a “client–server” architecture, allowed the creation of powerful graphical user
interfaces, which earlier terminal-based applications did not support. However,
softwarehadtobeinstalledoneachuser’smachinetorunanapplication,making
tasks such as upgrades harder. Even in the personal computer era, when client–
serverarchitecturesbecamepopular,mainframearchitecturecontinuedtobethe
9.2 WebFundamentals 377
choice for applications such as airline reservations, which are used from a large
numberofgeographicallydistributedlocations.
In the past 15 years, Web browsers have become the universal front end to
databaseapplications,connectingtothebackendthroughtheInternet.Browsers
use a standardized syntax, the HyperText Markup Language (HTML) standard,
which supports both formatted display of information, and creation of forms-
based interfaces. The HTML standard is independent of the operating system or
browser, and pretty much every computer today has a Web browser installed.
Thus a Web-based application can be accessed from any computer that is con-
nectedtothe Internet.
Unlikeclient–serverarchitectures,thereisnoneedtoinstallanyapplication-
speci?c software on client machines in order to use Web-based applications.
However,sophisticateduserinterfaces,supportingfeatureswellbeyondwhatis
possible using plain HTML, are now widely used, and are built with the script-
ing language JavaScript, which is supported by most Web browsers. JavaScript
programs,unlikeprogramswritteninC,canberuninasafemode,guarantee-
ing they cannot cause security problems. JavaScript programs are downloaded
transparently to the browser and do not need any explicit software installation
onthe user’scomputer.
WhiletheWebbrowserprovidesthefrontendforuserinteraction,application
programsconstitutethebackend.Typically,requestsfromabrowseraresenttoa
Webserver,whichinturnexecutesanapplicationprogramtoprocesstherequest.
Avarietyoftechnologiesareavailableforcreatingapplicationprogramsthatrun
at the back end, including Java servlets, Java Server Pages (JSP), Active Server
Page (ASP), orscriptinglanguagessuch as PHP, Perl,orPython.
Intherestofthischapter,wedescribehowtobuildsuchapplications,starting
with Web technologies and tools for building Web interfaces, and technologies
for building application programs, and then covering application architectures,
and performanceand securityissuesinbuildingapplications.
9.2 WebFundamentals
Inthissection,wereviewsomeofthefundamentaltechnologybehindtheWorld
Wide Web, for readers who are not familiar with the technology underlying the
Web.
9.2.1 UniformResourceLocators
A uniform resource locator (URL) is a globally unique name for each document
that can beaccessedonthe Web.Anexampleofa URLis:
http://www.acm.org/sigmod
The?rstpartoftheURLindicateshowthedocumentistobeaccessed: “http”
indicatesthatthedocumentistobeaccessedbytheHyperTextTransferProtocol
378 Chapter 9 Application DesignandDevelopment
<html>
<body>
<table border>
<tr>< th>ID</th>< th>Name</th>< th>Department</th>< /tr>
<tr>< td>00128</td>< td>Zhang</td>< td>Comp. Sci.</td>< /tr>
<tr>< td>12345</td>< td>Shankar</td>< td>Comp. Sci.</td>< /tr>
<tr>< td>19991</td>< td>Brandt</td>< td>History</td>< /tr>
</table>
</body>
</html>
Figure9.2 Tabular data in HTML format.
(HTTP), which is a protocol for transferring HTML documents. The second part
givesthenameofamachinethathasaWebserver.Therestofthe URListhepath
name of the ?le on the machine, or other unique identi?erof a document within
the machine.
A URL can contain the identi?er of a program located on the Web server
machine,aswellasargumentstobegiventotheprogram.Anexampleofsucha
URLis
http://www.google.com/search?q=silberschatz
which says that the program search on the server www.google.com should be
executedwiththeargumentq=silberschatz.OnreceivingarequestforsuchaURL,
the Web server executes the program, using the given arguments. The program
returns an HTML document to the Web server, which sends it back to the front
end.
9.2.2 HyperTextMarkupLanguage
Figure9.2isanexampleofatablerepresentedintheHTMLformat,whileFigure9.3
showsthedisplayedimagegeneratedbyabrowserfromtheHTMLrepresentation
of the table. The HTML source shows a few of the HTML tags. Every HTML page
should be enclosed in an html tag, while the body of the page is enclosed in a
bodytag.Atableisspeci?edbyatabletag,whichcontainsrowsspeci?edbyatr
tag.Theheaderrowofthetablehastablecellsspeci?edbyathtag,whileregular
ID
00128
12345
19991
Zhang
Shankar
Brandt
Name
Comp. Sci.
Comp. Sci.
History
Department
Figure9.3 Display of HTML source from Figure 9.2.
9.2 WebFundamentals 379
<html>
<body>
<form action="PersonQuery" method=get>
Search for:
<select name="persontype">
<option value="student" selected>Student</option>
<option value="instructor"> Instructor </option>
</select>< br>
Name: <input type=text size=20 name="name">
<input type=submit value="submit">
</form>
</body>
</html>
Figure9.4 An HTML form.
rows have table cells speci?ed by a td tag. We do not go into more details about
thetagshere;seethebibliographicnotesforreferencescontaining moredetailed
descriptionsof HTML.
Figure9.4showshowtospecifyan HTMLformthatallowsuserstoselectthe
person type (student or instructor) from a menu and to input a number in a text
box. Figure 9.5 shows how the above form is displayed in a Web browser. Two
methods of accepting input are illustrated in the form, but HTML also supports
several other input methods. The action attribute of the form tag speci?es that
when the form is submitted (by clicking on the submit button), the form data
should be sent to the URL PersonQuery (the URL is relative to that of the page).
TheWebserveriscon?guredsuchthatwhenthisURLisaccessed,acorresponding
applicationprogramisinvoked,withtheuser-providedvaluesforthearguments
persontype and name (speci?ed in the select and input ?elds). The application
programgeneratesan HTMLdocument,whichisthensentbackanddisplayedto
theuser;we shallseehowtoconstructsuch programslaterinthischapter.
HTTPde?nes two ways in which valuesenteredby auser at the browser can
besenttotheWebserver.Thegetmethodencodesthevaluesaspartofthe URL.
For example, if the Google search page used a form with an input parameter
namedqwiththegetmethod,andtheusertypedinthestring“silberschatz”and
submitted the form, the browser would requestthe following URL from the Web
server:
http://www.google.com/search?q=silberschatz
Search for:
Name:
Student
submit
Figure9.5 Display of HTML source from Figure 9.4.
380 Chapter 9 Application DesignandDevelopment
Thepostmethodwouldinsteadsendarequestforthe URL http://www.google.com,
and send the parameter values as part of the HTTP protocol exchange between
the Web server and the browser. The form in Figure 9.4 speci?es that the form
usesthegetmethod.
Although HTML code can be created using a plain text editor, there are a
number of editors that permit direct creation of HTML text by using a graphical
interface. Such editors allow constructs such as forms, menus, and tables to be
inserted into the HTML document from a menu of choices, instead of manually
typinginthe codetogeneratetheconstructs.
HTML supports stylesheets, which can alter the default de?nitions of how an
HTML formatting construct is displayed, as well as other display attributes such
as background color of the page. The cascading stylesheet (CSS) standard allows
thesamestylesheettobeusedformultipleHTMLdocuments,givingadistinctive
butuniformlooktoall thepagesonaWebsite.
9.2.3 WebServersandSessions
AWeb server isaprogramrunningontheservermachine,whichacceptsrequests
fromaWebbrowserandsendsbackresultsintheformof HTMLdocuments.The
browser and Web server communicate via HTTP. Web servers provide powerful
features, beyond the simple transfer of documents. The most important feature
is the ability to execute programs, with arguments supplied by the user, and to
delivertheresultsbackasan HTML document.
As a result, a Web server can act as an intermediary to provide access to a
variety of information services. A new service can be created by creating and
installing an application program that provides the service. The common gate-
way interface (CGI) standard de?nes how the Web server communicates with
application programs. The application program typically communicates with a
database server, through ODBC, JDBC, or other protocols, in order to get or store
data.
HTTP
browser
server
web server
application server
database server
data
network
network
Figure9.6 Three-layer Web application architecture.
9.2 WebFundamentals 381
HTTP
browser
server
data
database server
web server and
application server
network
Figure9.7 Two-layer Web application architecture.
Figure9.6showsaWebapplicationbuiltusingathree-layerarchitecture,with
a Web server,an application server,and a database server.Using multiple levels
of servers increases system overhead; the CGI interface starts a new process to
serviceeachrequest,whichresultsinevengreateroverhead.
Most Web applications today therefore use a two-layer Web application ar-
chitecture, where the application program runs within the Web server, as in
Figure 9.7. We study systems based on the two-layer architecture in more detail
insubsequentsections.
There is no continuous connection between the client and the Web server;
whenaWebserverreceivesarequest,aconnectionistemporarilycreatedtosend
the request and receive the response from the Web server. But the connection
may then be closed, and the next request could come over a new connection. In
contrast,whenauserlogsontoacomputer,orconnectstoadatabaseusingODBC
or JDBC,asessioniscreated,andsessioninformationisretainedattheserverand
theclientuntilthesessionisterminated—informationsuchastheuser-identi?er
of the user and session options that the user has set. One important reason that
HTTP is connectionless is that most computers have limits on the number of
simultaneous connections they can accommodate, and if a large number of sites
on the Web open connections to a single server, this limit would be exceeded,
denying service to further users. With a connectionless protocol, the connection
can be broken as soon as a request is satis?ed, leaving connections available for
otherrequests.
1
Most Web applications, however, need session information to allow mean-
ingful user interaction. For instance, applications typically restrict access to in-
formation, and therefore need to authenticate users. Authentication should be
1
For performance reasons, connections may be kept open for a short while, to allow subsequent requests to reuse the
connection. However, there is no guarantee that the connection will be kept open, and applications must be designed
assuming the connectionmaybe closedas soonas a requestisserviced.
382 Chapter 9 Application DesignandDevelopment
done once per session, and further interactions in the session should not require
reauthentication.
To implement sessions in spite of connections getting closed, extra informa-
tionhastobestoredattheclientandreturnedwitheachrequestinasession;the
server uses this information to identify that a request is part of a user session.
Extrainformationabout thesessionalsohastobemaintainedat theserver.
This extra information is usually maintained in the form of a cookie at the
client; a cookie issimplyasmallpiece oftextcontaining identifyinginformation
and with an associated name. For example, google.com may set a cookie with
thenameprefs,whichencodespreferencessetbytheusersuchasthepreferred
languageandthenumberofanswersdisplayedperpage.Oneachsearchrequest,
google.com can retrieve the cookie named prefs from the user’s browser, and
display results according to the speci?ed preferences. A domain (Web site) is
permittedtoretrieveonlycookiesthatithasset,notcookiessetbyotherdomains,
and cookienamescan bereusedacrossdomains.
For the purpose of tracking a user session, an application may generate a
session identi?er (usually a random number not currently in use as a session
identi?er), and send a cookie named (for instance) sessionid containing the
session identi?er. The session identi?er is also stored locally at the server.When
arequestcomesin,theapplicationserverrequeststhecookienamedsessionid
fromtheclient.Iftheclientdoesnothavethecookiestored,orreturnsavaluethat
isnotcurrentlyrecordedasavalidsessionidenti?erattheserver,theapplication
concludes that the request is not part of a current session. If the cookie value
matchesastoredsessionidenti?er,therequestisidenti?edaspartofanongoing
session.
If an application needs to identify users securely, it can set the cookie only
afterauthenticatingtheuser;forexampleausermaybeauthenticatedonlywhen
avalidusername and passwordaresubmitted.
2
For applications that do not require high security, such as publicly available
news sites, cookies can be stored permanently at the browser and at the server;
theyidentifytheuseronsubsequentvisitstothesamesite,withoutanyidenti?ca-
tioninformationbeingtypedin.Forapplicationsthatrequirehighersecurity,the
servermayinvalidate(drop)thesessionafteratime-outperiod,orwhentheuser
logsout.(Typicallyauserlogsoutbyclickingonalogoutbutton,whichsubmits
a logout form, whose action is to invalidate the current session.) Invalidating a
session merely consists of dropping the session identi?er from the list of active
sessionsattheapplicationserver.
2
The useridenti?ercouldbestoredatthe clientend,inacookienamed, forexample,userid. Suchcookiescanbe used
for low-security applications, such as free Web sites identifying their users. However, for applications that require a
higher level of security, such a mechanism creates a security risk: The value of a cookie can be changed at the browser
by a malicious user, who can then masquerade as a different user. Setting a cookie (named sessionid, for example) to
a randomly generated session identi?er (from a large space of numbers) makes it highly improbable that a user can
masqueradeas(thatis,pretendtobe)anotheruser.Asequentiallygeneratedsessionidenti?er,ontheotherhand,would
besusceptible to masquerading.
9.3 ServletsandJSP 383
9.3 ServletsandJSP
In a two-layer Web architecture, an application runs as part of the Web serverit-
self.OnewayofimplementingsuchanarchitectureistoloadJavaprogramsinto
the Web server. The Java servlet speci?cation de?nes an application program-
ming interface for communication between the Web server and the application
program.TheHttpServletclassinJavaimplementstheservletAPIspeci?cation;
servlet classes used to implement speci?c functions are de?ned as subclasses of
thisclass.
3
OftenthewordservletisusedtorefertoaJavaprogram(andclass)that
implementstheservletinterface.Figure9.8 shows a servletexample;weexplain
itindetailshortly.
ThecodeforaservletisloadedintotheWebserverwhentheserverisstarted,
orwhentheserverreceivesaremote HTTPrequesttoexecuteaparticularservlet.
The task of a servlet is to process such a request, which may involve accessing a
database to retrieve necessary information, and dynamically generate an HTML
pagetobereturnedtotheclientbrowser.
9.3.1 AServletExample
Servlets are commonly used to generate dynamic responses to HTTP requests.
They can access inputs providedthrough HTML forms, apply “business logic” to
decidewhatresponsetoprovide,andthengenerate HTMLoutputtobesentback
tothebrowser.
Figure 9.8 shows an example of servlet code to implement the form in Fig-
ure 9.4. The servlet is called PersonQueryServlet, while the form speci?es that
“action="PersonQuery".” The Web server must be told that this servlet is to be
used to handle requests for PersonQuery. The form speci?es that the HTTP get
mechanism is used for transmitting parameters. So the doGet() method of the
servlet,as de?nedinthe code,is invoked.
Each request results in a new thread within which the call is executed, so
multiple requests can be handled in parallel. Any values from the form menus
and input ?elds on the Web page, as well as cookies, pass through an object of
the HttpServletRequest class that is created for the request, and the reply to the
requestpassesthroughanobject oftheclass HttpServletResponse.
The doGet() method in the example extracts values of the parameter’s type
andnumberbyusingrequest.getParameter(),andusesthesevaluestorunaquery
againstadatabase.Thecodeusedtoaccessthedatabaseandtogetattributevalues
from the query result is not shown; refer to Section 5.1.1.4 for details of how to
use JDBC to access a database. The servlet code returns the results of the query
totherequesterbyoutputtingthemtothe HttpServletResponseobject response.
Outputtingtheresultsistoresponseisimplementedby?rstgettinga PrintWriter
object outfrom response,and thenprintingtheresultin HTMLformatto out.
3
The servletinterface can also supportnon-HTTP requests, although ourexampleuses onlyHTTP.
384 Chapter 9 Application DesignandDevelopment
import java.io.*;
import javax.servlet.*;
import javax.servlet.http.*;
public class PersonQueryServlet extends HttpServlet {
public void doGet(HttpServletRequest request,
HttpServletResponse response)
throws ServletException, IOException
{
response.setContentType("text/html");
PrintWriter out = response.getWriter();
out.println("<HEAD><TITLE> Query Result</TITLE></HEAD>");
out.println("<BODY>");
String persontype = request.getParameter("persontype");
String number = request.getParameter("name");
if(persontype.equals("student")) {
... code to ?nd students with the speci?ed name ...
... using JDBC to communicate with the database ..
out.println("<table BORDER COLS=3>");
out.println(" <tr>< td>ID</td>< td>Name:</td>"+
" <td>Department</td>< /tr>");
for(... each result ...){
... retrieve ID, name and dept name
... into variables ID, name and deptname
out.println("<tr>< td>"+ID+" </td>"+
"<td>"+name+" </td>"+
"<td>" + deptname + "</td></tr>");
};
out.println("</table>");
}
else {
... as above, but for instructors ...
}
out.println("</BODY>");
out.close();
}
}
Figure9.8 Example of servlet code.
9.3.2 ServletSessions
RecallthattheinteractionbetweenabrowserandaWebserverisstateless.Thatis,
eachtimethebrowsermakesarequesttotheserver,thebrowserneedstoconnect
totheserver,requestsomeinformation,thendisconnectfromtheserver.Cookies
can be used to recognize that a request is from the same browser session as an
9.3 ServletsandJSP 385
earlierrequest.However,cookiesformalow-levelmechanism,andprogrammers
requireabetterabstraction todealwithsessions.
The servlet API provides a method of tracking a session and storing infor-
mation pertaining to it. Invocation of the method getSession(false) of the
class HttpServletRequest retrieves the HttpSession object corresponding to the
browser that sent the request. An argument value of truewouldhavespeci?ed
thatanewsessionobjectmustbe createdifthe requestisanewrequest.
When the getSession() method is invoked, the server ?rst asks the client
to return a cookie with a speci?ed name. If the client does not have a cookie
of that name, or returns a value that does not match any ongoing session, then
the request is not part of an ongoing session. In this case, getSession() would
returnanullvalue,andtheservletcoulddirecttheusertoaloginpage.
Theloginpagecouldallowtheusertoprovideausernameandpassword.The
servlet corresponding to the login page could verify that the password matches
theuser(forexample,bylookingupauthenticationinformationinthedatabase).
If the user is properly authenticated, the login servlet would execute getSes-
sion(true), which would return a new session object. To create a new session
theWebserverwouldinternallycarryoutthefollowingtasks:setacookie(called,
forexample,sessionId)withasessionidenti?erasitsassociatedvalueattheclient
browser,createanewsessionobject,andassociatethesessionidenti?ervaluewith
thesessionobject.
The servlet code can also store and look up (attribute-name, value) pairs
in the HttpSession object, to maintain state across multiple requests within a
session. For example, after the user is authenticated and the session object has
been created, the login servlet could store the user-id of the user as a session
parameterbyexecutingthe method
session.setAttribute(“userid”, userid)
on the session object returned by getSession(); the Java variable userid is
assumedtocontain the useridenti?er.
If the request was part of an ongoing session, the browser would have re-
turned the cookie value, and the corresponding session object is returned by
getSession(). The servlet can then retrieve session parameters such as user-id
fromthesessionobject byexecutingthemethod
session.getAttribute(“userid”)
onthesessionobjectreturnedabove.Iftheattributeuseridisnotset,thefunction
wouldreturnanullvalue,whichwouldindicatethattheclientuserhasnotbeen
authenticated.
9.3.3 ServletLifeCycle
Thelifecycle ofaservletiscontrolledbytheWebserverinwhich theservlethas
been deployed. When there is a client request for a speci?c servlet, the server
386 Chapter 9 Application DesignandDevelopment
?rst checks if an instance of the servlet exists or not. If not, the Web serverloads
the servlet class into the Java virtual machine (JVM), and creates an instance of
the servlet class. In addition, the server calls the init() method to initialize the
servletinstance. Notice that each servletinstance isinitializedonly once when it
isloaded.
After making sure the servlet instance does exist, the server invokes the
service method of the servlet, with a request object and a response object as
parameters. By default, the server creates a new thread to execute the service
method; thus, multiple requests on a servlet can execute in parallel, without
having to wait for earlier requests to complete execution. The service method
callsdoGetordoPostasappropriate.
Whennolongerrequired,aservletcanbeshutdownbycallingthedestroy()
method. The server can be set up to automatically shut down a servlet if no
requests have been made on a servlet within a time-out period; the time-out
periodisaserverparameterthatcan besetasappropriatefortheapplication.
9.3.4 ServletSupport
Many application servers provide built-in support for servlets. One of the most
popular is the Tomcat Serverfrom the Apache Jakarta Project. Other application
servers that support servlets include Glass?sh, JBoss, BEAWeblogic Application
Server,OracleApplicationServer,and IBM’s WebSphereApplicationServer.
ThebestwaytodevelopservletapplicationsisbyusinganIDEsuchasEclipse
orNetBeans,which come withTomcatorGlass?shserversbuiltin.
Application servers usually provide a variety of useful services, in addition
tobasicservletsupport.Theyallowapplicationstobedeployedorstopped,and
provide functionality to monitor the status of the application server, including
performance statistics. If a servlet ?le is modi?ed, some application servers can
detectthis and recompileand reload the servlettransparently. Many application
servers also allow the server to run on multiple machines in parallel to improve
performance,androuterequeststoanappropriatecopy.Manyapplicationservers
alsosupporttheJava2EnterpriseEdition(J2EE)platform,whichprovidessupport
and APIs for a variety of tasks, such as for handling objects, parallel processing
acrossmultipleapplicationservers,andforhandling XMLdata(XMLisdescribed
laterinChapter23).
9.3.5 Server-SideScripting
Writing evena simple Web application in a programminglanguage such as Java
orCisatime-consumingtaskthatrequiresmanylinesofcodeandprogrammers
who are familiar with the intricacies of the language. An alternative approach,
that of server-side scripting, provides a much easier method for creating many
applications.Scriptinglanguagesprovideconstructsthatcanbeembeddedwithin
HTML documents. In server-side scripting, before delivering a Web page, the
serverexecutesthescriptsembeddedwithinthe HTMLcontentsofthepage.Each
pieceofscript,whenexecuted,cangeneratetextthatisaddedtothepage(ormay
even delete content from the page). The source code of the scripts is removed
9.3 ServletsandJSP 387
<html>
<head>< title> Hello </title>< /head>
<body>
< % if (request.getParameter(“name”) == null)
{ out.println(“Hello World”); }
else { out.println(“Hello, ” + request.getParameter(“name”)); }
%>
</body>
</html>
Figure9.9 A JSP page with embedded Java code.
from the page, so the client may not even be aware that the page originally had
anycodeinit.Theexecutedscriptmaycontain SQLcodethatisexecutedagainst
adatabase.
Some of the widely used scripting languages include Java ServerPages (JSP)
from Sun, Active Server Pages (ASP) and its successor ASP.NET from Microsoft,
the PHPscriptinglanguage,theColdFusionMarkupLanguage(CFML),andRuby
onRails. Manyscriptinglanguagesalsoallowcode writteninlanguagessuch as
Java,C#,VBScript,Perl,andPythontobeembeddedintoorinvokedfrom HTML
pages. For instance, JSP allows Java code to be embedded in HTML pages, while
Microsoft’s ASP.NET and ASPsupportembeddedC#and VBScript.Many ofthese
languagescomewithlibrariesandtools,thattogetherconstituteaframeworkfor
Webapplicationdevelopment.
We brie?y describe below Java Server Pages (JSP), a scripting language that
allowsHTMLprogrammerstomixstaticHTMLwithdynamicallygeneratedHTML.
Themotivationisthat,formanydynamicWebpages,mostoftheircontentisstill
static (that is, the same content is present whenever the page is generated). The
dynamic content of the Web pages (which are generated, for example, on the
basisofformparameters)isoftenasmallpartofthepage.Creatingsuchpagesby
writingservletcoderesultsinalargeamountofHTMLbeingcodedasJavastrings.
JSP instead allows Java code to be embeddedin static HTML; the embeddedJava
code generates the dynamic part of the page. JSP scripts are actually translated
into servletcode that isthen compiled, but the application programmerissaved
thetroubleofwritingmuchoftheJavacodetocreatetheservlet.
Figure 9.9 shows the source text of an JSP page that includes embeddedJava
code. The Java code in the script is distinguished from the surrounding HTML
codebybeingenclosedin<%...%>.Thecodeusesrequest.getParameter()
togetthe valueofthe attributename.
When a JSP page is requested by a browser, the application server generates
HTMLoutputfromthepage,whichissentbacktothebrowser.The HTMLpartof
the JSPpageisoutputasis.
4
WhereverJavacodeisembeddedwithin<%...%>,
4
JSP allows a more complex embedding, whereHTML code is within a Java if-else statement, and gets output condi-
tionallydependingonwhethertheif conditionevaluates totrue ornot.Weomitdetailshere.
388 Chapter 9 Application DesignandDevelopment
PHP
PHP is a scripting language that is widely used for server-side scripting. PHP
code can be intermixed with HTML in a manner similar to JSP. The characters
“<?php” indicate the start of PHP code, while the characters “?>” indicate the
endof PHP code.Thefollowingcodeperformsthesameactionsasthe JSPcode
in Figure 9.9.
<html>
<head>< title> Hello </title>< /head>
<body>
<?php if (!isset($ REQUEST[’name’]))
{ echo ’Hello World’; }
else {echo’Hello,’.$REQUEST[’name’]; }
?>
</body>
</html>
The array $ REQUEST contains the request parameters. Note that the array is
indexed by the parameter name; in PHP arrays can be indexed by arbitrary
strings,notjustnumbers.Thefunctionissetchecksiftheelementofthearray
hasbeeninitialized.TheechofunctionprintsitsargumenttotheoutputHTML.
Theoperator “.”betweentwo stringsconcatenatesthestrings.
Asuitablycon?guredWebserverwouldinterpretany?lewhosenameends
in “.php” to be a PHP ?le. If the ?le is requested, the Web server process it in a
mannersimilar tohow JSP?lesareprocessed,andreturnsthegenerated HTML
to thebrowser.
Anumberof librariesare available for the PHP language, including libraries
fordatabase accessusing ODBC (similar to JDBCinJava).
thecodeisreplacedintheHTML output by the text it prints to the object out.In
the JSP code in the above ?gure, if no value was entered for the form parameter
name, the script prints “Hello World”; if a value was entered, the script prints
“Hello”followedby thename.
Amorerealisticexamplemayperformmorecomplexactions,suchaslooking
up valuesfromadatabase using JDBC.
JSPalsosupportstheconceptofataglibrary,whichallowstheuseoftagsthat
look much like HTML tags, but are interpreted at the server, and are replaced by
appropriately generated HTML code. JSP provides a standard set of tags that de-
?ne variables and control ?ow (iterators, if-then-else), along with an expression
language based on JavaScript (but interpreted at the server). The set of tags is
extensible, and a number of tag libraries have been implemented. For example,
there is a tag library that supports paginated display of large data sets, and a li-
brarythatsimpli?esdisplayandparsingofdatesandtimes.Seethebibliographic
notes for references to more information on JSPtaglibraries.
9.3 ServletsandJSP 389
9.3.6 Client-SideScripting
Embedding of program code in documents allows Web pages to be active,car-
rying out activities such as animation by executing programs at the local site,
insteadofjustpresentingpassivetextandgraphics.Theprimaryuseofsuchpro-
grams is ?exible interaction with the user, beyond the limited interaction power
provided by HTML and HTML forms. Further, executing programs at the client
site speeds up interaction greatly, compared to every interaction being sent to a
serversiteforprocessing.
A danger in supporting such programs is that, if the design of the system
is done carelessly, program code embedded in a Web page (or equivalently, in
an email message) can perform malicious actions on the user’s computer. The
malicious actions could range from reading private information, to deleting or
modifying information on the computer, up to taking control of the computer
and propagating the code to other computers (through email, for example). A
numberofemailviruseshavespreadwidelyinrecentyearsinthisway.
One of the reasons that the Java language became very popular is that it
provides a safe mode for executing programs on users’ computers. Java code
can be compiled into platform-independent “byte-code” that can be executedon
any browser that supports Java. Unlike local programs, Java programs (applets)
downloadedaspartofaWebpagehavenoauthoritytoperformanyactionsthat
couldbedestructive.Theyarepermittedtodisplaydataonthescreen,ortomake
a network connection to the server from which the Web page was downloaded,
in order to fetch more information. However, they are not permitted to access
local ?les, to execute any system programs, or to make network connections to
anyothercomputers.
While Java is a full-?edged programming language, there are simpler lan-
guages, called scripting languages, that can enrich user interaction, while pro-
viding the same protection as Java. These languages provide constructs that can
beembeddedwithan HTMLdocument.Client-sidescriptinglanguagesarelan-
guagesdesignedtobeexecutedontheclient’sWebbrowser.
Of these, the JavaScript language is by far the most widely used. The current
generation of Web interfaces uses the JavaScript scripting language extensively
toconstructsophisticateduserinterfaces.JavaScriptisusedforavarietyoftasks.
Forexample,functionswritteninJavaScriptcanbeusedtoperformerrorchecks
(validation) on user input, such as a date string being properly formatted, or
a value entered (such as age) being in an appropriate range. These checks are
carriedoutonthebrowserasdataisentered,evenbeforethedataaresenttothe
Webserver.
Figure9.10showsanexampleofaJavaScriptfunctionusedtovalidateaform
input. The function is declared in the head section of the HTML document. The
function checks that the credits entered for a course is a number greater than
0, and less than 16. The form tag speci?es that the validation function is to be
invokedwhentheformissubmitted.Ifthevalidationfails,analertboxisshown
totheuser,andifitsucceeds,theformissubmittedtotheserver.
JavaScriptcanbeusedtomodifydynamicallytheHTMLcodebeingdisplayed.
The browser parses HTML code into an in-memory tree structure de?ned by
390 Chapter 9 Application DesignandDevelopment
<html>
<head>
<script type="text/javascript">
function validate() {
var credits=document.getElementById("credits").value;
if (isNaN(credits)|| credits<=0 || credits>=16) {
alert("Credits must be a number greater than 0 and less than 16");
return false
}
}
</script>
</head>
<body>
<form action="createCourse" onsubmit="return validate()">
Title: <input type="text" id="title" size="20"><br />
Credits: <input type="text" id="credits" size="2"><br />
<input type="submit" value="Submit">
</form>
</body>
</html>
Figure9.10 Example of JavaScript used to validate form input
a standard called the Document Object Model (DOM). JavaScript code can
modify the tree structure to carry out certain operations. For example, suppose
a user needs to enter a number of rows of data, for example multiple items in a
single bill. A table containing text boxes and other form input methods can be
usedtogatheruserinput.Thetablemayhaveadefaultsize,butifmorerowsare
needed, the user may click on a button labeled (for example) “Add Item.” This
button can be set up to invoke a JavaScript function that modi?es the DOM tree
byaddinganextrarowinthetable.
Although the JavaScript language has been standardized, there are differ-
encesbetweenbrowsers,particularlyinthedetailsoftheDOMmodel.Asaresult,
JavaScript code that works on one browser may not work on another. To avoid
such problems, it is best to use a JavaScript library, such as Yahoo’s YUI library,
which allows code to be written in a browser independent way. Internally, the
functions in the librarycan ?nd out which browser is inuse,and send appropri-
ately generated JavaScript to the browser. See the Tools section at the end of the
chapterformoreinformation on YUIand otherlibraries.
Today,JavaScriptiswidelyusedtocreatedynamicWebpages,using several
technologies that are collectively called Ajax. Programs written in JavaScript
communicate with the Web server asynchronously (that is, in the background,
withoutblockinguserinteractionwiththeWebbrowser),andcanfetchdataand
displayit.
9.4 Application Architectures 391
AsanexampleoftheuseofAjax,consideraWebsitewithaformthatallows
youtoselectacountry, andonce acountryhasbeenselected,youareallowedto
selectastatefromalistofstatesinthatcountry.Untilthecountryisselected,the
drop-downlistof statesisempty.The Ajaxframeworkallowsthe listof statesto
bedownloadedfromtheWebsiteinthebackgroundwhenthecountryisselected,
and as soon as the list has been fetched, it is added to the drop-down list, which
allowsyoutoselectthestate.
Therearealsospecial-purposescriptinglanguagesforspecializedtaskssuch
asanimation(forexample,FlashandShockwave)andthree-dimensionalmodel-
ing (Virtual Reality Markup Language (VRML)). Flash is very widely used today
not onlyfor animation, but alsofor handlingstreamingvideocontent.
9.4 ApplicationArchitectures
Tohandletheircomplexity,largeapplicationsareoftenbrokenintoseverallayers:
• The presentation or user interface layer, which deals with user interaction. A
single application may have several different versions of this layer, corre-
sponding to distinct kinds of interfaces such as Web browsers, and user
interfacesofmobilephones,which have muchsmallerscreens.
In many implementations, the presentation/user-interface layer is it-
selfconceptuallybrokenupintolayers,basedonthemodel-view-controller
(MVC)architecture.Themodel corresponds to the business-logic layer, de-
scribedbelow.Theviewde?nesthepresentationofdata;asingleunderlying
model can have different views depending on the speci?c software/device
used to access the application. The controller receives events (user actions),
executes actions on the model, and returns a view to the user. The MVC ar-
chitecture is used in a number of Web application frameworks, which are
discussedlaterinSection9.5.2.
• The business-logic layer, which provides a high-level view of data and ac-
tions on data. We discuss the business-logic layer in more detail in Sec-
tion9.4.1.
• Thedataaccesslayer,whichprovidestheinterfacebetweenthebusiness-logic
layerandtheunderlyingdatabase.Manyapplicationsuseanobject-oriented
language to code the business-logic layer, and use an object-oriented model
ofdata,whiletheunderlyingdatabaseisarelationaldatabase.Insuchcases,
thedata-accesslayeralsoprovidesthemappingfromtheobject-orienteddata
model used by the business logic to the relational model supported by the
database.Wediscusssuch mappingsinmoredetailinSection9.4.2.
Figure 9.11 shows the above layers, along with a sequence of steps taken to
process a request from the Web browser. The labels on the arrows in the ?gure
indicate the order of the steps. When the request is received by the application
server, the controller sends a request to the model. The model processes the
392 Chapter 9 Application DesignandDevelopment
Web browser
Internet
1
8
7
6
5
2
3
4
Database
Web/Application Server
View
Controller
Model
Data Access
Layer
Figure9.11 Web application architecture.
request,usingbusinesslogic,whichmayinvolveupdatingobjectsthatarepartof
the model, followed by creating a result object. The model in turn uses the data-
access layer to update or retrieve information from a database. The result object
createdby themodelissentto theviewmodule,which createsan HTML viewof
the result, to be displayed on the Web browser. The view may be tailored based
on the characteristics of the device used to view the result, for example whether
itisacomputermonitorwithalargescreen,orasmallscreenonaphone.
9.4.1 TheBusiness-Logic Layer
Thebusiness-logiclayerofanapplicationformanagingauniversitymayprovide
abstractions of entities such as students, instructors, courses, sections, etc., and
actions such as admitting a student to the university, enrolling a student in a
course, and so on. The code implementing these actions ensures that business
rulesaresatis?ed;forexamplethecodewouldensurethatastudentcanenroll
foracourseonlyifshehasalreadycompletedcourseprerequisites,andhaspaid
hertuitionfees.
In addition, the business logic includes work?ows, which describe how a
particular task that involves multiple participants is handled. For example, if a
candidate applies to the university, there is a work?ow that de?nes who should
see and approve the application ?rst, and if approved in the ?rst step, who
should see the application next, and so on until either an offer is made to the
student,orarejectionnoteissentout.Work?owmanagementalsoneedstodeal
with error situations; for example, if a deadline for approval/rejection is not
met, a supervisor may need to be informed so she can intervene and ensure the
applicationisprocessed.Work?owsarediscussedinmoredetailinSection26.2.
9.4 Application Architectures 393
9.4.2 TheData-AccessLayerandObject-Relational Mapping
Inthesimplestscenario,wherethebusiness-logiclayerusesthesamedatamodel
asthedatabase,thedata-accesslayersimplyhidesthedetailsofinterfacingwith
the database. However, when the business-logic layer is written using an object-
oriented programming language, it is natural to model data as objects, with
methodsinvokedon objects.
Inearlyimplementations,programmershadtowritecodeforcreatingobjects
by fetching data from the database, and for storing updated objects back in the
database. However, such manual conversions between data models is cumber-
some and error prone. One approach to handling this problem was to developa
database system that natively stores objects, and relationships between objects.
Such databases, called object-oriented databases, are discussed in more detail
in Chapter 22. However, object-oriented databases did not achieve commercial
successforavarietyoftechnical andcommercialreasons.
Analternativeapproachistousetraditionalrelationaldatabasestostoredata,
but to automate the mapping of data in relations to in-memory objects, which
are created on demand (since memory is usually not suf?cient to store all data
in the database), as well as the reversemapping to store updated objects back as
relationsinthedatabase.
Several systems have been developed to implement such object-relational
mappings.TheHibernate system is widely used for mapping from Java objects
to relations. In Hibernate, the mapping from each Java class to one or more
relationsisspeci?edinamapping?le.Themapping?lecanspecify,forexample,
that a Java class called Student is mapped to the relation student,withtheJava
attribute ID mapped to the attribute student.ID, and so on. Information about the
database, such as the host on which it is running, and user name and password
for connecting to the database, etc., are speci?ed in a properties ?le. The program
has to open a session, which sets up the connection to the database. Once the
sessionissetup,aStudentobjectstudcreatedinJavacanbestoredinthedatabase
byinvokingsession.save(stud).TheHibernatecodegeneratestheSQLcommands
requiredtostorecorrespondingdatainthe student relation.
A list of objects can be retrieved from the database by executing a query
writtenintheHibernatequerylanguage ; this is similar to executing a query us-
ing JDBC,whichreturnsaResultSet containing a set of tuples. Alternatively, a
singleobjectcan beretrievedbyprovidingitsprimarykey.The retrievedobjects
canbeupdatedinmemory;whenthetransactionontheongoingHibernateses-
sioniscommitted,Hibernateautomaticallysavestheupdatedobjectsbymaking
correspondingupdatesonrelationsinthedatabase.
While entities in an E-R model naturally correspond to objects in an object-
oriented language such as Java, relationships often do not. Hibernate supports
theabilitytomapsuchrelationshipsassetsassociatedwithobjects.Forexample,
the takes relationship between student and section can be modeled by associating
asetofsectionswitheachstudent,andasetofstudentswitheachsection.Oncethe
appropriate mapping is speci?ed, Hibernate populates these sets automatically
from the database relation takes, and updates to the sets are re?ected back to the
databaserelationon commit.
394 Chapter 9 Application DesignandDevelopment
HIBERNATEEXAMPLE
As an example of the use of Hibernate, we create a Java class corresponding to
the studentrelationasfollows.
public class Student {
String ID;
String name;
String department;
int tot cred;
Student(String id, String name, String dept, int totcreds); // constructor
}
Tobeprecise,theclassattributesshouldbedeclaredasprivate,andgetter/setter
methodsshouldbeprovidedto accesstheattributes,butweomitthesedetails.
The mapping of the class attributes of Student to attributes of the relation
studentisspeci?edinamapping?le,inan XMLformat.Again,weomitdetails.
The following code snippet then creates a Student object and saves it to the
database.
Session session = getSessionFactory().openSession();
Transaction txn = session.beginTransaction();
Student stud = new Student("12328", "John Smith", "Comp. Sci.", 0);
session.save(stud);
txn.commit();
session.close();
HibernateautomaticallygeneratestherequiredSQLinsertstatementtocreatea
studenttuple inthedatabase.
Toretrievestudents, wecanuse thefollowingcodesnippet
Session session = getSessionFactory().openSession();
Transaction txn = session.beginTransaction();
List students =
session.?nd("from Student as s order by s.ID asc");
for ( Iterator iter = students.iterator(); iter.hasNext(); ) {
Student stud = (Student) iter.next();
.. print out the Student information ..
}
txn.commit();
session.close();
The above code snippet uses a query in Hibernate’s HQL query language. The
HQL query is automatically translated to SQL by Hibernate and executed, and
theresultsareconvertedintoalistof Studentobjects.Theforloopiteratesover
theobjectsinthislist andprintsthemout.
9.4 Application Architectures 395
Theabovefeatureshelptoprovidetheprogrammerahigh-levelmodelofdata
withoutbotheringaboutthedetailsoftherelationalstorage.However,Hibernate,
likeotherobject-relationalmappingsystems,alsoallowsprogrammersdirectSQL
accesstotheunderlyingrelations.Suchdirectaccessisparticularlyimportantfor
writingcomplexqueriesusedforreportgeneration.
Microsoft has developed a data model called the Entity Data Model,which
can be viewed as a variant of the entity-relationship model, and an associated
frameworkcalledthe ADO.NET EntityFramework,whichcanmapdatabetween
theEntityDataModelandarelationaldatabase.Theframeworkalsoprovidesan
SQL-likequerylanguagecalledEntitySQL,whichoperatesdirectlyontheEntity
DataModel.
9.4.3 WebServices
In the past, most Web applications used only data available at the application
server and its associated database. In recent years, a wide variety of data is
available on the Web that is intended to be processed by a program, rather than
displayeddirectlytotheuser;theprogrammayberunningaspartofaback-end
application, or may be a script running in the browser. Such data are typically
accessedusingwhatisineffectaWebapplicationprogramminginterface;thatis,
afunctioncallrequestissentusingthe HTTPprotocol,executedatanapplication
server, and the results sent back to the calling program. A system that supports
such aninterfaceiscalleda Web service.
Two approaches are widely used to implement Web services. In the simpler
approach, called Representation State Transfer (or REST), Web service function
calls are executed by a standard HTTP request to a URL at an application server,
withparameterssentasstandardHTTPrequestparameters.Theapplicationserver
executes the request (which may involve updating the database at the server),
generates and encodes the result, and returns the result as the result of the HTTP
request. The server can use any encoding for a particular requested URL; XML,
and an encoding of JavaScript objects called JavaScript Object Notation(JSON),
arewidelyused.Therequestorparsesthereturnedpagetoaccessthereturned
data.
InmanyapplicationsofsuchRESTfulWebservices(thatis,Webservicesusing
REST),therequestorisJavaScriptcoderunninginaWebbrowser;thecodeupdates
the browser screen using the result of the function call. For example, when you
scrollthedisplayonamapinterfaceontheWeb,thepartofthemapthatneedsto
benewly displayedmay befetchedby JavaScriptcodeusinga RESTful interface,
and thendisplayedonthe screen.
Amorecomplexapproach,sometimesreferredtoas“BigWebServices,”uses
XMLencodingofparametersaswellasresults,hasaformalde?nitionoftheWeb
API using a special language, and uses a protocol layer built on top of the HTTP
protocol.ThisapproachisdescribedinmoredetailinSection23.7.3.
9.4.4 Disconnected Operation
Many applications wish to support some operations even when a client is dis-
connected from the application server. For example, a student may wish to ?ll
396 Chapter 9 Application DesignandDevelopment
an application form even if her laptop is disconnected from the network, but
have it saved back when the laptop is reconnected. As another example, if an
email client is built as a Web application, a user may wish to compose an email
even if her laptop is disconnected from the network, and have it sent when it
is reconnected. Building such applications requires local storage, preferably in
the form of a database, in the client machine. The Gears software (originally de-
veloped by Google) is a browser plug-in that provides a database, a local Web
server,andsupportforparallelexecutionofJavaScriptattheclient.Thesoftware
worksidenticallyonmultipleoperatingsystem/browserplatforms,allowingap-
plicationstosupportrichfunctionalitywithoutinstallationofanysoftware(other
than Gears itself). Adobe’s AIR software also provides similar functionality for
buildingInternetapplicationsthatcanrunoutsidethe Webbrowser.
9.5 RapidApplicationDevelopment
If Web applications are built without using tools or libraries for constructing the
userinterface,theprogrammingeffortrequiredtoconstructtheuserinterfacecan
be signi?cantly more than that required for business logic and database access.
Several approaches have been developed to reduce the effort required to build
applications:
• Providealibraryof functions togenerateuser-interface elementswith mini-
malprogramming.
• Provide drag-and-drop features in an integrated development environment
thatallowsuser-interfaceelementstobedraggedfromamenuintoadesign
viewofapage.Theintegrateddevelopmentenvironmentgeneratescodethat
createstheuser-interfaceelementbyinvokinglibraryfunctions.
• Automaticallygeneratecodefortheuserinterfacefromadeclarativespeci?-
cation.
Allthese approaches have beenusedfor creating user interfaces,well beforethe
Web was created, as part of Rapid Application Development (RAD) tools, and
arenow usedextensivelyforcreatingWebapplicationsaswell.
Examplesof toolsdesignedforrapid developmentof interfacesfordatabase
applicationsincludeOracleForms,SybasePowerBuilder,andOracleApplication
Express (APEX). In addition, tools designed for Web application development,
suchasVisualStudioandNetbeansVisualWeb,supportseveralfeaturesdesigned
forrapiddevelopmentofWebinterfacesfordatabasebacked applications.
We study tools for construction of user interfaces in Section 9.5.1, and study
frameworks that support automatic code generation from a system model, in
Section9.5.2.
9.5.1 ToolsforBuildingUserInterfaces
Many HTML constructs are best generated by using appropriately de?ned func-
tions,insteadofbeingwrittenaspartofthecodeofeachWebpage.Forexample,
9.5 RapidApplication Development 397
address forms typically require a menu containing country or state names. In-
stead of writing lengthy HTML code to create the required menu each time it is
used, it is preferable to de?ne a function that outputs the menu, and to call the
functionwhereverrequired.
Menus are often best generated from data in the database, such as a table
containingcountrynamesorstatenames.Thefunctiongeneratingthemenuexe-
cutesadatabasequeryandpopulatesthemenu,usingthequeryresult.Addinga
countryorstatethenrequiresonlyachangetothedatabase,nottotheapplication
code. This approach has the potential drawback of requiring increased database
interaction, but such overhead can be minimized by caching query resultsat the
applicationserver.
Formstoinputdatesandtimes,orinputsthatrequirevalidation,aresimilarly
best generated by calling appropriately de?ned functions. Such functions can
outputJavaScriptcodetoperformvalidationatthebrowser.
Displayingasetofresultsfromaqueryisacommontaskformanydatabase
applications.Itispossible tobuildagenericfunction thattakesan SQL query(or
ResultSet) as argument, and display the tuples in the query result (or ResultSet)
inatabularform. JDBCmetadatacallscanbeusedto?ndinformationsuchasthe
number of columns and the names and typesof the columns in the queryresult;
thisinformationisthenusedtodisplaythequeryresult.
Tohandlesituationswherethequeryresultisverylarge,suchaqueryresult
display function can provide for pagination of results. The function can display
a ?xed number of records in a page and provide controls to step to the next or
previouspageorjumptoaparticularpageofthe results.
There is unfortunately no (widely used) standard Java API for functions to
carryouttheuser-interfacetasksdescribedabove.Buildingsuchalibrarycanbe
aninterestingprogrammingproject.
However,thereareothertools,suchastheJavaServerFaces(JSF)framework,
that support the features listed above. The JSF framework includes a JSP tag
librarythatimplementsthesefeatures.TheNetbeans IDEhasacomponentcalled
VisualWebthatbuildsonJSF,providingavisualdevelopmentenvironmentwhere
user interface components can be dragged and dropped into a page, and their
properties customized. For example, JSF provides components to create drop-
down menus, or display a table, which can be con?gured to get their data from
a database query. JSF also supports validation speci?cation on components, for
example to make a selection or input mandatory, or to constrain a number or a
datetobe inaspeci?edrange.
Microsoft’s Active Server Pages (ASP), and its more recent version, Active
Server Pages.NET(ASP.NET), is a widely used alternative to JSP/Java. ASP.NET is
similar to JSP, in that code in a language such as Visual Basic or C# can be
embeddedwithin HTMLcode.Inaddition, ASP.NETprovidesavarietyofcontrols
(scripting commands) that are interpreted at the server, and generate HTML that
isthensenttotheclient.Thesecontrolscansigni?cantlysimplifytheconstruction
ofWebinterfaces.Weprovideabriefoverviewofthebene?tsthatthesecontrols
offer.
398 Chapter 9 Application DesignandDevelopment
Forexample,controlssuchasdrop-downmenusandlistboxescanbeasso-
ciatedwithaDataSetobject. TheDataSetobjectissimilartoa JDBCResultSet
object, and is typically created by executing a query on the database. The HTML
menu contents arethengeneratedfrom theDataSet object’s contents; for exam-
ple,aquerymayretrievethenamesofalldepartmentsinanorganizationintothe
DataSet,andtheassociatedmenuwouldcontainthesenames.Thus,menusthat
depend on database contents can be created in a convenient manner with very
littleprogramming.
Validatorcontrolscanbeaddedtoforminput?elds;thesedeclarativelyspec-
ify validity constraints such as value ranges, or whether the input is a required
input for which a value must be provided by the user. The server creates ap-
propriate HTML code combined with JavaScript to perform the validation at the
user’sbrowser.Errormessagestobedisplayedoninvalidinputcanbeassociated
witheachvalidatorcontrol.
User actions can be speci?ed to have an associated action at the server. For
example, a menu control can specify that selecting a value from a menu has an
associatedserver-sideaction(JavaScriptcodeisgeneratedtodetecttheselection
eventandinitiatetheserver-sideaction).VisualBasic/C#codethatdisplaysdata
pertaining to the selected value can be associated with the action at the server.
Thus, selecting a value from a menu can result in associated data on the page
gettingupdated,withoutrequiringthe usertoclickonasubmitbutton.
The DataGrid control provides a very convenient way of displaying query
results. A DataGrid is associated with a DataSet object, which is typically the
result of a query. The server generates HTML code that displays the query result
asatable.Columnheadingsaregeneratedautomaticallyfromqueryresultmeta-
data. In addition, DataGrids provide several features, such as pagination, and
allow the user to sort the result on chosen columns. All the HTML code as well
as server-side functionality to implement these features is generated automati-
cally by the server. The DataGrid even allows users to edit the data and submit
changes back to the server. The application developer can specify a function, to
be executedwhenarowisedited,thatcanperformthe updateonthe database.
Microsoft Visual Studio provides a graphical user interface for creating ASP
pagesusingthesefeatures,furtherreducingthe programmingeffort.
Seethe bibliographicnotesforreferencestomoreinformationon ASP.NET.
9.5.2 WebApplicationFrameworks
There are a variety of Web application development frameworks that provide
severalcommonlyusedfeaturessuchas:
• An object-oriented model with an object-relational mapping to store data in
arelationaldatabase(aswesawinSection9.4.2).
• A(relatively)declarativewayofspecifyingaformwithvalidationconstraints
on user inputs, from which the system generates HTML and Javascript/Ajax
codetoimplementtheform.
9.5 RapidApplication Development 399
• Atemplatescriptingsystem(similarto JSP).
• A controller that maps user interaction events such as form submits to ap-
propriate functions that handle the event. The controller also manages au-
thenticationandsessions.Someframeworksalsoprovidetoolsformanaging
authorizations.
Thus, these frameworks provide a variety of features that are required to build
Webapplications,inanintegratedmanner.Bygeneratingformsfromdeclarative
speci?cations,andmanagingdataaccesstransparently,theframeworksminimize
theamount ofcodingthataWebapplicationprogrammerhastocarryout.
There are a large number of such frameworks, based on differentlanguages.
Some of the more widely used frameworks include Ruby on Rails, which is
based on the Ruby programming language, JBoss Seam, Apache Struts, Swing,
Tapestry,andWebObjects,allbasedonJava/JSP.Someofthese,suchasRubyon
Rails and JBoss Seam provide a tool that can automatically create simple CRUD
Web interfaces; that is, interfaces that support create, read, update and delete
of objects/tuples, by generating code from an object model or a database. Such
tools are particularly useful to get simple applications running quickly, and the
generatedcodecan beeditedtobuildmoresophisticatedWebinterfaces.
9.5.3 ReportGenerators
Report generators are tools to generate human-readable summary reports from
a database. They integrate querying the database with the creation of formatted
text and summary charts (such as bar or pie charts). For example, a report may
show thetotalsalesineachofthepast 2months for eachsalesregion.
Theapplicationdevelopercanspecifyreportformatsbyusingtheformatting
facilities of the report generator. Variables can be used to store parameters such
as the month and the year and to de?ne ?elds in the report. Tables, graphs, bar
charts, or other graphics can be de?ned via queries on the database. The query
de?nitionscan makeuseofthe parametervaluesstoredinthevariables.
Once we have de?ned a report structure on a report-generator facility, we
can store it and can execute it at any time to generate a report. Report-generator
systems provide a variety of facilities for structuring tabular output, such as
de?ningtableandcolumnheaders,displayingsubtotalsforeachgroupinatable,
automatically splitting long tables into multiple pages, and displaying subtotals
at theendofeachpage.
Figure 9.12 is an example of a formatted report. The data in the report are
generatedbyaggregationoninformation aboutorders.
Report-generationtoolsareavailablefromavarietyofvendors,suchasCrys-
tal Reports and Microsoft (SQL Server Reporting Services). Several application
suites, such as Microsoft Of?ce, provide a way of embedding formatted query
resultsfromadatabasedirectlyintoadocument. Chart-generationfacilitiespro-
videdbyCrystalReports, orbyspreadsheetssuch asExcelcan be usedtoaccess
data from databases, and generate tabular depictionsof data or graphical depic-
tionsusingchartsorgraphs.Suchchartscanbeembeddedwithintextdocuments.
400 Chapter 9 Application DesignandDevelopment
Region Category Sales
North Computer Hardware 1,000,000
 Computer Sonullare 500,000
 All categories  1,500,000
South Computer Hardware 200,000
 Computer Sonullare 400,000
 All categories  600,000
   2,100,000
Acme Supply Company, Inc. 
Quarterly Sales Report
Period:  Jan. 1 to March 31, 2009
Total Sales
Subtotal
Figure9.12 A formatted report.
The charts are createdinitiallyfromdata generatedby executingqueriesagainst
the database; the queries can be re-executed and the charts regenerated when
required,togenerateacurrentversionoftheoverallreport.
In addition to generating static reports, report-generation tools support the
creationofinteractivereports.Forexample,ausercan “drilldown” intoareasof
interest,forexamplemovefromanaggregateviewshowingthetotalsalesacross
an entire year to the monthly sales ?gures for a particular year. Such operations
werediscussedearlier,inSection5.6.
9.6 ApplicationPerformance
Websitesmaybeaccessedbymillionsofpeoplefromacrosstheglobe,atratesof
thousandsofrequestspersecond,orevenmore,forthemostpopularsites.Ensur-
ingthatrequestsareservedwithlowresponsetimesisamajorchallengeforWeb
site developers. To do so, application developers try to speed up the processing
of individual requests by using techniques such as caching, and exploit parallel
processing by using multiple application servers. We describe these techniques
brie?ybelow.Tuningofdatabaseapplicationsisdescribedinmoredetaillater,in
Chapter24(Section24.1).
9.6.1 Reducing OverheadbyCaching
Caching techniques of various types are used to exploit commonalities between
transactions. For instance, suppose the application code for servicing each user
requestneedstocontactadatabasethroughJDBC.CreatinganewJDBCconnection
maytakeseveralmilliseconds,soopeninganewconnectionforeachuserrequest
isnotagood ideaifveryhightransaction ratesaretobe supported.
9.6 Application Performance 401
TheConnectionpoolingmethodisusedtoreducethisoverhead;itworksas
follows. The connection pool manager (a part of the application server) creates
apool(thatis,aset)ofopenODBC/JDBC connections. Instead of opening a new
connection to the database,thecode servicinga userrequest(typicallya servlet)
asks for (requests) a connection from the connection pool and returns the con-
nection to the pool when the code (servlet) completes its processing. If the pool
has no unused connections when a connection is requested, a new connection is
opened to the database (taking care not to exceed the maximum number of con-
nections that the database system can support concurrently). If there are many
open connections that have not been used for a while, the connection pool man-
agermayclosesomeoftheopendatabaseconnections.Manyapplicationservers,
andnewer ODBC/JDBCdriversprovideabuilt-inconnection poolmanager.
A common error that many programmers make when creating Web appli-
cations is to forget to close an opened JDBC connection (or equivalently, when
connection pooling is used, to forget to return the connection to the connection
pool).Eachrequestthenopensanewconnectiontothedatabase,andthedatabase
soon reachesthe limitofhow many openconnections it can have at atime.Such
problems often do not show up on small-scale testing, since databases often al-
low hundreds of open connections, but show up only on intensive usage. Some
programmersassumethatconnections,likememoryallocatedbyJavaprograms,
are garbage collected automatically. Unfortunately, this does not happen, and
programmersareresponsiblefor closingconnections that theyhaveopened.
Certainrequestsmayresultinexactlythesamequerybeingresubmittedtothe
database.Thecostofcommunicationwiththedatabasecanbegreatlyreducedby
cachingtheresultsofearlierqueriesandreusingthem,solongasthequeryresult
has not changed at the database. Some Web servers support such query-result
caching; caching can otherwisebedoneexplicitlyinapplicationcode.
Costs can be further reduced by caching the ?nal Web page that is sent in
responsetoarequest.Ifanewrequestcomeswithexactlythesameparametersas
a previous request, the request does not perform any updates, and the resultant
Web page is in the cache, then it can be reused to avoid the cost of recomputing
the page. Caching can be done atthe levelof fragmentsof Webpages, which are
thenassembledtocreatecompleteWebpages.
CachedqueryresultsandcachedWebpagesareformsofmaterializedviews.
Ifthe underlyingdatabasedatachange, thecached resultsmustbe discarded,or
recomputed,orevenincrementallyupdated,asinmaterialized-viewmaintenance
(described later, in Section 13.5). Some database systems (such as Microsoft SQL
Server) provide a way for the application server to register a query with the
database, and get a noti?cation from the database when the result of the query
changes. Such a noti?cation mechanism can be used to ensure that queryresults
cachedattheapplicationserverareup-to-date.
9.6.2 ParallelProcessing
A commonly used approach to handling such very heavy loads is to use a large
numberofapplicationserversrunninginparallel,eachhandlingafractionofthe
402 Chapter 9 Application DesignandDevelopment
requests.AWebserveroranetworkroutercanbeusedtorouteeachclientrequest
tooneoftheapplicationservers.Allrequestsfromaparticularclientsessionmust
go to the same application server, since the server maintains state for a client
session.Thispropertycanbeensured,forexample,byroutingallrequestsfroma
particular IP address to the same application server.The underlying database is,
however,sharedbyalltheapplicationservers,sothatusersseeaconsistentview
ofthedatabase.
Withtheabovearchitecture,thedatabasecouldeasilybecomethebottleneck,
since it is shared. Application designers pay particular attention to minimizing
thenumberofrequeststothedatabase,bycachingqueryresultsattheapplication
server, as discussed earlier. In addition, parallel database systems, described in
Chapter18, areusedwhen required.
9.7 ApplicationSecurity
Application security has to deal with several security threats and issues beyond
those handledby SQL authorization.
The ?rst point where security has to be enforced is in the application. To do
so, applications must authenticate users, and ensure that users are only allowed
tocarryoutauthorized tasks.
Therearemanywaysinwhichanapplication’ssecuritycanbecompromised,
even if the database system is itself secure, due to badly written application
code. In this section, we ?rst describe several security loopholes that can permit
hackers to carry out actions that bypass the authentication and authorization
checkscarriedoutbytheapplication,andexplainhowtopreventsuchloopholes.
Laterinthesection,wedescribetechniquesforsecureauthentication,andfor?ne-
grained authorization. We then describe audit trails that can help in recovering
from unauthorized access and from erroneous updates. We conclude the section
bydescribingissuesindataprivacy.
9.7.1 SQLInjection
In SQLinjectionattacks,theattackermanagestogetanapplicationtoexecutean
SQLquerycreatedbytheattacker.InSection5.1.1.4,wesawanexampleofanSQL
injectionvulnerabilityifuserinputsareconcatenateddirectlywithan SQLquery
andsubmittedtothedatabase.AsanotherexampleofSQLinjectionvulnerability,
consider the form source text shown in Figure 9.4. Suppose the corresponding
servlet shown in Figure 9.8 creates an SQL query string using the following Java
expression:
String query = “select * from student where name like ’%”
+ name + “%’”
wherenameisavariablecontainingthestringinputbytheuser,andthenexecutes
thequeryonthedatabase.AmaliciousattackerusingtheWebformcanthentype
9.7 Application Security 403
astringsuchas“’;<some SQL statement>; ?? ”,where<some SQL statement>
denotes any SQL statement that the attacker desires, in place of a valid student
name.The servletwould thenexecutethe followingstring.
select * fromstudentwhere namelike’’;<some SQL statement>; ??’
The quote inserted by the attacker closes the string, the following semicolon ter-
minatesthequery,andthefollowingtextinsertedbytheattackergetsinterpreted
asasecondSQLquery,whiletheclosingquotehasbeencommentedout.Thus,the
malicioususerhasmanagedtoinsertanarbitrary SQLstatementthatisexecuted
by the application. The statement can cause signi?cant damage, since it can per-
form any action on the database, bypassing all security measures implemented
intheapplicationcode.
AsdiscussedinSection5.1.1.4,toavoidsuchattacks,itisbesttouseprepared
statementstoexecuteSQLqueries.Whensettingaparameterofapreparedquery,
JDBCautomaticallyaddsescapecharacterssothattheuser-suppliedquotewould
no longerbe able to terminatethe string. Equivalently,a function that addssuch
escapecharacterscouldbeappliedoninputstringsbeforetheyareconcatenated
withthe SQL query,insteadofusingpreparedstatements.
Another source of SQL-injection risk comes from applications that create
queriesdynamically, based on selection conditions and orderingattributesspec-
i?ed in a form. For example, an application may allow a user to specify what
attribute should be used for sorting the results of a query. An appropriate SQL
query is constructed, based on the attribute speci?ed. Suppose the application
takestheattributenamefromaform,inthevariableorderAttribute,andcreatesa
querystringsuch asthe following:
String query = “select * from takes order by ” + orderAttribute;
A malicious user can send an arbitrary string in place of a meaningful or-
derAttribute value, even if the HTML form used to get the input tried to restrict
theallowedvaluesbyprovidingamenu.T oavoidthiskindofSQL injection,
the application should ensure that the orderAttribute variable value is one of the
allowedvalues(inourexample,attributenames),beforeappendingit.
9.7.2 CrossSiteScriptingandRequestForgery
AWebsitethatallowsuserstoentertext,suchasacommentoraname,andthen
stores it and later displaysit to other users, is potentially vulnerable to a kind of
attackcalledacross-sitescripting(XSS)attack.Insuchanattack,amalicioususer
enterscodewritteninaclient-sidescriptinglanguagesuchasJavaScriptorFlash
instead of entering a valid name or comment. When a different user views the
entered text, the browser would execute the script, which can carry out actions
such as sending private cookie information back to the malicious user, or even
executinganactiononadifferentWebserverthattheusermaybeloggedinto.
404 Chapter 9 Application DesignandDevelopment
Forexample,supposetheuserhappenstobeloggedintoherbankaccountat
the time the script executes. The script could send cookie information related to
thebankaccountloginbacktothemalicioususer,whocouldusetheinformation
toconnecttothebank’sWebserver,foolingitintobelievingthattheconnectionis
fromtheoriginaluser.Or,thescriptcouldaccessappropriatepagesonthebank’s
Web site, with appropriately set parameters, to execute a money transfer. In fact
this particular problem can occur even without scripting by simply using a line
ofcodesuch as
<imgsrc=
"http://mybank.com/transfermoney?amount=1000&toaccount=14523">
assumingthat the URL mybank.com/transfermoneyacceptsthespeci?edparam-
eters, and carries out a money transfer. This latter kind of vulnerability is also
calledcross-siterequestforgeryor XSRF(sometimesalsocalled CSRF).
XSS can be done in other ways, such as luring a user into visiting a Web site
that has malicious scripts embedded in its pages. There are other more complex
kindsof XSSand XSRFattacks,whichweshallnotgetintohere.Toprotectagainst
such attacks,twothingsneedtobe done:
• PreventyourWebsitefrombeingusedtolaunch XSSor XSRFattacks.
ThesimplesttechniqueistodisallowanyHTMLtagswhatsoeverintextinput
byusers.Therearefunctionsthatdetect,orstripallsuchtags.Thesefunctions
can be used to prevent HTML tags, and as a result, any scripts, from being
displayed to other users. In some cases HTML formatting is useful, and in
that case functions that parse the text and allow limited HTML constructs,
but disallow other dangerous constructs can be used instead; these must be
designedcarefully, since something as innocuous as an image include could
potentiallybedangerousincasethereisabugintheimagedisplaysoftware
thatcanbe exploited.
• ProtectyourWebsitefrom XSSor XSRFattackslaunchedfromothersites.
If the user has logged into your Web site, and visits a different Web site
vulnerable to XSS, the malicious code executing on the user’s browser could
executeactionsonyourWebsite,orpasssessioninformationrelatedtoyour
Web site back to the malicious user who could try to exploit it. This cannot
bepreventedaltogether,butyoucantakeafewstepstominimizetherisk.
?
TheHTTPprotocolallowsaservertochecktherefererofapageaccess,that
is,the URLofthepagethathadthelinkthattheuserclickedontoinitiate
thepageaccess.Bycheckingthattherefererisvalid,forexample,thatthe
referer URL is a page on the same Web site, XSS attacks that originated on
adifferentWebpageaccessedbytheusercanbeprevented.
?
Insteadofusingonlythecookietoidentifyasession,thesessioncouldalso
berestrictedtothe IPaddressfromwhichitwasoriginallyauthenticated.
9.7 Application Security 405
As a result, even if a malicious user gets a cookie, he may not be able to
loginfromadifferentcomputer.
?
Never use a GET method to perform any updates. This prevents attacks
using<img src ..> such as the one we saw earlier.In fact, the HTTP stan-
dard recommends that GETmethodsshouldneverperformanyupdates,
for other reasons such as a page refresh repeating an action that should
havehappenedonly once.
9.7.3 PasswordLeakage
Anotherproblemthatapplicationdevelopersmustdealwithisstoringpasswords
in clear text in the application code. For example, programs such as JSP scripts
often contain passwords in clear text. If such scripts are stored in a directory
accessible by a Web server, an external user may be able to access the source
code of the script, and get access to the password for the database account used
by the application. To avoid such problems, many application servers provide
mechanisms to store passwords in encrypted form, which the server decrypts
before passing it on to the database. Such a feature removes the need for storing
passwords as clear text in application programs. However, if the decryption key
isalsovulnerabletobeingexposed,thisapproachisnot fullyeffective.
As another measure against compromised database passwords, many data-
basesystemsallowaccesstothedatabasetoberestrictedtoagivensetofInternet
addresses, typically, the machines running the application servers. Attempts to
connect to the database from other Internet addresses are rejected. Thus, unless
the malicious user is able to log into the application server, she cannot do any
damageevenifshegainsaccesstothedatabasepassword.
9.7.4 Application Authentication
Authentication refers to the task of verifying the identity of a person/software
connecting to an application. The simplest form of authentication consists of a
secretpassword that must be presentedwhen auser connects tothe application.
Unfortunately, passwords are easily compromised, for example, by guessing, or
by snif?ng of packets on the network if the passwords are not sent encrypted.
More robust schemes are needed for critical applications, such as online bank
accounts. Encryption is the basis for more robust authentication schemes. Au-
thenticationthrough encryptionisaddressedinSection9.8.3.
Many applications use two-factor authentication, where two independent
factors(thatis,piecesofinformationorprocesses)areusedtoidentifyauser.The
two factors should not share a common vulnerability; for example, if a system
merelyrequiredtwopasswords,bothcouldbevulnerabletoleakageinthesame
manner(bynetworksnif?ng,orbyavirusonthecomputerusedbytheuser,for
example). While biometrics such as ?ngerprints or iris scanners can be used in
situations where a user is physically present at the point of authentication, they
arenot verymeaningfulacross anetwork.
406 Chapter 9 Application DesignandDevelopment
Passwords are used as the ?rst factor in most such two-factor authentication
schemes. Smart cards or other encryption devices connected through the USB
interface, which can be used for authentication based on encryption techniques
(seeSection9.8.3),arewidelyusedassecondfactors.
One-time password devices, which generate a new pseudo-random number
(say) every minute are also widely used as a second factor. Each user is given
one of these devices, and must enter the number displayed by the device at
the time of authentication, along with the password, to authenticate himself.
Each device generates a different sequence of pseudo-random numbers. The
application server can generate the same sequence of pseudo-random numbers
as the device given to the user, stopping at the number that would be displayed
at the time of authentication, and verify that the numbers match. This scheme
requiresthattheclockinthedeviceandattheserveraresynchronizedreasonably
closely.
Yetanothersecond-factorapproachistosendanSMSwitha(randomlygener-
ated)one-timepasswordtotheuser’sphone(whosenumberisregisteredearlier)
whenever the user wishes to log in to the application. The user must possess a
phonewiththatnumbertoreceivetheSMS,andthenentertheone-timepassword,
alongwithherregularpassword,tobeauthenticated.
Itisworthnotingthatevenwithtwo-factorauthentication,usersmaystillbe
vulnerable to man-in-the-middle attacks. In such attacks, a user attempting to
connect to the application is divertedto a fake Web site, which accepts the pass-
word(includingsecondfactorpasswords)fromtheuser,andusesitimmediately
toauthenticatetotheoriginalapplication.The HTTPSprotocol,describedlaterin
Section 9.8.3.2, is used to authenticate the Web site to the user (so the user does
notconnect toafakesitebelievingittobetheintendedsite).The HTTPSprotocol
alsoencryptsdata,andpreventsman-in-the-middleattacks.
WhenusersaccessmultipleWebsites,itisoftenannoyingforausertohaveto
authenticateherselftoeachsiteseparately,oftenwithdifferentpasswordsoneach
site. There are systems that allow the user to authenticate herself to one central
authenticationservice,andotherWebsitesandapplicationscanauthenticatethe
user through the central authentication service; the same password can then be
used to access multiple sites. The LDAP protocol is widely used to implement
such a central point of authentication; organizations implement an LDAP server
containingusernamesandpasswordinformation,andapplicationsusethe LDAP
servertoauthenticateusers.
In addition to authenticating users, a central authentication service can pro-
vide other services, for example, providing information about the user such as
name, email,and addressinformation, tothe application. Thisobviatesthe need
to enter this information separately in each application. LDAP can be used for
this task, as described later in Section 19.10.2. Other directory systems such Mi-
crosoft’sActiveDirectories,alsoprovidemechanismsforauthenticatingusersas
wellasforprovidinguserinformation.
Asinglesign-onsystemfurtherallowstheusertobeauthenticatedonce,and
multipleapplicationscanthenverifytheuser’sidentitythroughanauthentication
servicewithoutrequiringreauthentication.Inotherwords,once auserislogged
9.7 Application Security 407
in at one site, he does not have to enter his user name and password at other
sites that use the same single sign-on service. Such single sign-on mechanisms
have long been used in network authentication protocols such as Kerberos, and
implementationsarenow availableforWebapplications.
The Security Assertion Markup Language (SAML)isastandardforex-
changing authentication and authorization information between different secu-
ritydomains,toprovidecross-organizationsinglesign-on.Forexample,suppose
anapplicationneedstoprovideaccesstoallstudentsfromaparticularuniversity,
say Yale. The university can set up a Web-based service that carries out authen-
tication. Suppose a user connects to the application with a username such as
“joe@yale.edu”.Theapplication,insteadofdirectlyauthenticatingauser,diverts
theusertoYaleUniversity’sauthenticationservice,whichauthenticatestheuser,
and then tells the application who the user is and may provide some additional
information such as the categoryof the user(studentorinstructor) orotherrele-
vantinformation.Theuser’spasswordandotherauthenticationfactorsarenever
revealed to the application, and the user need not register explicitly with the
application. However, the application must trust the university’s authentication
servicewhen authenticatingauser.
TheOpenIDstandardisanalternativeforsinglesign-onacrossorganizations,
and has seen increasing acceptance in recent years. A large number of popular
Websites,suchasGoogle,Microsoft,Yahoo!,amongmanyothers,actasOpenID
authenticationproviders.AnyapplicationthatactsasanOpenIDclientcan then
use any of these providers to authenticate a user; for example, a user who has
a Yahoo! account can choose Yahoo! as the authentication provider. The user
is redirected to Yahoo! for authentication, and on successful authentication is
transparentlyredirectedbacktotheapplication,andcanthencontinueusingthe
application.
9.7.5 Application-Level Authorization
Although the SQL standard supports a fairly ?exible system of authorization
based on roles (described in Section 4.6), the SQL authorization model plays a
very limited role in managing user authorizations in a typical application. For
instance, suppose you want all students to be able to see their own grades, but
not the grades of anyone else. Such authorization cannot be speci?ed in SQL for
at leasttworeasons:
1. Lack of end-user information. With the growth in the Web, database ac-
cessescomeprimarilyfromWebapplicationservers.Theenduserstypically
do not have individual user identi?ers on the database itself, and indeed
there may only be a single user identi?er in the database corresponding to
all users of an application server. Thus, authorization speci?cation in SQL
cannot be usedinthe abovescenario.
Itispossibleforanapplicationservertoauthenticateendusers,andthen
pass the authentication information on to the database. In this section we
408 Chapter 9 Application DesignandDevelopment
willassumethatthefunctionsyscontext.user id()returnstheidenti?erofthe
applicationuseronwhose behalfaqueryisbeingexecuted.
5
2. Lack of ?ne-grained authorization. Authorization must be at the level of
individual tuples, if we are to authorize students to see only their own
grades.SuchauthorizationisnotpossibleinthecurrentSQLstandard,which
permits authorization only on an entire relation or view, or on speci?ed
attributesofrelationsorviews.
We could try to get around this limitation by creating for each student
aviewonthetakes relation that shows only that student’s grades. While
this would work in principle, it would be extremelycumbersome since we
wouldhavetocreateonesuchviewforeverysinglestudentenrolledinthe
university,which iscompletelyimpractical.
6
Analternativeistocreateaviewoftheform
createview studentTakesas
select *
fromtakes
where takes.ID= syscontext.user id()
Usersarethengivenauthorizationtothisview,ratherthantotheunderlying
takesrelation.However,queriesexecutedonbehalfofstudentsmustnowbe
writtenontheviewstudentTakes, rather than on the original takes relation,
whereasqueriesexecutedonbehalfofinstructorsmayneedtouseadifferent
view.Thetaskofdevelopingapplicationsbecomesmorecomplexasaresult.
Thetaskofauthorizationistodaytypicallycarriedoutentirelyintheapplica-
tion, bypassing the authorization facilities of SQL. At the application level, users
areauthorizedtoaccessspeci?cinterfaces,andmayfurtherberestrictedtoview
orupdatecertaindataitemsonly.
While carrying out authorization in the application gives a great deal of
?exibilitytoapplicationdevelopers,thereareproblems,too.
• Thecodeforcheckingauthorizationbecomesintermixedwiththerestofthe
applicationcode.
• Implementingauthorizationthroughapplicationcode,ratherthanspecifying
it declaratively in SQL, makes it hard to ensure the absence of loopholes.
Because of an oversight, one of the application programs may not check for
authorization, allowingunauthorized usersaccesstocon?dential data.
5
In Oracle, a JDBC connection using Oracle’s JDBC drivers can set the end user identi?er using the method
OracleConnection.setClientIdenti?er(userId),a n da nSQL query can use the function sys context(’USERENV’,
’CLIENT IDENTIFIER’) to retrievethe useridenti?er.
6
Database systemsaredesignedtomanage largerelations,butmanageschemainformationsuchasviewsinawaythat
assumes smallerdata volumesso as to enhance overallperformance.
9.7 Application Security 409
Verifying that all application programs make all required authorization checks
involves reading through all the application-server code, a formidable task in
a large system. In other words, applications have a very large “surface area,”
making the task of protecting the application signi?cantly harder. And in fact,
securityloopholeshave beenfound inavarietyofreal-lifeapplications.
In contrast, if a database directly supported ?ne-grained authorization, au-
thorization policies could be speci?ed and enforced at the SQL-level, which has
a much smaller surface area. Even if some of the application interfaces inad-
vertently omit required authorization checks, the SQL-level authorization could
preventunauthorizedactionsfrombeingexecuted.
Some database systems provide mechanisms for ?ne-grained authorization.
Forexample,theOracleVirtualPrivateDatabase (VPD)allowsasystemadmin-
istrator to associate a function with a relation; the function returns a predicate
that must be added to any query that uses the relation (different functions can
be de?ned for relations that are being updated). For example, using our syntax
for retrieving application user identi?ers, the function for the takes relation can
return a predicate such as:
ID= sys context.user id()
This predicate is added to the where clause of every query that uses the takes
relation.Asaresult(assumingthattheapplicationprogramsetstheuser idvalue
tothestudent’sID),eachstudentcanseeonlythetuplescorrespondingtocourses
that shetook.
Thus, VPD provides authorization at the level of speci?c tuples, or rows,
of a relation, and is therefore said to be a row-level authorization mechanism. A
potentialpitfallwithaddingapredicateasdescribedaboveisthatitmaychange
the meaningofaquerysigni?cantly. Forexample,ifauserwrote aqueryto?nd
the average grade over all courses, she would end up getting the average of her
grades, not all grades. Although the system would give the “right” answer for
therewrittenquery,thatanswerwouldnotcorrespondtothequerytheusermay
havethought shewas submitting.
Seethebibliographic notesforpointerstomoreinformation onOracle VPD.
9.7.6 AuditTrails
An audit trail is a log of all changes (inserts, deletes, and updates) to the appli-
cation data, along with information such as which user performed the change
andwhenthechangewasperformed.Ifapplicationsecurityisbreached,oreven
if security was not breached, but some update was carried out erroneously, an
audit trail can (a) help ?nd out what happened, and who may have carried out
the actions, and (b) aid in ?xing the damage caused by the security breach or
erroneousupdate.
For example, if a student’s grade is found to be incorrect, the audit log can
be examined to locate when and how the grade was updated, as well as to ?nd
which user carried out the updates. The university could then also use the audit
410 Chapter 9 Application DesignandDevelopment
trailtotracealltheupdatesperformedbythisuser,inorderto?ndotherincorrect
orfraudulentupdates,and thencorrectthem.
Audittrailscanalsobeusedtodetectsecuritybreacheswhereauser’saccount
is compromised and accessed by an intruder. For example, each time a user logs
in,shemaybeinformedaboutallupdatesintheaudittrailthatweredonefrom
thatloginintherecentpast;iftheuserseesaupdatethatshedidnotcarryout,it
islikelytheaccount hasbeencompromised.
It is possible to create a database-level audit trail by de?ning appropriate
triggersonrelationupdates(usingsystem-de?nedvariablesthatidentifytheuser
name and time).However,manydatabase systemsprovidebuilt-inmechanisms
to create audit trails that are much more convenient to use. Details of how to
create audit trails vary across database systems, and you should refer to the
database-systemmanualsfordetails.
Database-levelaudittrailsareusuallyinsuf?cientforapplications,sincethey
are usually unable to track who was the end user of the application. Further,
updates are recorded at a low level, in terms of updates to tuples of a relation,
ratherthanatahigherlevel,intermsofthebusinesslogic.Applicationstherefore
usually create a higher-levelaudit trail, recording, for example, what action was
carriedout,by whom, when, and from which IPaddresstherequestoriginated.
A related issue is that of protecting the audit trail itself from being modi?ed
or deleted by users who breach application security. One possible solution is to
copytheaudittrailtoadifferentmachine,towhichtheintruderwouldnothave
access, witheachrecordinthe trailcopiedassoonasitisgenerated.
9.7.7 Privacy
In a world where an increasing amount of personal data are available online,
people are increasingly worried about the privacy of their data. For example,
most people would want their personal medical data to be kept private and
not revealed publicly. However, the medical data must be made available to
doctorsandemergencymedicaltechnicianswhotreatthepatient.Manycountries
have laws on privacy of such data that de?ne when and to whom the data may
be revealed. Violation of privacy law can result in criminal penalties in some
countries. Applications that access such private data must be built carefully,
keepingtheprivacylawsinmind.
On the other hand, aggregated private data can play an important role in
many tasks such as detecting drug side effects, or in detecting the spread of
epidemics. How to make such data available to researchers carrying out such
tasks, without compromising the privacy of individuals, is an important real-
worldproblem.Asanexample,supposeahospitalhidesthenameofthepatient,
but provides a researcher with the date of birth and the zip code (postal code)
of the patient (both of which may be useful to the researcher). Just these two
pieces of information can be used to uniquely identify the patient in many cases
(usinginformationfromanexternaldatabase),compromisinghisprivacy.Inthis
particular situation, one solution would be to give the year of birth but not the
9.8 Encryption andItsApplications 411
date of birth, along with the zip code, which may suf?ce for the researcher. This
would notprovideenough informationtouniquelyidentifymostindividuals.
7
As another example, Web sites often collect personal data such as address,
telephone,email,andcredit-cardinformation.Suchinformationmayberequired
to carry out a transaction such as purchasing an itemfrom a store. However, the
customer may not want the information to be made available to other organiza-
tions, or may want part of the information (such as credit-card numbers) to be
erasedaftersomeperiodoftimeasawaytopreventitfromfallingintounautho-
rized hands in the event of a security breach. Many Web sites allow customers
tospecifytheirprivacypreferences,andmustthenensurethatthesepreferences
arerespected.
9.8 EncryptionandItsApplications
Encryption refers to the process of transforming data into a form that is unread-
able, unless the reverse process of decryption is applied. Encryption algorithms
useanencryptionkeytoperformencryption,andrequireadecryptionkey(which
could be the same as the encryptionkeydependingonthe encryption algorithm
used)toperformdecryption.
The oldest uses of encryption were for transmitting messages, encrypted
using a secret key known only to the sender and the intended receiver. Even if
themessageisinterceptedbyanenemy,theenemy,notknowingthekey,willnot
beabletodecryptandunderstandthemessage.Encryptioniswidelyusedtoday
for protecting data in transit in a variety of applications such as data transfer on
theInternet,andoncellularphonenetworks.Encryptionisalsousedtocarryout
othertasks,such asauthentication, aswe willseeinSection9.8.3.
In the context of databases, encryption is used to store data in a secure way,
sothatevenifthedataisacquiredbyanunauthorizeduser(forexample,alaptop
computer containing the dataisstolen), the datawill not be accessible without a
decryptionkey.
Many databases today store sensitive customer information, such as credit-
card numbers, names, ?ngerprints, signatures, and identi?cation numbers such
as, in the United States, social-security numbers. A criminal who gets access to
such data can use it for a variety of illegal activities such as purchasing goods
using a credit-card number, or even acquiring a credit card in someone else’s
name. Organizations such as credit-card companies use knowledge of personal
informationasawayofidentifyingwhoisrequestingaserviceorgoods.Leakage
ofsuchpersonalinformationallowsacriminaltoimpersonatesomeoneelseand
getaccesstoserviceorgoods;suchimpersonationisreferredtoasidentitytheft.
Thus, applications that store such sensitive data must take great care to protect
themfromtheft.
7
For extremely old people, who are relatively rare, even the year of birth plus postal code may be enough to uniquely
identify the individual, so a range of values, such as 80 years or older, may be provided instead of the actual age for
peopleolderthan 80years.
412 Chapter 9 Application DesignandDevelopment
To reduce the chance of sensitive information being acquired by criminals,
many countries and states today require by law that any database storing such
sensitiveinformationmuststoretheinformationinanencryptedform.Abusiness
that does not protect its data thus could be held criminally liable in case of data
theft.Thus,encryptionisacriticalcomponentofanyapplicationthatstoressuch
sensitiveinformation.
9.8.1 EncryptionTechniques
Thereareavastnumberoftechniquesfortheencryptionofdata.Simpleencryp-
tion techniques may not provide adequate security, since it may be easy for an
unauthorized user to break the code. As an example of a weak encryption tech-
nique, consider the substitution of each character with the next character in the
alphabet.Thus,
Perryridge
becomes
Qfsszsjehf
If an unauthorized user sees only “Qfsszsjehf,” she probably has insuf?cient
information to break the code. However, if the intruder sees a large number of
encrypted branch names, she could use statistical data regarding the relative
frequencyofcharacterstoguesswhatsubstitutionisbeingmade(forexample,E
isthe mostcommon letterinEnglishtext,followedby T,A,O,N,I,andsoon).
Agood encryptiontechniquehasthefollowingproperties:
• Itisrelativelysimpleforauthorizeduserstoencryptand decryptdata.
• It depends not on the secrecy of the algorithm, but rather on a parameter
of the algorithm called the encryption key, which is used to encrypt data. In
a symmetric-key encryption technique, the encryption key is also used to
decrypt data. In contrast, in public-key (also known as asymmetric-key)
encryption techniques, there are two different keys, the public key and the
privatekey,usedtoencryptand decryptthedata.
• Its decryption key is extremely dif?cult for an intruder to determine, even
if the intruder has access to encrypted data. In the case of asymmetric-key
encryption,it isextremelydif?cult toinfer theprivatekeyevenifthe public
keyisavailable.
The Advanced Encryption Standard (AES) is a symmetric-key encryption
algorithmthatwasadoptedasanencryptionstandardbytheU.S.governmentin
2000, and is now widely used. The standard is based on the Rijndael algorithm
(namedfortheinventorsV.Rijmenand J.Daemen).Thealgorithmoperatesona
128-bitblockofdataatatime,whilethekeycanbe128,192,or256bitsinlength.
9.8 Encryption andItsApplications 413
Thealgorithmrunsaseriesofstepstojumbleupthebitsinadatablockinaway
that can be reversed during decryption, and performs an XOR operation with a
128-bit “round key” that is derived from the encryption key. A new round key
is generated from the encryption key for each block of data that is encrypted.
During decryption, the round keys are generated again from the encryption key
and the encryption process is reversed to recover the original data. An earlier
standard called the Data Encryption Standard (DES), adopted in 1977, was very
widelyusedearlier.
For any symmetric-key encryption scheme to work, authorized users must
be provided with the encryption key via a secure mechanism. This requirement
is a major weakness, since the scheme is no more secure than the security of the
mechanismbywhich the encryptionkeyistransmitted.
Public-keyencryptionisanalternativeschemethatavoidssomeoftheprob-
lems faced by symmetric-key encryption techniques. It is based on two keys: a
publickeyandaprivatekey.EachuserU
i
hasapublickey E
i
andaprivatekey D
i
.
All public keys are published: They can be seen by anyone. Each private key is
known to only the one user to whom the key belongs. If user U
1
wants to store
encrypted data, U
1
encrypts them using public key E
1
. Decryption requires the
privatekey D
1
.
Because the encryption key for each user is public, it is possible to exchange
information securely by this scheme. If user U
1
wants to share data with U
2
, U
1
encrypts the data using E
2
, the public key of U
2
. Since only user U
2
knows how
todecryptthedata,information canbe transferredsecurely.
For public-key encryption to work, there must be a scheme for encryption
suchthatitisinfeasible(thatis,extremelyhard)todeducetheprivatekey,given
thepublickey.Such ascheme doesexistand isbasedontheseconditions:
• Thereisanef?cientalgorithmfortestingwhetherornotanumberisprime.
• Noef?cient algorithmis known for?nding theprimefactors ofanumber.
For purposes of this scheme, data are treated as a collection of integers. We
createapublickeybycomputingtheproductoftwolargeprimenumbers: P
1
and
P
2
.Theprivatekeyconsistsofthepair(P
1
, P
2
).Thedecryptionalgorithmcannot
be used successfully if only the product P
1
P
2
is known; it needs the individual
values P
1
and P
2
.Sinceallthatispublishedistheproduct P
1
P
2
,anunauthorized
user would need to be able to factor P
1
P
2
to steal data. By choosing P
1
and P
2
to be suf?ciently large (over 100 digits), we can make the cost of factoring P
1
P
2
prohibitivelyhigh(ontheorderofyearsofcomputationtime,oneventhefastest
computers).
Thedetailsofpublic-keyencryptionandthemathematicaljusti?cationofthis
technique’spropertiesarereferencedinthe bibliographicnotes.
Although public-keyencryption by this scheme is secure,itis alsocomputa-
tionallyveryexpensive.Ahybridschemewidelyusedforsecurecommunication
is as follows: a symmetric encryption key (based, for example, on AES)isran-
414 Chapter 9 Application DesignandDevelopment
domly generated and exchanged in a secure manner using a public-key encryp-
tion scheme, and symmetric-key encryption using that key is used on the data
transmittedsubsequently.
Encryption of small values, such as identi?ers or names, is made compli-
catedbythepossibilityofdictionaryattacks,particularlyiftheencryptionkeyis
publicly available. For example, if date-of-birth ?elds are encrypted, an attacker
tryingtodecryptaparticularencryptedvaluee couldtryencryptingeverypossi-
ble dateof birthuntil he ?ndsone whose encryptedvalue matches e. Evenifthe
encryptionkeyisnotpubliclyavailable,statisticalinformationaboutdatadistri-
butions can be used to ?gure out what an encrypted value represents in some
cases,suchasageorzipcode.Forexample,iftheage18isthemostcommonage
in a database, the encrypted age value that occurs most often can be inferred to
represent18.
Dictionary attacks can be deterred by adding extra random bits to the end
ofthe valuebeforeencryption(and removingthemafterdecryption).Such extra
bits,referredtoasaninitializationvectorin AES,orassaltbitsinothercontexts,
providegoodprotectionagainstdictionaryattack.
9.8.2 EncryptionSupportinDatabases
Many ?le systemsand database systemstodaysupportencryptionof data.Such
encryption protects the data from someone who is able to access the data, but is
not able to access the decryption key. In the case of ?le-system encryption, the
datatobeencryptedareusuallylarge?lesanddirectoriescontaininginformation
about?les.
Inthecontextofdatabases,encryptioncanbedoneatseveraldifferentlevels.
At the lowest level, the disk blocks containing database data can be encrypted,
usingakeyavailabletothe database-systemsoftware. Whenablock isretrieved
fromdisk,itis?rstdecryptedandthenusedintheusualfashion.Suchdisk-block-
level encryption protects against attackers who can access the disk contents but
donot haveaccesstothe encryptionkey.
Atthenexthigherlevel,speci?ed(orall)attributesofarelationcanbestored
in encryptedform.In thiscase, eachattribute ofa relationcould havea different
encryptionkey.Manydatabasestodaysupportencryptionatthelevelofspeci?ed
attributesaswellasatthelevelofanentirerelation,orallrelationsinadatabase.
Encryption of speci?ed attributes minimizes the overhead of decryption, by al-
lowing applications to encrypt only attributes that contain sensitive values such
ascredit-cardnumbers.However,whenindividualattributesorrelationsareen-
crypted,databasestypicallydonotallowprimaryandforeignkeyattributestobe
encrypted,anddonotsupportindexingonencryptedattributes.Encryptionalso
then needs to use extra random bits to prevent dictionary attacks, as described
earlier.
A decryption key is obviously required to get access to encrypted data. A
singlemasterencryptionkeymaybeusedforalltheencrypteddata;withattribute
levelencryption,differentencryptionkeyscouldbeusedfordifferentattributes.
9.8 Encryption andItsApplications 415
In this case, the decryption keys for different attributes can be stored in a ?le or
relation (often referred to as “wallet”), which is itself encrypted using a master
key.
A connection to the database that needs to access encrypted attributes must
then provide the master key; unless this is provided, the connection will not be
abletoaccessencrypteddata.Themasterkeywouldbestoredintheapplication
program(typicallyonadifferentcomputer),ormemorizedbythedatabaseuser,
and providedwhenthe userconnects tothe database.
Encryption at the database level has the advantage of requiring relatively
lowtimeandspaceoverhead,and doesnot requiremodi?cationofapplications.
For example, if data in a laptop computer database need to be protected from
theftofthecomputeritself,suchencryptioncanbeused.Similarly,someonewho
gets access to backup tapes of a database would not be able to access the data
contained inthebackups without knowing the decryptionkey.
Analternativetoperformingencryptioninthedatabaseistoperformitbefore
the data are sent to the database. The application must then encrypt the data
before sending it to the database, and decrypt the data when it is retrieved.This
approach to data encryption requires signi?cant modi?cations to be done to the
application,unlikeencryptionperformedinadatabasesystem.
9.8.3 EncryptionandAuthentication
Password-based authentication is used widely by operating systems as well as
databases.However,theuseofpasswordshassomedrawbacks,especiallyovera
network.Ifaneavesdropperisableto“sniff”thedatabeingsentoverthenetwork,
shemaybeableto?ndthepasswordasitisbeingsentacrossthenetwork.Once
theeavesdropperhasausernameandpassword,shecanconnecttothedatabase,
pretendingtobethelegitimateuser.
Amoresecureschemeinvolvesachallenge–response system.The database
system sends a challenge string to the user. The user encrypts the challenge
stringusingasecretpasswordasencryptionkeyandthenreturnstheresult.The
database system can verify the authenticity of the user by decrypting the string
withthesamesecretpasswordandcheckingtheresultwiththeoriginalchallenge
string.Thisschemeensuresthatnopasswordstravelacrossthe network.
Public-key systems can be used for encryption in challenge–response sys-
tems.Thedatabasesystemencryptsachallengestringusingtheuser’spublickey
and sends it to the user. The user decrypts the string using her private key, and
returns the result to the database system. The database system then checks the
response. This scheme has the added bene?t of not storing the secret password
inthedatabase,whereitcould potentiallybeseenbysystemadministrators.
Storing the private key of a user on a computer (even a personal computer)
has the risk that if the computer is compromised, the key may be revealed to an
attackerwhocanthenmasqueradeastheuser.Smartcardsprovideasolutionto
this problem. In a smart card, the key can be stored on an embedded chip; the
operatingsystemofthesmartcardguaranteesthatthekeycanneverberead,but
416 Chapter 9 Application DesignandDevelopment
allows data to be sent to the card for encryption or decryption, using the private
key.
8
9.8.3.1 DigitalSignatures
Another interestingapplication of public-key encryption is indigital signatures
toverifyauthenticityofdata;digitalsignaturesplaytheelectronicroleofphysical
signaturesondocuments.Theprivatekeyisusedto “sign,”thatis,encrypt,data,
and the signed data can be made public. Anyone can verify the signature by
decryptingthedatausingthepublickey ,butnoonecouldhavegeneratedthe
signeddata without having the privatekey.(Note the reversalof the roles of the
publicandprivatekeysinthisscheme.)Thus,wecanauthenticatethedata;that
is,wecanverifythatthedatawereindeedcreatedbythepersonwhoissupposed
tohavecreatedthem.
Furthermore,digitalsignaturesalsoservetoensurenonrepudiation.Thatis,
in case the person who created the data later claims she did not create it (the
electronic equivalent of claiming not to have signed the check), we can prove
thatthatpersonmusthavecreatedthedata(unlessherprivatekeywasleakedto
others).
9.8.3.2 DigitalCerti?cates
Authentication is, in general, a two-way process, where each of a pair of inter-
acting entities authenticates itself to the other. Such pairwise authentication is
needed even when a client contacts a Web site, to prevent a malicious site from
masquerading as a legal Web site. Such masquerading could be done, for exam-
ple,ifthenetworkrouterswerecompromised,anddatareroutedtothemalicious
site.
For a user to ensure that she is interacting with an authentic Web site, she
musthavethesite’spublickey.Thisraisestheproblemofhowtheusercangetthe
publickey–ifitisstoredontheWebsite,themalicioussitecouldsupplyadifferent
key, and the user would have no way of verifying if the supplied public key is
itselfauthentic.Authenticationcanbehandledbyasystemofdigitalcerti?cates,
whereby public keys are signed by a certi?cation agency, whose public key is
well known. Forexample, the public keys of the root certi?cation authorities are
storedinstandardWebbrowsers.Acerti?cateissuedbythemcanbeveri?edby
usingthestoredpublickeys.
A two-level system would place an excessive burden of creating certi?cates
on the root certi?cation authorities, so a multilevel system is used instead, with
one or more root certi?cation authorities and a tree of certi?cation authorities
below each root. Each authority (other than the root authorities) has a digital
certi?cate issuedbyitsparent.
A digital certi?cate issued by a certi?cation authority Aconsists of a public
key K
A
and an encrypted text E that can be decoded by using the public key
8
Smartcardsprovideotherfunctionalitytoo,suchastheabilitytostorecashdigitallyandmakepayments,whichisnot
relevantin ourcontext.
9.9 Summary 417
K
A
. The encrypted text contains the name of the party to whom the certi?cate
was issued and her public key K
c
. In case the certi?cation authority Ais not a
root certi?cation authority, the encryptedtext also contains the digital certi?cate
issued to Aby its parent certi?cation authority; this certi?cate authenticates the
key K
A
itself. (That certi?cate may in turn contain a certi?cate from a further
parentauthority,and soon.)
To verify a certi?cate, the encrypted text E is decrypted by using the public
key K
A
to retrieve the name of the party (that is, the name of the organization
owning the Web site); additionally, if A is not a root authority whose public
key is known to the veri?er, the public key K
A
is veri?ed recursively by using
the digitalcerti?catecontained within E;recursionterminateswhenacerti?cate
issued by the root authority is reached. Verifying the certi?cate establishes the
chain through which a particular site was authenticated, and providesthe name
and authenticatedpublickeyforthe site.
Digital certi?cates are widely used to authenticate Web sites to users, to
prevent malicious sites from masquerading as other Web sites. In the HTTPS
protocol (the secure version of the HTTP protocol), the site provides its digital
certi?cate to the browser, which then displays it to the user. If the user accepts
the certi?cate, the browser then uses the provided public key to encrypt data.
A malicious site will have access to the certi?cate, but not the private key, and
willthusnotbeabletodecryptthedatasentbythebrowser.Onlytheauthentic
site, which has the corresponding private key, can decrypt the data sent by the
browser. We note that public-/private-key encryption and decryption costs are
muchhigherthanencryption/decryptioncostsusingsymmetricprivatekeys.To
reduce encryption costs, HTTPS actually creates a one-time symmetric key after
authentication, and usesittoencryptdataforthe restofthesession.
Digital certi?cates can also be used for authenticating users. The user must
submit a digitalcerti?cate containing her publickeyto asite, which veri?esthat
the certi?cate has been signed by a trusted authority. The user’s public key can
then be used in a challenge–response system to ensure that the user possesses
thecorrespondingprivatekey,therebyauthenticatingthe user.
9.9 Summary
• Applicationprogramsthatusedatabasesasbackendsandinteractwithusers
have been around since the 1960s. Application architectures have evolved
over this period. Today most applications use Web browsers as their front
end,andadatabaseastheirbackend,withanapplicationserverinbetween.
• HTML provides the ability to de?ne interfaces that combine hyperlinks with
forms facilities. Web browsers communicate with Web servers by the HTTP
protocol. Web servers can pass on requests to application programs, and
returntheresultstothe browser.
• Web servers execute application programs to implement desired function-
ality. Servlets are a widely used mechanism to write application programs
418 Chapter 9 Application DesignandDevelopment
that run as part of the Web server process, in order to reduce overhead.
There are also many server-side scripting languages that are interpreted by
theWebserverand provideapplication-programfunctionality aspartofthe
Webserver.
• There are several client-side scripting languages—JavaScript is the most
widelyused—that providericheruserinteractionatthebrowserend.
• Complex applications usually have a multilayer architecture, including a
model implementing business logic, a controller, and a view mechanism to
display results. They may also include a data access layer that implements
an object-relational mapping. Many applications implement and use Web
services,allowingfunctions tobeinvokedover HTTP.
• A number of tools have been developedfor rapid application development,
and inparticulartoreducetheeffortrequiredtobuilduserinterfaces.
• Techniquessuchascachingofvariousforms,includingqueryresultcaching
and connection pooling, and parallel processing are used to improve appli-
cationperformance.
• Application developers must pay careful attention to security, to prevent
attackssuchas SQL injectionattacksand cross-sitescriptingattacks.
• SQLauthorizationmechanismsarecoarsegrainedandoflimitedvaluetoap-
plicationsthatdealwithlargenumbersofusers.Todayapplicationprograms
implement?ne-grained,tuple-levelauthorization,dealingwithalargenum-
ber of application users, completely outside the database system. Database
extensions to provide tuple-levelaccess control and to deal with large num-
bersofapplicationusershavebeendeveloped,butarenotstandardasyet.
• Protectingtheprivacyofdataisanimportanttaskfordatabaseapplications.
Many countries have legal requirements on protection of certain kinds of
data,suchascredit-cardinformation ormedicaldata.
• Encryption plays a key role in protecting information and in authentication
ofusersandWebsites.Symmetric-keyencryptionandpublic-keyencryption
are two contrasting but widely used approaches to encryption. Encryption
of certain sensitive data stored in databases is a legal requirement in many
countriesandstates.
• Encryptionalsoplaysakeyroleinauthenticationofuserstoapplications,of
Websitestousers,and fordigitalsignatures.
ReviewTerms
• Applicationprograms
• Webinterfacestodatabases
• HyperTextMarkupLanguage
(HTML)
PracticeExercises 419
• Hyperlinks
• Uniformresourcelocator(URL)
• Forms
• HyperTextTransferProtocol
(HTTP)
• CommonGatewayInterface
(CGI)
• Connectionlessprotocols
• Cookie
• Session
• ServletsandServletsessions
• Server-sidescripting
• JSP
• PHP
• ASP.NET
• Client-sidescripting
• JavaScript
• Document ObjectModel(DOM)
• Applets
• Applicationarchitecture
• Presentationlayer
• Model-view-controller(MVC)
architecture
• Business-logiclayer
• Data-accesslayer
• Object-relationalmapping
• Hibernate
• Webservices
• RESTfulservices
• Rapidapplicationdevelopment
• Webapplicationframeworks
• Reportgenerators
• Connection pooling
• Queryresultcaching
• Applicationsecurity
• SQL injection
• Cross-sitescripting(XSS)
• Cross-siterequestforgery(XSRF)
• Authentication
• Two-factor authentication
• Man-in-the-middleattack
• Centralauthentication
• Singlesign-on
• OpenID
• Virtual PrivateDatabase (VPD)
• Audittrail
• Encryption
• Symmetric-keyencryption
• Public-keyencryption
• Dictionaryattack
• Challenge–response
• Digitalsignatures
• Digitalcerti?cates
PracticeExercises
9.1 What is the main reason why servlets give better performance than pro-
grams that use the common gateway interface (CGI), even though Java
programsgenerallyrunslowerthan CorC++programs?
9.2 List some bene?ts and drawbacks of connectionless protocols over proto-
colsthatmaintain connections.
9.3 ConsideracarelesslywrittenWebapplicationforanonline-shoppingsite,
which stores the price of each item as a hidden form variable in the Web
page sent to the customer; when the customer submits the form, the in-
420 Chapter 9 Application DesignandDevelopment
formation from the hidden form variable is used to compute the bill for
the customer. What is the loophole in this scheme? (There was a real in-
stance where the loophole was exploitedby some customers of an online-
shoppingsite,beforetheproblemwasdetectedand?xed.)
9.4 Consider another carelessly writtenWeb application, which uses a servlet
that checks if there was an active session, but does not check if the user is
authorized to access that page, instead depending on the fact that a link
to the page is shown only to authorized users. What is the risk with this
scheme?(Therewasarealinstancewhereapplicantstoacollegeadmissions
site could, after logging into the Web site, exploit this loophole and view
information theywere not authorized tosee;the unauthorized accesswas
howeverdetected,andthosewhoaccessedtheinformationwerepunished
bybeingdeniedadmission.)
9.5 List three ways in which caching can be used to speed up Web server
performance.
9.6 The netstat command (available on Linux and on Windows) shows the
activenetworkconnectionsonacomputer.Explainhowthiscommandcan
beusedto?nd outifaparticularWebpageisnot closingconnections that
it opened, or if connection pooling is used, not returning connections to
the connection pool. You should account for the fact that with connection
pooling,the connection maynot getclosedimmediately.
9.7 Testing for SQL-injection vulnerability:
a. Suggest an approach for testing an application to ?nd if it is vulner-
ableto SQL injectionattacksontextinput.
b. Can SQL injection occur with other forms of input? If so, how would
youtestforvulnerability?
9.8 A database relation may have the values of certain attributes encrypted
forsecurity.Why dodatabasesystemsnotsupportindexingonencrypted
attributes?Using your answer to this question, explainwhy database sys-
temsdonotallowencryptionofprimary-keyattributes.
9.9 Exercise9.8addressestheproblemofencryptionofcertainattributes.How-
ever, some database systems support encryption of entire databases. Ex-
plainhow theproblemsraisedinExercise9.8areavoidedwhentheentire
databaseisencrypted.
9.10 Suppose someone impersonates a company and gets a certi?cate from a
certi?cate-issuingauthority.What istheeffectonthings(suchaspurchase
ordersorprograms)certi?edbytheimpersonatedcompany,andonthings
certi?edbyothercompanies?
9.11 Perhaps the most important data items in any database system are the
passwords that control access to the database. Suggest a scheme for the
secure storage of passwords. Be sure that your scheme allows the system
Exercises 421
to test passwords supplied by users who are attempting to log into the
system.
Exercises
9.12 Write a servlet and associated HTML code for the following very simple
application: A user is allowed to submit a form containing a value, say n,
and shouldgetaresponsecontaining n “*”symbols.
9.13 Write a servlet and associated HTML code for the following simple appli-
cation: A user is allowed to submit a form containing a number, say n,
and should get a response saying how many times the value n has been
submittedpreviously.Thenumberoftimeseachvaluehasbeensubmitted
previouslyshouldbe storedinadatabase.
9.14 Write a servlet that authenticates a user (based on user names and pass-
wordsstoredinadatabaserelation),andsetsasessionvariablecalleduserid
afterauthentication.
9.15 What is an SQL injection attack? Explain how it works, and what precau-
tionsmustbetakentoprevent SQL injectionattacks.
9.16 Write pseudocode to manage a connection pool. Your pseudocode must
includeafunctiontocreateapool(providingadatabaseconnectionstring,
database username, and passwordas parameters),afunction torequesta
connection fromthepool,aconnection toreleaseaconnection tothepool,
and afunction toclosethe connection pool.
9.17 Explaintheterms CRUDand REST.
9.18 Many Web sites today provide rich user-interfaces using Ajax. List two
featureseachofwhichrevealsifasiteusesAjax,withouthavingtolookat
the source code. Usingthe above features, ?nd three siteswhich use Ajax;
you can view the HTML source of the page to check if the site is actually
usingAjax.
9.19 XSSattacks:
a. What isan XSS attack?
b. Howcanthereferer?eldbeusedtodetectsome XSSattacks?
9.20 What is multi-factor authentication? How does it help safeguard against
stolenpasswords?
9.21 Consider the Oracle Virtual Private Database (VPD)featuredescribedin
Section9.7.5,and anapplicationbased onouruniversityschema.
a. What predicate (using a subquery) should be generated to allow
eachfacultymembertoseeonlytakestuplescorrespondingtocourse
sectionsthattheyhave taught?
422 Chapter 9 Application DesignandDevelopment
b. GiveanSQLquerysuchthatthequerywiththepredicateaddedgives
aresultthatisasubsetoftheoriginalqueryresultwithouttheadded
predicate.
c. GiveanSQLquerysuchthatthequerywiththepredicateaddedgives
aresultcontainingatuplethatisnotintheresultoftheoriginalquery
withouttheaddedpredicate.
9.22 Whataretwoadvantagesofencryptingdatastoredinthedatabase?
9.23 Supposeyouwishtocreateanaudittrailofchangestothe takes relation.
a. De?netriggerstocreateanaudittrail,loggingtheinformationintoa
relationcalled,forexample,takes trail.Theloggedinformationshould
includetheuser-id(assumeafunctionuser id()providesthisinforma-
tion) and a timestamp, in addition to old and new values. You must
alsoprovidetheschemaofthe takes trail relation.
b. Can the above implementation guarantee that updates made by a
malicious database administrator (or someone who manages to get
theadministrator’spassword)willbeintheaudittrail?Explainyour
answer.
9.24 HackersmaybeabletofoolyouintobelievingthattheirWebsiteisactually
a Web site (such as a bank or credit card Web site) that you trust. This
may be done by misleading email, or even by breaking into the network
infrastructureandreroutingnetworktraf?cdestinedfor,saymybank.com,to
thehacker’ssite.Ifyouenteryourusernameandpasswordonthehacker’s
site,the sitecan recordit, and use itlatertobreakintoyouraccount at the
realsite.WhenyouuseaURLsuchashttps://mybank.com,theHTTPSprotocol
isusedtopreventsuchattacks.Explainhowtheprotocolmightusedigital
certi?catestoverifyauthenticityofthe site.
9.25 Explainwhatisachallenge–response systemforauthentication. Whyisit
moresecurethanatraditionalpassword-basedsystem?
ProjectSuggestions
Eachofthefollowingisalargeproject,whichcanbeasemester-longprojectdone
by a group of students. The dif?culty of the project can be adjusted easily by
addingordeletingfeatures.
Project9.1 Pick your favorite interactive Web site, such as Bebo, Blogger, Face-
book, Flickr, Last.FM, Twitter, Wikipedia; these are just a few examples,
there are many more. Most of these sites manage a large amount of data,
and use databasesto store and process the data. Implementa subset of the
functionalityoftheWebsiteyoupicked.Clearly,implementingevenasig-
ni?cantsubsetofthefeaturesofsuchasiteiswellbeyondacourseproject,
ProjectSuggestions 423
butitispossibleto?ndasetoffeaturesthatisinterestingtoimplement,yet
smallenoughforacourseproject.
Most of today’s popular Web sites make extensive use of Javascript to
create rich interfaces. You may wish to go easy on this for your project, at
leastinitially,sinceittakestimetobuildsuchintefaces,andthenaddmore
features to your interfaces, as time permits. Make use of Web application
developmentframeworks,orJavascriptlibrariesavailableontheWeb,such
astheYahoo UserInterface library,tospeedupyourdevelopment.
Project9.2 Create a “mashup” which uses Web services such as Google or Ya-
hoomapsAPIstocreateaninteractiveWebsites.Forexample,themap APIs
provide a way to display a map on the Web page, with other information
overlayed on the maps. You could implement a restaurant recommenda-
tionsystem,withuserscontributinginformationaboutrestaurantssuchas
location, cuisine, price range, and ratings. Results of user searches could
be displayed on the map. You could allow Wikipedia-like features, such
as allowing users to add information and edit information added by other
users, along with moderators who can weed out malicious updates. You
could also implement social features, such as giving more importance to
ratingsprovidedbyyourfriends.
Project9.3 Your university probably uses a course-management systems such
as Moodle,Blackboard, orWebCT. Implementa subsetof the functionality
of such a course-management system. For example, you can provide as-
signmentsubmissionand gradingfunctionality,includingmechanismsfor
studentsandteachers/teaching-assistantstodiscussgradingofaparticular
assignment.Youcouldalsoprovidepollsandothermechanismsforgetting
feedback.
Project9.4 Consider the E-R schema of Practice Exercise 7.3 (Chapter 7), which
represents information about teams in a league. Design and implement a
Web-basedsystemtoenter,update,and viewthedata.
Project9.5 Design and implement a shopping cart system that lets shoppers
collectitemsintoashoppingcart(youcandecidewhatinformationistobe
suppliedforeachitem)andpurchasedtogether.Youcanextendandusethe
E-R schema of Exercise 7.20 of Chapter 7. You should check for availability
oftheitemand dealwithnonavailable itemsasyoufeelappropriate.
Project9.6 Design and implement a Web-based system to record student regis-
trationandgradeinformationforcoursesatauniversity.
Project9.7 Design and implement a system that permits recording of course
performanceinformation—speci?cally, themarksgiventoeachstudentin
eachassignmentorexamofacourse,andcomputationofa(weighted)sum
of marksto getthe total course marks. The number of assignments/exams
should not be prede?ned; that is, more assignments/exams can be added
atany time.The systemshould alsosupportgrading, permittingcutoffs to
bespeci?edforvariousgrades.
424 Chapter 9 Application DesignandDevelopment
Youmayalsowishtointegrateitwiththestudentregistrationsystemof
Project9.6(perhapsbeingimplementedbyanotherprojectteam).
Project9.8 Design and implement a Web-based system for booking classrooms
at your university. Periodic booking (?xed days/times each week for a
whole semester) must be supported. Cancellation of speci?c lectures in a
periodicbooking shouldalsobe supported.
You may also wish to integrate it with the student registration system
of Project 9.6 (perhaps being implemented by another project team) so
that classrooms can be booked for courses, and cancellations of a lecture
or addition of extra lectures can be noted at a single interface, and will be
re?ectedintheclassroombookingandcommunicatedtostudentsviaemail.
Project9.9 Designandimplementasystemformanagingonlinemultiple-choice
tests.Youshouldsupportdistributedcontributionofquestions(byteaching
assistants,forexample),editingofquestionsbywhoeverisinchargeofthe
course,andcreationoftestsfromtheavailablesetofquestions.Youshould
alsobeabletoadministertestsonline,eitherata?xedtimeforallstudents,
or at any time but with a time limit from start to ?nish (support one or
both), and give students feedback on their scores at the end of the allotted
time.
Project9.10 Design and implementa system for managing email customer ser-
vice. Incoming mail goes to a common pool. There is a set of customer
service agents who reply to email. If the email is part of an ongoing se-
ries of replies (tracked using the in-reply-to ?eld of email) the mail should
preferably be replied to by the same agent who replied earlier. The system
should track all incoming mail and replies, so an agent can see the history
ofquestionsfromacustomerbeforereplyingtoanemail.
Project9.11 Designandimplementasimpleelectronicmarketplacewhereitems
canbelistedforsaleorforpurchaseundervariouscategories(whichshould
formahierarchy).Youmayalsowishtosupportalertingservices,whereby
a user can register interest in items in a particular category, perhaps with
other constraints as well, without publicly advertising her interest, and is
noti?edwhen suchan itemislistedforsale.
Project9.12 DesignandimplementaWeb-basednewsgroupsystem.Usersshould
beabletosubscribetonewsgroups,andbrowsearticlesinnewsgroups.The
systemtracks which articleswere read by a user, so they are not displayed
again. Also providesearch for old articles. You may also wish to providea
rating service for articles, so that articles with high rating are highlighted,
permittingthe busyreadertoskiplow-ratedarticles.
Project9.13 Design and implement a Web-based system for managing a sports
“ladder.” Many people register, and may be given some initial rankings
(perhapsbasedonpastperformance).Anyonecanchallengeanyoneelseto
a match, and the rankings are adjusted according to the result. One simple
system for adjusting rankings just moves the winner ahead of the loser in
ProjectSuggestions 425
the rank order,incase thewinner was behind earlier.You can trytoinvent
morecomplicatedrank-adjustmentsystems.
Project9.14 Design and implement a publication-listing service. The service
should permit entering of information about publications, such as title,
authors, year, where the publication appeared, and pages. Authors should
be a separate entity with attributes such as name, institution, department,
email,address,andhome page.
Your application should support multiple views on the same data. For
instance, you should provide all publications by a given author (sorted by
year, for example), or all publications by authors from a given institution
ordepartment.Youshouldalsosupportsearchbykeywords,ontheoverall
databaseaswellaswithineachoftheviews.
Project9.15 A common task in any organization is to collect structured infor-
mation from a group of people. For example, a manager may need to ask
employees to enter their vacation plans, a professor may wish to collect
feedback on a particular topic from students, or a student organizing an
event may wish to allow other students to register for the event, or some-
one maywishtoconduct anonline voteonsome topic.
Createasystemthatwillallowuserstoeasilycreateinformationcollection
events.Whencreatinganevent,theeventcreatormustde?newhoiseligible
to participate; to do so, your system must maintain user information, and
allowpredicatesde?ningasubsetofusers.Theeventcreatorshouldbeable
tospecifyasetofinputs(withtypes,defaultvalues,andvalidationchecks)
that the users will have to provide. The event should have an associated
deadline, and the system should have the ability to send reminders to
users who have not yet submitted their information. The event creator
may be given the option of automatic enforcement of the deadline based
on a speci?ed date/time, or choosing to login and declare the deadline is
over. Statistics about the submissions should be generated—to do so, the
event creator may be allowed to create simple summaries on the entered
information.Theeventcreatormaychoosetomakesomeofthesummaries
public, viewable by all users, either continually (e.g., how many people
haveresponded)orafterthedeadline(e.g.,whatwastheaveragefeedback
score).
Project9.16 Create a library of functions to simplify creation of Web interfaces.
You must implement at least the following functions: a function to display
a JDBC result set (with tabular formatting), functions to create different
typesoftextandnumericinputs(withvalidationcriteriasuchasinputtype
and optional range, enforced at the client by appropriate JavaScript code),
functionstoinputdateandtimevalues(withdefaultvalues),andfunctions
to create menu items based on a result set. For extra credit, allow the user
to set style parameters such as colors and fonts, and provide pagination
supportinthetables(hiddenformparameterscanbeusedtospecifywhich
page is to be displayed). Build a sample database application to illustrate
theuseofthesefunctions.
426 Chapter 9 Application DesignandDevelopment
Project9.17 DesignandimplementaWeb-basedmultiusercalendarsystem.The
systemmusttrackappointmentsforeachperson,includingmultioccurrence
events,suchasweeklymeetings,andsharedevents(whereanupdatemade
bytheeventcreatorgetsre?ectedtoallthosewhosharetheevent).Provide
interfaces to schedule multiuser events, where an event creator can add a
number of users who are invited to the event. Provide email noti?cation
of events. For extra credits implement a Web service that can be used by a
reminderprogramrunningon theclientmachine.
Tools
Development of a Web application requires several software tools such as an
application server, a compiler, and an editor for a programming language such
as Java or C#, and other optional tools such as a Web server. There are several
integrateddevelopmentenvironmentsthatprovidesupportforWebapplication
development.Thetwomostpopularopen-source IDEsareEclipse,developedby
IBM,andNetbeans,developedbySunMicrosystems.Microsoft’sVisualStudiois
the mostwidelyused IDEintheWindows world.
TheApacheT omcat( jakarta.apache.org), Glass?sh (glass?sh.dev.java.net), JBoss
(jboss.org),andCaucho’sResin(www.caucho.com),areapplicationserversthatsup-
portservletsand JSP.TheApacheWebserver(apache.org)isthemostwidelyused
Webservertoday.Microsoft’s IIS(InternetInformationServices)isaWebandap-
plicationserverthatiswidelyusedonMicrosoftWindowsplatforms,supporting
Microsoft’s ASP.NET (msdn.microsoft.com/asp.net/).
IBM’s WebSphere (www.software.ibm.com) software provides a variety of soft-
ware tools for Web application development and deployment, including an ap-
plication server, an IDE, application integration middleware, business process
managementsoftwareand systemmanagementtools.
Some of the above tools are open-source software that can be used free of
cost, some arefreefornoncommercial use orforpersonal use,while othersneed
tobepaidfor.SeetherespectiveWebsitesformoreinformation.
The Yahoo! User Interface (YUI) JavaScript library (developer.yahoo.com/yui)is
widelyusedforcreatingJavaScriptprogramsthatworkacrossmultiplebrowsers.
BibliographicalNotes
Informationaboutservlets,includingtutorials,standardspeci?cations,andsoft-
ware,isavailableonjava.sun.com/products/servlet.InformationaboutJSPisavailable
at java.sun.com/products/jsp. Information on JSP tag libraries can also be found at
this URL. Information about the .NET framework and about Web application de-
velopmentusing ASP.NET canbe found at msdn.microsoft.com.
Atreyaetal.[2002]providetextbookcoverageofdigitalsignatures,including
X.509digitalcerti?catesandpublic-keyinfrastructure.
PART
3
DATASTORAGEAND
QUERYING
Although a database system provides a high-level view of data, ultimately data
have to be stored as bits on one or more storage devices. A vast majority of
databases today store data on magnetic disk (and, increasingly, on ?ash storage)
and fetch data into main memory for processing, or copy data onto tapes and
other backup devices for archival storage. The physical characteristics of storage
devices play a major role in the way data are stored, in particular because access
toarandompieceofdataondiskismuchslowerthanmemoryaccess:Diskaccess
takestensofmilliseconds,whereasmemoryaccesstakesatenthofamicrosecond.
Chapter 10 begins with an overview of physical storage media, including
mechanisms to minimize the chance of data loss due to device failures. The
chapterthendescribeshowrecordsaremappedto?les,whichinturnaremapped
to bits on the disk.
Many queries reference only a small proportion of the records in a ?le. An
indexisastructurethathelpslocatedesiredrecordsofarelationquickly,without
examining all records.The index in this textbook is an example, although, unlike
database indices, it is meant for human use. Chapter 11 describes several types
of indices used in database systems.
User queries have to be executed on the database contents, which reside on
storage devices. It is usually convenient to break up queries into smaller oper-
ations, roughly corresponding to the relational-algebra operations. Chapter 12
describes how queries are processed, presenting algorithms for implementing
individual operations, and then outlining how the operations are executed in
synchrony, to process a query.
Therearemanyalternativewaysofprocessingaquery,whichcanhavewidely
varying costs. Query optimizationrefersto the process of ?nding the lowest-cost
method of evaluating a given query. Chapter 13 describes the process of query
optimization.
427
This page intentionally left blank 
CHAPTER
10
Storage and File Structure
Inprecedingchapters,wehaveemphasizedthehigher-levelmodelsofadatabase.
For example, at the conceptual or logical level, we viewed the database, in the re-
lationalmodel,asacollectionoftables.Indeed,thelogicalmodelofthedatabase
is the correct level for database users to focus on. This is because the goal of a
database system is to simplify and facilitate access to data; users of the system
shouldnotbeburdenedunnecessarilywiththephysicaldetailsoftheimplemen-
tation of thesystem.
In this chapter, however, as well as in Chapters 11, 12, and 13, we probe be-
lowthe higher levelsas we describevariousmethods for implementingthe data
models and languages presentedin precedingchapters. We start with character-
istics of the underlying storage media, such as disk and tape systems. We then
de?ne various data structures that allow fast access to data. We consider several
alternative structures, each best suited to a different kind of access to data. The
?nalchoiceofdatastructureneedstobemadeonthebasisoftheexpecteduseof
the systemand of the physical characteristics of the speci?c machine.
10.1 OverviewofPhysicalStorageMedia
Severaltypesofdatastorageexistinmostcomputersystems.Thesestoragemedia
are classi?ed by the speed with which data can be accessed, by the cost per unit
of data to buy the medium, and by the medium’s reliability. Among the media
typicallyavailablearethese:
• Cache.Thecacheisthefastestandmostcostlyformofstorage.Cachememory
is relatively small; its use is managed by the computer system hardware.
We shall not be concerned about managing cache storage in the database
system. It is, however, worth noting that database implementors do pay
attention to cache effects when designing query processing data structures
and algorithms.
• Mainmemory.Thestoragemediumusedfordatathatareavailabletobeop-
eratedonismainmemory.Thegeneral-purposemachineinstructionsoperate
429
430 Chapter 10 Storage and File Structure
on main memory. Although main memory may contain severalgigabytes of
data on a personal computer, or even hundreds of gigabytes of data in large
server systems, it is generally too small (or too expensive) for storing the
entire database. The contents of main memory are usually lost if a power
failureorsystemcrash occurs.
• Flashmemory.Flashmemorydiffersfrommainmemoryinthat storeddata
areretainedevenifpoweristurnedoff(orfails).Therearetwotypesof?ash
memory, called NAND and NOR ?ash. Of these, NAND ?ash has a much
higher storage capacity for a given cost, and is widely used for data storage
indevicessuchascameras,musicplayers,andcellphones,andincreasingly,
in laptop computers as well. Flash memory has a lower cost per byte than
main memory, in addition to being nonvolatile; that is, it retains stored data
evenifpoweris switchedoff.
Flashmemoryisalsowidelyusedforstoringdatain“USBkeys,”whichcan
be plugged into the Universal Serial Bus (USB) slots of computing devices.
Such USB keys have become a popular means of transporting data between
computer systems (“?oppy disks” played the same role in earlier days, but
theirlimitedcapacity has madethemobsoletenow).
Flash memory is also increasingly used as a replacement for magnetic
disks for storing moderate amounts of data. Such disk-drive replacements
are called solid-state drives. As of 2009, a 64 GB solid-state hard drive costs
less than $200, and capacities range up to 160 GB. Further, ?ash memory
is increasingly being used in server systems to improve performance by
caching frequently used data, since it provides faster access than disk, with
largerstorage capacity thanmainmemory(for a givencost).
• Magnetic-disk storage. The primary medium for the long-term online stor-
age of data is the magnetic disk. Usually, the entire database is stored on
magnetic disk. The system must move the data from disk to main memory
so that they can be accessed. Afterthe systemhas performedthe designated
operations,the data thathave beenmodi?ed mustbe writtentodisk.
As of 2009, the size of magnetic disks ranges from 80 gigabytes to 1.5
terabytes, and a 1 terabyte disk costs about $100. Disk capacities have been
growingatabout50percentperyear,andwecanexpectdisksofmuchlarger
capacityeveryyear.Diskstoragesurvivespowerfailuresandsystemcrashes.
Disk-storage devices themselves may sometimes fail and thus destroy data,
butsuchfailuresusuallyoccurmuchlessfrequentlythandosystemcrashes.
• Optical storage. The most popular forms of optical storage are the compact
disk(CD),which canholdabout700megabytesofdataandhasaplaytimeof
about 80 minutes, and the digital video disk (DVD), which can hold 4.7 or 8.5
gigabytes of data per side of the disk (or up to 17 gigabytes on a two-sided
disk). The expression digital versatile disk is also used in place of digital
video disk,sinceDVDs can hold any digital data, not just video data. Data
are stored optically on a disk, and are read by a laser. A higher capacity
formatcalled Blu-ray DVDcanstore27gigabytesperlayer,or54gigabytesin
a double-layerdisk.
10.1 Overview of Physical Storage Media 431
Theoptical disksusedinread-only compact disks(CD-ROM)orread-only
digitalvideodisks(DVD-ROM)cannot be written,but aresuppliedwith data
prerecorded. There are also “record-once” versions of compact disk (called
CD-R)anddigitalvideodisk(called DVD-Rand DVD+R),whichcanbewritten
only once; such disks are also called write-once, read-many (WORM)disks.
Therearealso “multiple-write”versionsofcompact disk(called CD-RW)and
digital video disk (DVD-RW, DVD+RW,andDVD-RAM), which can be written
multipletimes.
Optical disk jukebox systems contain a few drives and numerous disks
that can be loaded into one of the drives automatically (by a robot arm) on
demand.
• Tape storage. Tape storage is used primarily for backup and archival data.
Althoughmagnetictapeischeaperthandisks,accesstodataismuchslower,
because the tape must be accessed sequentially from the beginning. For this
reason, tape storage is referred to as sequential-access storage. In contrast,
disk storage is referred to as direct-access storage because it is possible to
readdata fromany location on disk.
Tapes have a high capacity (40- to 300-gigabyte tapes are currently avail-
able), and can be removed from the tape drive, so they are well suited to
cheaparchivalstorage.Tapelibraries(jukeboxes)areusedtoholdexception-
allylargecollectionsofdatasuchasdatafromsatellites,whichcouldinclude
as much as hundreds of terabytes (1 terabyte = 10
12
bytes), or even multiple
petabytes(1 petabyte=10
15
bytes)of data ina fewcases.
The various storage media can be organized in a hierarchy (Figure 10.1)
according to their speed and their cost. The higher levels are expensive, but are
fast. As we move down the hierarchy, the cost per bit decreases, whereas the
accesstimeincreases.Thistrade-offisreasonable;ifagivenstoragesystemwere
both faster and less expensive than another—other properties being the same
—then there would be no reason to use the slower, more expensive memory. In
fact, many early storage devices, including paper tape and core memories, are
relegatedtomuseumsnowthatmagnetictapeandsemiconductormemoryhave
become faster and cheaper. Magnetic tapes themselves were used to store active
databackwhendiskswereexpensiveandhadlowstoragecapacity.Today,almost
allactivedataarestoredondisks,exceptinveryrarecaseswheretheyarestored
on tapeor in opticaljukeboxes.
The fastest storage media—for example, cache and main memory—are re-
ferred to as primary storage. The media in the next level in the hierarchy—for
example, magnetic disks—are referred to as secondary storage,oronline stor-
age. Themediainthe lowestlevelinthehierarchy—for example,magnetictape
andoptical-diskjukeboxes—arereferredtoastertiarystorage,orof?inestorage.
Inadditiontothespeedandcost ofthevariousstoragesystems,thereisalso
the issue of storage volatility.Volatile storage loses its contents when the power
to the device is removed. In the hierarchy shown in Figure 10.1, the storage
systems from main memory up are volatile, whereas the storage systems below
432 Chapter 10 Storage and File Structure
cache
main memory
flash memory
magnetic disk
optical disk
magnetic tapes
Figure10.1 Storage device hierarchy.
main memory are nonvolatile. Data must be written to nonvolatile storage for
safekeeping.Weshall returnto thissubject inChapter16.
10.2 MagneticDiskandFlashStorage
Magnetic disks provide the bulk of secondary storage for modern computer
systems.Althoughdiskcapacitieshavebeengrowingyearafteryear,thestorage
requirementsoflargeapplicationshavealsobeengrowingveryfast,insomecases
even faster than the growth rate of disk capacities. A very large database may
requirehundredsofdisks.Inrecentyears,?ash-memorystoragesizeshavegrown
rapidly,and?ashstorageisincreasinglybecomingacompetitortomagneticdisk
storagefor severalapplications.
10.2.1 PhysicalCharacteristicsofDisks
Physically, disks are relatively simple (Figure 10.2). Each disk platter has a ?at,
circular shape. Its two surfaces are covered with a magnetic material, and infor-
mationisrecordedonthe surfaces.Plattersaremadefrom rigidmetalor glass.
When the disk is in use, a drive motor spins it at a constant high speed
(usually60,90,or120revolutionspersecond,butdisksrunningat250revolutions
per second are available). There is a read–write head positioned just above the
surface of the platter. The disk surface is logically divided into tracks,which
are subdivided into sectors.Asector is the smallest unit of information that can
be read from or written to the disk. In currently available disks, sector sizes are
10.2 Magnetic Disk and Flash Storage 433
track t
sector s
spindle
cylinder c
platter
arm
read–write
head
arm assembly
rotation
Figure10.2 Moving head disk mechanism.
typically 512 bytes; there are about 50,000 to 100,000 tracks per platter, and 1 to
5 platters per disk. The inner tracks (closer to the spindle) are of smaller length,
and in current-generation disks, the outer tracks contain more sectors than the
innertracks;typicalnumbersarearound500to1000sectorspertrackintheinner
tracks,andaround1000to2000sectorspertrackintheoutertracks.Thenumbers
varyamong differentmodels;higher-capacity modelsusually havemoresectors
pertrackandmoretracksoneachplatter.
Theread–writeheadstoresinformationonasectormagneticallyasreversals
of thedirectionof magnetization ofthe magneticmaterial.
Each side of a platter of a disk has a read–write head that moves across the
plattertoaccessdifferenttracks.Adisktypicallycontainsmanyplatters,andthe
read–writeheadsofallthetracksaremountedonasingleassemblycalledadisk
arm, and move together. The disk platters mounted on a spindle and the heads
mounted on a disk arm are together known as head–disk assemblies.Sincethe
headsonalltheplattersmovetogether,whentheheadononeplatterisontheith
track, the heads on all other platters are also on the ith track of their respective
platters.Hence,theithtracksofalltheplatterstogetherarecalledtheithcylinder.
Today, disks with a platter diameter of 3
1
2
inches dominate the market. They
have a lower cost and faster seek times (due to smaller seek distances) than do
the larger-diameter disks (up to 14 inches) that were common earlier, yet they
provide high storage capacity. Disks with even smaller diameters are used in
portable devices such as laptop computers, and some handheld computers and
portablemusic players.
The read–write heads are kept as close as possible to the disk surface to
increase the recording density. The head typically ?oats or ?ies only microns
434 Chapter 10 Storage and File Structure
from the disk surface; the spinning of the disk creates a small breeze, and the
head assembly is shaped so that the breeze keeps the head ?oating just above
the disk surface. Because the head ?oats so close to the surface, platters must be
machined carefullyto be?at.
Head crashes can be a problem. If the head contacts the disk surface, the
head can scrape the recordingmediumoff the disk,destroyingthe data that had
been there. In older-generation disks, the head touching the surface caused the
removedmedium to become airborne and tocome between the other heads and
their platters, causing more crashes; a head crash could thus result in failure of
the entire disk. Current-generation disk drives use a thin ?lm of magnetic metal
as recording medium. They are much less susceptible to failure by head crashes
than theolderoxide-coateddisks.
A disk controller interfaces between the computer system and the actual
hardware of the disk drive; in modern disk systems, the disk controller is im-
plemented within the disk drive unit. A disk controller accepts high-level com-
mands to read or write a sector, and initiates actions, such as moving the disk
arm to the right track and actually reading or writing the data. Disk controllers
also attach checksums to each sector that is written; the checksum is computed
from the data written to the sector. When the sector is read back, the controller
computes the checksum again from the retrieved data and compares it with the
stored checksum; if the data are corrupted, with a high probability the newly
computedchecksumwillnotmatchthestoredchecksum.Ifsuchanerroroccurs,
thecontrollerwillretrythereadseveraltimes;iftheerrorcontinues tooccur,the
controllerwill signala readfailure.
Another interesting task that disk controllers perform is remapping of bad
sectors.Ifthecontrollerdetectsthatasectorisdamagedwhenthediskisinitially
formatted,orwhenanattemptismadetowritethesector,itcanlogicallymapthe
sector to a different physical location (allocated from a pool of extra sectors set
asideforthispurpose).Theremappingisnotedondiskorinnonvolatilememory,
and the write is carriedout on the new location.
Disksareconnectedtoacomputersystemthroughahigh-speedinterconnec-
tion.Thereareanumberofcommoninterfacesforconnectingdiskstocomputers
of which the most commonly used today are (1) SATA (which stands for serial
ATA,
1
andanewerversionofSATAcalledSATAIIorSATA3Gb(olderversionsofthe
ATA standardcalled PATA,orParallel ATA,andIDE,werewidelyusedearlier,and
are still available), (2) small-computer-system interconnect (SCSI;pronounced
“scuzzy”),(3)SAS(whichstandsforserialattachedSCSI),and(4)theFibreChan-
nel interface. Portable external disk systems often use the USB interface or the
IEEE1394 FireWireinterface.
Whiledisksareusuallyconnecteddirectlybycablestothediskinterfaceofthe
computer system, they can be situated remotely and connected by a high-speed
network to the disk controller. In thestorageareanetwork(SAN)architecture,
large numbers of disks are connected by a high-speed network to a number
1
ATA is astorage-device connectionstandard fromthe 1980s.
10.2 Magnetic Disk and Flash Storage 435
of server computers. The disks are usually organized locally using a storage
organization technique called redundant arrays of independent disks (RAID)
(describedlater,inSection10.3), to givethe serversa logicalviewof a verylarge
andveryreliabledisk.Thecomputerandthedisksubsystemcontinuetousethe
SCSI, SAS, or Fiber Channel interface protocols to talk with each other, although
theymaybeseparatedbyanetwork.Remoteaccesstodisksacrossastoragearea
network means that disks can be shared by multiple computers that could run
different parts of an application in parallel. Remote access also means that disks
containingimportantdatacanbekeptinacentralserverroomwheretheycanbe
monitored and maintained by system administrators, instead of being scattered
in differentpartsof an organization.
Network attached storage (NAS) is an alternative to SAN. NAS is much like
SAN, except that instead of the networked storage appearing to be a large disk,
it provides a ?le system interface using networked ?le system protocols such as
NFS or CIFS.
10.2.2 PerformanceMeasuresofDisks
Themainmeasuresofthequalitiesofadiskarecapacity,accesstime,data-transfer
rate,andreliability.
Access time is the time from when a read or write request is issued to when
data transfer begins. To access (that is, to read or write) data on a given sector of
adisk,thearm?rstmustmovesothatitispositionedoverthecorrecttrack,and
then must wait for the sector to appear under it as the disk rotates. The time for
repositioning the arm is called the seek time, and it increases with the distance
that the arm must move. Typical seek times range from 2 to 30 milliseconds,
depending on how far the track is from the initial arm position. Smaller disks
tendto havelowerseektimessince theheadhas totravela smallerdistance.
The average seek time is the average of the seek times, measured over a
sequence of (uniformly distributed)random requests.If all tracks have the same
numberofsectors,andwedisregardthetimerequiredfortheheadtostart
movingandtostopmoving,wecanshowthattheaverageseektimeisone-third
theworst-caseseektime.Takingthesefactorsintoaccount,theaverageseektime
isaroundone-halfofthemaximumseektime.Averageseektimescurrentlyrange
between4 and 10 milliseconds,dependingonthediskmodel.
Once the head has reached the desired track, the time spent waiting for the
sector to be accessed to appear under the head is called the rotational latency
time. Rotational speeds of disks today range from 5400 rotations per minute (90
rotationspersecond)upto15,000rotationsperminute(250rotationspersecond),
or, equivalently, 4 milliseconds to 11.1 milliseconds per rotation. On an average,
one-halfofarotationofthediskisrequiredforthebeginningofthedesiredsector
to appear under the head. Thus, the average latency time of the disk is one-half
thetimeforafullrotationofthedisk.
The access time is then the sum of the seek time and the latency, and ranges
from8to20milliseconds.Oncethe?rstsectorofthedatatobeaccessedhascome
under the head, data transfer begins. The data-transfer rate is the rate at which
436 Chapter 10 Storage and File Structure
data can be retrieved from or stored to the disk. Current disk systems support
maximum transfer rates of 25 to 100 megabytes per second; transfer rates are
signi?cantly lower than the maximum transfer rates for inner tracks of the disk,
sincethey havefewersectors.Forexample,a diskwithamaximum transferrate
of 100 megabytes per second may have a sustained transfer rate of around 30
megabytespersecondon itsinnertracks.
The?nalcommonlyusedmeasureofadiskisthemeantimetofailure(MTTF),
whichisameasureofthereliabilityofthedisk.Themeantimetofailureofadisk
(orofanyothersystem)istheamountoftimethat,onaverage,wecanexpectthe
system to run continuously without any failure. According to vendors’ claims,
the mean timeto failure of diskstoday ranges from 500,000 to 1,200,000 hours—
about 57 to 136 years. In practice the claimed mean time to failure is computed
on the probability of failure when the disk is new—the ?gure means that given
1000 relatively new disks, if the MTTF is 1,200,000 hours, on an average one of
them will fail in 1200 hours. A mean time to failure of 1,200,000 hours does not
implythatthediskcanbeexpectedtofunctionfor136years!Mostdiskshavean
expected life span of about 5 years, and have signi?cantly higher rates of failure
once they become more than a fewyearsold.
Disk drives for desktop machines typically support the Serial ATA(SATA)in-
terface, which supports 150 megabytes per second, or the SATA-II 3Gb interface,
whichsupports300megabytespersecond.The PATA 5interfacesupportedtrans-
fer rates of 133 megabytes per second. Disk drives designed for server systems
typicallysupportthe Ultra320 SCSI interface,which providestransferratesof up
to320megabytespersecond,ortheSerialAttached SCSI(SAS)interface,versions
ofwhichprovidetransferratesof3or6gigabitspersecond.Storageareanetwork
(SAN) devices, which are connected to servers by a network, typically use Fiber
Channel FC 2-Gb or 4-Gb interface, which provides transfer rates of up to 256 or
512 megabytespersecond. Thetransfer rateof aninterfaceis sharedbetweenall
disks attached to the interface, except for the serial interfaces which allow only
one disktobe connected toeachinterface.
10.2.3 Optimization ofDisk-BlockAccess
Requests for disk I/O are generated both by the ?le system and by the virtual
memory manager found in most operating systems. Each request speci?es the
addressonthedisktobereferenced;thataddressisintheformofa block number.
Ablockisalogicalunitconsistingofa?xednumberofcontiguoussectors.Block
sizesrangefrom512bytestoseveralkilobytes.Dataaretransferredbetweendisk
and main memory in units of blocks. The term page is often used to refer to
blocks, although in a few contexts (such as ?ash memory) they refer to different
things.
Asequenceof requestsfor blocks fromdiskmay beclassi?edasasequential
accesspatternorarandomaccesspattern.Inasequentialaccesspattern,succes-
siverequestsareforsuccessiveblocknumbers,whichareonthesametrack,oron
adjacent tracks. To read blocks in sequential access, a disk seek may be required
for the ?rst block, but successive requests would either not require a seek, or
10.2 Magnetic Disk and Flash Storage 437
require a seek to an adjacent track, which is faster than a seek to a track that is
fartheraway.
In contrast, in a random access pattern, successive requests are for blocks
that are randomly located on disk. Each such request would require a seek. The
numberofrandomblockaccessesthatcanbesatis?edbyasinglediskinasecond
depends on the seek time, and is typically about 100 to 200 accesses per second.
Since only a small amount (one block) of data is read per seek, the transfer rate
issigni?cantly lowerwitha randomaccess patternthan witha sequentialaccess
pattern.
A number of techniques have been developed for improving the speed of
access to blocks.
• Buffering. Blocks that are read from disk are stored temporarily in an in-
memory buffer, to satisfy future requests. Buffering is done by both the op-
erating system and the database system. Database buffering is discussed in
moredetailinSection10.8.
• Read-ahead.Whenadiskblockisaccessed,consecutiveblocksfromthesame
track are read into an in-memory buffer even if there is no pending request
for the blocks. In the case of sequential access, such read-ahead ensures that
manyblocksarealreadyinmemorywhentheyarerequested,andminimizes
the time wasted in disk seeks and rotational latency per block read. Operat-
ing systems also routinely perform read-ahead for consecutive blocks of an
operating system ?le. Read-ahead is, however, not very useful for random
block accesses.
• Scheduling. If several blocks from a cylinder need to be transferred from
disk to main memory, we may be able to save access time by requesting the
blocks in the order in which they will pass under the heads. If the desired
blocksareondifferentcylinders,itisadvantageoustorequesttheblocksinan
orderthatminimizesdisk-armmovement.Disk-arm–schedulingalgorithms
attempt to order accesses to tracks in a fashion that increases the number of
accesses that can be processed. A commonly used algorithm is the elevator
algorithm, which works in the same way many elevators do. Suppose that,
initially, the arm is moving from the innermost track toward the outside of
the disk. Under the elevator algorithm’s control, for each track for which
there is an access request, the arm stops at that track, services requests for
the track, and then continues moving outward until there are no waiting
requests for tracks farther out. At this point, the arm changes direction, and
moves toward the inside, again stopping at each track for which there is a
request, until it reaches a track where there is no request for tracks farther
toward the center. Then, it reverses direction and starts a new cycle. Disk
controllers usually perform the task of reordering read requests to improve
performance,sincetheyareintimatelyawareoftheorganizationofblockson
disk, of the rotational position of the disk platters, and of the position of the
diskarm.
438 Chapter 10 Storage and File Structure
• File organization. To reduce block-access time, we can organize blocks on
disk in a way that corresponds closely to the way we expect data to be
accessed. For example, if we expect a ?le to be accessed sequentially, then
we should ideally keep all the blocks of the ?le sequentially on adjacent
cylinders. Older operating systems, such as the IBM mainframe operating
systems, providedprogrammers ?ne control on placement of ?les, allowing
a programmer to reserve a set of cylinders for storing a ?le. However, this
controlplacesaburdenontheprogrammerorsystemadministratortodecide,
forexample,howmanycylinderstoallocatefora?le,andmayrequirecostly
reorganizationifdataareinsertedto ordeletedfrom the?le.
Subsequent operating systems, such as Unix and Microsoft Windows,
hide the disk organization from users, and manage the allocation internally.
Although they do not guarantee that all blocks of a ?le are laid out sequen-
tially,they allocatemultipleconsecutiveblocks (anextent) at atimetoa?le.
Sequential access to the ?le then only needs one seek per extent, instead of
one seek per block. Over time, a sequential ?le that has multiple small ap-
pends may become fragmented; that is, its blocks become scattered all over
thedisk.Toreducefragmentation,thesystemcanmakeabackupcopyofthe
data on disk and restore the entire disk. The restore operation writes back
theblocksofeach?lecontiguously(ornearlyso).Somesystems(suchasdif-
ferentversionsofthe Windowsoperatingsystem)haveutilitiesthatscanthe
disk and then move blocks to decrease the fragmentation. The performance
increasesrealizedfromthesetechniquescan belarge.
• Nonvolatile write buffers. Since the contents of main memory are lost in
a power failure, information about database updates has to be recorded on
disk to survive possible system crashes. For this reason, the performance of
update-intensive database applications, such as transaction-processing sys-
tems,is heavilydependentonthe speedofdiskwrites.
We can use nonvolatile random-access memory (NVRAM) to speed up
disk writes drastically. The contents of NVRAM are not lost in power failure.
A common way to implement NVRAM is to use battery–backed-up RAM,al-
though ?ash memory is also increasingly being used for nonvolatile write
buffering. The idea is that, when the database system (or the operating sys-
tem) requests that a block be written to disk, the disk controller writes the
block to an NVRAM buffer, and immediately noti?es the operating system
that the write completed successfully. The controller writes the data to their
destination on disk whenever the disk does not have any other requests, or
when the NVRAM buffer becomes full. When the database system requests a
block write, it notices a delay only if the NVRAM buffer is full. On recovery
from a system crash, any pending buffered writes in the NVRAM are written
back to the disk. NVRAM buffers are found in certain high end disks, but are
morefrequentlyfound in “RAID controllers”;westudyRAID inSection10.3.
• Logdisk.Anotherapproachtoreducingwritelatenciesistousealogdisk—
thatis,adiskdevotedtowritingasequentiallog—inmuchthesamewayas
a nonvolatile RAM buffer. All access to the log disk is sequential, essentially
10.2 Magnetic Disk and Flash Storage 439
eliminatingseektime,andseveralconsecutiveblockscanbewrittenatonce,
making writes to the log disk several times faster than random writes. As
before, the data have to be written to their actual location on disk as well,
but the log disk can do the write later, without the database system having
to wait for the write to complete. Furthermore, the log disk can reorder the
writes to minimize disk-arm movement. If the system crashes before some
writes to the actual disk location have completed, when the system comes
backupitreadsthelogdiskto?ndthosewritesthathadnotbeencompleted,
and carriesthemout then.
File systems that support log disks as above are called journaling ?le
systems.Journaling?lesystemscanbeimplementedevenwithoutaseparate
log disk, keeping data and the log on the same disk. Doing so reduces the
monetarycost, atthe expenseof lowerperformance.
Mostmodern?lesystemsimplementjournaling,andusethelogdiskwhen
writing internal ?le system information such as ?le allocation information.
Earlier-generation?lesystemsallowedwrite reorderingwithout usinga log
disk, and ran the risk that the ?le system data structures on disk would be
corruptedifthesystemcrashed.Suppose,forexample,thata?lesystemused
a linked list, and inserted a new node at the end by ?rst writing the data for
the new node, then updating the pointer from the previous node. Suppose
also that the writes were reordered, so the pointer was updated ?rst, and
the system crashes before the new node is written. The contents of the node
would then be whatever junk was on disk earlier, resulting in a corrupted
datastructure.
To deal with the possibility of such data structure corruption, earlier-
generation ?le systems had to perform a ?le system consistency check on
system restart, to ensure that the data structures were consistent. And if
they were not, extra steps had to be taken to restore them to consistency.
These checks resulted in long delays in system restart after a crash, and the
delaysbecameworseasdisksystemsgrewtohighercapacities.Journaling?le
systemsallowquickrestartwithouttheneedforsuch?lesystemconsistency
checks.
However,writesperformedbyapplications areusuallynot writtentothe
logdisk.Databasesystemsimplementtheirownformsoflogging,whichwe
studylaterinChapter16.
10.2.4 FlashStorage
AsmentionedinSection10.1,therearetwotypesof?ashmemory,NOR?ashand
NAND?ash.NOR?ashallowsrandomaccesstoindividualwordsofmemory,and
has read time comparable to main memory. However, unlike NOR ?ash, reading
from NAND ?ash requires an entire page of data, typically consisting of between
512 and 4096 bytes, to be fetched from NAND ?ash into main memory. Pages in
a NAND ?ash are thus similar to sectors in a magnetic disk. But NAND ?ash is
signi?cantly cheaper than NOR ?ash, and has much higher storage capacity, and
isby far themorewidelyused.
440 Chapter 10 Storage and File Structure
Storage systems built using NAND ?ash provide the same block-oriented
interfaceasdiskstorage.Comparedtomagneticdisks,?ashmemorycanprovide
much faster random access: a page of data can be retrieved in around 1 or 2
microseconds from ?ash, whereas a random access on disk would take 5 to 10
milliseconds. Flash memory has a lower transfer rate than magnetic disks, with
20megabytespersecondbeingcommon.Somemorerecent?ashmemorieshave
increasedtransferratesof100to200megabytespersecond.However,solidstate
drives use multiple ?ash memory chips in parallel, to increase transfer rates to
over200 megabytespersecond, which isfasterthantransfer ratesofmost disks.
Writes to ?ash memory are a little more complicated. A write to a page of
?ashmemorytypicallytakesafewmicroseconds.However,oncewritten,apage
of ?ash memory cannot be directly overwritten. Instead, it has to be erased and
rewritten subsequently. The erase operation can be performed on a number of
pages,calledaneraseblock,atonce,andtakesabout1to2milliseconds.Thesize
of an erase block (often referred to as just “block” in ?ash literature) is usually
signi?cantly larger than the block size of the storage system. Further, there is a
limit to how many times a ?ash page can be erased, typically around 100,000 to
1,000,000times.Oncethislimitisreached,errorsinstoringbitsarelikelytooccur.
Flashmemorysystemslimitthe impact of boththe slow erasespeedand the
updatelimitsbymappinglogicalpagenumberstophysicalpagenumbers.When
alogicalpageisupdated,itcanberemappedtoanyalreadyerasedphysicalpage,
and the original location can be erasedlater.Each physical page has a smallarea
of memory where its logical addressis stored; if the logical address is remapped
toadifferentphysicalpage,theoriginalphysicalpageismarkedasdeleted.Thus
byscanningthephysicalpages,wecan?ndwhereeachlogicalpageresides.The
logical-to-physicalpagemappingisreplicatedinanin-memorytranslationtable
for quickaccess.
Blockscontainingmultipledeletedpagesareperiodicallyerased,takingcare
to ?rst copy nondeleted pages in those blocks to a different block (the transla-
tion table is updated for these nondeleted pages). Since each physical page can
be updated only a ?xed number of times, physical pages that have been erased
many times are assigned “cold data,” that is, data that are rarely updated, while
pages that have not been erased many times are used to store “hot data,” that is,
data that are updated frequently. This principle of evenly distributing erase op-
erationsacross physical blocks iscalledwear leveling,andisusually performed
transparently by ?ash-memory controllers. If a physical page is damaged due to
anexcessivenumberofupdates,itcanberemovedfromusage,withoutaffecting
the ?ash memoryasa whole.
All the above actions are carried out by a layer of software called the ?ash
translation layer; above this layer, ?ash storage looks identical to magnetic disk
storage,providingthesamepage/sector-orientedinterface,exceptthat?ashstor-
age is much faster. File systems and database storage structures can thus see an
identical logical view of the underlying storage structure, regardless of whether
itis ?ashormagneticstorage.
Hybrid disk drives are hard-disk systems that combine magnetic storage
with a smaller amount of ?ash memory, which is used as a cache for frequently
10.3 RAID 441
accessed data. Frequently accessed data that are rarely updated are ideal for
caching in ?ash memory.
10.3 RAID
Thedata-storagerequirementsofsomeapplications(inparticularWeb,database,
and multimedia applications) have been growing so fast that a large number of
disksareneededtostoretheirdata,eventhoughdisk-drivecapacitieshavebeen
growing veryfast.
Having a large number of disks in a system presents opportunities for im-
provingtherateatwhich datacan bereadorwritten,ifthedisksareoperatedin
parallel. Several independent reads or writes can also be performed in parallel.
Furthermore, this setup offers the potential for improving the reliability of data
storage, because redundant information can be stored on multiple disks. Thus,
failureof onediskdoesnot leadto lossof data.
A variety of disk-organization techniques, collectively called redundant ar-
rays of independent disks (RAID), have been proposed to achieve improved
performanceand reliability.
In the past, system designers viewed storage systems composed of several
small, cheap disks as a cost-effective alternative to using large, expensive disks;
thecostpermegabyteofthesmallerdiskswaslessthanthatoflargerdisks.Infact,
the I in RAID, which now stands for independent, originally stood for inexpensive.
Today,however,alldisksarephysicallysmall,andlarger-capacitydisksactually
havealowercostpermegabyte.RAIDsystemsareusedfortheirhigherreliability
and higher performance rate, rather than for economic reasons. Another key
justi?cationfor RAIDuse iseasiermanagement andoperations.
10.3.1 Improvement ofReliabilityviaRedundancy
Let us ?rst consider reliability. The chance that at least one disk out of a set of
N disks will fail is much higher than the chance that a speci?c single disk will
fail. Suppose that the mean time to failure of a disk is 100,000 hours, or slightly
over11years.Then,themeantimetofailureofsomediskinanarrayof100disks
will be 100,000/100 = 1000 hours, or around 42 days, which is not long at all! If
we store only one copy of the data, then each disk failure will result in loss of a
signi?cantamountofdata(asdiscussedinSection10.2.1).Suchahighfrequency
of dataloss isunacceptable.
The solution to the problem of reliabilityis to introduce redundancy;thatis,
we store extra information that is not needed normally, but that can be used in
the event of failure of a disk to rebuild the lost information. Thus, even if a disk
fails,dataarenotlost,sotheeffectivemeantimetofailureisincreased,provided
that we count only failuresthatlead to lossof dataor tononavailability of data.
Thesimplest(butmostexpensive)approachtointroducingredundancyisto
duplicate every disk. This technique is called mirroring (or, sometimes, shadow-
ing).Alogicaldiskthenconsistsoftwophysicaldisks,andeverywriteiscarried
442 Chapter 10 Storage and File Structure
out on both disks. If one of the disks fails, the data can be read from the other.
Datawillbelostonlyiftheseconddiskfailsbeforethe?rstfaileddiskisrepaired.
The meantimeto failure(where failureistheloss of data)of a mirroreddisk
depends on the mean time to failure of the individual disks, as well as on the
meantimetorepair,whichisthetimeittakes(onanaverage)toreplaceafailed
disk and to restore the data on it. Suppose that the failures of the two disks are
independent;thatis,thereisnoconnectionbetweenthefailureofonediskandthe
failure of the other. Then, if the mean time to failure of a single disk is 100,000
hours, and the mean time to repair is 10 hours, the mean time to data loss of
a mirrored disk system is 100,000
2
/(2 ?10) = 500 ? 10
6
hours, or 57,000 years!
(We do not go into the derivations here; references in the bibliographical notes
providethedetails.)
You should be aware that the assumption of independence of disk failures
is not valid. Power failures, and natural disasterssuch as earthquakes, ?res, and
?oods, may result in damage to both disks at the same time. As disks age, the
probability of failure increases, increasing the chance that a second disk will fail
while the ?rst is being repaired. In spite of all these considerations, however,
mirrored-disksystemsoffermuch higher reliabilitythan do single-disksystems.
Mirrored-disk systems with mean time to data loss of about 500,000 to 1,000,000
hours, or 55to 110 years,areavailabletoday.
Power failures are a particular source of concern, since they occur far more
frequentlythandonaturaldisasters.Powerfailuresarenotaconcernifthereisno
datatransfertodiskinprogresswhentheyoccur.However,evenwithmirroring
ofdisks,ifwritesareinprogresstothesameblockinbothdisks,andpowerfails
beforebothblocksarefullywritten,thetwoblockscanbeinaninconsistentstate.
The solution to this problem is to write one copy ?rst, then the next, so that one
of the two copies is always consistent. Some extra actions are required when we
restart after a power failure, to recover from incomplete writes. This matter is
examinedinPractice Exercise10.3.
10.3.2 Improvement inPerformanceviaParallelism
Now let us consider the bene?t of parallel access to multiple disks. With disk
mirroring,therateatwhichreadrequestscanbehandledisdoubled,sinceread
requests can be sent to either disk (as long as both disks in a pair are functional,
as is almost always the case). The transfer rate of each read is the same as in a
single-disksystem,but the number of readsperunit timehas doubled.
With multiple disks, we can improve the transfer rate as well (or instead) by
stripingdata across multipledisks.Inits simplestform,data stripingconsists of
splitting the bits of each byte across multiple disks; such striping is called bit-
level striping. For example, if we have an array of eight disks, we write bit i of
each byte to disk i. The array of eight disks can be treated as a single disk with
sectors that are eight times the normal size, and, more important, that has eight
times the transfer rate. In such an organization, every disk participates in every
access(readorwrite),sothenumberofaccessesthatcanbeprocessedpersecond
isaboutthesameasonasingledisk,buteachaccesscanreadeighttimesasmany
10.3 RAID 443
data in the same time as on a single disk. Bit-levelstriping can be generalized to
anumberofdisksthateitherisamultipleof8orafactorof8.Forexample,ifwe
useanarray of four disks,bits i and 4 +i of each byte goto disk i.
Block-levelstripingstripesblocksacrossmultipledisks.Ittreatsthearrayof
disks as a single large disk, and it gives blocks logical numbers; we assume the
block numbersstartfrom0.With anarrayof ndisks,block-levelstripingassigns
logical block i of the diskarray to disk(i mod n) +1; it uses the null i/nnull th physical
(a) RAID 0: nonredundant striping
(b) RAID 1: mirrored disks
(c) RAID 2: memory-style error-correcting codes
(d) RAID 3: bit-interleaved parity
(e) RAID 4: block-interleaved parity
(f) RAID 5: block-interleaved distributed parity
(g) RAID 6: P + Q redundancy
P
PP
P
P
PP
P
P
P
C C C C
P P P
P
PP
Figure10.3 RAID levels.
444 Chapter 10 Storage and File Structure
blockofthedisktostorelogicalblock i.Forexample,with8disks,logicalblock0
is stored in physical block 0 of disk 1, while logical block 11 is stored in physical
block1ofdisk4.Whenreadingalarge?le,block-levelstripingfetchesnblocksat
atimeinparallelfromthendisks,givingahighdata-transferrateforlargereads.
Whenasingleblockisread,thedata-transferrateisthe sameasonone disk,but
the remaining n ?1 disksare freeto performotheractions.
Block-level striping is the most commonly used form of data striping. Other
levelsof striping,suchas bytesof a sectoror sectorsof a block, alsoarepossible.
Insummary,therearetwo maingoals of parallelismina disksystem:
1. Load-balance multiple small accesses (block accesses), so that the through-
put of such accessesincreases.
2. Parallelize large accesses so that the response time of large accesses is re-
duced.
10.3.3 RAIDLevels
Mirroring provides high reliability, but it is expensive. Striping provides high
data-transfer rates, but does not improve reliability. Various alternative schemes
aimtoprovideredundancyatlowercostbycombiningdiskstripingwith“parity”
bits (which we describe next). These schemes have different cost–performance
trade-offs. The schemes are classi?ed into RAID levels, as in Figure 10.3. (In the
?gure, P indicates error-correcting bits, and C indicates a second copy of the
data.) For all levels, the ?gure depicts four disks’ worth of data, and the extra
disks depicted are used to store redundant information for failure recovery.
• RAID level 0 refers to disk arrays with striping at the level of blocks, but
withoutanyredundancy(suchasmirroringorparitybits).Figure10.3ashows
an array of size 4.
• RAIDlevel1referstodiskmirroringwithblockstriping.Figure10.3bshows
a mirroredorganization that holdsfour disks’worth ofdata.
NotethatsomevendorsusethetermRAIDlevel1+0or RAIDlevel10torefer
tomirroringwithstriping,andusethetermRAIDlevel1torefertomirroring
without striping. Mirroring without striping can also be used with arrays of
disks, to give the appearance of a single large, reliable disk: if each disk has
Mblocks,logicalblocks0to M ?1arestoredondisk0, Mto2M ?1ondisk
1(the second disk),and so on, and each diskis mirrored.
2
• RAID level 2, known as memory-style error-correcting-code (ECC) organiza-
tion, employs parity bits. Memory systems have long used parity bits for
2
Note that some vendors use the term RAID 0+1 to refer to a version of RAID that uses striping to create a RAID 0
array, and mirrors the array onto another array, with the difference from RAID 1 being that if a disk fails, the RAID
0 array containing the disk becomes unusable. The mirrored array can still be used, so there is no loss of data. This
arrangement is inferiortoRAID 1 when a disk has failed, since the other disks in theRAID 0 array can continue to be
used inRAID 1, but remain idleinRAID 0+1.
10.3 RAID 445
error detection and correction. Each byte in a memory system may have a
parity bit associated with it that records whether the numbers of bits in the
byte that are set to 1 is even(parity = 0) or odd (parity = 1). If one of the bits
in the byte gets damaged (either a 1 becomes a 0, or a 0 becomes a 1), the
parityofthebytechangesandthuswillnotmatchthestoredparity.Similarly,
if the stored parity bit gets damaged, it will not match the computed parity.
Thus,all1-biterrorswillbedetectedbythememorysystem.Error-correcting
schemesstore2ormoreextrabits,andcanreconstructthedataifasinglebit
getsdamaged.
The idea of error-correcting codes can be used directly in disk arrays by
striping bytes across disks. For example, the ?rst bit of each byte could be
stored in disk 0, the second bit in disk 1, and so on until the eighth bit is
storedindisk7,and theerror-correctionbits arestoredinfurtherdisks.
Figure10.3cshows thelevel2scheme.Thediskslabeled P store the error-
correctionbits.Ifoneofthedisksfails,theremainingbitsofthebyteandthe
associatederror-correctionbitscanbereadfromotherdisks,andcanbeused
to reconstruct the damaged data. Figure 10.3c shows an array of size 4; note
RAIDlevel2requiresonlythreedisks’overheadforfourdisksofdata,unlike
RAID level1,which requiredfourdisks’overhead.
• RAID level 3, bit-interleaved parity organization, improves on level 2 by
exploiting the fact that disk controllers, unlike memory systems, can detect
whether a sector has been read correctly, so a single parity bit can be used
for error correction, as well as for detection. The idea is as follows: If one of
the sectors gets damaged, the system knows exactly which sector it is, and,
for each bit in the sector, the system can ?gure out whether it is a 1 or a 0
by computing the parity of the corresponding bits from sectors in the other
disks. If the parity of the remaining bits is equal to the stored parity, the
missingbit is0;otherwise,itis1.
RAID level 3 is as good as level 2, but is less expensive in the number of
extradisks(ithasonlyaone-diskoverhead),solevel2isnotusedinpractice.
Figure10.3d shows thelevel3 scheme.
RAID level 3 has two bene?ts over level 1. It needs only one parity disk
for several regular disks, whereas level 1 needs one mirror disk for every
disk, and thus level 3 reduces the storage overhead. Since reads and writes
of a byte are spread out over multiple disks, with N-way striping of data,
the transfer rate for reading or writing a single block is N times faster than
a RAID level 1 organization using N-way striping. On the other hand, RAID
level 3 supports a lower number of I/O operations per second, since every
diskhas toparticipateinevery I/O request.
• RAIDlevel4,block-interleavedparityorganization,usesblock-levelstriping,
like RAID 0, and in addition keeps a parity block on a separate disk for
corresponding blocks from N other disks. This scheme is shown pictorially
inFigure10.3e.Ifoneofthedisksfails,theparityblockcan beusedwiththe
corresponding blocks from the other disks to restore the blocks of the failed
disk.
446 Chapter 10 Storage and File Structure
Ablockreadaccessesonlyonedisk,allowingotherrequeststobeprocessed
by the other disks. Thus, the data-transfer rate for each access is slower, but
multiplereadaccessescanproceedinparallel,leadingtoahigheroverallI/O
rate.Thetransferratesforlargereadsishigh,sinceallthediskscanbereadin
parallel; large writes also have high transfer rates, since the data and parity
can bewritteninparallel.
Small independent writes, on the other hand, cannot be performed in
parallel.Awriteofablockhastoaccessthediskonwhichtheblockisstored,
aswellastheparitydisk,sincetheparityblockhastobeupdated.Moreover,
both the old value of the parity block and the old value of the block being
written have to be read for the new parity to be computed. Thus, a single
write requiresfour disk accesses: two to read the two old blocks, and two to
write the two blocks.
• RAID level 5, block-interleaved distributed parity, improves on level 4 by
partitioningdataandparityamongall N +1disks,insteadofstoringdatain
Ndisksandparityinonedisk.Inlevel5,alldiskscanparticipateinsatisfying
read requests, unlike RAID level 4, where the parity disk cannot participate,
so level 5 increases the total number of requests that can be met in a given
amount of time. For each set of N logical blocks, one of the disks stores the
parity,and theother N disksstore the blocks.
Figure 10.3f shows the setup. The P’s are distributed across all the disks.
Forexample,withanarrayof5disks,theparityblock,labeledPk,forlogical
blocks 4k,4k +1,4k +2,4k +3isstoredindiskk mod 5; the corresponding
blocksoftheotherfourdisksstorethe4datablocks4kto4k+3.Thefollowing
table indicates how the ?rst 20 blocks, numbered 0 to 19, and their parity
blocks arelaidout.The patternshown getsrepeatedonfurtherblocks.
P0
4
8
12
16
0
P1
9
13
17
1
5
P2
14
18
2
6
10
P3
19
3
7
11
15
P4
Notethataparityblockcannotstoreparityforblocksinthesamedisk,since
thenadiskfailurewouldresultinlossofdataaswellasofparity,andhence
wouldnotberecoverable.Level5subsumeslevel4,sinceitoffersbetterread
–writeperformance atthe samecost, solevel4 isnot usedinpractice.
• RAID level 6, the P + Q redundancy scheme, is much like RAID level 5, but
stores extra redundant information to guard against multiple disk failures.
Insteadof using parity,level6useserror-correctingcodessuchastheReed–
Solomoncodes(seethebibliographicalnotes).IntheschemeinFigure10.3g,
2 bits of redundant data are stored for every 4 bits of data—unlike 1 parity
bit inlevel5—and thesystemcan toleratetwo diskfailures.
10.3 RAID 447
Finally,wenotethatseveralvariationshavebeenproposedtothebasic RAID
schemesdescribedhere,anddifferentvendorsusedifferentterminologiesforthe
variants.
10.3.4 ChoiceofRAIDLevel
The factors to be takeninto account in choosing a RAID levelare:
• Monetarycost of extradisk-storagerequirements.
• Performance requirementsin termsof number of I/O operations.
• Performance when a diskhas failed.
• Performance during rebuild(that is,while the datain a faileddiskare being
rebuilton anewdisk).
The time to rebuild the data of a failed disk can be signi?cant, and it varies
with the RAID levelthat is used. Rebuilding is easiestfor RAID level1, since data
can be copied from another disk; for the other levels, we need to access all the
otherdisksinthearraytorebuilddataofafaileddisk.Therebuildperformance
of a RAID system may be an important factor if continuous availability of data
is required, as it is in high-performance database systems. Furthermore, since
rebuild time can form a signi?cant part of the repair time, rebuild performance
also in?uencesthe meantimeto dataloss.
RAID level 0 is used in high-performance applications where data safety is
not critical. Since RAID levels 2 and 4 are subsumed by RAID levels 3 and 5, the
choice of RAID levels is restricted to the remaining levels. Bit striping (level 3) is
inferiortoblockstriping(level5),sinceblockstripinggivesasgooddata-transfer
rates for large transfers, while using fewer disks for small transfers. For small
transfers, the diskaccess timedominates anyway, so the bene?t of parallelreads
diminishes. In fact, level 3 may perform worse than level 5 for a small transfer,
since the transfer completes only when corresponding sectors on all disks have
been fetched; the average latency for the disk array thus becomes very close to
the worst-case latency for a single disk, negating the bene?ts of higher transfer
rates. Level 6 is not supported currently by many RAID implementations, but it
offers better reliability than level 5 and can be used in applications where data
safetyisveryimportant.
Thechoicebetween RAIDlevel1andlevel5ishardertomake. RAIDlevel1is
popularfor applications such as storageof log?lesin a databasesystem,sinceit
offersthebestwriteperformance. RAIDlevel5hasalowerstorageoverheadthan
level 1, but has a higher time overhead for writes. For applications where data
arereadfrequently,and writtenrarely,level5isthe preferredchoice.
Disk-storagecapacitieshavebeengrowingatarateofover50percentperyear
formanyyears,andthecostperbytehasbeenfallingatthesamerate.Asaresult,
formanyexistingdatabaseapplicationswithmoderatestoragerequirements,the
monetarycostoftheextradiskstorageneededformirroringhasbecomerelatively
small (the extra monetary cost, however, remains a signi?cant issue for storage-
448 Chapter 10 Storage and File Structure
intensiveapplicationssuchasvideodatastorage).Accessspeedshaveimproved
at a much slower rate (around a factor of 3 over 10 years), while the number of
I/O operations required per second has increased tremendously, particularly for
Webapplicationservers.
RAIDlevel5,whichincreasesthenumberof I/Ooperationsneededtowritea
singlelogicalblock,paysasigni?canttimepenaltyintermsofwriteperformance.
RAID level 1 is therefore the RAID level of choice for many applications with
moderatestorage requirementsand high I/O requirements.
RAID system designers have to make several other decisions as well. For
example, how many disks should there be in an array? How many bits should
be protected by each parity bit? If there are more disks in an array, data-transfer
rates are higher, but the system will be more expensive. If there are more bits
protectedbyaparitybit,thespaceoverheadduetoparitybitsislower,butthere
is an increased chance that a second disk will fail before the ?rst failed disk is
repaired,and that willresultindataloss.
10.3.5 HardwareIssues
Another issue in the choice of RAID implementations is at the level of hardware.
RAID canbe implementedwithno change atthe hardwarelevel,using only soft-
ware modi?cation. Such RAID implementations are called software RAID.How-
ever,therearesigni?cantbene?tstobehadbybuildingspecial-purposehardware
tosupportRAID,whichweoutlinebelow;systemswithspecialhardwaresupport
arecalledhardware RAID systems.
Hardware RAID implementations can use nonvolatile RAM to record writes
beforetheyareperformed.Incaseofpowerfailure,whenthesystemcomesback
up, it retrieves information about any incomplete writes from nonvolatile RAM
andthencompletesthewrites.Withoutsuchhardwaresupport,extraworkneeds
to be done to detect blocks that may have been partially written before power
failure(seePractice Exercise10.3).
Even if all writes are completed properly, there is a small chance of a sector
in a disk becoming unreadable at some point, even though it was successfully
written earlier. Reasons for loss of data on individual sectors could range from
manufacturing defects, to data corruption on a track when an adjacent track
is written repeatedly. Such loss of data that were successfully written earlier is
sometimesreferredtoasalatent failure,orasbitrot.Whensuchafailurehappens,
if it is detected early the data can be recovered from the remaining disks in the
RAID organization. However, if such a failure remains undetected, a single disk
failure could lead to data loss if a sector in one of the other disks has a latent
failure.
To minimize the chance of such data loss, good RAID controllers perform
scrubbing;thatis,duringperiodswhendisksareidle,everysectorofeverydisk
is read, and if any sector is found to be unreadable, the data are recovered from
the remaining disks in the RAID organization, and the sector is written back. (If
thephysicalsectorisdamaged,thediskcontrollerwouldremapthelogicalsector
addresstoadifferentphysical sectoron disk.)
10.4 TertiaryStorage 449
Some hardware RAID implementations permit hot swapping;thatis,faulty
diskscan be removedand replacedby newones without turning poweroff. Hot
swapping reduces the mean time to repair, since replacement of a disk does not
havetowaituntilatimewhenthesystemcanbeshutdown.Infactmanycritical
systemstodayrunona24 ×7schedule;thatis,theyrun24hoursaday,7daysa
week, providing no time for shutting down and replacing a failed disk. Further,
manyRAIDimplementationsassignasparediskforeacharray(orforasetofdisk
arrays). If a disk fails, the spare disk is immediately used as a replacement. As a
result, the mean time to repair is reduced greatly, minimizing the chance of any
dataloss.The faileddiskcanbe replacedatleisure.
The power supply, or the disk controller, or even the system interconnection
inaRAIDsystemcouldbecomeasinglepointoffailurethatcouldstopfunctioning
of the RAID system. To avoid this possibility, good RAID implementations have
multiple redundant power supplies (with battery backups so they continue to
function even if power fails). Such RAID systems have multiple disk interfaces,
andmultipleinterconnectionstoconnecttheRAIDsystemtothecomputersystem
(ortoanetworkofcomputersystems).Thus,failureofanysinglecomponentwill
not stop the functioning of the RAID system.
10.3.6 OtherRAIDApplications
The concepts of RAID have been generalized to other storage devices, including
arrays of tapes, and even to the broadcast of data over wireless systems. When
appliedtoarraysoftapes,the RAIDstructuresareabletorecoverdataevenifone
of the tapes in an array of tapes is damaged. When applied to broadcast of data,
a block of data is split into short units and is broadcast along with a parity unit;
if one of the units is not received for any reason, it can be reconstructed from the
otherunits.
10.4 TertiaryStorage
Inalargedatabasesystem,someofthedatamayhavetoresideontertiarystorage.
The two most common tertiary storage media are optical disks and magnetic
tapes.
10.4.1 OpticalDisks
Compactdiskshavebeenapopularmediumfordistributingsoftware,multime-
dia data such as audio and images, and other electronically published informa-
tion. They have a storage capacity of 640 to 700 megabytes, and they are cheap
to mass-produce. Digital video disks (DVDs) have now replaced compact disks
in applications that require larger amounts of data. Disks in the DVD-5 format
can store 4.7 gigabytes of data (in one recording layer), while disks in the DVD-9
format can store 8.5 gigabytes of data (in two recording layers). Recording on
both sides of a disk yields even larger capacities; DVD-10 and DVD-18 formats,
which are the two-sided versions of DVD-5 and DVD-9, can store 9.4 gigabytes
450 Chapter 10 Storage and File Structure
and 17gigabytes,respectively.The Blu-ray DVD formathas a signi?cantly higher
capacity of 27 to 54 gigabytesperdisk.
CDandDVDdriveshavemuchlongerseektimes(100millisecondsiscommon)
than do magnetic-disk drives, since the head assembly is heavier. Rotational
speeds are typically lower than those of magnetic disks, although the faster CD
and DVD drives have rotation speeds of about 3000 rotations per minute, which
is comparable to speeds of lower-end magnetic-disk drives. Rotational speeds
of CD drives originally corresponded to the audio CD standards, and the speeds
of DVD drives originally corresponded to the DVD video standards, but current-
generationdrivesrotateat many timesthe standardrate.
Data-transfer rates are somewhat less than for magnetic disks. Current CD
drives read at around 3 to 6 megabytes per second, and current DVD drivesread
at 8 to 20 megabytes per second. Like magnetic-disk drives, optical disks store
more data in outside tracks and less data in inner tracks. The transfer rate of
optical drives is characterized as n×, which means the drive supports transfers
at ntimesthestandardrate;ratesofaround50×for CDand16×for DVDarenow
common.
Therecord-onceversionsofopticaldisks(CD-R, DVD-R,andDVD+R)arepop-
ular for distribution of data and particularly for archival storage of data because
theyhaveahighcapacity,havealongerlifetimethanmagneticdisks,andcanbe
removed and stored at a remote location. Since they cannot be overwritten, they
canbeusedtostoreinformationthatshouldnotbemodi?ed,suchasaudittrails.
The multiple-write versions (CD-RW, DVD-RW, DVD+RW,andDVD-RAM)arealso
usedfor archivalpurposes.
Jukeboxesaredevicesthatstorealargenumberofopticaldisks(uptoseveral
hundred) and load them automatically on demand to one of a small number of
drives (usually 1 to 10). The aggregate storage capacity of such a system can be
manyterabytes.Whenadiskisaccessed,itisloadedbyamechanicalarmfroma
rackontoadrive(anydiskthatwasalreadyinthedrivemust?rstbeplacedback
on the rack). The disk load/unload time is usually of the order of a few seconds
—verymuch longerthan diskaccess times.
10.4.2 MagneticTapes
Although magnetic tapes are relatively permanent, and can hold large volumes
ofdata,theyareslowincomparisontomagneticandopticaldisks.Evenmoreim-
portant,magnetictapesarelimitedtosequentialaccess.Thus,theycannotprovide
random access for secondary-storage requirements, although historically, prior
to theuseof magneticdisks,tapeswereusedas a secondary-storagemedium.
Tapes are used mainly for backup, for storage of infrequently used informa-
tion, and as an off-line medium for transferring information from one system to
another. Tapes are also used for storing large volumes of data, such as video or
imagedata,thateitherdonotneedtobeaccessiblequicklyoraresovoluminous
that magnetic-diskstoragewouldbe too expensive.
A tape is kept in a spool, and is wound or rewound past a read–write head.
Movingtothecorrectspotonatapecantakesecondsorevenminutes,ratherthan
10.5 File Organization 451
milliseconds; once positioned, however, tape drives can write data at densities
and speeds approaching those of disk drives. Capacities vary, depending on the
length and width of the tape and on the density at which the head can read and
write.Themarketiscurrentlyfragmentedamongawidevarietyoftapeformats.
Currently available tape capacities range from a few gigabytes with the Digital
Audio Tape (DAT) format, 10 to 40 gigabytes with the Digital Linear Tape (DLT)
format,100gigabytesandhigherwiththeUltriumformat,to330gigabyteswith
Ampex helical scan tape formats. Data-transfer rates are of the order of a few to
tensof megabytespersecond.
Tapedevicesarequitereliable,andgoodtapedrivesystemsperformareadof
thejust-writtendatatoensurethatithasbeenrecordedcorrectly.Tapes,however,
have limitson the number of timesthat theycan be read orwritten reliably.
Tapejukeboxes,likeopticaldiskjukeboxes,holdlargenumbersoftapes,with
afewdrivesontowhichthetapescanbemounted;theyareusedforstoringlarge
volumesofdata,ranginguptomanypetabytes(10
15
bytes),withaccesstimeson
theorderofsecondstoafewminutes.Applicationsthatneedsuchenormousdata
storageincludeimagingsystemsthat gatherdatafromremote-sensingsatellites,
and largevideolibrariesfor televisionbroadcasters.
Some tape formats (such as the Accelis format) support faster seek times
(of the order of tens of seconds), and are intended for applications that retrieve
information from jukeboxes. Most other tape formats provide larger capacities,
at the cost of slower access; such formats are ideal for data backup, where fast
seeksare not important.
Tape drives have been unable to keep up with the enormous improvements
indiskdrivecapacityandcorrespondingreductioninstoragecost.Whilethecost
of tapes is low, the cost of tape drives and tape libraries is signi?cantly higher
thanthe cost of adiskdrive:a tapelibrarycapable of storinga fewterabytescan
costs tens of thousands of dollars. Backing up data to disk drives has become a
cost-effectivealternativeto tapebackup for anumber of applications.
10.5 FileOrganization
A databaseis mappedinto anumber of different?les that aremaintained by the
underlying operating system. These ?les reside permanently on disks. A ?le is
organizedlogicallyasasequenceofrecords.Theserecordsaremappedontodisk
blocks. Files are provided as a basic construct in operating systems, so we shall
assume the existence of an underlying ?le system. We need to consider ways of
representinglogicaldatamodelsintermsof ?les.
Each ?le is also logically partitioned into ?xed-length storage units called
blocks, which are the units of both storage allocation and data transfer. Most
databasesuseblocksizesof4to8kilobytesbydefault,butmanydatabasesallow
the block size to be speci?ed when a database instance is created. Larger block
sizescan beusefulin somedatabaseapplications.
A block may contain several records; the exact set of records that a block
contains isdeterminedbytheformofphysicaldataorganizationbeingused.We
452 Chapter 10 Storage and File Structure
shallassumethatnorecordislargerthanablock.Thisassumptionisrealisticformost
data-processingapplications,suchasouruniversityexample.Therearecertainly
several kinds of large data items, such as images, that can be signi?cantly larger
than a block. We brie?y discuss how to handle such large data items later, in
Section10.5.2,by storinglargedataitemsseparately,andstoring apointertothe
dataitemintherecord.
In addition, we shall require that each record is entirely contained in a single
block; that is, no record is contained partly in one block, and partly in another.
This restrictionsimpli?esand speedsup access to dataitems.
In a relational database, tuples of distinct relations are generally of different
sizes. One approach to mapping the database to ?les is to use several ?les, and
to store records of only one ?xed length in any given ?le. An alternative is to
structure our ?les so that we can accommodate multiple lengths for records;
however, ?les of ?xed-length records are easier to implement than are ?les of
variable-lengthrecords.Manyofthetechniquesusedfortheformercanbeapplied
to the variable-length case. Thus, we begin by considering a ?le of ?xed-length
records,and considerstorage of variable-lengthrecordslater.
10.5.1 Fixed-Length Records
As an example, let us consider a ?le of instructor records for our university
database.Eachrecordof this?le isde?ned(inpseudocode)as:
type instructor =record
IDvarchar(5);
name varchar(20);
dept name varchar(20);
salary numeric(8,2);
end
Assume that each character occupies 1 byte and that numeric (8,2) occupies
8 bytes. Suppose that instead of allocating a variable amount of bytes for the
attributes ID, name,anddept name, we allocate the maximum number of bytes
thateachattributecanhold.Then,the instructorrecord is 53 bytes long. A simple
approach is to use the ?rst 53 bytes for the ?rst record, the next 53 bytes for the
second record, and so on (Figure 10.4). However, there are two problems with
this simpleapproach:
1. Unless the block size happens to be a multiple of 53 (which is unlikely),
some records will cross block boundaries. That is, part of the record will
be stored in one block and part in another. It would thus require two block
accesses toreador writesuch a record.
2. It is dif?cult to delete a record from this structure. The space occupied by
the recordtobedeletedmustbe?lledwithsomeother recordof the?le,or
wemusthaveawayofmarkingdeletedrecordssothattheycanbeignored.
10.5 File Organization 453
Srinivasan Comp. Sci. 65000
Wu Finance 90000
Mozart Music 40000
Einstein Physics 95000
El Said History 60000
Gold Physics 87000
Katz Comp. Sci. 75000
Cali?eri History 62000
Singh Finance 80000
Crick Biology 72000
Brandt Comp. Sci. 92000
15151
10101
12121
22222
32343
33456
45565
58583
76543
76766
83821
98345 Kim Elec. Eng. 80000
record 0
record 1
record 2
record 3
record 4
record 5
record 6
record 7
record 8
record 9
record 10
record 11
Figure10.4 File containing instructor records.
To avoid the ?rst problem, we allocate only as many records to a block as
would?tentirelyintheblock(thisnumbercanbecomputedeasilybydividingthe
block size by the record size, and discarding the fractional part). Any remaining
bytesof each block areleftunused.
Whenarecordisdeleted,wecouldmovetherecordthatcameafteritintothe
space formerlyoccupied by the deletedrecord,and soon, until everyrecordfol-
lowingthedeletedrecordhasbeenmovedahead(Figure10.5).Suchanapproach
requiresmovingalargenumberofrecords.Itmightbeeasiersimplytomovethe
?nalrecordofthe?leintothespaceoccupiedbythedeletedrecord(Figure10.6).
Itisundesirabletomoverecordstooccupythespacefreedbyadeletedrecord,
sincedoingsorequiresadditionalblockaccesses.Sinceinsertionstendtobemore
frequent than deletions, it is acceptable to leave open the space occupied by the
Srinivasan Comp. Sci. 65000
Wu Finance 90000
Mozart Music 40000
El Said History 60000
Gold Physics 87000
Katz Comp. Sci. 75000
Cali?eri History 62000
Singh Finance 80000
Crick Biology 72000
Brandt Comp. Sci. 92000
15151
10101
12121
32343
33456
45565
58583
76543
76766
83821
98345 Kim Elec. Eng. 80000
record 0
record 1
record 2
record 4
record 5
record 6
record 7
record 8
record 9
record 10
record 11
Figure10.5 File of Figure 10.4, with record 3 deleted and all records moved.
454 Chapter 10 Storage and File Structure
Srinivasan Comp. Sci. 65000
Wu Finance 90000
Mozart Music 40000
El Said History 60000
Gold Physics 87000
Katz Comp. Sci. 75000
Cali?eri History 62000
Singh Finance 80000
Crick Biology 72000
Brandt Comp. Sci. 92000
15151
10101
12121
32343
33456
45565
58583
76543
76766
83821
record 0
record 1
record 2
record 4
record 5
record 6
record 7
record 8
record 9
record 10
98345 Kim Elec. Eng. 80000 record 11
Figure10.6 File of Figure 10.4, with record 3 deleted and ?nal record moved.
deleted record, and to wait for a subsequent insertion before reusing the space.
A simple marker on a deleted record is not suf?cient, since it is hard to ?nd this
available space when an insertion is being done. Thus, we need to introduce an
additionalstructure.
At the beginning of the ?le, we allocate a certain number of bytes as a ?le
header.Theheaderwillcontainavarietyofinformationaboutthe?le.Fornow,all
weneedtostorethereistheaddressofthe?rstrecordwhosecontentsaredeleted.
Weusethis?rstrecordtostoretheaddressofthesecondavailablerecord,andso
on.Intuitively,wecanthinkofthesestoredaddressesas pointers,sincetheypoint
to the location of a record. The deleted records thus form a linked list, which is
often referred to as a free list. Figure 10.7 shows the ?le of Figure 10.4, with the
freelist,afterrecords1,4,and 6havebeendeleted.
On insertion of a new record, we use the record pointed to by the header.
We change the header pointer to point to the next available record. If no space is
available,we add the new record tothe end of the ?le.
Insertion and deletion for ?les of ?xed-length records are simple to imple-
ment, because the space made available by a deleted record is exactly the space
needed to insert a record. If we allow records of variable length in a ?le, this
match no longer holds. An inserted record may not ?t in the space left free by a
deletedrecord,or itmay ?llonly partof that space.
10.5.2 Variable-LengthRecords
Variable-lengthrecordsariseindatabase systemsinseveralways:
• Storageofmultiplerecordtypesina ?le.
• Record typesthat allow variablelengths for one or more ?elds.
• Recordtypesthat allow repeating?elds,such asarrays or multisets.
10.5 File Organization 455
header
record 0
record 1
record 2
record 3
record 4
record 5
record 6
record 7
record 8
record 9
record 10
record 11
72000
92000
80000
65000
40000
95000
87000
62000
76766
83821
98345
10101
15151
22222
33456
58583
76543
Crick
Brandt
Kim
Srinivasan
Mozart
Einstein
Gold
Cali?eri
Singh
Biology
Elec. Eng.
Comp. Sci.
Comp. Sci.
Music
Physics
Physics
History
Finance 80000
Figure10.7 File of Figure 10.4, with free list after deletion of records 1, 4, and 6.
Differenttechniquesforimplementingvariable-lengthrecordsexist.Twodifferent
problemsmustbe solvedby any suchtechnique:
• Howtorepresentasinglerecordinsuchawaythatindividualattributescan
beextractedeasily.
• How to store variable-length records within a block, such that records in a
block can beextractedeasily.
The representation of a record with variable-length attributes typically has
twoparts:aninitialpartwith?xedlengthattributes,followedbydataforvariable-
lengthattributes.Fixed-lengthattributes,suchasnumericvalues,dates,or?xed-
length character strings are allocated as many bytes as required to store their
value. Variable-length attributes, such as varchar types, are represented in the
initial part of the record by a pair (offset, length), where offset denotes where
the data for that attribute begins within the record, and length is the length in
bytes of the variable-sized attribute. The values for these attributes are stored
consecutively,aftertheinitial?xed-lengthpartoftherecord.Thus,theinitialpart
of the record stores a ?xed size of information about each attribute, whether it is
?xed-lengthor variable-length.
AnexampleofsucharecordrepresentationisshowninFigure10.8.The?gure
showsaninstructorrecord,whose?rstthreeattributes ID,name,anddept nameare
variable-lengthstrings,andwhosefourthattributesalaryisa?xed-sizednumber.
We assume that the offset and length values are stored in two bytes each, for a
totalof4bytesperattribute.Thesalaryattributeisassumedtobestoredin8bytes,
and each stringtakesas many bytesas ithas characters.
456 Chapter 10 Storage and File Structure
21, 5 26, 10 36, 10 65000 10101 Srinivasan Comp. Sci.
Bytes 0 4 8 12 20 21 26 36 45
0000
Null bitmap (stored in 1 byte)
Figure10.8 Representation of variable-length record.
The ?gure also illustrates the use of a null bitmap, which indicates which
attributes of the record have a null value. In this particular record, if the salary
werenull,thefourthbitofthebitmapwouldbesetto1,andthesalaryvaluestored
inbytes12through19wouldbeignored.Sincetherecordhasfourattributes,the
nullbitmapforthisrecord?tsin1byte,althoughmorebytesmayberequiredwith
moreattributes.Insomerepresentations,thenullbitmapisstoredatthebeginning
oftherecord,andforattributesthatarenull,nodata(value,oroffset/length)are
storedat all. Such a representationwould savesome storage space,at the cost of
extra work to extract attributes of the record. This representation is particularly
usefulforcertainapplicationswhererecordshavealargenumberof?elds,most
of which are null.
Wenextaddresstheproblemofstoringvariable-lengthrecordsinablock.The
slotted-page structure is commonly used for organizing records within a block,
and is shown in Figure 10.9.
3
There is a header at the beginning of each block,
containing the following information:
1. The number of recordentriesinthe header.
2. Theendoffreespaceintheblock.
3. An array whose entriescontain the location and sizeof each record.
The actual records are allocated contiguously in the block, starting from the
endoftheblock.Thefreespaceintheblockiscontiguous,betweenthe?nalentry
in the header array, and the ?rst record. If a record is inserted, space is allocated
for it at the end of free space, and an entry containing its size and location is
addedto the header.
If a recordis deleted,the space that it occupies is freed,and its entryis set to
deleted (its size is set to ?1, for example). Further, the records in the block before
the deleted record are moved, so that the free space created by the deletion gets
occupied, and all free space is again between the ?nal entry in the header array
and the ?rst record. The end-of-free-space pointer in the header is appropriately
updated as well. Records can be grown or shrunk by similar techniques, as long
asthereisspaceintheblock.Thecostofmovingtherecordsisnottoohigh,since
the sizeof a block is limited:typicalvaluesarearound4 to 8 kilobytes.
Theslotted-pagestructurerequiresthattherebenopointersthatpointdirectly
torecords.Instead,pointersmustpointtotheentryintheheaderthatcontainsthe
3
Here, “page”is synonymouswith “block.”
10.6 Organization of Records in Files 457
# Entries Size
Location
Block Header Records
Free Space
End of Free Space
Figure10.9 Slotted-page structure.
actuallocationoftherecord.Thislevelofindirectionallowsrecordstobemovedto
preventfragmentationofspaceinsideablock,whilesupportingindirectpointers
to therecord.
Databases often store data that can be much larger than a disk block. For
instance, an image or an audio recording may be multiple megabytes in size,
while a video object may be multiple gigabytes in size. Recall that SQL supports
thetypesblob and clob, which storebinary and character largeobjects.
Most relational databases restrict the size of a record to be no larger than
the size of a block, to simplify buffer management and free-space management.
Large objects are often stored in a special ?le (or collection of ?les) instead of
being stored with the other (short) attributes of records in which they occur. A
(logical) pointer to the object is then stored in the record containing the large
object.LargeobjectsareoftenrepresentedusingB
+
-tree?leorganizations,which
we study in Section 11.4.1. B
+
-tree ?le organizations permit us to read an entire
object, or speci?ed byte ranges in the object, as well as to insert and delete parts
of the object.
10.6 OrganizationofRecordsinFiles
So far, we have studiedhow recordsare representedin a ?le structure.A relation
is a set of records. Given a set of records, the next question is how to organize
themina?le.Severalof thepossibleways of organizing recordsin?lesare:
• Heap?leorganization.Anyrecordcanbeplacedanywhereinthe?lewhere
there is space for the record. There is no ordering of records. Typically, there
isa single?lefor each relation.
• Sequential?leorganization.Recordsarestoredinsequentialorder,accord-
ingtothevalueofa “searchkey”ofeachrecord.Section10.6.1describesthis
organization.
• Hashing ?le organization. A hash function is computed on some attribute
ofeachrecord.Theresultofthehashfunctionspeci?esinwhichblockofthe
458 Chapter 10 Storage and File Structure
10101 Srinivasan
45565 Katz
58583 Cali?eri
76543 Singh
76766 Crick
83821 Brandt
98345 Kim
12121 Wu
15151 Mozart
22222 Einstein
32343 El Said
33456 Gold
Comp. Sci.
Comp. Sci.
Comp. Sci.
History
Finance
Biology
Elec. Eng.
Finance
Music
Physics
History
Physics
65000
75000
62000
80000
72000
92000
80000
90000
40000
95000
60000
87000
Figure10.10 Sequential ?le for instructor records.
?le the record should be placed. Chapter 11 describes this organization; it is
closelyrelatedtothe indexingstructuresdescribedinthat chapter.
Generally,aseparate?leisusedtostoretherecordsofeachrelation.However,
inamultitableclustering?leorganization,recordsofseveraldifferentrelations
are stored in the same ?le; further, related records of the different relations are
stored on the same block, so that one I/O operation fetches related records from
alltherelations.Forexample,recordsofthetworelationscanbeconsideredtobe
relatediftheywouldmatchinajoinofthetworelations.Section10.6.2describes
this organization.
10.6.1 Sequential FileOrganization
A sequential ?le is designed for ef?cient processing of records in sorted order
basedonsomesearchkey .Asearch key is any attribute or set of attributes; it
neednotbetheprimarykey,orevenasuperkey.Topermitfastretrievalofrecords
in search-key order, we chain together records by pointers. The pointer in each
recordpointstothenextrecordinsearch-keyorder.Furthermore,tominimizethe
numberofblockaccessesinsequential?leprocessing,westorerecordsphysically
insearch-keyorder,or as closeto search-keyorderas possible.
Figure 10.10 shows a sequential ?le of instructor records taken from our uni-
versityexample.Inthatexample,therecordsarestoredinsearch-keyorder,using
IDasthe searchkey.
The sequential ?le organization allows records to be read in sorted order;
that can be useful for display purposes, as well as for certain query-processing
algorithms thatwe shall studyinChapter12.
It is dif?cult, however, to maintain physical sequential order as records are
insertedanddeleted,sinceitiscostlytomovemanyrecordsasaresultofasingle
10.6 Organization of Records in Files 459
10101
45565
76543
76766
83821
98345
12121
15151
22222
32343
33456
58583
Srinivasan
Katz
Singh
Crick
Brandt
Kim
Wu
Mozart
Einstein
El Said
Gold
Cali?eri
Comp. Sci.
Comp. Sci.
Finance
Biology
Comp. Sci.
Elec. Eng.
Finance
Music
Physics
History
Physics
History
65000
75000
80000
72000
92000
80000
90000
40000
95000
60000
87000
62000
48000 Music Verdi 32222
Figure10.11 Sequential ?le after an insertion.
insertionordeletion.Wecanmanagedeletionbyusingpointerchains,aswesaw
previously.For insertion,we applythe following rules:
1. Locatetherecordinthe?lethatcomesbeforetherecordtobeinsertedin
search-keyorder.
2. If there is a free record (that is, space left after a deletion) within the same
block as this record, insert the new record there. Otherwise, insert the new
record in an over?ow block. In either case, adjust the pointers so as to chain
togethertherecordsinsearch-keyorder.
Figure 10.11 shows the ?le of Figure 10.10 after the insertion of the record
(32222,Verdi,Music,48000).ThestructureinFigure10.11allowsfastinsertionof
newrecords,butforcessequential?le-processingapplicationstoprocessrecords
in an orderthat doesnot match the physical orderof the records.
If relatively few records need to be stored in over?ow blocks, this approach
works well. Eventually, however, the correspondence between search-key order
and physical order may be totally lost over a period of time, in which case se-
quentialprocessingwillbecomemuch lessef?cient.Atthispoint,the?le should
bereorganizedsothatitisonceagainphysicallyinsequentialorder.Suchreorga-
nizationsarecostly,andmustbedoneduringtimeswhenthesystemloadislow.
Thefrequencywithwhichreorganizationsareneededdependsonthefrequency
of insertion of new records. In the extreme case in which insertions rarely occur,
itispossiblealwaystokeepthe?leinphysicallysortedorder.Insuchacase,the
pointer?eldinFigure10.10isnot needed.
460 Chapter 10 Storage and File Structure
10.6.2 MultitableClusteringFileOrganization
Manyrelationaldatabasesystemsstoreeachrelationinaseparate?le,sothatthey
can take full advantage of the ?le system that the operating system provides.
Usually, tuples of a relation can be represented as ?xed-length records. Thus,
relationscanbemappedtoasimple?lestructure.Thissimpleimplementationof
arelationaldatabasesystemiswellsuitedtolow-costdatabaseimplementations
as in, for example, embedded systems or portable devices. In such systems, the
sizeofthedatabaseissmall,solittleisgainedfromasophisticated?lestructure.
Furthermore,insuchenvironments,itisessentialthattheoverallsizeoftheobject
codeforthedatabasesystembesmall.Asimple?lestructurereducestheamount
of codeneededtoimplementthesystem.
This simple approach to relational database implementation becomes less
satisfactoryasthesizeofthedatabaseincreases.Wehaveseenthatthereareper-
formance advantages to be gained from careful assignment of records to blocks,
and from careful organization of the blocks themselves. Clearly, a more compli-
cated?lestructuremaybebene?cial,evenifweretainthestrategyofstoringeach
relationina separate?le.
However, many large-scale database systems do not rely directly on the un-
derlying operating system for ?le management. Instead, one large operating-
system ?le is allocated to the database system. The database system stores all
relationsin thisone ?le,and manages the?leitself.
Evenifmultiplerelationsarestoredinasingle?le,bydefaultmostdatabases
store records of only one relation in a given block. This simpli?es data man-
agement. However, in some cases it can be useful to store records of more than
one relation in a single block. To see the advantage of storing records of multi-
ple relations in one block, consider the following SQL query for the university
database:
select dept name, building, budget, ID, name, salary
from department naturaljoin instructor;
This query computes a join of the department and instructor relations. Thus, for
each tuple of department, the system must locate the instructor tuples with the
same value for dept name. Ideally, these records will be located with the help of
indices,whichweshalldiscussinChapter11.Regardlessofhowtheserecordsare
located,however,theyneedtobetransferredfromdiskintomainmemory.Inthe
worstcase, each recordwillresideon adifferentblock, forcingustodoone block
readforeach recordrequiredby the query.
dept name building budget
Comp.Sci. Taylor 100000
Physics Watson 70000
Figure10.12 The department relation.
10.6 Organization of Records in Files 461
ID name dept name salary
10101 Srinivasan Comp.Sci. 65000
33456 Gold Physics 87000
45565 Katz Comp.Sci. 75000
83821 Brandt Comp.Sci. 92000
Figure10.13 The instructor relation.
As a concrete example, consider the department and instructor relations of
Figures 10.12 and 10.13, respectively(for brevity,we include only a subset of the
tuples of the relations we have used thus far). In Figure 10.14, we show a ?le
structure designed for ef?cient execution of queries involving the natural join
of department and instructor.Theinstructor tuples for each ID are stored near the
department tuple for the corresponding dept name. This structure mixes together
tuples of two relations, but allows for ef?cient processing of the join. When a
tuple of the department relation is read, the entire block containing that tuple is
copiedfromdiskintomainmemory.Sincethecorrespondinginstructortuplesare
stored on the disk near the department tuple, the block containing the department
tuple contains tuples of the instructor relation needed to process the query. If a
department has so many instructors that the instructor records do not ?t in one
block, the remaining recordsappearon nearby blocks.
A multitable clustering ?le organization is a ?le organization, such as that
illustrated in Figure 10.14, that stores related records of two or more relations in
each block. Such a ?le organization allows us to read records that would satisfy
the join condition by using one block read. Thus, we are able to process this
particularquerymoreef?ciently.
IntherepresentationshowninFigure10.14,thedept nameattributeisomitted
from instructor records since it can be inferred from the associated department
record;theattributemayberetainedinsomeimplementations,tosimplifyaccess
totheattributes.Weassumethateachrecordcontainstheidenti?eroftherelation
to which itbelongs,although thisis not shown inFigure 10.14.
Our use of clustering of multiple tables into a single ?le has enhanced pro-
cessing of a particular join (that of department and instructor), but it results in
slowing processingof othertypesof queries.For example,
Katz
Srinivasan
Taylor
Brandt
Watson
Gold
100000
70000
87000
92000
75000
65000
Comp. Sci.
45564
10101
83821
Physics
33456
Figure10.14 Multitable clustering ?le structure.
462 Chapter 10 Storage and File Structure
Katz
Srinivasan
Taylor
Brandt
Watson
Gold
100000
70000
87000
92000
75000
65000
Comp. Sci.
45564
10101
83821
Physics
33456
Figure10.15 Multitable clustering ?le structure with pointer chains.
select *
from department;
requires more block accesses than it did in the scheme under which we stored
each relation in a separate ?le, since each block now contains signi?cantly fewer
departmentrecords.Tolocateef?cientlyalltuplesofthe departmentrelationinthe
structure of Figure 10.14, we can chain together all the records of that relation
using pointers,as in Figure10.15.
When multitable clustering is to be used depends on the types of queries
thatthedatabasedesignerbelievestobemostfrequent.Carefuluseofmultitable
clusteringcan produce signi?cant performance gains in queryprocessing.
10.7 Data-DictionaryStorage
So far, we have considered only the representation of the relations themselves.
A relational database system needs to maintain data about the relations, such as
the schema of the relations. In general, such “data about data” is referred to as
metadata.
Relational schemas and other metadata about relations are stored in a struc-
turecalledthedatadictionaryorsystemcatalog.Amongthetypesofinformation
that thesystemmust storearethese:
• Namesof the relations.
• Namesof theattributesof each relation.
• Domains and lengthsof attributes.
• Namesof viewsde?ned on the database, and de?nitions of those views.
• Integrityconstraints (for example,keyconstraints).
Inaddition,many systemskeepthe following dataonusersof the system:
• Namesof authorizedusers.
• Authorization and accounting information about users.
10.7 Data-Dictionary Storage 463
• Passwordsor other information usedtoauthenticate users.
Further,thedatabasemaystorestatisticalanddescriptivedataabouttherelations,
such as:
• Numberof tuplesineachrelation.
• Methodofstorageforeachrelation(forexample,clusteredornonclustered).
The data dictionary may also note the storage organization (sequential, hash, or
heap) ofrelations, and the location where each relationisstored:
• Ifrelationsarestoredinoperatingsystem?les,thedictionarywouldnotethe
namesof the ?le (or ?les)containing each relation.
• Ifthedatabasestoresallrelationsinasingle?le,thedictionarymaynotethe
blockscontainingrecordsofeachrelationinadatastructuresuchasalinked
list.
InChapter11,inwhichwestudyindices,weshallseeaneedtostoreinformation
about each indexon each of the relations:
• Nameof theindex.
• Nameof therelationbeing indexed.
• Attributesonwhich the indexisde?ned.
• Typeof indexformed.
All this metadata information constitutes, in effect, a miniature database.
Somedatabasesystemsstoresuchmetadatabyusingspecial-purposedatastruc-
tures and code. It is generally preferable to store the data about the database as
relations in the database itself.By using database relations to store system meta-
data, we simplify the overall structure of the system and harness the full power
of thedatabase forfast access tosystemdata.
The exact choice of how to represent system metadata by relations must be
made by the system designers. One possible representation, with primary keys
underlined, is shown in Figure 10.16. In this representation, the attribute index
attributesoftherelationIndex metadataisassumedtocontainalistofoneormore
attributes, which can be represented by a character string such as “dept name,
building”.TheIndex metadata relation is thus not in ?rst normal form; it can be
normalized, but the above representation is likely to be more ef?cient to access.
Thedatadictionaryisoftenstoredinanonnormalizedformtoachievefastaccess.
Whenever the database system needs to retrieve records from a relation, it
must ?rst consult the Relation metadata relation to ?nd the location and storage
organizationofthe relation,and thenfetchrecordsusingthisinformation.How-
ever, the storage organization and location of the Relation metadata relation itself
464 Chapter 10 Storage and File Structure
Relation_metadata
relation_name
number_of_anullibutes
storage_organization
location
Index_metadata
index_name
relation_name
index_type
index_anullibutes
View_metadata
view_name
de?nition
Anullibute_metadata
relation_name
anullibute_name
domain_type
position
length
User_metadata
user_name
encrypted_password
group
Figure10.16 Relational schema representing system metadata.
mustberecordedelsewhere(forexample,inthedatabasecodeitself,orina?xed
location in the database), since we need this information to ?nd the contents of
Relation metadata.
10.8 DatabaseBuffer
Amajorgoalofthedatabasesystemistominimizethenumberofblocktransfers
betweenthediskandmemory.Onewaytoreducethenumberofdiskaccessesis
to keepas many blocks as possiblein main memory.The goalis tomaximizethe
chancethat,whenablockisaccessed,itisalreadyinmainmemory,and,thus,no
diskaccess isrequired.
Sinceitisnotpossibletokeepallblocksinmainmemory,weneedtomanage
the allocation of the space available in main memory for the storage of blocks.
The buffer is that part of main memory available for storage of copies of disk
blocks. There is always a copy kept on disk of every block, but the copy on disk
maybeaversionoftheblockolderthantheversioninthebuffer.Thesubsystem
responsible forthe allocation of bufferspace iscalled thebuffermanager.
10.8.1 BufferManager
Programsinadatabasesystemmakerequests(thatis,calls)onthebuffermanager
when they needablock from disk.Ifthe block is alreadyinthe buffer,the buffer
manager passes the address of the block in main memory to the requester. If the
10.8 Database Buffer 465
blockisnotinthebuffer,thebuffermanager?rstallocatesspaceinthebufferfor
theblock,throwingoutsomeotherblock,ifnecessary,tomakespaceforthenew
block. The thrown-out block is written back to disk only if it has been modi?ed
sincethemostrecenttimethatitwaswrittentothedisk.Then,thebuffermanager
reads in the requested block from the disk to the buffer, and passes the address
of the block in main memory to the requester. The internal actions of the buffer
manager aretransparentto theprogramsthat issuedisk-blockrequests.
If you are familiar with operating-system concepts, you will note that the
buffer manager appears to be nothing more than a virtual-memory manager,
like those found in most operating systems. One difference is that the size of
the database might be larger than the hardware address space of a machine,
so memory addresses are not suf?cient to address all disk blocks. Further, to
serve the database system well, the buffer manager must use techniques more
sophisticatedthantypicalvirtual-memorymanagement schemes:
• Buffer replacement strategy. When there is no room left in the buffer, a
block must be removed from the buffer before a new one can be read in.
Most operating systems use a least recently used (LRU) scheme, in which
the block that was referenced least recently is written back to disk and is
removed from the buffer. This simple approach can be improved on for
databaseapplications.
• Pinned blocks. For the database system to be able to recover from crashes
(Chapter 16), it is necessary to restrict those times when a block may be
writtenbacktodisk.Forinstance,mostrecoverysystemsrequirethatablock
should not be written to disk while an update on the block is in progress.
A block that is not allowed to be written back to disk is said to be pinned.
Although many operating systems do not support pinned blocks, such a
featureis essentialfor adatabase systemthat is resilientto crashes.
• Forcedoutputofblocks.Therearesituationsinwhichitisnecessarytowrite
back the block to disk, even though the buffer space that it occupies is not
needed. This write is called the forced output of a block. We shall see the
reason for forced output in Chapter 16; brie?y, main-memory contents and
thusbuffercontentsarelostinacrash,whereasdataondiskusuallysurvive
acrash.
10.8.2 Buffer-Replacement Policies
Thegoalofareplacementstrategyforblocksinthebufferistominimizeaccesses
tothedisk.Forgeneral-purposeprograms,itisnotpossibletopredictaccurately
whichblockswillbereferenced.Therefore,operatingsystemsusethepastpattern
of block references as a predictor of future references. The assumption generally
madeisthatblocks thathavebeenreferencedrecentlyarelikelytobereferenced
again.Therefore,ifablockmustbereplaced,theleastrecentlyreferencedblockis
replaced.Thisapproachiscalledtheleastrecentlyused(LRU)block-replacement
scheme.
466 Chapter 10 Storage and File Structure
foreach tuple i of instructor do
foreach tuple d of department do
if i[dept name]=d[dept name]
thenbegin
let xbe a tuplede?nedasfollows:
x[ID]:=i[ID]
x[dept name]:=i[dept name]
x[name]:=i[name]
x[salary]:=i[salary]
x[building]:=d[building]
x[budget]:=d[budget]
includetuple xas partof resultof instructor  department
end
end
end
Figure10.17 Procedure for computing join.
LRU is an acceptable replacement scheme in operating systems. However, a
databasesystemisabletopredictthepatternoffuturereferencesmoreaccurately
thananoperatingsystem.Auserrequesttothedatabasesysteminvolvesseveral
steps. The database system is often able to determine in advance which blocks
will be needed by looking at each of the steps required to perform the user-
requestedoperation.Thus,unlikeoperatingsystems,whichmustrelyonthepast
to predict the future, database systems may have information regarding at least
the short-termfuture.
Toillustratehowinformationaboutfutureblockaccessallowsustoimprove
the LRU strategy,consider theprocessingof the SQL query:
select *
from instructor naturaljoin department;
Assumethat thestrategychosentoprocessthisrequestisgivenby thepseu-
docode program shown in Figure 10.17. (We shall study other, more ef?cient,
strategiesinChapter12.)
Assume that the two relations of this example are stored in separate ?les.
In this example, we can see that, once a tuple of instructor has been processed,
that tuple is not needed again. Therefore, once processing of an entire block of
instructor tuples is completed, that block is no longer needed in main memory,
even though it has been used recently. The buffer manager should be instructed
to free the space occupied by an instructor block as soon as the ?nal tuple has
been processed. This buffer-management strategy is called the toss-immediate
strategy.
Nowconsiderblockscontainingdepartmenttuples.Weneedtoexamineevery
block of department tuples once for each tuple of the instructor relation. When
10.8 Database Buffer 467
processing of a department block is completed, we know that that block will not
beaccessedagainuntilallotherdepartmentblockshavebeenprocessed.Thus,the
most recently used department block will be the ?nal block to be re-referenced,
and the least recently used department block is the block that will be referenced
next. Thisassumption setisthe exactoppositeof the one that formsthe basis for
theLRUstrategy.Indeed,theoptimalstrategyforblockreplacementfortheabove
procedureisthemostrecentlyused(MRU)strategy.Ifadepartmentblockmustbe
removedfrom the buffer, the MRU strategychooses the most recently usedblock
(blocks arenot eligiblefor replacementwhile theyarebeing used).
Forthe MRUstrategytoworkcorrectlyforourexample,thesystemmustpin
the department block currently being processed. After the ?nal department tuple
hasbeenprocessed,theblockisunpinned,anditbecomesthemostrecentlyused
block.
In addition to using knowledge that the system may have about the request
being processed, the buffer manager can use statistical information about the
probabilitythatarequestwillreferenceaparticularrelation.Forexample,thedata
dictionary that (as we will see in detail in Section 10.7) keeps track of the logical
schemaofthe relationsaswellastheirphysicalstorageinformationisone ofthe
mostfrequentlyaccessedpartsofthedatabase.Thus,thebuffermanagershould
trynottoremovedata-dictionaryblocksfrommainmemory,unlessotherfactors
dictate that it do so. In Chapter 11, we discuss indices for ?les. Since an index
for a ?le may be accessed more frequentlythan the ?le itself,the buffer manager
should, in general, not remove index blocks from main memory if alternatives
areavailable.
Theidealdatabaseblock-replacementstrategyneedsknowledgeofthedata-
base operations—both those being performed and those that will be performed
in the future. No single strategy is known that handles all the possible scenarios
well. Indeed, a surprisingly large number of database systems use LRU,despite
that strategy’s faults. The practice questions and exercises explore alternative
strategies.
Thestrategythatthebuffermanagerusesforblockreplacementisin?uenced
by factors other than the time at which the block will be referenced again. If
thesystemisprocessingrequestsbyseveralusersconcurrently,theconcurrency-
control subsystem (Chapter 15) may need to delay certain requests, to ensure
preservation of database consistency. If the buffer manager is given informa-
tionfromtheconcurrency-controlsubsystemindicatingwhichrequestsarebeing
delayed,itcanusethisinformationtoalteritsblock-replacementstrategy.Specif-
ically,blocksneededbyactive(nondelayed)requestscanberetainedinthebuffer
at theexpenseof blocks neededby thedelayedrequests.
The crash-recovery subsystem (Chapter 16) imposes stringent constraints
on block replacement. If a block has been modi?ed, the buffer manager is not
allowedtowritebackthenewversionoftheblockinthebuffertodisk,sincethat
woulddestroytheoldversion.Instead,theblockmanagermustseekpermission
fromthecrash-recoverysubsystembeforewritingoutablock.Thecrash-recovery
subsystemmaydemandthatcertainotherblocksbeforce-outputbeforeitgrants
permission to the buffer manager to output the block requested. In Chapter 16,
468 Chapter 10 Storage and File Structure
we de?ne precisely the interaction between the buffer manager and the crash-
recoverysubsystem.
10.9 Summary
• Several types of data storage exist in most computer systems. They are clas-
si?ed by the speed with which they can access data, by their cost per unit of
data to buy the memory,and by their reliability.Among the mediaavailable
are cache, main memory, ?ash memory, magnetic disks, optical disks, and
magnetictapes.
• Two factors determine the reliability of storage media: whether a power
failure or system crash causes data to be lost, and what the likelihood is of
physical failureof the storagedevice.
• Wecan reducethelikelihoodofphysicalfailureby retainingmultiplecopies
of data. For disks, we can use mirroring. Or we can use more sophisticated
methodsbasedonredundantarraysofindependentdisks(RAID).Bystriping
dataacrossdisks,thesemethodsofferhighthroughputratesonlargeaccesses;
by introducing redundancy across disks, they improve reliability greatly.
Several different RAID organizations are possible, each with different cost,
performance,andreliabilitycharacteristics.RAIDlevel1(mirroring)andRAID
level5arethe mostcommonly used.
• We can organize a ?le logically as a sequence of records mapped onto disk
blocks. One approach to mapping the database to ?les is to use several ?les,
andtostorerecordsofonlyone?xedlengthinanygiven?le.Analternativeis
to structure?les so that they can accommodate multiplelengths for records.
The slotted-page method is widely used to handle varying length records
within a diskblock.
• Since data are transferred between disk storage and main memory in units
of a block, itis worthwhile to assign ?le recordsto blocks in such a way that
asingleblockcontainsrelatedrecords.Ifwecanaccessseveraloftherecords
wewantwithonlyoneblockaccess,wesavediskaccesses.Sincediskaccesses
are usually the bottleneck in the performance of a database system, careful
assignmentof recordsto blocks can pay signi?cant performance dividends.
• The data dictionary, also referred to as the system catalog, keeps track of
metadata, that is data about data, such as relation names, attribute names
and types,storageinformation, integrityconstraints, and userinformation.
• One way to reducethe number of diskaccesses is to keepas many blocks as
possible in main memory. Since it is not possible to keep all blocks in main
memory, we need to manage the allocation of the space available in main
memory for the storage of blocks. The buffer is that part of main memory
available for storage of copies of disk blocks. The subsystem responsible for
the allocation ofbuffer space is calledthe buffer manager.
Review Terms 469
ReviewTerms
• Physical storagemedia
?
Cache
?
Mainmemory
?
Flashmemory
?
Magneticdisk
?
Optical storage
• Magneticdisk
?
Platter
?
Harddisks
?
Floppydisks
?
Tracks
?
Sectors
?
Read–write head
?
Diskarm
?
Cylinder
?
Diskcontroller
?
Checksums
?
Remapping ofbad sectors
• Performance measuresof disks
?
Access time
?
Seektime
?
Rotational latency
?
Data-transfer rate
?
Meantimetofailure(MTTF)
• Diskblock
• Optimizationof disk-blockaccess
?
Disk-armscheduling
?
Elevatoralgorithm
?
Fileorganization
?
Defragmenting
?
Nonvolatilewritebuffers
?
Nonvolatilerandom-access
memory(NVRAM)
?
Logdisk
• Redundant arrays of independent
disks(RAID)
?
Mirroring
?
Datastriping
?
Bit-levelstriping
?
Block-levelstriping
• RAID levels
?
Level0(block striping,no
redundancy)
?
Level1(block striping,
mirroring)
?
Level3(bitstriping,parity)
?
Level5(block striping,
distributedparity)
?
Level6(blockstriping,P+Qre-
dundancy)
• Rebuild performance
• Software RAID
• Hardware RAID
• Hot swapping
• Tertiarystorage
?
Opticaldisks
?
Magnetictapes
?
Jukeboxes
• File
• Fileorganization
?
Fileheader
470 Chapter 10 Storage and File Structure
?
Freelist
• Variable-lengthrecords
?
Slotted-pagestructure
• Largeobjects
• Heap?le organization
• Sequential?leorganization
• Hashing ?le organization
• Multitable clustering ?le organiza-
tion
• Searchkey
• Data dictionary
• Systemcatalog
• Buffer
?
Buffermanager
?
Pinned blocks
?
Forcedoutputof blocks
• Buffer-replacementpolicies
?
Leastrecentlyused(LRU)
?
Toss-immediate
?
Mostrecentlyused(MRU)
PracticeExercises
10.1 Considerthedataandparity-blockarrangementonfourdisksdepictedin
Figure10.18.The B
i
srepresentdatablocks;the P
i
srepresentparityblocks.
Parityblock P
i
istheparityblockfordatablocks B
4i?3
to B
4i
.What,ifany,
problemmightthis arrangementpresent?
10.2 Flashstorage:
a. Howisthe?ashtranslationtable,whichisusedtomaplogicalpage
numbers tophysical pagenumbers, createdinmemory?
b. Suppose you have a 64 gigabyte ?ash storage system, with a 4096
byte page size. How big would the ?ash translation table be, as-
suming each page has a 32 bit address,and the table is stored as an
array.
c. Suggest how to reduce the size of the translation table if very often
long ranges of consecutive logical page numbers are mapped to
consecutive physical page numbers.
Disk 1 Disk 2 Disk 3 Disk 4
B
1
P
1
B
8
…
B
2
B
5
P
2
…
B
3
B
6
B
9
…
B
4
B
7
B
10
…
Figure10.18 Data and parity block arrangement.
PracticeExercises 471
10.3 Apowerfailurethatoccurswhileadiskblockisbeingwrittencouldresult
in the block being only partially written. Assume that partially written
blocks can be detected. An atomic block write is one where either the
disk block is fully written or nothing is written (i.e., there are no partial
writes). Suggest schemes for getting the effect of atomic block writes
with the following RAID schemes. Your schemes should involve work on
recovery from failure.
a. RAID level1(mirroring)
b. RAID level5(block interleaved,distributedparity)
10.4 Considerthedeletionofrecord5fromthe?leofFigure10.6.Comparethe
relativemeritsofthefollowingtechniquesforimplementingthedeletion:
a. Move record 6 to the space occupied by record 5, and move record
7 tothe space occupied by record 6.
b. Move record 7 to the space occupied by record 5.
c. Markrecord5asdeleted,andmovenorecords.
10.5 Show the structure of the ?le of Figure 10.7 after each of the following
steps:
a. Insert(24556, Turnamian, Finance, 98000).
b. Delete record 2.
c. Insert(34556, Thompson, Music, 67000).
10.6 Considertherelationssectionand takes.Giveanexampleinstanceofthese
tworelations,withthreesections,eachofwhich has?vestudents.Givea
?lestructureof theserelationsthat usesmultitableclustering.
10.7 Consider the following bitmap technique for tracking free space in a ?le.
For each block in the ?le, two bits are maintained in the bitmap. If the
block is between 0 and 30 percent full the bits are 00, between 30 and 60
percent the bits are 01, between 60 and 90 percent the bits are 10, and
above 90 percent the bits are 11. Such bitmaps can be kept in memory
evenforquitelarge?les.
a. Describehowtokeepthebitmapuptodateonrecordinsertionsand
deletions.
b. Outlinethebene?t ofthebitmaptechniqueoverfreelistsin search-
ing forfree space and in updatingfreespace information.
10.8 It is important to be able to quickly ?nd out if a block is present in the
buffer, and if so where in the buffer it resides.Giventhat database buffer
sizes are very large, what (in-memory) data structure would you use for
theabove task?
472 Chapter 10 Storage and File Structure
10.9 Giveanexampleofarelational-algebraexpressionandaquery-processing
strategyineach ofthe following situations:
a. MRU ispreferableto LRU.
b. LRU ispreferableto MRU.
Exercises
10.10 List the physical storage media available on the computers you use rou-
tinely.Givethe speedwithwhich datacan beaccessedoneach medium.
10.11 How does the remapping of bad sectors by disk controllers affect data-
retrievalrates?
10.12 RAIDsystemstypicallyallowyoutoreplacefaileddiskswithoutstopping
access tothesystem.Thus, thedatain thefaileddiskmustberebuiltand
writtentothereplacementdiskwhilethesystemisinoperation.Whichof
theRAIDlevelsyieldstheleastamountofinterferencebetweentherebuild
and ongoing diskaccesses? Explainyour answer.
10.13 What is scrubbing, in the context of RAID systems, and why is scrubbing
important?
10.14 In the variable-length record representation, a null bitmap is used to
indicateifan attributehas thenull value.
a. For variablelength ?elds,ifthe valueis null, what would be stored
in theoffsetand length?elds?
b. Insomeapplications,tupleshaveaverylargenumberofattributes,
most of which are null. Can you modify the record representation
such that the only overhead for a null attribute is the single bit in
thenull bitmap.
10.15 Explain why the allocation of records to blocks affects database-system
performance signi?cantly.
10.16 Ifpossible,determinethebuffer-managementstrategyusedbytheoperat-
ingsystemrunningonyourlocalcomputersystemandwhatmechanisms
it provides to control replacement of pages. Discuss how the control on
replacement that it provides would be useful for the implementation of
databasesystems.
10.17 Listtwoadvantagesandtwodisadvantagesofeachofthefollowingstrate-
giesforstoring a relationaldatabase:
a. Storeeach relationin one ?le.
b. Store multiple relations (perhaps even the entire database) in one
?le.
Bibliographical Notes 473
10.18 In the sequential ?le organization, why is an over?ow block used even if
thereis,at themoment,only one over?owrecord?
10.19 GiveanormalizedversionoftheIndex metadatarelation,andexplainwhy
using thenormalizedversionwouldresultinworseperformance.
10.20 If you have data that should not be lost on disk failure, and the data are
writeintensive,howwould you storethedata?
10.21 In earlier generation disks the number of sectors per track was the same
acrossalltracks.Currentgenerationdiskshavemoresectorspertrackon
outer tracks, and fewer sectors per track on inner tracks (since they are
shorterinlength).What istheeffectofsuchachange oneachofthethree
mainindicatorsof diskspeed?
10.22 Standardbuffermanagersassumeeachblockisofthesamesizeandcosts
thesametoread.Considerabuffermanagerthat,insteadofLRU,usesthe
rate of reference to objects, that is, how often an object has been accessed
in the last n seconds. Suppose we want to store in the buffer objects of
varyingsizes,andvaryingreadcosts(suchasWebpages,whosereadcost
depends on the site from which they are fetched). Suggest how a buffer
manager maychoose which block toevictfrom the buffer.
BibliographicalNotes
Hennessy et al. [2006] is a popular textbook on computer architecture, which
includes coverage of hardware aspects of translation look-aside buffers, caches,
and memory-management units. Rosch [2003] presents an excellent overview of
computerhardware,includingextensivecoverageofalltypesofstoragetechnol-
ogysuchasmagneticdisks,opticaldisks,tapes,andstorageinterfaces.Patterson
[2004] provides a good discussion on how latency improvements have lagged
behind bandwidth(transferrate)improvements.
Withtherapidincreasein CPU speeds,cache memorylocatedalong withthe
CPUhasbecomemuchfasterthanmainmemory .Althoughdatabasesystems
do not control what data is kept in cache, there is an increasing motivation to
organizedatainmemoryandwriteprogramsinsuchawaythatcacheutilization
is maximized. Work in this area includes Rao and Ross [2000], Ailamaki et al.
[2001], Zhou and Ross [2004], Garcia and Korth [2005], and Cieslewicz et al.
[2009].
Thespeci?cationsofcurrent-generationdiskdrivescanbeobtainedfromthe
Websitesoftheirmanufacturers,suchasHitachi,LaCie,Iomega,Seagate,Maxtor,
and WesternDigital.
Discussions of redundant arrays of inexpensive disks (RAID) are presented
by Patterson et al. [1988]. Chen et al. [1994] presents an excellent survey of RAID
principlesandimplementation.Reed–SolomoncodesarecoveredinPless[1998].
Buffering data in mobile systems is discussed in Imielinski and Badrinath
[1994], Imielinskiand Korth[1996], and Chandrasekaran etal.[2003].
474 Chapter 10 Storage and File Structure
The storage structure of speci?c database systems, such as IBM DB2, Oracle,
MicrosoftSQLServer,andPostgreSQLaredocumentedintheirrespectivesystem
manuals.
Buffermanagementisdiscussedinmostoperating-systemtexts,includingin
Silberschatz et al. [2008]. Chou and Dewitt [1985] presents algorithms for buffer
management in databasesystems,and describesa performanceevaluation.
CHAPTER
11
Indexing and Hashing
Many queries reference only a small proportion of the records in a ?le. For ex-
ample, a query like “Find all instructors in the Physics department”or “Find the
total number of credits earned by the student with ID 22201” references only a
fraction of the student records. It is inef?cient for the system to read every tuple
in the instructor relation to check if the dept name value is “Physics”. Likewise, it
is inef?cient to read the entire student relation just to ?nd the one tuple for the
ID “32556,”. Ideally, the system should be able to locate these records directly. To
allow these forms of access, we design additional structures that we associate
with?les.
11.1 Basic Concepts
Anindexfora?leinadatabasesystemworksinmuchthesamewayastheindex
inthistextbook.Ifwewant tolearnaboutaparticulartopic(speci?edbyaword
or a phrase) in this textbook, we can search for the topic in the index at the back
of the book, ?nd the pages where it occurs, and then read the pages to ?nd the
informationforwhichwearelooking.Thewordsintheindexareinsortedorder,
making it easy to ?nd the word we want. Moreover, the index is much smaller
than thebook, furtherreducingthe effortneeded.
Database-system indices play the same role as book indices in libraries. For
example,toretrieveastudentrecordgivenan ID,thedatabasesystemwouldlook
up an index to ?nd on which disk block the corresponding record resides, and
thenfetchthediskblock, togetthe appropriatestudent record.
Keeping a sorted list of students’ ID would not work well on very large
databases with thousands of students, since the index would itself be very big;
further, eventhough keepingthe indexsorted reducesthe search time,?nding a
studentcanstillberathertime-consuming.Instead,moresophisticatedindexing
techniques may be used. We shall discuss several of these techniques in this
chapter.
Therearetwo basic kindsof indices:
• Orderedindices.Basedonasortedorderingofthevalues.
475
476 Chapter 11 Indexing andHashing
• Hash indices. Based on a uniform distribution of values across a range of
buckets.Thebuckettowhichavalueisassignedisdeterminedbyafunction,
calledahashfunction.
Weshallconsiderseveraltechniquesforbothorderedindexingandhashing.
No one technique is the best. Rather, each technique is best suited to particular
database applications. Each technique must be evaluated on the basis of these
factors:
• Accesstypes:Thetypesofaccessthataresupportedef?ciently.Accesstypes
can include ?nding records with a speci?ed attribute value and ?nding
recordswhose attributevaluesfallinaspeci?edrange.
• Access time: The time it takes to ?nd a particular data item, or set of items,
using thetechniqueinquestion.
• Insertiontime:Thetimeittakestoinsertanewdataitem.Thisvalueincludes
the time it takes to ?nd the correct place to insert the new data item, as well
as thetimeittakestoupdatetheindexstructure.
• Deletion time: The time it takes to delete a data item. This value includes
the time it takes to ?nd the item to be deleted, as well as the time it takes to
updatethe indexstructure.
• Space overhead: The additional space occupied by an index structure. Pro-
vided that the amount of additional space is moderate, it is usually worth-
whileto sacri?ce the spaceto achieveimprovedperformance.
We often want to have more than one index for a ?le. For example, we may
wishto searchfora book by author, by subject,or by title.
An attribute or set of attributes used to look up records in a ?le is called a
search key. Note that this de?nition of key differs from that used in primary key,
candidatekey,andsuperkey.Thisduplicatemeaningforkeyis(unfortunately)well
established in practice. Using our notion of a search key, we see that if there are
severalindicesona?le,thereareseveralsearchkeys.
11.2 Ordered Indices
Togainfastrandomaccesstorecordsina?le,wecanuseanindexstructure.Each
index structure is associated with a particular search key. Just like the index of a
bookoralibrarycatalog,anorderedindexstoresthevaluesofthesearchkeysin
sortedorder,andassociates witheachsearchkeytherecordsthat contain it.
Therecordsintheindexed?lemaythemselvesbestoredinsomesortedorder,
justasbooksinalibraryarestoredaccordingtosomeattributesuchastheDewey
decimal number. A ?le may have several indices, on different search keys. If the
?lecontaining the recordsissequentiallyordered,aclusteringindexis anindex
whose search key also de?nes the sequential order of the ?le. Clustering indices
11.2 OrderedIndices 477
10101 Srinivasan
45565 Katz
58583 Cali?eri
76543 Singh
76766 Crick
83821 Brandt
98345 Kim
12121 Wu
15151 Mozart
22222 Einstein
32343 El Said
33456 Gold
Comp. Sci.
Comp. Sci.
Comp. Sci.
History
Finance
Biology
Elec. Eng.
Finance
Music
Physics
History
Physics
65000
75000
62000
80000
72000
92000
80000
90000
40000
95000
60000
87000
Figure 11.1 Sequential ?le for instructor records.
arealsocalledprimaryindices;thetermprimaryindexmayappeartodenotean
index on a primary key, but such indices can in fact be built on any search key.
The search key of a clustering index is often the primary key, although that is
notnecessarilyso.Indiceswhosesearchkeyspeci?esanorderdifferentfromthe
sequentialorderofthe?learecallednonclusteringindices,orsecondaryindices.
The terms “clustered” and “nonclustered” are often used in place of “clustering”
and “nonclustering.”
InSections11.2.1through11.2.3,weassumethatall?lesareorderedsequen-
tially on some search key. Such ?les, with a clustering index on the search key,
arecalledindex-sequential?les.Theyrepresentoneoftheoldestindexschemes
used in database systems. They are designed for applications that require both
sequential processing of the entire ?le and random access to individual records.
InSection11.2.4wecoversecondary indices.
Figure 11.1 shows a sequential ?le of instructor records taken from our uni-
versity example. In the example of Figure 11.1, the records are stored in sorted
orderof instructor ID,which isusedas thesearchkey.
11.2.1 Dense and Sparse Indices
An index entry,orindex record, consists of a search-key value and pointers to
one or more records with that value as their search-key value. The pointer to a
recordconsistsoftheidenti?erofadiskblockandanoffsetwithinthediskblock
to identifytherecordwithinthe block.
Therearetwotypesoforderedindicesthat we canuse:
478 Chapter 11 Indexing andHashing
10101
12121
15151
22222
32343
33456
45565
58583
76543
76766
83821
98345
10101 Srinivasan
45565 Katz
58583 Cali?eri
76543 Singh
76766 Crick
83821 Brandt
98345 Kim
12121 Wu
15151 Mozart
22222 Einstein
32343 El Said
33456 Gold
Comp. Sci.
Comp. Sci.
Comp. Sci.
History
Finance
Biology
Elec. Eng.
Finance
Music
Physics
History
Physics
65000
75000
62000
80000
72000
92000
80000
90000
40000
95000
60000
87000
Figure 11.2 Dense index.
• Dense index: In a dense index, an index entry appears for every search-key
value in the ?le. In a dense clustering index, the index record contains the
search-key value and a pointer to the ?rst data record with that search-key
value.Therestoftherecordswiththesamesearch-keyvaluewouldbestored
sequentiallyafterthe?rstrecord,since,becausetheindexisaclusteringone,
records are sorted on the same search key.
In a dense nonclustering index, the index must store a list of pointers to
allrecordswiththesamesearch-key value.
• Sparseindex:Inasparseindex,anindexentryappearsforonly someof the
search-key values.Sparseindicescan be usedonly ifthe relationis storedin
sorted order of the search key, that is, if the index is a clustering index. As
is true in dense indices, each index entry contains a search-key value and a
pointertothe?rstdatarecordwiththatsearch-keyvalue.Tolocatearecord,
we ?nd the index entry with the largest search-key value that is less than or
equaltothesearch-keyvalueforwhichwearelooking.Westartattherecord
pointedtobythatindexentry,andfollowthepointersinthe?leuntilwe?nd
thedesiredrecord.
Figures 11.2 and 11.3 show dense and sparse indices, respectively, for the
instructor ?le. Suppose that we are looking up the record of instructor with ID
“22222”. Using the dense index of Figure 11.2, we follow the pointer directly to
the desired record. Since ID is a primary key, there exists only one such record
and the search is complete. If we are using the sparse index (Figure 11.3), we
do not ?nd an index entry for “22222”. Since the last entry (in numerical order)
before “22222” is “10101”, we follow that pointer. We then read the instructor ?le
insequentialorderuntilwe ?nd thedesiredrecord.
11.2 OrderedIndices 479
Consider a (printed) dictionary. The header of each page lists the ?rst word
alphabetically on that page. The words at the top of each page of the book index
togetherformasparseindexonthecontents ofthe dictionarypages.
As another example, suppose that the search-key value is not not a primary
key. Figure 11.4 shows a dense clustering index for the instructor ?le with the
search key being dept name. Observe that in this case the instructor ?le is sorted
on the search key dept name,insteadofID, otherwise the index on dept name
would be a nonclustering index. Suppose that we are looking up records for
the History department. Using the dense index of Figure 11.4, we follow the
pointerdirectlytothe?rstHistoryrecord.Weprocessthisrecord,andfollowthe
pointerinthatrecordtolocatethenextrecordinsearch-key(dept name)order.We
continue processing records until we encounter a record for a department other
than History.
As we have seen, it is generally faster to locate a record if we have a dense
indexrather than a sparse index. However,sparse indiceshave advantagesover
dense indices in that they require less space and they impose less maintenance
overheadfor insertionsand deletions.
Thereisa trade-offthat thesystemdesignermust makebetweenaccess time
and space overhead. Although the decision regarding this trade-off depends on
the speci?c application, a good compromise is to have a sparse index with one
index entry per block. The reason this design is a good trade-off is that the
dominant cost in processing a database request is the time that it takes to bring
a block from disk into main memory. Once we have brought in the block, the
time to scan the entire block is negligible. Using this sparse index, we locate the
block containing the record that we are seeking. Thus, unless the record is on an
over?ow block (see Section 10.6.1), we minimize block accesses while keeping
thesizeof the index(and thus our spaceoverhead)assmallas possible.
10101
32343
76766
10101 Srinivasan
45565 Katz
58583 Cali?eri
76543 Singh
76766 Crick
83821 Brandt
98345 Kim
12121 Wu
15151 Mozart
22222 Einstein
32343 El Said
33456 Gold
Comp. Sci.
Comp. Sci.
Comp. Sci.
History
Finance
Biology
Elec. Eng.
Finance
Music
Physics
History
Physics
65000
75000
62000
80000
72000
92000
80000
90000
40000
95000
60000
87000
Figure 11.3 Sparse index.
480 Chapter 11 Indexing andHashing
Biology
Comp. Sci.
Elec. Eng.
Finance
History
Music
Physics
76766 Crick
76543 Singh
32343 El Said
58583 Cali?eri
15151 Mozart
22222 Einstein
33465 Gold
10101 Srinivasan
45565 Katz
83821 Brandt
98345 Kim
12121 Wu
Biology
Physics
Finance
History
History
Music
Physics
Comp. Sci.
Comp. Sci.
Comp. Sci.
Elec. Eng.
Finance
72000
80000
60000
62000
40000
95000
87000
65000
75000
92000
80000
90000
Figure 11.4 Dense index with search key dept name.
For the preceding technique to be fully general, we must consider the case
whererecordsforonesearch-keyvalueoccupyseveralblocks.Itiseasytomodify
ourschemetohandlethissituation.
11.2.2 Multilevel Indices
Supposewebuildadenseindexonarelationwith1,000,000tuples.Indexentries
are smaller than data records, so let us assume that 100 index entries ?t on a
4 kilobyte block. Thus, our index occupies 10,000 blocks. If the relation instead
had 100,000,000 tuples, the index would instead occupy 1,000,000 blocks, or 4
gigabytesof space.Suchlargeindicesarestoredas sequential?lesondisk.
If an index is small enough to be kept entirely in main memory, the search
time to ?nd an entry is low. However, if the index is so large that not all of it
can be kept in memory, index blocks must be fetched from disk when required.
(Evenifanindexissmallerthan themainmemoryofacomputer,mainmemory
is also required for a number of other tasks, so it may not be possible to keep
the entire index in memory.) The search for an entry in the index then requires
severaldisk-blockreads.
Binary search can be used on the index ?le to locate an entry, but the search
stillhasalargecost.Iftheindexwouldoccupybblocks,binarysearchrequiresas
many as null log
2
(b)null blocks to be read. (null xnull denotes the least integer that is greater
than or equal to x; that is, we round upward.) For a 10,000-block index, binary
search requires 14 block reads. On a disk system where a block read takes on
average 10 milliseconds, the index search will take 140 milliseconds. This may
not seem much, but we would be able to carry out only seven index searches a
second, whereas a more ef?cient search mechanism would let us carry out far
more searches per second, as we shall see shortly. Note that, if over?ow blocks
have been used, binary search is not possible. In that case, a sequential search is
typicallyused,andthatrequiresbblockreads,whichwilltakeevenlonger.Thus,
the processofsearching alargeindexmaybe costly.
11.2 OrderedIndices 481
…
… …
…
outer index
index
block 0
index
block 1
data
block 0
data
block 1
inner index
Figure 11.5 Two-level sparse index.
Todealwiththisproblem,wetreattheindexjustaswewouldtreatanyother
sequential?le,andconstructasparseouterindexontheoriginalindex,whichwe
now callthe inner index,as shown inFigure11.5. Notethat theindexentriesare
always in sorted order, allowing the outer index to be sparse. To locate a record,
we ?rst use binary search on the outer index to ?nd the record for the largest
search-key value less than or equal to the one that we desire. The pointer points
to a block of the inner index. We scan this block until we ?nd the record that
has the largest search-key value less than or equal to the one that we desire.The
pointer in this record points to the block of the ?le that contains the record for
which we arelooking.
Inourexample,aninnerindexwith10,000blockswouldrequire10,000entries
in the outer index, which would occupy just 100 blocks. If we assume that the
outer index is already in main memory, we would read only one index block for
a search using a multilevel index, rather than the 14 blocks we read with binary
search.As aresult,we canperform14timesasmany indexsearches persecond.
Ifour?leisextremelylarge,eventheouterindexmaygrowtoolargeto?tin
main memory. With a 100,000,000 tuple relation, the inner index would occupy
482 Chapter 11 Indexing andHashing
1,000,000 blocks, and the outer index occupies 10,000 blocks, or 40 megabytes.
Sincetherearemanydemandsonmainmemory,itmaynotbepossibletoreserve
that much main memory just for this particular outer index. In such a case, we
can createyetanother levelofindex.Indeed,wecan repeatthis processasmany
timesasnecessary.Indiceswithtwoormorelevelsarecalledmultilevelindices.
Searching for records with a multilevel index requires signi?cantly fewer I/O
operationsthan doessearching forrecordsby binary search.
1
Multilevel indices are closely related to tree structures, such as the binary
trees used for in-memory indexing. We shall examine the relationship later, in
Section11.3.
11.2.3 Index Update
Regardlessofwhatformofindexisused,everyindexmustbeupdatedwhenever
arecordiseitherinsertedintoordeletedfromthe?le.Further,incasearecordin
the?leisupdated,anyindexwhosesearch-keyattributeisaffectedbytheupdate
mustalsobeupdated;forexample,ifthedepartmentofaninstructorischanged,
anindexonthedept nameattributeofinstructormustbeupdatedcorrespondingly.
Such a record update can be modeled as a deletion of the old record, followed
byaninsertionofthenewvalueoftherecord,which resultsinanindexdeletion
followed by an index insertion. As a result we only need to consider insertion
and deletiononanindex,and donot needtoconsiderupdatesexplicitly.
We?rst describealgorithmsfor updatingsingle-levelindices.
• Insertion. First, the system performs a lookup using the search-key value
that appears in the record to be inserted. The actions the system takes next
dependon whether theindexisdenseorsparse:
?
Denseindices:
1. Ifthesearch-keyvaluedoesnotappearintheindex,thesysteminserts
anindexentrywiththesearch-keyvalueintheindexattheappropriate
position.
2. Otherwisethefollowing actions aretaken:
a. Iftheindexentrystorespointerstoallrecordswiththesamesearch-
keyvalue,thesystemaddsapointertothenewrecordintheindex
entry.
b. Otherwise, the index entry stores a pointer to only the ?rst record
withthesearch-keyvalue.Thesystemthenplacestherecordbeing
insertedaftertheother recordswiththe samesearch-keyvalues.
?
Sparse indices: We assume that the index stores an entry for each block.
If the system creates a new block, it inserts the ?rst search-key value (in
1
Intheearlydaysofdisk-basedindices,eachleveloftheindexcorrespondedtoaunitofphysicalstorage.Thus,wemay
have indices at the track, cylinder, and disk levels. Such a hierarchy does not make sense today since disk subsystems
hide the physical details of disk storage, and the number of disks and platters per disk is very small compared to the
number ofcylindersorbytes pertrack.
11.2 OrderedIndices 483
search-keyorder)appearinginthenewblockintotheindex.Ontheother
hand, if the new record has the least search-key value in its block, the
system updates the index entry pointing to the block; if not, the system
makesno change to theindex.
• Deletion.Todeletearecord,thesystem?rstlooksuptherecordtobedeleted.
The actions the system takes next depend on whether the index is dense or
sparse:
?
Denseindices:
1. Ifthedeletedrecordwastheonlyrecordwithitsparticularsearch-key
value,thenthesystemdeletesthecorrespondingindexentryfromthe
index.
2. Otherwisethefollowing actions aretaken:
a. Iftheindexentrystorespointerstoallrecordswiththesamesearch-
keyvalue,thesystemdeletesthepointertothedeletedrecordfrom
theindexentry.
b. Otherwise, the index entry stores a pointer to only the ?rst record
withthesearch-keyvalue.Inthiscase,ifthedeletedrecordwasthe
?rstrecordwiththesearch-keyvalue,thesystemupdatestheindex
entrytopointtothenext record.
?
Sparseindices:
1. Iftheindexdoesnotcontainanindexentrywiththesearch-keyvalue
ofthedeletedrecord,nothing needstobedone tothe index.
2. Otherwisethesystemtakesthefollowing actions:
a. If the deleted record was the only record with its search key, the
system replaces the corresponding index record with an index rec-
ord for the next search-key value (in search-key order). If the next
search-key value already has an index entry, the entry is deleted
insteadofbeingreplaced.
b. Otherwise, if the index entry for the search-key value points to the
record being deleted, the system updates the index entry to point
tothe nextrecordwiththesamesearch-key value.
Insertionanddeletionalgorithmsformultilevelindicesareasimpleextension
of the scheme just described. On deletion or insertion, the system updates the
lowest-level index as described. As far as the second level is concerned, the
lowest-levelindexismerelya?lecontainingrecords—thus,ifthereisanychange
inthelowest-levelindex,thesystemupdatesthesecond-levelindexasdescribed.
Thesame techniqueappliestofurtherlevelsofthe index,ifthereareany.
11.2.4 Secondary Indices
Secondaryindicesmustbedense,withanindexentryforeverysearch-keyvalue,
andapointertoeveryrecordinthe?le.Aclusteringindexmaybesparse,storing
484 Chapter 11 IndexingandHashing
only some of the search-key values, since it is always possible to ?nd records
with intermediatesearch-key values by a sequential access to a part of the ?le, as
described earlier. If a secondary index stores only some of the search-key values,
records with intermediate search-key values may be anywhere in the ?le and, in
general, we cannot ?nd them without searching the entire ?le.
A secondary index on a candidate key looks just like a dense clustering
index,exceptthat therecordspointedtobysuccessivevaluesintheindexarenot
stored sequentially. In general, however, secondary indices may have a different
structure from clustering indices. If the search key of a clustering index is not a
candidate key, it suf?ces if the index points to the ?rst record with a particular
value for the search key, since the other records can be fetched by a sequential
scan of the ?le.
In contrast, if the search key of a secondary index is not a candidate key, it
is not enough to point to just the ?rst record with each search-key value. The
remainingrecordswiththesamesearch-keyvaluecouldbeanywhereinthe?le,
since the records are ordered by the search key of the clustering index, rather
thanbythesearchkeyofthesecondaryindex.Therefore,asecondaryindexmust
contain pointers to all the records.
We can use an extra level of indirection to implement secondary indices on
search keys that are not candidate keys. The pointers in such a secondary index
do not point directly to the ?le. Instead, each points to a bucket that contains
pointerstothe ?le.Figure11.6 shows the structureofa secondaryindexthat uses
an extra level of indirection on the instructor ?le, on the search key salary.
A sequential scan in clustering index order is ef?cient because records in
the ?le are stored physically in the same order as the index order. However,
we cannot (except in rare special cases) store a ?le physically ordered by both
the search key of the clustering index and the search key of a secondary index.
40000
60000
62000
65000
72000
75000
80000
87000
90000
92000
95000
10101 Srinivasan Comp.Sci. 65000
12121 Wu Finance 90000
15151 Mozart Music 40000
22222 Einstein Physics 95000
32343 ElSaid History 60000
33456 Gold Physics 87000
45565 Katz Comp.Sci. 75000
58583 Cali?eri History 62000
76543 Singh Finance 80000
76766 Crick Biology 72000
83821 Brandt Comp.Sci. 92000
98345 Kim Elec.Eng. 80000
Figure 11.6 Secondary index on instructor ?le, on noncandidate key salary.
11.3 B
+
-TreeIndex Files 485
AUTOMATICCREATIONOF INDICES
If a relation is declared to have a primary key, most database implementations
automatically createan index ontheprimarykey.Wheneveratuple is inserted
intotherelation,theindexcanbeusedtocheckthattheprimarykeyconstraintis
notviolated(thatis,therearenoduplicatesontheprimarykeyvalue).Without
the index on the primary key, whenever a tuple is inserted, the entire relation
would haveto bereadtoensurethat theprimary-keyconstraintis satis?ed.
Becausesecondary-keyorderandphysical-keyorderdiffer,ifweattempttoscan
the ?le sequentially in secondary-key order, the reading of each record is likely
torequirethe readingof a newblock from disk,which isveryslow.
Theproceduredescribedearlierfordeletionandinsertioncanalsobeapplied
to secondary indices; the actions taken are those described for dense indices
storingapointertoeveryrecordinthe?le.Ifa?lehasmultipleindices,whenever
the?le ismodi?ed,everyindexmust beupdated.
Secondary indices improve the performance of queries that use keys other
than the search key of the clustering index. However, they impose a signi?cant
overhead on modi?cation of the database. The designer of a database decides
which secondary indices are desirable on the basis of an estimate of the relative
frequencyof queriesandmodi?cations.
11.2.5 Indices on Multiple Keys
Althoughtheexampleswehaveseensofarhavehadasingleattributeina
searchkey,ingeneralasearchkeycanhavemorethanoneattribute.Asearchkey
containing more than one attribute is referredto as a composite search key.The
structure of the index is the same as that of any other index, the only difference
being that the searchkey isnot a single attribute,but rather is a list of attributes.
The search key can be represented as a tuple of values, of the form (a
1
,...,a
n
),
wheretheindexedattributesare A
1
,...,A
n
.Theorderingofsearch-keyvaluesis
the lexicographic ordering. For example, for the case of two attribute search keys,
(a
1
,a
2
) < (b
1
,b
2
)ifeithera
1
< b
1
or a
1
= b
1
and a
2
< b
2
. Lexicographic ordering
isbasically thesame asalphabeticorderingof words.
Asanexample,consideranindexonthetakesrelation,onthecompositesearch
key (course id, semester, year). Such an index would be useful to ?nd all students
who have registered for a particular course in a particular semester/year. An
orderedindexonacompositekeycanalsobeusedtoanswerseveralotherkinds
of queriesef?ciently,as weshall seelaterinSection11.5.2.
11.3 B
+
-Tree Index Files
The main disadvantage of the index-sequential ?le organization is that perfor-
mancedegradesasthe?legrows,bothforindexlookupsandforsequentialscans
486 Chapter 11 Indexing andHashing
through the data. Although this degradation can be remediedby reorganization
of the?le,frequentreorganizationsareundesirable.
TheB
+
-treeindexstructureisthemostwidelyusedofseveralindexstructures
that maintain their ef?ciency despite insertion and deletion of data. A B
+
-tree
index takes the form of a balanced tree in which every path from the root of the
tree to a leaf of the tree is of the same length. Each nonleaf node in the tree has
between null n/2null andnchildren,wheren is?xedfor aparticular tree.
We shall seethat the B
+
-treestructureimposes performance overhead on in-
sertion and deletion, and adds space overhead. The overhead is acceptable even
for frequentlymodi?ed?les,since the cost of?le reorganizationis avoided.Fur-
thermore,since nodes may be as much as half empty(if theyhave the minimum
number of children), there is some wasted space. This space overhead, too, is
acceptable giventhe performancebene?ts of theB
+
-treestructure.
11.3.1 Structure of a B
+
-Tree
AB
+
-tree index is a multilevel index, but it has a structure that differs from that
of the multilevel index-sequential ?le. Figure 11.7 shows a typical node of a B
+
-
tree.Itcontains up ton ? 1search-keyvaluesK
1
, K
2
,...,K
n?1
,andn pointers
P
1
, P
2
,...,P
n
.Thesearch-keyvalueswithinanodearekeptinsortedorder;thus,
ifi < j,thenK
i
< K
j
.
Weconsider?rstthestructureoftheleafnodes.Fori = 1,2,...,n?1,pointer
P
i
pointstoa?lerecordwithsearch-keyvalueK
i
.PointerP
n
hasaspecialpurpose
that we shalldiscussshortly.
Figure 11.8 shows one leaf node of a B
+
-tree for the instructor ?le, in which
we have chosenn to be4,and the searchkeyisname.
Now that we have seen the structure of a leaf node, let us consider how
search-keyvaluesareassignedtoparticularnodes.Eachleafcanholdupton ? 1
values. We allow leaf nodes to contain as few as null (n ? 1)/2null values. With n = 4
in our example B
+
-tree, each leaf must contain at least 2 values, and at most 3
values.
Therangesofvaluesineachleafdonotoverlap,exceptifthereareduplicate
search-key values, in which case a value may be present in more than one leaf.
Speci?cally, if L
i
and L
j
are leaf nodes and i < j, then every search-key value
in L
i
is less than or equal to every search-key value in L
j
.IftheB
+
-tree index is
usedasadenseindex(asisusuallythecase)everysearch-keyvaluemustappear
insomeleafnode.
Nowwecanexplaintheuseofthepointer P
n
.Sincethereisalinearorderon
the leaves based on the search-key values that they contain, we use P
n
to chain
P
1
K
1
P
2
P
n-1
K
n-1
P
n
…
Figure 11.7 Typical node of a B
+
-tree.
11.3 B
+
-TreeIndex Files 487
leaf node
Pointer to next leaf node
instructor ?le
Brandt
Srinivasan
Cali?eri Crick
Comp. Sci. 65000
Wu Finance 90000
Mozart Music 40000
Einstein Physics 95000
El Said History 80000
Gold Physics 87000
Katz Comp. Sci. 75000
Cali?eri History 60000
Singh Finance 80000
Crick Biology 72000
Brandt Comp. Sci. 92000
15151
10101
12121
22222
32343
33456
45565
58583
76543
76766
83821
98345 Kim Elec. Eng. 80000
Figure 11.8 A leaf node for instructor B
+
-tree index (n = 4).
together the leaf nodes in search-key order. This ordering allows for ef?cient
sequentialprocessingof the ?le.
ThenonleafnodesoftheB
+
-treeformamultilevel(sparse)indexontheleaf
nodes. The structure of nonleaf nodes is the same as that for leaf nodes, except
that all pointers are pointers to tree nodes. A nonleaf node may hold up to n
pointers,andmustholdatleast null n/2null pointers.Thenumberofpointersinanode
is called the fanout of the node. Nonleaf nodes are also referred to as internal
nodes.
Letusconsideranodecontainingmpointers(m ? n).Fori = 2,3,...,m ?1,
pointer P
i
points to the subtree that contains search-key values less than K
i
and
greater than or equal to K
i ?1
.PointerP
m
points to the part of the subtree that
containsthosekeyvaluesgreaterthanorequalto K
m?1
,andpointerP
1
pointsto
thepart ofthe subtreethatcontains those search-keyvalueslessthan K
1
.
Unlikeothernonleafnodes,therootnodecanholdfewerthan null n/2null pointers;
however, it must hold at least two pointers, unless the tree consists of only one
node. It is always possible to construct a B
+
-tree, for any n, that satis?es the
precedingrequirements.
Figure 11.9 shows a complete B
+
-tree for the instructor ?le (with n = 4). We
have shown instructor names abbreviated to 3 characters in order to depict the
treeclearly; in reality,the treenodes would contain the full names. We have also
omitted null pointers for simplicity; any pointer ?eld in the ?gure that does not
haveanarrow is understoodto havea null value.
Figure11.10showsanotherB
+
-treefortheinstructor?le,thistimewithn = 6.
Asbefore,wehaveabbreviatedinstructornamesonlyforclarityofpresentation.
488 Chapter 11 Indexing andHashing
Gold Katz Kim Mozart Singh Srinivasan Wu
Internal nodes
Root node
Leaf nodes
Einstein
Einstein El Said
Gold
Mozart
Srinivasan
Srinivasan Comp. Sci. 65000
Wu Finance 90000
Mozart Music 40000
Einstein Physics 95000
El Said History 80000
Gold Physics 87000
Katz Comp. Sci. 75000
Cali?eri History 60000
Singh Finance 80000
Crick Biology 72000
Brandt Comp. Sci. 92000
15151
10101
Brandt Cali?eri Crick
12121
22222
32343
33456
45565
58583
76543
76766
83821
98345 Kim Elec. Eng. 80000
Figure 11.9 B
+
-tree for instructor ?le (n = 4).
Observe that the height of this tree is less than that of the previous tree, which
hadn = 4.
TheseexamplesofB
+
-treesareallbalanced. That is,the lengthofeverypath
from the root to a leaf node is the same. This property is a requirement for a B
+
-
tree.Indeed,the “B”inB
+
-treestandsfor “balanced.”Itisthebalancepropertyof
B
+
-treesthat ensuresgoodperformance for lookup,insertion,anddeletion.
11.3.2 Queries on B
+
-Trees
Let us consider how we process queries on a B
+
-tree. Suppose that we wish to
?nd records with a search-key value of V. Figure 11.11 presents pseudocode for
afunctionfind()tocarry out thistask.
Intuitively, the function starts at the root of the tree, and traverses the tree
downuntilitreachesaleafnodethatwouldcontainthespeci?edvalueifitexists
in the tree. Speci?cally, starting with the root as the current node, the function
repeats the following steps until a leaf node is reached. First, the current node
is examined, looking for the smallest i such that search-key value K
i
is greater
Brandt Crick Cali?eri Einstein El Said Gold Katz Kim Mozart Singh Srinivasan Wu
El Said Mozart
Figure 11.10 B
+
-tree for instructor ?le with n = 6.
11.3 B
+
-TreeIndex Files 489
function?nd(valueV)
/*Returns leafnodeC and indexi such thatC.P
i
points to ?rstrecord
* withsearchkeyvalue V */
SetC =root node
while (C isnot aleafnode)begin
Leti =smallestnumber suchthat V ? C.K
i
ifthereisnosuch numberi thenbegin
Let P
m
=last non-null pointerinthe node
SetC =C.P
m
end
elseif (V = C.K
i
)
thenSetC =C.P
i+1
elseC =C.P
i
/* V < C.K
i
*/
end
/*C isaleafnode*/
Leti be theleastvaluesuchthat K
i
= V
if thereissuch avaluei
then return (C,i)
else returnnull ;/*Norecordwithkeyvalue V exists*/
procedureprintAll(valueV)
/*prints allrecordswithsearchkeyvalue V */
Setdone=false;
Set(L,i)= ?nd(V);
if ((L,i)is null)return
repeat
repeat
Printrecordpointedtoby L.P
i
Seti = i +1
until(i > number of keysin L or L.K
i
> V)
if(i > numberof keysin L)
then L = L.P
n
else Setdone =true;
until(doneor L isnull)
Figure 11.11 Querying a B
+
-tree.
than or equal to V. Suppose such a value is found; then, if K
i
is equal to V,the
currentnodeissettothenodepointedtobyP
i+1
,otherwiseK
i
> V,andthe
currentnodeissettothenodepointedtoby P
i
.IfnosuchvalueK
i
isfound,then
clearly V > K
m?1
,whereP
m
is the last nonnull pointer in the node. In this case
thecurrentnodeissettothatpointedtoby P
m
. The above procedure is repeated,
traversingdown thetreeuntil aleafnode isreached.
Attheleafnode,ifthereisasearch-keyvalueequaltoV,letK
i
bethe?rstsuch
value; pointer P
i
directs us to a record with search-key value K
i
. The function
490 Chapter 11 Indexing andHashing
then returns the leaf node L and the index i.Ifnosearch-keywithvalueV is
found in the leaf node, no record with key value V exists in the relation, and
functionfindreturnsnull,toindicatefailure.
If there is at most one record with a search key value V (for example, if the
indexisonaprimarykey)theprocedurethatcallsthefindfunctionsimplyuses
thepointer L.P
i
toretrievetherecordandisdone.However,incasetheremaybe
morethan onematching record,the remainingrecordsalsoneedtobe fetched.
Procedure printAll shown in Figure 11.11 shows how to fetch all records
with a speci?ed search key V. The procedure ?rst steps through the remaining
keys in the node L, to ?nd other records with search-key value V.IfnodeL
contains at least one search-key value greater than V, then there are no more
records matching V.Otherwise,thenextleaf,pointedtobyP
n
may contain
further entries for V. The node pointed to by P
n
must then be searched to ?nd
further records with search-key value V. If the highest search-key value in the
nodepointedtoby P
n
isalsoV,furtherleavesmayhavetobetraversed,inorder
to ?nd all matching records. The repeat loop inprintAll carries out the task of
traversingleafnodesuntilall matchingrecordshavebeenfound.
Arealimplementationwouldprovideaversionoffindsupportinganiterator
interface similar to that provided by the JDBC ResultSet, which we saw in
Section 5.1.1. Such an iterator interface would provide a method next(), which
canbecalledrepeatedlytofetchsuccessiverecordswiththespeci?edsearch-key.
The next() method would step through the entries at the leaf level, in a manner
similar to printAll, but each call takes only one step, and records where it left
off,sothatsuccessivecallsnextstepthroughsuccessiverecords.Weomitdetails
for simplicity, and leave the pseudocode for the iterator interface as an exercise
for the interested reader.
B
+
-trees can also be used to ?nd all records with search key values in a
speci?ed range (L,U). For example, with a B
+
-tree on attribute salary of instruc-
tor, we can ?nd all instructor records with salary in a speci?ed range such as
(50000,100000) (in other words, all salaries between 50000 and 100000). Such
queries are called range queries. To execute such queries, we can create a proce-
dureprintRange(L,U),whosebodyisthesameasprintAllexceptforthesedif-
ferences: printRange calls find(L), instead of find(V), and then steps through
records as in procedure printAll, but with the stopping condition being that
L.K
i
>U,insteadofL.K
i
> V.
Inprocessingaquery,wetraverseapathinthetreefromtheroottosomeleaf
node. Ifthere areN recordsin the ?le,the path isno longer than null log
null n/2null
(N)null .
Inpractice,onlyafewnodesneedtobeaccessed.Typically,anodeismadeto
bethesamesizeasadiskblock,whichistypically4kilobytes.Withasearch-key
size of 12 bytes, and a disk-pointer size of 8 bytes, n is around 200. Even with a
more conservative estimate of 32 bytes for the search-key size, n is around 100.
With n = 100,ifwehave1millionsearch-keyvaluesinthe?le,alookuprequires
only null log
50
(1,000,000)null 4 nodes to be accessed. Thus, at most four blocks need
to be read from disk for the lookup. The root node of the tree is usually heavily
accessed and is likely to be in the buffer, so typically only three or fewer blocks
needtobe readfrom disk.
11.3 B
+
-TreeIndex Files 491
AnimportantdifferencebetweenB
+
-treestructuresandin-memorytreestruc-
tures, such as binary trees, is the size of a node, and as a result, the height of the
tree.Inabinarytree,eachnodeissmall,andhasatmosttwopointers.InaB
+
-tree,
each node is large—typically a disk block—and a node can have a large number
of pointers. Thus, B
+
-trees tend to be fat and short, unlike thin and tall binary
trees. In a balanced binary tree, the path for a lookup can be of length null log
2
(N)null ,
where Nisthenumberofrecordsinthe?lebeingindexed.With N = 1,000,000as
inthepreviousexample,abalancedbinarytreerequiresaround20nodeaccesses.
If each node were on a different disk block, 20 block reads would be required to
processalookup,incontrasttothefourblockreadsfortheB
+
-tree.Thedifference
is signi?cant, since each block read could require a disk arm seek, and a block
read together with the disk arm seek takes about 10 milliseconds on a typical
disk.
11.3.3 Updates on B
+
-Trees
When a record is inserted into, or deletedfrom a relation, indices on the relation
mustbeupdatedcorrespondingly.Recallthatupdatestoarecordcanbemodeled
asadeletionoftheoldrecordfollowedbyinsertionoftheupdatedrecord.Hence
we onlyconsider thecase ofinsertionand deletion.
Insertion and deletion are more complicated than lookup, since it may be
necessary to split a node that becomes too large as the result of an insertion, or
to coalesce nodes (that is, combine nodes) if a node becomes too small (fewer
than null n/2null pointers). Furthermore, when a node is split or a pair of nodes is
combined,wemustensurethatbalanceispreserved.Tointroducetheideabehind
insertionanddeletioninaB
+
-tree,weshallassumetemporarilythatnodesnever
becometoolargeortoosmall.Underthisassumption,insertionanddeletionare
performedasde?nednext.
• Insertion. Using the same technique as for lookup from the find() function
(Figure11.11),we?rst?ndtheleafnodeinwhichthesearch-keyvaluewould
appear.Wetheninsertanentry(thatis,asearch-keyvalueandrecordpointer
pair)intheleafnode,positioningitsuchthatthesearchkeysarestillinorder.
• Deletion. Using the same technique as for lookup, we ?nd the leaf node
containingtheentrytobedeleted,byperformingalookuponthesearch-key
valueofthedeletedrecord;iftherearemultipleentrieswiththesamesearch-
key value, we search across all entries with the same search-key value until
we?ndtheentrythatpointstotherecordbeingdeleted.Wethenremovethe
entry from the leaf node. All entries in the leaf node that are to the right of
the deletedentryare shiftedleftby one position, sothat thereareno gaps in
theentriesaftertheentryisdeleted.
Wenowconsiderthegeneralcaseofinsertionanddeletion,dealingwithnode
splittingandnode coalescing.
.
492 Chapter 11 Indexing andHashing
Adams Cali?eri Crick Brandt
Figure 11.12 Split of leaf node on insertion of “Adams”
11.3.3.1 Insertion
Wenowconsideranexampleofinsertioninwhichanodemustbesplit.Assume
that a record is inserted on the instructor relation, with the name value being
Adams.Wethenneedtoinsertanentryfor“Adams”intotheB
+
-treeofFigure11.9.
Using the algorithm for lookup, we ?nd that “Adams” should appear in the leaf
nodecontaining“Brandt”,“Cali?eri”,and“Crick.”Thereisnoroominthisleafto
insert the search-key value “Adams.” Therefore,the node is split into two nodes.
Figure11.12showsthetwoleafnodesthatresultfromthesplitoftheleafnodeon
inserting “Adams”.Thesearch-keyvalues“Adams”and “Brandt”areinoneleaf,
and “Cali?eri” and “Crick” are in the other. In general, we take the n search-key
values (the n ?1 values in the leaf node plus the value being inserted), and put
the ?rst null n/2null in the existing node and the remaining values in a newly created
node.
Having split a leaf node, we must insert the new leaf node into the B
+
-tree
structure.Inourexample,thenewnodehas “Cali?eri”asitssmallestsearch-key
value.Weneedtoinsertanentrywiththissearch-keyvalue,andapointertothe
new node, into the parent of the leaf node that was split. The B
+
-tree of Figure
11.13 shows the result of the insertion. It was possible to perform this insertion
withnofurthernodesplit,becausetherewasroomintheparentnodeforthenew
entry. If there were no room, the parent would have had to be split, requiringan
entry to be added to its parent. In the worst case, all nodes along the path to the
root mustbe split.Ifthe rootitselfissplit,theentiretreebecomesdeeper.
Splitting of a nonleaf node is a little different from splitting of a leaf node.
Figure11.14showstheresultofinsertingarecordwithsearchkey“Lamport”into
thetreeshowninFigure11.13.Theleafnodeinwhich“Lamport”istobeinserted
already has entries “Gold”, “Katz”,and“Kim”, and as a result the leaf node has
to be split. The new right-hand-side node resulting from the split contains the
search-keyvalues“Kim”and “Lamport”.Anentry(Kim,n1)mustthenbeadded
Adams Brandt Einstein El Said Gold Katz Kim Mozart Singh Srinivasan Wu
Gold Srinivasan
Mozart
Einstein Cali?eri
Crick Cali?eri
Figure 11.13 Insertion of “Adams” into the B
+
-tree of Figure 11.9.
11.3 B
+
-TreeIndex Files 493
Srinivasan
Gold
Cali?eri Einstein
Mozart
Kim
Adams Brandt Einstein El Said Gold Katz Kim Lamport Mozart Singh Srinivasan Wu Crick Cali?eri
Figure 11.14 Insertion of “Lamport” into the B
+
-tree of Figure 11.13.
to the parent node, where n1 is a pointer to the new node, However, there is no
space in the parent node to add a new entry, and the parent node has to be split.
Todoso,theparentnodeisconceptuallyexpandedtemporarily,theentryadded,
and theoverfullnode isthenimmediatelysplit.
Whenanoverfullnonleafnodeissplit,thechildpointersaredividedamong
the original and the newly created nodes; in our example, the original node is
left with the ?rst three pointers, and the newly created node to the right gets
the remaining two pointers. The search key values are, however, handled a little
differently.Thesearchkeyvaluesthatliebetweenthepointersmovedtotheright
node(inourexample,thevalue“Kim”)aremovedalongwiththepointers,while
thosethatliebetweenthepointersthatstayontheleft(inourexample,“Cali?eri”
and “Einstein”)remainundisturbed.
However,thesearchkeyvaluethatliesbetweenthepointersthatstayonthe
left, and the pointers that move to the right node is treated differently. In our
example,thesearchkeyvalue“Gold”liesbetweenthethreepointersthatwentto
theleftnode,andthetwopointersthatwenttotherightnode.Thevalue “Gold”
is not added to either of the split nodes. Instead, an entry (Gold, n2) is added to
the parent node, where n2 is a pointer to the newly created node that resulted
from the split. In this case, the parent node is the root, and it has enough space
for the new entry.
ThegeneraltechniqueforinsertionintoaB
+
-treeistodeterminetheleafnode
l into which insertion must occur. If a split results, insert the new node into the
parent of node l. If this insertion causes a split, proceed recursively up the tree
until eitheraninsertiondoesnot cause asplitor anewrootis created.
Figure 11.15 outlines the insertion algorithm in pseudocode. The procedure
insert inserts a key-value pointer pair into the index, using two subsidiary
proceduresinsert in leafandinsert in parent.Inthepseudocode,L,N,P
andT denotepointerstonodes,withLbeingusedtodenotealeafnode.L.K
i
and
L.P
i
denote the ith value and the ith pointer in node L, respectively; T.K
i
and
T.P
i
areusedsimilarly.Thepseudocodealsomakesuseofthefunctionparent(N)
to ?nd the parent of a node N. We can compute a list of nodes in the path from
theroottotheleafwhileinitially?ndingtheleafnode,andcanuseitlaterto?nd
theparent ofany nodeinthepathef?ciently.
Theprocedureinsert in parenttakesasparameters N,K
null
,N
null
,wherenode
N was split into N and N
null
,withK
null
being the least value in N
null
.Theprocedure
494 Chapter 11 Indexing andHashing
procedureinsert(valueK, pointer P)
if (tree is empty) create an empty leaf node L, which isalso theroot
else Find theleafnode L that should containkeyvalue K
if(L has lessthann ?1keyvalues)
theninsert in leaf(L,K,P)
elsebegin/* L hasn ?1keyvaluesalready,splitit*/
Createnode L
null
Copy L.P
1
...L.K
n?1
to a block of memory T that can
holdn(pointer,key-value)pairs
insert in leaf(T,K,P)
Set L
null
.P
n
= L.P
n
;SetL.P
n
= L
null
Erase L.P
1
through L.K
n?1
from L
Copy T.P
1
through T.K
null n/2null
from T into L startingat L.P
1
Copy T.P
null n/2null1
through T.K
n
from T into L
null
startingat L
null
.P
1
Let K
null
be thesmallestkey-valuein L
null
insert in parent(L, K
null
, L
null
)
end
procedureinsert in leaf(node L,value K, pointer P)
if(K < L.K
1
)
theninsert P,K into L justbefore L.P
1
elsebegin
Let K
i
be thehighest valuein L that islessthan K
Insert P,K into L justafter T.K
i
end
procedureinsert in parent(node N,valueK
null
,node N
null
)
if(N is theroot ofthe tree)
thenbegin
Createanewnode Rcontaining N,K
null
,N
null
/* N and N
null
arepointers*/
Make Rthe rootof thetree
return
end
Let P =parent (N)
if(P has lessthannpointers)
theninsert(K
null
,N
null
)inP justafter N
elsebegin/*Split P */
Copy P to ablock of memory T that can hold P and (K
null
,N
null
)
Insert(K
null
,N
null
)intoT justafter N
Eraseallentriesfrom P;CreatenodeP
null
Copy T.P
1
...T.P
null n/2null
into P
Let K
null
= T.K
null n/2null
Copy T.P
null n/2null1
...T.P
n+1
into P
null
insert in parent(P, K
null
, P
null
)
end
Figure 11.15 Insertion of entry in a B
+
-tree.
11.3 B
+
-TreeIndex Files 495
Adams Brandt Cali?eri Crick Einstein El Said Gold Katz Kim Mozart Singh Wu
Cali?eri
Gold
Mozart Einstein
Figure 11.16 Deletion of “Srinivasan” from the B
+
-tree of Figure 11.13.
modi?estheparentof N to record the split. The proceduresinsert into index
andinsert in parent use a temporary area of memory T to store the contents
of a node being split. The procedures can be modi?ed to copy data from the
node being split directly to the newly created node, reducing the time required
for copying data. However, the use of the temporary space T simpli?es the
procedures.
11.3.3.2 Deletion
Wenowconsiderdeletionsthatcausetreenodestocontaintoofewpointers.First,
let us delete “Srinivasan” from the B
+
-tree of Figure 11.13. The resulting B
+
-tree
appearsinFigure11.16.Wenowconsiderhowthedeletionisperformed.We?rst
locatetheentryfor “Srinivasan”byusingourlookupalgorithm.Whenwedelete
the entry for “Srinivasan”from its leafnode,the node isleft withonly one entry,
“Wu”. Since, in our example, n = 4and1 < null (n ?1)/2null ,wemusteithermerge
the node with a sibling node, or redistribute the entries between the nodes, to
ensurethateachnodeisatleasthalf-full.Inourexample,theunderfullnodewith
the entry for “Wu” can be mergedwith its left sibling node. We merge the nodes
by moving the entries from both the nodes into the left sibling, and deleting the
nowemptyrightsibling.Oncethenodeisdeleted,wemustalsodeletetheentry
intheparentnode thatpointedtothejustdeletednode.
In our example, the entry to be deleted is (Srinivasan, n3), where n3isa
pointer to the leaf containing “Srinivasan”. (In this case the entry to be deleted
in the nonleaf node happens to be the same value as that deleted from the leaf;
that would not be the case for most deletions.) After deleting the above entry,
the parent node, which had a search key value “Srinivasan” and two pointers,
nowhasonepointer(theleftmostpointerinthenode)andnosearch-keyvalues.
Since1 < null n/2null forn = 4,theparentnodeisunderfull.(Forlargern,anodethat
becomes underfullwould stillhave somevaluesas wellas pointers.)
In this case, we look at a sibling node; in our example, the only sibling is
the nonleaf node containing the search keys “Cali?eri”, “Einstein”,and“Gold”.
If possible, we try to coalesce the node with its sibling. In this case, coalescing is
not possible, since the node and its sibling together have ?ve pointers, against a
maximumoffour.Thesolutioninthiscaseistoredistributethepointersbetween
496 Chapter 11 Indexing andHashing
Adams Brandt Cali?eri Crick Einstein El Said Gold Katz Kim      Mozart
Cali?eri Einstein Kim
Gold
Figure 11.17 Deletion of “Singh” and “Wu” from the B
+
-tree of Figure 11.16.
the node and its sibling, such that each has at least null n/2null 2 child pointers. To
do so, we move the rightmost pointer from the left sibling (the one pointing to
theleafnodecontaining“Mozart”) to the underfull right sibling. However, the
underfullrightsiblingwouldnowhavetwopointers,namelyitsleftmostpointer,
and the newly moved pointer, with no value separating them. In fact, the value
separating them is not present in either of the nodes, but is present in the parent
node, between the pointers from the parent to the node and its sibling. In our
example,thevalue“Mozart”separatesthetwopointers,andispresentintheright
siblingaftertheredistribution.Redistributionofthepointersalsomeansthatthe
value “Mozart” in the parent no longer correctly separates search-key values in
thetwosiblings.Infact,thevaluethatnowcorrectlyseparatessearch-keyvalues
in the two sibling nodes is the value “Gold”, which was in the left sibling before
redistribution.
Asaresult,ascanbeseenintheB
+
-treeinFigure11.16,afterredistributionof
pointersbetweensiblings,thevalue “Gold”hasmovedupinto theparent,while
thevaluethatwasthereearlier,“Mozart”,hasmoveddownintotherightsibling.
We next delete the search-key values “Singh” and “Wu” from the B
+
-tree of
Figure11.16.TheresultisshowninFigure11.17.Thedeletionofthe?rstofthese
values does not make the leaf node underfull, but the deletion of the second
value does. It is not possible to merge the underfull node with its sibling, so a
redistribution of values is carried out, moving the search-key value “Kim” into
the node containing “Mozart”, resulting in the tree shown in Figure 11.17. The
valueseparatingthetwosiblingshasbeenupdatedintheparent,from “Mozart”
to “Kim”.
Nowwedelete“Gold”fromtheabovetree;theresultisshowninFigure11.18.
This results in an underfull leaf, which can now be merged with its sibling. The
resultant deletionof an entry from the parent node (the nonleaf node containing
“Kim”) makes the parent underfull (it is left with just one pointer). This time
around,theparentnodecanbemergedwithitssibling.Thismergeresultsinthe
search-key value “Gold” moving down from the parent into the merged node.
As a result of this merge, an entry is deleted from its parent, which happens to
be the root of the tree. And as a result of that deletion, the root is left with only
one child pointer and no search-key value, violating the condition that the root
11.3 B
+
-TreeIndex Files 497
Adams Brandt Einstein El Said Katz Kim Mozart
Gold Cali?eri
Cali?eri
Einstein
Crick
Figure 11.18 Deletion of “Gold” from the B
+
-tree of Figure 11.17.
have at least two children. As a result, the root node is deleted and its sole child
becomes theroot,and thedepthof theB
+
-treehas beendecreasedby1.
It is worth noting that, as a result of deletion, a key value that is present in a
nonleafnodeoftheB
+
-treemaynotbepresentatanyleafofthetree.Forexample,
in Figure 11.18, the value “Gold” has been deleted from the leaf level, but is still
presentinanonleafnode.
In general, to delete a value in a B
+
-tree, we perform a lookup on the value
and delete it. If the node is too small, we delete it from its parent. This deletion
resultsinrecursiveapplicationofthedeletionalgorithmuntiltherootisreached,
aparentremainsadequatelyfullafterdeletion,orredistributionisapplied.
Figure 11.19 outlines the pseudocode for deletion from a B
+
-tree. The proce-
dureswap variables(N,N
null
) merely swaps the values of the (pointer) variables
N and N
null
; this swap has no effect on the tree itself. The pseudocode uses the
condition “toofewpointers/values.”Fornonleafnodes,thiscriterionmeansless
than null n/2null pointers; for leaf nodes, it means less than null (n ? 1)/2null values. The
pseudocode redistributes entries by borrowing a single entry from an adjacent
node. We can also redistribute entries by repartitioning entries equally between
the two nodes. The pseudocode refers to deleting an entry (K,P)fromanode.
In the case of leaf nodes, the pointer to an entry actually precedes the key value,
so the pointer P precedesthe key value K. For nonleaf nodes, P follows the key
value K.
11.3.4 Nonunique Search Keys
Ifarelationcanhavemorethanonerecordcontainingthesamesearchkeyvalue
(thatis,twoormorerecordscanhavethesamevaluesfortheindexedattributes),
thesearchkeyissaidtobe anonuniquesearchkey.
One problem with nonunique search keys is in the ef?ciency of record dele-
tion. Suppose a particular search-key value occurs a large number of times, and
one of the records with that search key is to be deleted. The deletion may have
to search through a number of entries, potentially across multiple leaf nodes, to
?nd theentry correspondingto theparticularrecordbeing deleted.
Asimplesolutiontothisproblem,usedbymostdatabasesystems,istomake
search keys unique by creating a composite search key containing the original
search key and another attribute, which together are unique across all records.
The extra attribute can be a record-id, which is a pointer to the record, or any
other attribute whose value is unique among all records with the same search-
498 Chapter 11 Indexing andHashing
proceduredelete(valueK, pointer P)
?nd the leafnode L that contains (K,P)
delete entry(L, K, P)
proceduredelete entry(node N,value K, pointer P)
delete(K,P)fromN
if (N istherootand N has only oneremaining child)
thenmakethe childof N thenewroot of thetreeand delete N
else if(N has too fewvalues/pointers)thenbegin
Let N
null
be thepreviousor nextchild of parent(N)
Let K
null
bethe valuebetweenpointers Nand N
null
in parent(N)
if(entriesin N and N
null
can ?t inasinglenode)
thenbegin/*Coalescenodes*/
if(N isapredecessorof N
null
)then swap variables(N,N
null
)
if(N isnot aleaf)
thenappend K
null
andall pointersand valuesin N to N
null
else appendall(K
i
,P
i
)pairsinN to N
null
;setN
null
.P
n
= N.P
n
delete entry(parent(N), K
null
, N); deletenode N
end
elsebegin /*Redistribution:borrowanentryfrom N
null
*/
if (N
null
isapredecessorof N)thenbegin
if(N isanonleaf node)thenbegin
letm be such that N
null
.P
m
is thelastpointerin N
null
remove (N
null
.K
m?1
,N
null
.P
m
)fromN
null
insert(N
null
.P
m
,K
null
) as the?rst pointerandvaluein N,
by shiftingother pointersand valuesright
replace K
null
in parent(N)byN
null
.K
m?1
end
elsebegin
letm be such that (N
null
.P
m
,N
null
.K
m
)is thelastpointer/value
pairin N
null
remove (N
null
.P
m
,N
null
.K
m
)fromN
null
insert(N
null
.P
m
,N
null
.K
m
) as the?rst pointerandvaluein N,
by shiftingother pointersand valuesright
replace K
null
in parent(N)byN
null
.K
m
end
end
else ...symmetrictothethencase...
end
end
Figure 11.19 Deletion of entry from a B
+
-tree.
key value. The extra attribute is called a uniqui?er attribute. When a record is
to be deleted, the composite search-key value is computed from the record, and
thenusedtolookuptheindex.Sincethevalueisunique,thecorrespondingleaf-
11.3 B
+
-TreeIndex Files 499
levelentry can be found with asingletraversalfrom root toleaf,withno further
accessesat theleaflevel.Asaresult,recorddeletioncan be doneef?ciently.
A search with the original search-key attribute simply ignores the value of
theuniqui?er attributewhencomparing search-keyvalues.
With nonunique search keys, our B
+
-tree structure stores each key value
as many times as there are records containing that value. An alternative is to
storeeachkeyvalueonly once inthetree,and tokeepabucket (or list)ofrecord
pointerswithasearch-keyvalue,tohandlenonuniquesearchkeys.Thisapproach
is more space ef?cient since it stores the key value only once; however, it creates
several complications when B
+
-trees are implemented. If the buckets are kept
in the leaf node, extra code is needed to deal with variable-size buckets, and to
deal with buckets that grow larger than the size of the leaf node. If the buckets
are stored in separate blocks, an extra I/O operation may be required to fetch
records.Inadditiontotheseproblems,thebucketapproachalsohastheproblem
of inef?ciency for record deletion if a search-key value occurs a large number of
times.
11.3.5 Complexity of B
+
-Tree Updates
Although insertion and deletion operations on B
+
-trees are complicated, they
require relatively few I/O operations, which is an important bene?t since I/O
operations are expensive. It can be shown that the number of I/O operations
neededintheworstcaseforaninsertionisproportionaltolog
null n/2null
(N),wherenis
the maximum number of pointers in a node, and N is the number of records in
the?le beingindexed.
The worst-case complexity of the deletion procedure is also proportional to
log
null n/2null
(N), provided there are no duplicate values for the search key. If there
are duplicate values, deletion may have to search across multiple records with
the same search-key value to ?nd the correct entry to be deleted, which can
be inef?cient. However, making the search key unique by adding a uniqui?er
attribute, as described in Section 11.3.4, ensures the worst-case complexity of
deletionisthe sameeveniftheoriginal searchkeyisnonunique.
In other words, the cost of insertion and deletion operations in terms of I/O
operations is proportional to the height of the B
+
-tree, and is therefore low. It
is the speed of operation on B
+
-trees that makes them a frequently used index
structureindatabase implementations.
In practice, operations on B
+
-trees result in fewer I/O operations than the
worst-case bounds. With fanout of 100, and assuming accesses to leaf nodes are
uniformly distributed, the parent of a leaf node is 100 times more likely to get
accessed than the leaf node. Conversely, with the same fanout, the total number
of nonleaf nodes in a B
+
-tree would be just a little more than 1/100th of the
number of leaf nodes. As a result, with memory sizes of several gigabytes being
common today, for B
+
-trees that are used frequently, even if the relation is very
large it is quite likely that most of the nonleaf nodes are already in the database
buffer when they are accessed. Thus, typically only one or two I/O operations
are required to perform a lookup. For updates, the probability of a node split
500 Chapter 11 Indexing andHashing
occurring is correspondingly very small. Depending on the ordering of inserts,
with a fanout of 100, only between 1 in 100 to 1 in 50 insertions will result in a
nodesplit,requiringmorethanoneblocktobewritten.Asaresult,onanaverage
an insert will require just a little more than one I/O operation to write updated
blocks.
AlthoughB
+
-treesonlyguaranteethatnodeswillbeatleasthalffull,ifentries
are inserted in random order, nodes can be expected to be more than two-thirds
full on average. If entries are inserted in sorted order, on the other hand, nodes
will be only half full. (We leave it as an exercise to the reader to ?gure out why
nodeswould beonly half full inthe lattercase.)
11.4 B
+
-Tree Extensions
In this section, we discuss several extensions and variations of the B
+
-tree index
structure.
11.4.1 B
+
-Tree File Organization
As mentioned in Section 11.3, the main drawback of index-sequential ?le orga-
nization is the degradation of performance as the ?le grows: With growth, an
increasing percentage of index entries and actual records become out of order,
andarestoredinover?owblocks.Wesolvethedegradationofindexlookupsby
using B
+
-tree indices on the ?le. We solve the degradation problem for storing
the actual records by using the leaf level of the B
+
-tree to organize the blocks
containing the actual records. We use the B
+
-tree structure not only as an index,
butalsoasanorganizerforrecordsina?le.InaB
+
-tree?leorganization,theleaf
nodesofthetreestorerecords,insteadofstoringpointerstorecords.Figure11.20
shows an example of a B
+
-tree ?le organization. Since records are usually larger
than pointers, the maximum number of records that can be stored in a leaf node
islessthanthenumberofpointersinanonleafnode.However,theleafnodesare
stillrequiredtobe at leasthalffull.
I
C
M K
(A,4) (C,1) (B,8) (D,9) (E,4) (F,7) (G,3) (H,3)
(I,4) (J,8) (K,1) (L,6) (M,4) (N,8) (P,6)
F
Figure 11.20 B
+
-tree ?le organization.
11.4 B
+
-TreeExtensions 501
Insertionand deletionof recordsfrom a B
+
-tree?leorganizationarehandled
in the same way as insertion and deletion of entries in a B
+
-tree index. When
a record with a given key value v is inserted, the system locates the block that
should contain the record by searching the B
+
-tree for the largest key in the tree
that is ? v. If the block located has enough free space for the record, the system
storestherecordintheblock.Otherwise,asinB
+
-treeinsertion,thesystemsplits
the block in two, and redistributes the records in it (in the B
+
-tree–key order) to
createspaceforthenewrecord.ThesplitpropagatesuptheB
+
-treeinthenormal
fashion. When we delete a record, the system ?rst removes it from the block
containing it. If a block B becomes less than half full as a result, the records in B
are redistributed with the records in an adjacent block B
null
. Assuming ?xed-sized
records, each block will hold at least one-half as many records as the maximum
thatitcanhold.ThesystemupdatesthenonleafnodesoftheB
+
-treeintheusual
fashion.
When we use a B
+
-tree for ?le organization, space utilization is particularly
important,sincethespaceoccupiedbytherecordsislikelytobemuchmorethan
thespaceoccupiedbykeysandpointers.Wecanimprovetheutilizationofspace
in a B
+
-tree by involving more sibling nodes in redistribution during splits and
merges. The technique is applicable to both leaf nodes and nonleaf nodes, and
worksasfollows:
During insertion, if a node is full the system attempts to redistribute some
of its entries to one of the adjacent nodes, to make space for a new entry. If this
attempt fails because the adjacent nodes are themselves full, the system splits
the node, and splits the entries evenly among one of the adjacent nodes and the
two nodes that it obtained by splitting the original node. Since the three nodes
together contain one more record than can ?t in two nodes, each node will be
about two-thirdsfull.More precisely,each nodewill haveat least null 2n/3null entries,
where n is the maximum number of entries that the node can hold. (null xnull denotes
the greatest integer that is less than or equal to x; that is, we drop the fractional
part,ifany.)
During deletion of a record, if the occupancy of a node falls below null 2n/3null ,
the system attempts to borrow an entry from one of the sibling nodes. If both
sibling nodes have null 2n/3null records, instead of borrowing an entry, the system
redistributes the entries in the node and in the two siblings evenly between two
of the nodes, and deletes the third node. We can use this approach because the
total number of entries is 3null 2n/3null 1, which is less than 2n. With three adjacent
nodesusedforredistribution,eachnodecanbeguaranteedtohave null 3n/4null entries.
In general, if m nodes (m ? 1 siblings) are involved in redistribution, each node
can be guaranteed to contain at least null (m ?1)n/mnull entries. However, the cost of
updatebecomeshigher asmoresibling nodesareinvolvedintheredistribution.
Note that in a B
+
-tree index or ?le organization, leaf nodes that are adjacent
to each other in the tree may be located at different places on disk. When a ?le
organization is newly created on a set of records, it is possible to allocate blocks
that are mostly contiguous on disk to leaf nodes that are contiguous in the tree.
Thusasequentialscanofleafnodeswouldcorrespondtoamostlysequentialscan
ondisk.Asinsertionsanddeletionsoccuronthetree,sequentialityisincreasingly
502 Chapter 11 Indexing andHashing
lost,andsequentialaccesshastowaitfordiskseeksincreasinglyoften.Anindex
rebuild may be required to restore sequentiality.
B
+
-tree ?le organizations can also be used to store large objects, such as SQL
clobs and blobs, which may be larger than a disk block, and as large as multiple
gigabytes. Such large objects can be stored by splitting them into sequences of
smaller records that are organized in a B
+
-tree ?le organization. The records can
be sequentially numbered, or numbered by the byte offset of the record within
the largeobject, andthe recordnumber can beusedas thesearchkey.
11.4.2 Secondary Indices and Record Relocation
Some ?le organizations, such as the B
+
-tree ?le organization, may change the
locationofrecordsevenwhentherecordshavenotbeenupdated.Asanexample,
when a leaf node is split in a B
+
-tree ?le organization, a number of records are
moved to a new node. In such cases, all secondary indices that store pointers to
the relocated records would have to be updated, even though the values in the
recordsmaynothavechanged.Eachleafnodemaycontainafairlylargenumber
of records, and each of them may be in different locations on each secondary
index.Thusaleaf-nodesplitmayrequiretensorevenhundredsofI/Ooperations
toupdateall affectedsecondary indices,making itaveryexpensiveoperation.
A widely used solution for this problem is as follows: In secondary indices,
in place of pointers to the indexed records, we store the values of the primary-
indexsearch-keyattributes.Forexample,supposewehaveaprimaryindexonthe
attributeIDofrelationinstructor;thenasecondaryindexondept namewouldstore
with each department name a list of instructor’s ID values of the corresponding
records,insteadof storingpointerstotherecords.
Relocation of records because of leaf-node splits then does not require any
update on any such secondary index. However, locating a record using the sec-
ondary index now requires two steps: First we use the secondary index to ?nd
the primary-index search-key values, and then we use the primary index to ?nd
the corresponding records.
The aboveapproach thus greatlyreducesthe cost ofindexupdatedueto?le
reorganization,althoughitincreasesthecostofaccessingdatausingasecondary
index.
11.4.3 Indexing Strings
Creating B
+
-tree indices on string-valued attributes raises two problems. The
?rst problemisthat stringscan beof variablelength.Thesecond problemisthat
stringscanbelong,leadingtoalowfanoutandacorrespondinglyincreasedtree
height.
With variable-length search keys, different nodes can have different fanouts
eveniftheyarefull.Anodemustthenbesplitifitisfull,thatis,thereisnospace
toaddanewentry,regardlessofhowmanysearchentriesithas.Similarly,nodes
can be merged or entries redistributed depending on what fraction of the space
in the nodes is used, instead of being based on the maximum number of entries
that thenode canhold.
11.4 B
+
-TreeExtensions 503
Thefanoutofnodescanbeincreasedbyusingatechniquecalledpre?x
compression. With pre?x compression, we do not store the entire search key
value at nonleaf nodes. We only store a pre?x of each search key value that is
suf?cient to distinguish between the key values in the subtrees that it separates.
Forexample,ifwehadanindexonnames,thekeyvalueatanonleafnodecould
beapre?xofaname;itmaysuf?cetostore“Silb” at a nonleaf node, instead of
the full “Silberschatz” if the closest values in the two subtrees that it separates
are,say, “Silas”and “Silver”respectively.
11.4.4 Bulk Loading of B
+
-Tree Indices
As we saw earlier, insertion of a record in a B
+
-tree requires a number of I/O
operations that in the worst case is proportional to the height of the tree, which
isusually fairlysmall(typically?veorless,evenfor largerelations).
Now consider the case where a B
+
-tree is being built on a large relation.
Suppose the relation is signi?cantly larger than main memory, and we are con-
structing a nonclustering index on the relation such that the index is also larger
than main memory. In this case, as we scan the relation and add entries to the
B
+
-tree,itisquitelikelythateachleafnodeaccessedisnotinthedatabasebuffer
whenitisaccessed,sincethereisnoparticularorderingoftheentries.Withsuch
randomly ordered accesses to blocks, each time an entry is added to the leaf, a
disk seek will be required to fetch the block containing the leaf node. The block
willprobablybeevictedfromthediskbufferbeforeanotherentryisaddedtothe
block,leadingtoanotherdiskseektowritetheblockbacktodisk.Thusarandom
readandarandom writeoperationmay berequiredfor eachentryinserted.
For example, if the relation has 100 million records, and each I/O operation
takes about 10 milliseconds, it would take at least 1 million seconds to build the
index, counting only the cost of reading leaf nodes, not even counting the cost
of writing the updatednodes back to disk. This is clearly a very large amount of
time; in contrast, if each record occupies 100 bytes, and the disk subsystem can
transfer data at 50 megabytes per second, it would take just 200 seconds to read
theentirerelation.
Insertion of a large number of entries at a time into an index is referredto as
bulkloadingoftheindex.Anef?cientwaytoperformbulkloadingofanindexis
as follows. First, create a temporary ?le containing index entries for the relation,
thensortthe?leonthesearchkeyoftheindexbeingconstructed,and?nallyscan
the sorted?le and insertthe entriesinto theindex.Thereareef?cient algorithms
for sorting large relations, which are described later in Section 12.4, which can
sortevenalarge?lewithan I/O cost comparableto that ofreadingthe?lea few
times,assuming areasonableamount ofmainmemoryisavailable.
There is a signi?cant bene?t to sorting the entries before inserting them into
the B
+
-tree. When the entries are inserted in sorted order, all entries that go to a
particular leaf node will appear consecutively, and the leaf needs to be written
out only once; nodes will never have to be read from disk during bulk load, if
the B
+
-tree was empty to start with. Each leaf node will thus incur only one I/O
operation even though many entries may be inserted into the node. If each leaf
504 Chapter 11 Indexing andHashing
contains100entries,theleaflevelwillcontain1millionnodes,resultinginonly1
millionI/Ooperationsforcreatingtheleaflevel.EventheseI/Ooperationscanbe
expectedtobesequential,ifsuccessiveleafnodesareallocatedonsuccessivedisk
blocks, and few disk seeks would be required. With current disks, 1 millisecond
perblockisareasonableestimateformostlysequentialI/Ooperations,incontrast
to10millisecondsperblock for random I/O operations.
We shall study the cost of sorting a large relationlater, in Section12.4, but as
a rough estimate, the index which would have taken a million seconds to build
otherwise, can be constructed in well under 1000 seconds by sorting the entries
beforeinsertingthemintotheB
+
-tree,incontrasttomorethan1,000,000seconds
for insertinginrandomorder.
If the B
+
-tree is initially empty, it can be constructed faster by building it
bottom-up, from the leaf level, instead of using the usual insert procedure. In
bottom-up B
+
-tree construction, after sorting the entries as we just described,
we break up the sorted entries into blocks, keeping as many entries in a block
as can ?t in the block; the resulting blocks form the leaf level of the B
+
-tree. The
minimumvalueineachblock,alongwiththepointertotheblock,isusedtocreate
entries in the next level of the B
+
-tree, pointing to the leaf blocks. Each further
level of the tree is similarly constructed using the minimum values associated
with each node one level below, until the root is created. We leave details as an
exercisefor thereader.
Mostdatabasesystemsimplementef?cienttechniquesbasedonsortingofen-
tries,andbottom-upconstruction,whencreatinganindexonarelation,although
theyusethenormalinsertionprocedurewhentuplesareaddedoneatatimetoa
relationwithanexistingindex.Somedatabasesystemsrecommendthatifavery
large number of tuples are added at once to an already existing relation, indices
on the relation (other than any index on the primary key) should be dropped,
and then re-created after the tuples are inserted, to take advantage of ef?cient
bulk-loading techniques.
11.4.5 B-Tree Index Files
B-treeindicesaresimilartoB
+
-treeindices.Theprimarydistinctionbetweenthe
twoapproachesisthataB-treeeliminatestheredundantstorageofsearch-keyval-
ues.IntheB
+
-treeofFigure11.13,thesearchkeys “Cali?eri”, “Einstein”, “Gold”,
“Mozart”,and“Srinivasan” appear in nonleaf nodes, in addition to appearing
in the leaf nodes. Every search-key value appears in some leaf node; several are
repeatedinnonleafnodes.
A B-tree allows search-key values to appear only once (if they are unique),
unlike a B
+
-tree, where a value may appear in a nonleaf node, in addition to
appearing in a leaf node. Figure 11.21 shows a B-tree that represents the same
search keys as the B
+
-tree of Figure 11.13. Since search keys are not repeated
in the B-tree, we may be able to store the index in fewer tree nodes than in the
corresponding B
+
-tree index. However, since search keys that appear in nonleaf
nodes appear nowhere else in the B-tree, we are forced to include an additional
11.4 B
+
-TreeExtensions 505
Brandt Cali?eri Crick El Said Gold Kim Mozart Srinivasan Wu
Einstein Katz Singh
Einstein
record
Katz
record
Singh
record
Brandt
record
Cali?eri
record
... and soon for other records...
Figure 11.21 B-tree equivalent of B
+
-tree in Figure 11.13.
pointer ?eld for each search key in a nonleaf node. These additional pointers
point toeither?lerecordsorbuckets for theassociatedsearchkey.
It is worth noting that many database system manuals, articles in industry
literature, and industry professionals use the term B-tree to refer to the data
structure that we call the B
+
-tree. In fact, it would be fair to say that in current
usage, the term B-tree is assumed to be synonymous with B
+
-tree. However, in
this book we use the terms B-treeand B
+
-treeas they were originally de?ned,to
avoidconfusion betweenthe two datastructures.
A generalized B-tree leaf node appears in Figure 11.22a; a nonleaf node ap-
pears in Figure 11.22b. Leaf nodes are the same as in B
+
-trees. In nonleaf nodes,
the pointers P
i
are the tree pointers that we used also for B
+
-trees, while the
pointers B
i
are bucket or ?le-record pointers. In the generalized B-tree in the
?gure,therearen?1keysintheleafnode,buttherearem?1keysinthenonleaf
node. This discrepancy occurs because nonleaf nodes must include pointers B
i
,
thusreducingthenumberofsearchkeysthatcanbeheldinthesenodes.Clearly,
m < n,buttheexactrelationshipbetweenmandndependsontherelativesizeof
searchkeysandpointers.
The number of nodes accessed in a lookup in a B-tree depends on where the
searchkeyislocated.Alookup ona B
+
-treerequirestraversalofapathfromthe
rootof thetreetosomeleafnode.Incontrast, itissometimespossibleto?nd the
desired value in a B-tree before reaching a leaf node. However, roughly n times
as many keys are stored in the leaf level of a B-tree as in the nonleaf levels, and,
since n is typically large, the bene?t of ?nding certain values early is relatively
P
1 K
1
P
2
P
n-1
K
n-1
P
n
…
P
1
B
1
K
1
P
2
B
2
K
2
…
P
m-1
B
m-1
K
m-1
P
m
(a)
(b)
Figure 11.22 Typical nodes of a B-tree. (a) Leaf node. (b) Nonleaf node.
506 Chapter 11 Indexing andHashing
small. Moreover,the fact that fewer search keys appear in a nonleaf B-tree node,
compared to B
+
-trees, implies that a B-tree has a smaller fanout and therefore
may have depth greater than that of the corresponding B
+
-tree. Thus, lookup in
aB-treeisfasterforsomesearchkeysbutslowerforothers,although,ingeneral,
lookup timeis stillproportionaltothelogarithm of thenumber of searchkeys.
DeletioninaB-treeismorecomplicated.InaB
+
-tree,thedeletedentryalways
appearsinaleaf.InaB-tree,thedeletedentrymayappearinanonleafnode.The
proper value must be selected as a replacement from the subtree of the node
containing the deletedentry. Speci?cally,ifsearch key K
i
is deleted,the smallest
search key appearing in the subtree of pointer P
i +1
must be moved to the ?eld
formerlyoccupiedbyK
i
.Furtheractionsneedtobetakeniftheleafnodenowhas
toofewentries.Incontrast,insertioninaB-treeisonlyslightlymorecomplicated
than isinsertioninaB
+
-tree.
The space advantages of B-trees are marginal for large indices, and usually
do not outweigh the disadvantages that we have noted. Thus, pretty much all
database-system implementations use the B
+
-tree data structure, even if (as we
discussedearlier)they refertothedatastructureas aB-tree.
11.4.6 Flash Memory
In our description of indexing so far, we have assumed that data are resident on
magneticdisks.Althoughthisassumptioncontinuestobetrueforthemostpart,
?ash memory capacities have grown signi?cantly, and the cost of ?ash memory
per gigabyte has dropped equally signi?cantly, making ?ash memory storage a
serious contender for replacing magnetic-disk storage for many applications. A
natural questionis,how wouldthis change affect theindexstructure.
Flash-memorystorageisstructuredasblocks,andtheB
+
-treeindexstructure
canbeusedfor?ash-memorystorage.Thebene?tofthemuchfasteraccessspeeds
is clear for index lookups. Instead of requiring an average of 10 milliseconds to
seektoandreadablock,arandomblockcanbereadinaboutamicrosecondfrom
?ash-memory. Thus lookups run signi?cantly faster than with disk-based data.
The optimum B
+
-tree node size for ?ash-memory is typically smaller than that
withdisk.
The only real drawback with ?ash memory is that it does not permit in-
placeupdatestodataatthephysical level,although itappearstodosologically.
Everyupdateturnsintoacopy+writeofanentire?ash-memoryblock,requiring
the old copy of the block to be erased subsequently; a block erase takes about
1 millisecond. There is ongoing research aimed at developing index structures
thatcanreducethenumberofblockerases.Meanwhile,standardB
+
-treeindices
can continue to be used even on ?ash-memory storage, with acceptable update
performance, and signi?cantly improvedlookup performance compared to disk
storage.
11.5 Multiple-Key Access
Until now, we have assumed implicitly that only one index on one attribute is
used to process a query on a relation. However, for certain types of queries, it is
11.5 Multiple-Key Access 507
advantageous to use multiple indices if they exist, or to use an index built on a
multiattributesearchkey.
11.5.1 Using Multiple Single-Key Indices
Assumethattheinstructor?lehastwoindices:onefordept nameandoneforsalary.
Considerthefollowingquery:“FindallinstructorsintheFinancedepartmentwith
salaryequalto$80,000.” We write
select ID
frominstructor
wheredept name=’Finance’ andsalary= 80000;
Therearethreestrategiespossiblefor processingthisquery:
1. Use the index on dept name to ?nd all records pertaining to the Finance
department.Examineeachsuchrecordto seewhether salary= 80000.
2. Use the index on salary to ?nd all records pertaining to instructors with
salaryof $80,000. Examineeachsuchrecordtoseewhetherthedepartment
name is “Finance”.
3. Use the index on dept name to ?nd pointers to all records pertaining to the
Finance department. Also, use the index on salary to ?nd pointers to all
recordspertaining to instructors with a salary of $80,000. Take the intersec-
tionofthesetwo setsofpointers.Thosepointersthatareintheintersection
point to records pertaining to instructors of the Finance department and
withsalary of$80,000.
Thethirdstrategyistheonlyoneofthethreethattakesadvantageoftheexistence
ofmultipleindices.However,eventhisstrategymaybeapoorchoiceifallofthe
following hold:
• Therearemany recordspertainingto theFinance department.
• Therearemany recordspertainingto instructorswitha salaryof $80,000.
• There are only a few records pertaining to both the Finance department and
instructorswitha salaryof $80,000.
If these conditions hold, we must scan a large number of pointers to produce a
smallresult.Anindexstructurecalleda“bitmapindex”caninsomecasesgreatly
speeduptheintersectionoperationusedinthethirdstrategy.Bitmapindicesare
outlinedinSection11.9.
508 Chapter 11 Indexing andHashing
11.5.2 Indices on Multiple Keys
An alternativestrategy for this case is to create and use an index on a composite
searchkey(dept name,salary)—thatis,thesearchkeyconsistingofthedepartment
name concatenated withtheinstructor salary.
We can use an ordered (B
+
-tree) index on the above composite search key to
answer ef?cientlyqueriesoftheform
select ID
frominstructor
wheredept name= ’Finance’ andsalary= 80000;
Queriessuchasthefollowingquery,whichspeci?esanequalityconditiononthe
?rstattributeofthesearchkey(dept name)andarangeonthesecondattributeof
the search key (salary), can also be handled ef?ciently since they correspond to a
range queryon thesearchattribute.
select ID
frominstructor
wheredept name=’Finance’ andsalary< 80000;
Wecanevenuseanorderedindexonthesearchkey(dept name,salary)toanswer
the following queryon onlyone attributeef?ciently:
select ID
frominstructor
wheredept name =’Finance’;
An equality condition dept name = “Finance” is equivalent to a range query on
the range with lower end (Finance, ??) and upper end (Finance, +?). Range
queriesonjust thedept nameattributecan be handledina similarmanner.
The use of an ordered-index structure on a composite search key, however,
has afewshortcomings. Asanillustration,considerthequery
select ID
frominstructor
wheredept name< ’Finance’ andsalary< 80000;
We can answer this query by using an ordered index on the search key (dept
name,salary):Foreachvalueofdept namethatislessthan “Finance”inalphabetic
order, the system locates records with a salary value of 80000. However, each
record is likely to be in a different disk block, because of the ordering of records
inthe?le,leadingtomany I/O operations.
The difference between this query and the previous two queries is that the
conditionon the?rst attribute(dept name)is a comparisoncondition, ratherthan
11.6 Static Hashing 509
anequalitycondition.Theconditiondoesnotcorrespondtoarangequeryonthe
searchkey.
To speed the processing of general composite search-key queries (which can
involve one or more comparison operations), we can use several special struc-
tures. We shall consider bitmap indices in Section 11.9. There is another structure,
called the R-tree, that can be used for this purpose. The R-tree is an extension of
the B
+
-tree to handle indexing on multiple dimensions. Since the R-tree is used
primarilywithgeographical datatypes,we describethe structureinChapter25.
11.5.3 Covering Indices
Covering indices are indices that store the values of some attributes (other than
the search-key attributes) along with the pointers to the record. Storing extra
attribute values is useful with secondary indices, since they allow us to answer
somequeriesusing justthe index,without evenlookingup theactual records.
For example,supposethat we have a nonclustering indexon the IDattribute
oftheinstructorrelation.Ifwestorethevalueofthesalaryattributealongwiththe
record pointer, we can answer queries that require the salary (but not the other
attribute,dept name)without accessing theinstructor record.
Thesameeffectcouldbeobtainedbycreatinganindexonthesearchkey(ID,
salary), but a covering index reduces the size of the search key, allowing a larger
fanout inthe nonleaf nodes,andpotentiallyreducingtheheight of theindex.
11.6 Static Hashing
Onedisadvantageof sequential?leorganizationisthat wemustaccess anindex
structure to locate data, or must use binary search, and that results in more I/O
operations. File organizations based on the technique of hashing allow us to
avoidaccessing an indexstructure.Hashing also providesa way of constructing
indices.Westudy?leorganizationsandindicesbasedonhashinginthefollowing
sections.
In our description of hashing, we shall use the term bucket to denote a unit
of storage that can store one or more records. A bucket is typically a disk block,
but could bechosen tobe smalleror largerthan a diskblock.
Formally,letKdenotethesetofallsearch-keyvalues,andletBdenotetheset
ofallbucketaddresses.AhashfunctionhisafunctionfromK toB.Lethdenote
ahashfunction.
To insert a record with search key K
i
,wecomputeh(K
i
), which gives the
address of the bucket for that record. Assume for now that there is space in the
bucket tostorethe record.Then,the recordis storedin that bucket.
To perform a lookup on a search-key value K
i
, we simply compute h(K
i
),
then search the bucket with that address. Suppose that two search keys, K
5
and
K
7
, have the same hash value; that is, h(K
5
) = h(K
7
). If we perform a lookup
on K
5
,thebucketh(K
5
) contains records with search-key values K
5
and records
510 Chapter 11 Indexing andHashing
withsearch-keyvalues K
7
.Thus,wehavetocheckthesearch-keyvalueofevery
record in the bucket to verify that the record is one that we want.
Deletion is equally straightforward. If the search-key value of the record to
bedeletedis K
i
,wecomputeh(K
i
),thensearchthecorrespondingbucketforthat
record, and delete the record from the bucket.
Hashing can be used for two different purposes. In ahash ?le organization,
we obtain the address of the disk block containing a desired record directly by
computing a function on the search-key value of the record. In a hash index
organization we organize the search keys, with their associated pointers, into a
hash ?lestructure.
11.6.1 Hash Functions
Theworst possiblehashfunction mapsallsearch-keyvaluestothesamebucket.
Suchafunctionisundesirablebecausealltherecordshavetobekeptinthesame
bucket. A lookup has to examine every such record to ?nd the one desired. An
ideal hash function distributes the stored keys uniformly across all the buckets,
sothat everybucket has thesamenumber ofrecords.
Since we do not know at design time precisely which search-key values will
be stored in the ?le, we want to choose a hash function that assigns search-key
valuestobuckets insucha way that the distributionhas thesequalities:
• The distribution is uniform. That is, the hash function assigns each bucket
thesamenumber ofsearch-keyvaluesfromthesetofall possiblesearch-key
values.
• Thedistributionisrandom.Thatis,intheaveragecase,eachbucketwillhave
nearly the same number of values assigned to it, regardless of the actual
distribution of search-key values. More precisely, the hash value will not be
correlated to any externally visible ordering on the search-key values, such
as alphabetic ordering or orderingby the length of the search keys;the hash
function willappearto berandom.
As an illustration of these principles, let us choose a hash function for the
instructor ?le using the search key dept name. The hash function that we choose
musthavethedesirablepropertiesnotonlyontheexampleinstructor?lethatwe
havebeenusing,butalsoonaninstructor?leofrealisticsizeforalargeuniversity
withmany departments.
Assume that we decide to have 26 buckets, and we de?ne a hash function
that maps names beginning with the ith letter of the alphabet to the ith bucket.
This hash function has the virtue of simplicity, but it fails to provide a uniform
distribution, since we expect more names to begin with such letters as B and R
than Qand X, forexample.
Nowsupposethatwewantahashfunctiononthesearchkeysalary.Suppose
that the minimum salary is $30,000 and the maximum salary is $130,000, and
we use a hash function that divides the values into 10 ranges, $30,000–$40,000,
$40,001–$50,000andsoon.Thedistributionofsearch-keyvaluesisuniform(since
11.6 Static Hashing 511
bucket 0
bucket 1
bucket 2
bucket 3
bucket 4
bucket 5
bucket 6
bucket 7
45565
15151 Mozart Music 40000
80000
Wu 12121 Finance 90000
76543 Finance Singh
10101 Comp. Sci. Srinivasan
Katz Comp. Sci. 75000
92000
65000
32343
58583
El Said
Cali?eri
History
History
80000
60000
Einstein
Gold
Kim
22222
33456
98345
Physics
Physics
Elec. Eng.
95000
87000
80000
Brandt 83821 Comp. Sci.
76766 Crick Biology 72000
Figure 11.23 Hash organization of instructor ?le, with deptname as the key.
each bucket has the same number of different salary values), but is not random.
Recordswithsalariesbetween$60,001and$70,000arefarmorecommonthanare
records with salaries between $30,001 and $40,000. As a result, the distribution
of records is not uniform—some buckets receive more records than others do. If
the function has a random distribution, even if there are such correlations in the
search keys, the randomness of the distribution will make it very likely that all
buckets will have roughly the same number of records, as long as each search
key occurs in only a small fraction of the records. (If a single search key occurs
in a large fraction of the records, the bucket containing it is likely to have more
records than other buckets, regardless of the hash function used.)
Typical hash functions perform computation on the internal binary machine
representationofcharactersinthesearchkey.Asimplehashfunctionofthistype
?rst computes the sum of the binary representations of the characters of a key,
then returnsthesum modulothe number ofbuckets.
Figure 11.23 shows the application of such a scheme, with eight buckets,
to the instructor ?le, under the assumption that the ith letter in the alphabet is
representedbytheintegeri.
The following hash function is a better alternative for hashing strings. Let s
be a string of length n,andlets[i]denotetheith byte of the string. The hash
function isde?nedas:
s[0] ?31
(n?1)
+s[1] ?31
(n?2)
+···+s[n ?1]
512 Chapter 11 Indexing andHashing
The function can be implemented ef?ciently by setting the hash result initially
to 0, and iterating from the ?rst to the last character of the string, at each step
multiplying the hash value by 31 and then adding the next character (treated as
aninteger).Theaboveexpressionwouldappeartoresultinaverylargenumber,
but it is actually computed with ?xed-size positive integers; the result of each
multiplication and addition is thus automatically computed modulo the largest
possibleintegervalueplus1.Theresultoftheabovefunctionmodulothenumber
ofbuckets canthenbe usedfor indexing.
Hash functions require careful design. A bad hash function may result in
lookup taking timeproportional tothe number of searchkeys inthe ?le. A well-
designed function gives an average-case lookup time that is a (small) constant,
independentof thenumber of searchkeysinthe?le.
11.6.2 Handling of Bucket Over?ows
Sofar,wehaveassumedthat,whenarecordisinserted,thebucket towhich itis
mapped has space to store the record. If the bucket does not have enough space,
abucketover?owissaidtooccur.Bucketover?owcanoccurforseveralreasons:
• Insuf?cient buckets. The number of buckets, which we denote n
B
,mustbe
chosen such that n
B
> n
r
/f
r
,wheren
r
denotes the total number of records
that will be stored and f
r
denotes the number of records that will ?t in a
bucket.Thisdesignation,ofcourse,assumesthatthetotalnumberofrecords
isknown when thehash function ischosen.
• Skew. Some buckets are assigned more records than are others, so a bucket
may over?ow even when other buckets still have space. This situation is
calledbucketskew. Skewcan occur for two reasons:
1. Multiplerecordsmay havethe samesearchkey.
2. The chosen hash function may result in nonuniform distribution of
searchkeys.
So that the probability of bucket over?ow is reduced,the number of buckets
is chosen to be (n
r
/f
r
) ? (1 +d), where d is a fudge factor, typically around 0.2.
Somespaceiswasted:About20percentofthespaceinthebucketswillbeempty.
Butthe bene?tis that theprobabilityof over?owis reduced.
Despiteallocation of a few more buckets than required,bucket over?ow can
still occur. We handle bucket over?ow by using over?ow buckets. If a record
must be inserted into a bucket b,andb is already full, the system provides an
over?ow bucket for b, and inserts the record into the over?ow bucket. If the
over?owbucketisalsofull,thesystemprovidesanotherover?owbucket,andso
on.Alltheover?owbucketsofagivenbucketarechainedtogetherinalinkedlist,
as in Figure 11.24. Over?ow handling using such a linked list is called over?ow
chaining.
We must change the lookup algorithm slightly to handle over?ow chaining.
Asbefore,thesystemusesthehashfunctiononthesearchkeytoidentifyabucket
11.6 Static Hashing 513
over?ow buckets for bucket 1
bucket 0
bucket 1
bucket 2
bucket 3
Figure 11.24 Over?ow chaining in a hash structure.
b.Thesystemmustexaminealltherecordsinbucketb toseewhethertheymatch
thesearchkey,asbefore.Inaddition,ifbucketb hasover?owbuckets,thesystem
mustexaminethe recordsinallthe over?owbuckets also.
Theformofhashstructurethatwehavejustdescribedissometimesreferred
to as closed hashing. Under an alternative approach, called open hashing,the
set of buckets is ?xed, and there are no over?ow chains. Instead, if a bucket is
full,thesysteminsertsrecordsinsomeotherbucketintheinitialsetofbuckets B.
One policy is to use the next bucket (in cyclic order) that has space; this policy is
calledlinearprobing.Otherpolicies,suchascomputingfurtherhashfunctions,are
also used. Open hashing has been used to construct symbol tables for compilers
and assemblers, but closed hashing is preferable for database systems. The rea-
son is that deletion under open hashing is troublesome. Usually, compilers and
assemblersperformonlylookupandinsertionoperationsontheirsymboltables.
However, in a database system, it is important to be able to handle deletion as
well as insertion. Thus, open hashing is of only minor importance in database
implementation.
An important drawback to the form of hashing that we have described is
that we must choose the hash function when we implement the system, and it
cannot be changed easily thereafter if the ?le being indexed grows or shrinks.
Since the function h maps search-key values to a ?xed setB of bucket addresses,
we waste space if B is made large to handle future growth of the ?le. If B is
too small, the buckets contain records of many different search-key values, and
bucket over?ows can occur. As the ?le grows, performance suffers. We study
later, in Section 11.7, how the number of buckets and the hash function can be
changed dynamically.
514 Chapter 11 IndexingandHashing
bucket 0
bucket 1
bucket 2
bucket 3
bucket 4
bucket 5
bucket 6
76766
45565
76543
10101
15151
33456
58583
83821
22222
98345
bucket 7
12121
32343
76766 Crick
76543 Singh
32343 El Said
58583 Cali?eri
15151 Mozart
22222 Einstein
33465 Gold
10101 Srinivasan
45565 Katz
83821 Brandt
98345 Kim
12121 Wu
Biology
Physics
Finance
History
History
Music
Physics
Comp. Sci.
Comp. Sci.
Comp. Sci.
Elec. Eng.
Finance
72000
80000
60000
62000
40000
95000
87000
65000
75000
92000
80000
90000
Figure 11.25 Hash index on search key ID of instructor ?le.
11.6.3 Hash Indices
Hashing can be used not only for ?le organization, but also for index-structure
creation. A hashindex organizes the search keys, with their associated pointers,
into a hash ?le structure. We construct a hash index as follows. We apply a hash
function on a search key to identify a bucket, and store the key and its associated
pointers in the bucket (or in over?ow buckets). Figure 11.25 shows a secondary
hash index on the instructor ?le, for the search key ID. The hash function in the
?gure computes the sum of the digits of the ID modulo 8. The hash index has
eight buckets, each of size 2 (realistic indices would, of course, have much larger
bucketsizes).Oneofthebucketshasthreekeysmappedtoit,soithasanover?ow
bucket. In this example, ID is a primary key for instructor,soeachsearchkeyhas
11.7 Dynamic Hashing 515
only one associated pointer. In general, multiple pointers can be associated with
eachkey.
We use the term hashindextodenotehash?lestructuresaswellassecondary
hashindices.Strictlyspeaking,hashindicesareonlysecondaryindexstructures.
A hash index is never needed as a clustering index structure, since, if a ?le itself
isorganizedbyhashing,thereisnoneedforaseparatehashindexstructureonit.
However, since hash ?le organization provides the same direct access to records
that indexing provides, we pretend that a ?le organized by hashing also has a
clusteringhash indexonit.
11.7 Dynamic Hashing
As we have seen, the need to ?x the set B of bucket addresses presents a serious
problemwiththestatichashingtechniqueoftheprevioussection.Mostdatabases
growlargerovertime.Ifwearetousestatichashingforsuchadatabase,wehave
threeclassesof options:
1. Chooseahashfunctionbasedonthecurrent?lesize.Thisoptionwillresult
inperformancedegradationas thedatabasegrows.
2. Chooseahashfunctionbasedontheanticipatedsizeofthe?leatsomepoint
in the future. Although performance degradation is avoided, a signi?cant
amount ofspace maybe wastedinitially.
3. Periodically reorganize the hash structure in response to ?le growth. Such
a reorganization involves choosing a new hash function, recomputing the
hash function on every record in the ?le, and generating new bucket as-
signments. This reorganization is a massive, time-consuming operation.
Furthermore, it is necessary to forbid access to the ?le during reorganiza-
tion.
Severaldynamichashingtechniquesallow thehash function to be modi?ed
dynamically to accommodate the growth or shrinkage of the database. In this
section we describe one form of dynamic hashing, called extendable hashing.
Thebibliographical notes providereferencesto otherforms ofdynamichashing.
11.7.1 Data Structure
Extendablehashingcopeswithchangesindatabasesizebysplittingandcoalesc-
ing buckets as the database grows and shrinks. As a result, space ef?ciency is
retained. Moreover, since the reorganization is performed on only one bucket at
atime,theresultingperformanceoverheadisacceptably low.
With extendable hashing, we choose a hash function h with the desirable
propertiesofuniformityandrandomness.However,thishashfunctiongenerates
values over a relatively large range—namely, b-bit binary integers. A typical
valueforb is32.
516 Chapter 11 Indexing andHashing
i
i
1
i
2
i
3
bucket 1
bucket 2
bucket 3
00..
01..
10..
11..
bucket address table
hash prefix
…
…
Figure 11.26 General extendable hash structure.
We do not create a bucket for each hash value. Indeed, 2
32
is over 4 billion,
and that many buckets is unreasonable for all but the largest databases. Instead,
we create buckets on demand, as records are inserted into the ?le. We do not
use the entire b bits of the hash value initially. At any point, we use i bits, where
0 ? i ? b.Thesei bits are used as an offset into an additional table of bucket
addresses.The valueofigrows and shrinkswith thesizeof thedatabase.
Figure11.26showsageneralextendablehashstructure.Theiappearingabove
the bucket address table in the ?gure indicates that i bits of the hash value h(K)
are required to determine the correct bucket for K. This number will, of course,
change as the ?le grows. Although i bits are required to ?nd the correct entry
in the bucket address table, several consecutive table entries may point to the
same bucket. All such entries will have a common hash pre?x, but the length of
thispre?xmaybelessthani.Therefore,weassociatewitheachbucketaninteger
givingthelengthofthecommonhashpre?x.InFigure11.26theintegerassociated
withbucketjisshownasi
j
.Thenumberofbucket-address-tableentriesthatpoint
tobucketjis
2
(i ?i
j
)
11.7.2 Queries and Updates
We now see how to perform lookup, insertion, and deletion on an extendable
hash structure.
11.7 Dynamic Hashing 517
Tolocatethebucketcontainingsearch-keyvalue K
l
,thesystemtakesthe?rst
ihigh-orderbitsofh(K
l
),looksatthecorrespondingtableentryforthisbitstring,
and follows thebucket pointerinthetableentry.
To insert a record with search-key value K
l
, the system follows the same
procedureforlookupasbefore,endingupinsomebucket—say,j.Ifthereisroom
in the bucket, the system inserts the record in the bucket. If, on the other hand,
the bucket is full, it must split the bucket and redistribute the current records,
plus the new one. To split the bucket, the system must ?rst determine from the
hash valuewhether itneedstoincreasethe number ofbits that ituses.
• If i = i
j
, only one entry in the bucket address table points to bucket j.
Therefore,thesystemneedstoincreasethesizeofthebucketaddresstableso
thatitcanincludepointerstothetwobucketsthatresultfromsplittingbucket
j. It does so by considering an additional bit of the hash value. It increments
the value of i by 1, thus doubling the size of the bucket address table. It
replaces each entry by two entries, both of which contain the same pointer
as the original entry. Now two entries in the bucket address table point to
bucket j. The system allocates a new bucket (bucket z), and sets the second
entry to point to the new bucket. It sets i
j
and i
z
to i. Next, it rehashes each
recordinbucketjand,dependingonthe?rstibits(rememberthesystemhas
added 1 to i), either keeps it in bucket j or allocates it to the newly created
bucket.
The system now reattempts the insertion of the new record. Usually, the
attemptwillsucceed.However,ifalltherecordsinbucketj,aswellasthenew
record,havethesamehash-valuepre?x,itwillbenecessarytosplitabucket
again, since all the records in bucket j and the new record are assigned to
thesamebucket.Ifthehashfunctionhasbeenchosencarefully,itisunlikely
that a single insertion will require that a bucket be split more than once,
unlesstherearealargenumberofrecordswiththesamesearchkey.Ifallthe
records in bucket j have the same search-key value, no amount of splitting
will help. In such cases, over?owbuckets are used to store the records,as in
statichashing.
• If i > i
j
, then more than one entry in the bucket address table points to
bucket j. Thus, the system can split bucket j without increasing the size of
the bucket address table. Observe that all the entries that point to bucket j
correspond to hash pre?xes that have the same value on the leftmosti
j
bits.
The system allocates a new bucket (bucket z), and sets i
j
and i
z
to the value
that results from adding 1 to the original i
j
value. Next, the system needs
to adjust the entries in the bucket address table that previously pointed to
bucket j. (Note that with the new value for i
j
, not all the entries correspond
tohashpre?xesthathavethesamevalueontheleftmosti
j
bits.)Thesystem
leaves the ?rst half of the entries as they were (pointing to bucket j), and
setsalltheremainingentriestopointtothenewlycreatedbucket(bucket z).
Next, as in the previous case, the system rehashes each record in bucket j,
and allocatesiteithertobucketjor tothe newlycreatedbucketz.
518 Chapter 11 Indexing andHashing
dept_name h(dept_name)
Biology 0010  1101   1111  1011   0010  1100   0011   0000
Comp.   Sci. 1111   0001   0010  0100   1001  0011   0110   1101
Elec.   Eng. 0100  0011   1010  1100   1100  0110   1101   1111
Finance 1010  0011   1010  0000   1100  0110   1001   1111
History 1100   0111   1110  1101   1011  1111   0011   1010
Music 0011   0101   1010  0110   1100  1001   1110   1011
Physics                          1001   1000  0011   1111  1001   1100  0000   0001
Figure 11.27 Hash function for deptname.
The system then reattempts the insert. In the unlikely case that it again
fails,itappliesone of thetwo cases,i = i
j
ori > i
j
,as appropriate.
Notethat,inbothcases,thesystemneedstorecomputethehashfunctionononly
the recordsinbucketj.
To delete a record with search-key value K
l
, the system follows the same
procedure for lookup as before, ending up in some bucket—say, j. It removes
both the search key from the bucket and the record from the ?le. The bucket,
too, is removed if it becomes empty. Note that, at this point, several buckets can
be coalesced, and the size of the bucket address table can be cut in half. The
procedure for deciding on which buckets can be coalesced and how to coalesce
bucketsislefttoyoutodoasanexercise.Theconditionsunderwhichthebucket
address table can be reduced in size are also left to you as an exercise. Unlike
coalescing of buckets, changing the size of the bucket address table is a rather
expensiveoperationifthetableislarge.Thereforeitmaybeworthwhiletoreduce
the bucket-address-tablesizeonly ifthenumber ofbuckets reducesgreatly.
Toillustratetheoperationofinsertion,weusetheinstructor?leinFigure11.1
andassumethatthesearchkeyisdept namewiththe32-bithashvaluesasappear
in Figure 11.27. Assume that, initially, the ?le is empty, as in Figure 11.28. We
insert the records one by one. To illustrate all the features of extendable hashing
in a small structure, we shall make the unrealistic assumption that a bucket can
hold only two records.
0
0
bucket 1
bucket address table
hash pre?x
Figure 11.28 Initial extendable hash structure.
11.7 Dynamic Hashing 519
1
1
bucket address table
hash pre?x
1
15151 Music 40000
10101
12121
Srinivasan 90000
Wu 90000
Mozart
Comp. Sci.
Finance
Figure 11.29 Hash structure after three insertions.
Weinserttherecord(10101,Srinivasan,Comp.Sci.,65000).Thebucketaddress
tablecontainsapointertotheonebucket,andthesysteminsertstherecord.Next,
we insert the record (12121, Wu, Finance, 90000). The system also places this
recordinthe one bucketof our structure.
Whenweattempttoinsertthenextrecord(15151,Mozart,Music,40000),we
?nd that the bucket is full. Since i = i
0
, we need to increase the number of bits
that we use from the hash value. We now use 1 bit, allowing us 2
1
= 2buckets.
This increase in the number of bits necessitates doubling the size of the bucket
address table to two entries. The system splits the bucket, placing in the new
bucket those records whose search key has a hash value beginning with 1, and
leaving in the original bucket the other records. Figure 11.29 shows the state of
our structureafterthe split.
Next,weinsert(22222,Einstein,Physics,95000).Sincethe?rstbitofh(Physics)
is 1, we must insert this record into the bucket pointed to by the “1” entry in the
2
1
2
2
bucket address table
hash pre?x
15151 Music 40000 Mozart
12121 Finance 90000 Wu
10101 Comp. Sci. 65000 Srinivasan
22222 Einstein Physics 95000
Figure 11.30 Hash structure after four insertions.
520 Chapter 11 Indexing andHashing
3
1
3
3
bucket address table
hash pre?x
2
22222
33456
Physics
95000
Physics 87000
Music 15151 40000
Mozart
Einstein
Gold
12121 Wu 90000 Finance
10101
32343
Srinivasan
El Said
Comp. Sci.
History 60000
65000
Figure 11.31 Hash structure after six insertions.
3
1
3
3
bucket address table
hash pre?x
3
22222
33456
Physics 95000
Physics 87000
Music 15151
40000 Mozart
Einstein
Gold
12121 Wu 90000 Finance
10101
45565
Srinivasan
Katz
Comp. Sci.
Comp. Sci. 75000
65000
32343 El Said
History 60000
3
Figure 11.32 Hash structure after seven insertions.
11.7 Dynamic Hashing 521
bucketaddresstable.Onceagain,we?ndthebucketfullandi = i
1
.Weincrease
thenumberofbitsthatweusefromthehash to2.Thisincreaseinthenumberof
bits necessitates doubling the size of the bucket address table to four entries, as
in Figure 11.30. Since the bucket of Figure 11.29 for hash pre?x 0 was not split,
thetwoentriesof thebucketaddresstableof00and01bothpointtothisbucket.
For each record in the bucket of Figure 11.29 for hash pre?x 1 (the bucket
being split), the system examines the ?rst 2 bits of the hash value to determine
which bucket of thenewstructureshouldhold it.
Next,weinsert(32343,ElSaid,History,60000),whichgoesinthesamebucket
asComp.Sci.Thefollowinginsertionof(33456,Gold,Physics,87000)resultsina
bucket over?ow, leading to an increase in the number of bits, and a doubling of
thesizeof the bucketaddresstable (seeFigure11.31).
Theinsertionof(45565,Katz,Comp.Sci.,75000)leadstoanotherbucketover-
?ow; this over?ow, however, can be handled without increasing the number of
bits,sincethebucketinquestionhastwopointerspointingtoit(seeFigure11.32).
Next, we insert the records of “Cali?eri”, “Singh”,and“Crick” without any
bucket over?ow. The insertion of the third Comp. Sci. record (83821, Brandt,
Comp. Sci., 92000), however, leads to another over?ow. This over?ow cannot
be handled by increasing the number of bits, since there are three records with
exactly the same hash value. Hence the system uses an over?ow bucket, as in
3
bucket address table
hash pre?x
2
3
3
3
22222
33456
Physics 95000
Physics 87000
Music
Biology
15151 40000
72000
Mozart
Einstein
Gold
12121 Wu 90000 Finance
10101
45565
Srinivasan
Katz
Comp. Sci.
Comp. Sci. 75000
65000
Crick 76766
Singh 76543 Finance
92000 Comp. Sci. Brandt 83821
32343
58583
El Said
Cali?eri
History
History
60000
62000
80000
3
Figure 11.33 Hash structure after eleven insertions.
522 Chapter 11 Indexing andHashing
3
2
3
3
bucket address table
hash pre?x
3
22222
33456
Physics 95000
Physics 87000
Music 15151 40000 Mozart
Einstein
Gold
12121 Wu 90000 Finance
10101
45565
Srinivasan
Katz
Comp. Sci.
Comp. Sci. 75000
65000
Crick Biology 72000 76766
Singh 76543 Finance
80000 Elec. Eng. Kim 98345
92000 Comp. Sci. Brandt 83821
32343
58583
El Said
Cali?eri
History
History
60000
62000
2
80000
3
Figure 11.34 Extendable hash structure for the instructor ?le.
Figure11.33.Wecontinue inthismanner until wehaveinsertedallthe instructor
recordsof Figure11.1.The resultingstructureappearsinFigure11.34.
11.7.3 Static Hashing versus Dynamic Hashing
Wenowexaminetheadvantagesanddisadvantagesofextendablehashing,com-
pared with static hashing. The main advantage of extendable hashing is that
performance does not degrade as the ?le grows. Furthermore, there is minimal
spaceoverhead.Althoughthebucketaddresstableincursadditionaloverhead,it
contains one pointer for each hash value for the current pre?x length. This table
is thus small. The main space saving of extendable hashing over other forms of
hashing is that no buckets need to be reservedfor future growth; rather, buckets
can beallocateddynamically.
A disadvantage of extendable hashing is that lookup involves an additional
levelof indirection,since the systemmust access the bucket addresstable before
accessing the bucket itself. This extra reference has only a minor effect on per-
formance. Although the hash structures that we discussed in Section 11.6 do not
11.8 Comparisonof OrderedIndexing andHashing 523
havethisextralevelofindirection,theylosetheirminorperformanceadvantage
as theybecome full.
Thus, extendable hashing appears to be a highly attractive technique, pro-
vided that we are willing to accept the added complexity involved in its im-
plementation. The bibliographical notes reference more detailed descriptions of
extendablehashing implementation.
Thebibliographicalnotesalsoprovidereferencestoanotherformofdynamic
hashing calledlinearhashing,whichavoidstheextralevelofindirectionassoci-
atedwithextendablehashing, at thepossiblecost of moreover?owbuckets.
11.8 Comparison of Ordered Indexing and Hashing
We have seen several ordered-indexing schemes and several hashing schemes.
Wecanorganize?lesofrecordsasordered?lesbyusingindex-sequentialorgani-
zation or B
+
-tree organizations. Alternatively,we can organize the ?les by using
hashing. Finally, we can organize them as heap ?les, where the records are not
orderedinany particular way.
Each scheme has advantages in certain situations. A database-system imple-
mentorcouldprovidemanyschemes,leavingthe?naldecisionofwhichschemes
to use to the database designer. However, such an approach requires the imple-
mentor to write more code, adding both to the cost of the system and to the
spacethatthesystemoccupies.MostdatabasesystemssupportB
+
-treesandmay
additionallysupportsome formof hash ?leorganizationor hash indices.
To make a choice of ?le organization and indexing techniques for a relation,
theimplementororthe databasedesignermustconsider thefollowing issues:
• Is the cost of periodic reorganization of the index or hash organization ac-
ceptable?
• What isthe relativefrequencyof insertionand deletion?
• Is it desirable to optimize average access time at the expense of increasing
theworst-case access time?
• What typesofqueriesareuserslikelytopose?
We have already examined the ?rst three of these issues, ?rst in our review of
therelativemeritsofspeci?cindexingtechniques,andagaininourdiscussionof
hashingtechniques.Thefourthissue,theexpectedtypeofquery,iscriticaltothe
choice of orderedindexingor hashing.
Ifmost queriesareof theform:
select A
1
, A
2
,...,A
n
fromr
where A
i
= c;
524 Chapter 11 Indexing andHashing
then,toprocessthisquery,thesystemwillperformalookuponanorderedindex
orahashstructureforattribute A
i
,forvaluec.Forqueriesofthisform,ahashing
scheme is preferable. An ordered-index lookup requires time proportional to
the log of the number of values in r for A
i
. In a hash structure, however, the
average lookup time is a constant independent of the size of the database. The
only advantage to an index over a hash structure for this form of query is that
theworst-caselookuptimeisproportionaltothelogofthenumberofvaluesinr
for A
i
.Bycontrast,forhashing,theworst-caselookuptimeisproportionaltothe
number of values inr for A
i
. However, the worst-case lookup time is unlikely to
occur withhashing, and hashing ispreferableinthiscase.
Ordered-indextechniquesarepreferabletohashingincaseswherethequery
speci?esarangeofvalues.Suchaquerytakesthefollowing form:
select A
1
, A
2
,..., A
n
fromr
where A
i
? c
2
and A
i
? c
1
;
Inotherwords,theprecedingquery?ndsalltherecordswith A
i
valuesbetween
c
1
andc
2
.
Let us consider how we process this query using an ordered index. First, we
perform a lookup on value c
1
.Oncewehavefoundthebucketforvaluec
1
,we
follow the pointer chain in the index to read the next bucket in order, and we
continue inthis manner untilwe reachc
2
.
If, instead of an ordered index, we have a hash structure, we can perform
a lookup on c
1
and can locate the corresponding bucket—but it is not easy, in
general,todeterminethenextbucketthatmustbeexamined.Thedif?cultyarises
because a good hash function assigns valuesrandomly to buckets.Thus, thereis
no simple notion of “next bucket in sorted order.” The reason we cannot chain
bucketstogetherinsortedorderon A
i
isthateachbucketisassignedmanysearch-
key values.Since values are scatteredrandomly by the hash function, the values
in the speci?ed range are likely to be scattered across many or all of the buckets.
Therefore,wehave toreadallthebuckets to?nd therequiredsearchkeys.
Usually the designer will choose ordered indexing unless it is known in
advance that range queries will be infrequent, in which case hashing would be
chosen. Hash organizations are particularly useful for temporary ?les created
during query processing, if lookups based on a key value are required, but no
range querieswillbeperformed.
11.9 Bitmap Indices
Bitmap indices are a specialized type of index designed for easy querying on
multiplekeys,althougheachbitmap indexisbuilton asinglekey.
For bitmap indices to be used, records in a relation must be numbered se-
quentially, starting from, say, 0. Given a number n, it must be easy to retrieve
11.9 BitmapIndices 525
the recordnumberedn. This is particularlyeasyto achieveif recordsare?xed in
size, and allocated on consecutive blocks of a ?le. The record number can then
be translated easily into a block number and a number that identi?es the record
withinthe block.
Consider a relation r,withanattributeA that can take on only one of a
smallnumber (forexample,2to20)values.Forinstance,arelationinstructor info
may have an attribute gender, which can take only values m (male) or f (female).
Another example would be an attribute income level, where income has been
brokenup into5levels: L1:$0?9999, L2:$10,000?19,999, L3: 20,000?39,999, L4:
40,000?74,999, and L5: 75,000??. Here, the raw data can take on many values,
but a data analyst has split the values into a small number of ranges to simplify
analysis of thedata.
11.9.1 Bitmap Index Structure
A bitmap is simply an array of bits. In its simplest form, a bitmap index on the
attribute A of relation r consists of one bitmap for each value that A can take.
Eachbitmaphasasmanybitsasthenumberofrecordsintherelation.Theithbit
of the bitmapfor valuev
j
issetto1if therecordnumberedi has the valuev
j
for
attribute A.Allother bitsof thebitmap aresetto0.
In our example,there is one bitmap for the value m and one for f.Theith bit
of the bitmap for m is set to 1 if the gender value of the record numbered i is m.
All other bits of the bitmap for m are set to 0. Similarly, the bitmap for f has the
value1 forbitscorrespondingtorecordswiththevalue fforthegenderattribute;
all other bits have the value 0. Figure 11.35 shows an example of bitmap indices
onarelationinstructor info.
We now consider when bitmaps are useful. The simplest way of retrieving
all records with value m (or value f) would be to simply read all records of the
relation and select those records with value m (or f, respectively). The bitmap
indexdoesn’treallyhelptospeedupsuchaselection.Whileitwouldallowusto
ID income_level gender
76766
22222
12121
15151
58583
m
m
f
f
f
L1
L1
L2
L4
L3
record
number
1
0
2
3
4
m
f
Bitmaps for gender
10010
01101
Bitmaps for
income_level
L1
L2
L3
L4
L5
10100
01000
00001
00010
00000
Figure 11.35 Bitmap indices on relation instructor info.
526 Chapter 11 Indexing andHashing
read only those records for a speci?c gender, it is likely that every disk block for
the?lewouldhavetobereadanyway.
In fact, bitmap indices are useful for selections mainly when there are selec-
tions on multiple keys. Suppose we create a bitmap index on attribute income
level, which we describedearlier,inadditiontothebitmap indexongender.
Considernowaquerythatselectswomenwithincomeintherange$10,000to
$19,999. This querycan be expressedas
select *
fromr
wheregender=’f’andincome level=’L2’;
Toevaluatethis selection,wefetchthebitmapsforgendervalue fandthebitmap
for income level value L2, and perform an intersection (logical-and) of the two
bitmaps. In other words, we compute a new bitmap where bit i has value 1 if
the ith bit of the two bitmaps are both 1, and has a value 0 otherwise. In the
exampleinFigure11.35,theintersectionofthebitmapforgender = f(01101)and
the bitmapforincomelevel = L2 (01000) givesthe bitmap01000.
Since the ?rst attribute can take two values, and the second can take ?ve
values, we would expect only about 1 in 10 records, on an average, to satisfy
a combined condition on the two attributes. If there are further conditions, the
fraction of records satisfying all the conditions is likely to be quite small. The
system can then compute the query result by ?nding all bits with value 1 in the
intersection bitmap and retrieving the corresponding records. If the fraction is
large,scanning theentirerelationwould remainthecheaper alternative.
Anotherimportantuseofbitmapsistocountthenumberoftuplessatisfying
a given selection. Such queries are important for data analysis. For instance, if
wewishto?ndouthowmanywomenhaveanincomelevel L2,wecomputethe
intersection of the two bitmaps and then count the number of bits that are 1 in
theintersectionbitmap.Wecanthusgetthedesiredresultfromthebitmapindex,
without evenaccessing therelation.
Bitmapindicesaregenerallyquitesmallcomparedtotheactualrelationsize.
Records are typically at least tens of bytes to hundreds of bytes long, whereas a
single bit represents the record in a bitmap. Thus the space occupied by a single
bitmap is usually less than 1 percent of the space occupied by the relation. For
instance,iftherecordsizeforagivenrelationis100bytes,thenthespaceoccupied
by a singlebitmap wouldbe
1
8
of 1 percentof the space occupied by the relation.
If an attribute A of the relation can take on only one of eight values, a bitmap
indexonattribute Awouldconsistofeightbitmaps,whichtogetheroccupyonly
1percentofthesizeoftherelation.
Deletion of records creates gaps in the sequence of records, since shifting
records(orrecordnumbers)to?llgapswouldbeextremelyexpensive.Torecog-
nizedeletedrecords,wecanstoreanexistencebitmap,inwhichbiti is0ifrecord
i does not exist and 1 otherwise. We shall see the need for existence bitmaps in
Section 11.9.2. Insertion of records should not affect the sequence numbering of
11.9 BitmapIndices 527
other records. Therefore, we can do insertion either by appending records to the
endof the?le orby replacingdeletedrecords.
11.9.2 Ef?cient Implementation of Bitmap Operations
Wecancomputetheintersectionoftwobitmapseasilybyusingaforloop:theith
iteration of the loop computes the and of theith bits of the two bitmaps. We can
speed up computation of the intersection greatly by using bit-wise and instruc-
tionssupportedbymostcomputerinstructionsets.Awordusuallyconsistsof 32
or64bits,dependingonthearchitectureofthecomputer.Abit-wiseandinstruc-
tion takes two words as input and outputs a word where each bit is the logical
andof thebitsincorrespondingpositionsoftheinputwords.What isimportant
tonoteisthatasinglebit-wiseandinstructioncancomputetheintersectionof32
or 64 bitsatonce.
If a relation had 1 million records, each bitmap would contain 1 million bits,
orequivalently128kilobytes.Only31,250instructionsareneededtocomputethe
intersectionoftwobitmapsforourrelation,assuminga32-bitwordlength.Thus,
computing bitmap intersectionsisanextremelyfast operation.
Justasbitmapintersectionisusefulforcomputingtheandoftwoconditions,
bitmapunionisusefulforcomputingtheoroftwoconditions.Theprocedurefor
bitmap union is exactly the same as for intersection, except we use bit-wise or
instructions insteadof bit-wise andinstructions.
Thecomplementoperationcanbeusedtocomputeapredicateinvolvingthe
negation of a condition, such as not (income-level = L1). The complement of a
bitmap is generatedby complementing everybit of the bitmap (the complement
of1is0andthecomplementof0is1).Itmayappearthatnot(income level=L1)can
beimplementedbyjustcomputingthecomplementofthebitmapforincomelevel
L1.Ifsomerecordshavebeendeleted,however,justcomputingthecomplement
ofabitmapisnotsuf?cient.Bitscorrespondingtosuchrecordswouldbe0inthe
original bitmap, but would become 1 in the complement, although the records
don’t exist. A similar problem also arises when the value of an attribute is null.
For instance, if the value of income level is null, the bit would be 0 in the original
bitmapfor value L1,and 1inthecomplementbitmap.
To make sure that the bits corresponding to deleted records are set to 0
in the result, the complement bitmap must be intersected with the existence
bitmaptoturnoffthebitsfordeletedrecords.Similarly,tohandlenullvalues,the
complementbitmapmustalsobeintersectedwiththecomplementofthebitmap
for thevaluenull.
2
Counting the number of bits that are 1 in a bitmap can be done quickly by a
clever technique. We can maintain an array with 256 entries, where theith entry
stores the number of bits that are 1 in the binary representationof i.Setthetotal
countinitiallyto0.Wetakeeachbyteofthebitmap,useittoindexintothisarray,
andaddthestoredcounttothetotalcount.Thenumberofadditionoperationsis
2
Handlingpredicatessuchasisunknownwouldcausefurthercomplications,whichwouldingeneralrequireuseofan
extra bitmap to track whichoperationresults areunknown.
528 Chapter 11 Indexing andHashing
1
8
ofthenumberoftuples,andthusthecountingprocessisveryef?cient.Alarge
array (using 2
16
= 65,536 entries), indexed by pairs of bytes, would give even
higher speedup,but at ahigher storagecost.
11.9.3 Bitmaps and B
+
-Trees
Bitmaps can be combined with regular B
+
-tree indices for relations where a few
attribute values are extremely common, and other values also occur, but much
lessfrequently.InaB
+
-treeindexleaf,foreachvaluewewouldnormallymaintain
a list of all records with that value for the indexed attribute. Each element of the
list would be a record identi?er, consisting of at least 32 bits, and usually more.
For a value that occurs in many records, we store a bitmap instead of a list of
records.
Suppose a particular value v
i
occurs in
1
16
of the records of a relation. Let N
be the number of records in the relation, and assume that a record has a 64-bit
number identifying it. The bitmap needs only 1 bit per record, or N bits in total.
In contrast, the list representation requires 64 bits per record where the value
occurs, or 64 ? N/16 = 4N bits. Thus, a bitmap is preferable for representing
the list of records for value v
i
. In our example (with a 64-bit record identi?er),
if fewer than 1 in 64 records have a particular value, the list representation is
preferableforidentifyingrecordswiththatvalue,sinceitusesfewerbitsthanthe
bitmap representation. If more than 1 in 64 records have that value, the bitmap
representationispreferable.
Thus, bitmaps can be used as a compressed storage mechanism at the leaf
nodesof B
+
-treesfor those valuesthat occur veryfrequently.
11.10 Index De?nition in SQL
The SQLstandarddoesnotprovideanywayforthedatabaseuseroradministra-
tor to control what indices are created and maintained in the database system.
Indicesare notrequiredforcorrectness,since theyareredundantdatastructures.
However, indices are important for ef?cient processing of transactions, includ-
ing both update transactions and queries.Indices are also important for ef?cient
enforcementofintegrityconstraints.
In principle, a database system can decide automatically what indices to
create. However, because of the space cost of indices, as well as the effect of
indicesonupdateprocessing,itisnoteasytoautomaticallymaketherightchoices
about what indices to maintain. Therefore, most SQL implementations provide
theprogrammercontrolovercreationandremovalofindicesviadata-de?nition-
language commands.
We illustrate the syntax of these commands next. Although the syntax that
we show is widely used and supported by many database systems, it is not part
of the SQL standard. The SQL standard does not support control of the physical
databaseschema; itrestrictsitselfto thelogical databaseschema.
Wecreateanindexwiththe createindexcommand, which takestheform:
11.11 Summary 529
createindex<index-name>on<relation-name>(<attribute-list>);
The attribute-list is the list of attributes of the relations that form the search key
for theindex.
To de?ne an index named dept index on the instructor relation with dept name
as thesearchkey,we write:
createindexdept index oninstructor(dept name);
If we wish to declare that the search key is a candidate key, we add the
attributeuniquetothe indexde?nition.Thus,the command:
create uniqueindexdept indexoninstructor (dept name);
declaresdept nametobeacandidatekeyforinstructor(whichisprobablynotwhat
we actually would want for our university database). If, at the time we enter the
create uniqueindex command, dept name is not a candidate key, the system will
display an error message, and the attempt to create the index will fail. If the
index-creation attempt succeeds, any subsequent attempt to insert a tuple that
violatesthekeydeclarationwillfail.Notethattheuniquefeatureisredundantif
thedatabase systemsupportstheuniquedeclarationof the SQL standard.
Many database systems also provide a way to specify the type of index to
be used (such as B
+
-tree or hashing). Some database systems also permit one of
theindicesonarelationtobedeclaredtobeclustered;thesystemthenstoresthe
relationsortedby the search-keyof theclusteredindex.
The index name we speci?ed for an index is required to drop an index. The
dropindexcommand takestheform:
dropindex<index-name>;
11.11 Summary
• Many queries reference only a small proportion of the records in a ?le. To
reduce the overhead in searching for these records, we can construct indices
for the?lesthat storethedatabase.
• Index-sequential ?les are one of the oldest index schemes used in database
systems. To permit fast retrieval of records in search-key order, records are
stored sequentially, and out-of-order records are chained together. To allow
fastrandom access, weusean indexstructure.
• There are two types of indices that we can use: dense indices and sparse
indices. Dense indices contain entries for every search-key value, whereas
sparseindicescontain entriesonly forsome search-keyvalues.
530 Chapter 11 Indexing andHashing
• Ifthesortorderofasearchkeymatchesthesortorderofarelation,anindex
on the search key is called a clustering index. The other indices are called
nonclusteringorsecondaryindices.Secondaryindicesimprovetheperformance
of queries that use search keys other than the search key of the clustering
index.However,theyimposeanoverheadonmodi?cation ofthe database.
• The primary disadvantage of the index-sequential ?le organization is that
performancedegradesas the?legrows.Toovercomethisde?ciency,wecan
usea B
+
-treeindex.
• AB
+
-tree index takes the form of a balanced tree, in which every path from
the root of the tree to a leaf of the tree is of the same length. The height of
aB
+
-tree is proportional to the logarithm to the base N of the number of
records in the relation, where each nonleaf node stores N pointers;thevalue
of Nisoftenaround50or100.B
+
-treesaremuchshorterthanotherbalanced
binary-tree structures such as AVL trees, and therefore require fewer disk
accessesto locaterecords.
• Lookup on B
+
-trees is straightforward and ef?cient. Insertion and deletion,
however, are somewhat more complicated, but still ef?cient. The number of
operationsrequiredforlookup,insertion,anddeletiononB
+
-treesispropor-
tionaltothelogarithmtothebase Nofthenumberofrecordsintherelation,
whereeachnonleaf nodestores N pointers.
• We can use B
+
-trees for indexing a ?le containing records, as well as to
organizerecordsinto a ?le.
• B-treeindicesaresimilartoB
+
-treeindices.TheprimaryadvantageofaB-tree
is that the B-tree eliminates the redundant storage of search-key values. The
major disadvantages are overall complexity and reduced fanout for a given
node size. System designers almost universally prefer B
+
-tree indices over
B-treeindicesinpractice.
• Sequential ?le organizations require an index structure to locate data. File
organizations based on hashing, by contrast, allow us to ?nd the address of
a data item directly by computing a function on the search-key value of the
desiredrecord.Sincewedonotknowatdesigntimepreciselywhichsearch-
key values will be stored in the ?le, a good hash function to choose is one
that assigns search-key values to buckets such that the distribution is both
uniform and random.
• Static hashing uses hash functions in which the set of bucket addresses is
?xed. Such hash functions cannot easily accommodate databases that grow
signi?cantlylargerovertime.Thereareseveraldynamichashingtechniquesthat
allow the hash function to be modi?ed. One example is extendable hashing,
whichcopeswithchangesindatabasesizebysplittingandcoalescingbuckets
as thedatabasegrows and shrinks.
ReviewTerms 531
• We can also use hashing to create secondary indices; such indices are called
hash indices. For notational convenience, we assume hash ?le organizations
haveanimplicithashindexonthe searchkeyusedfor hashing.
• Ordered indices such as B
+
-trees and hash indices can be used for selec-
tions based on equality conditions involving single attributes. When multi-
ple attributes are involved in a selection condition, we can intersect record
identi?ersretrievedfrommultipleindices.
• Bitmapindicesprovideaverycompactrepresentationforindexingattributes
with very few distinct values. Intersection operations are extremely fast on
bitmaps,making themidealfor supportingqueriesonmultipleattributes.
Review Terms
• Accesstypes
• Accesstime
• Insertiontime
• Deletiontime
• Spaceoverhead
• Orderedindex
• Clusteringindex
• Primaryindex
• Nonclusteringindex
• Secondaryindex
• Index-sequential?le
• Indexentry/record
• Dense index
• Sparseindex
• Multilevelindex
• Compositekey
• Sequentialscan
• B
+
-treeindex
• Leafnode
• Nonleafnode
• Balancedtree
• Range query
• Nodesplit
• Nodecoalesce
• Nonunique searchkey
• B
+
-tree?le organization
• Bulkload
• Bottom-upB
+
-treeconstruction
• B-treeindex
• Statichashing
• Hash?le organization
• Hashindex
• Bucket
• Hashfunction
• Bucketover?ow
• Skew
• Closedhashing
• Dynamic hashing
• Extendablehashing
• Multiple-keyaccess
• Indicesonmultiplekeys
• Bitmapindex
• Bitmapoperations
?
Intersection
?
Union
?
Complement
?
Existencebitmap
532 Chapter 11 Indexing andHashing
Practice Exercises
11.1 Indices speed query processing, but it is usually a bad idea to create
indices on every attribute, and every combinations of attributes, that is a
potentialsearchkeys.Explainwhy.
11.2 Isitpossibleingeneraltohavetwoclusteringindicesonthesamerelation
for differentsearchkeys?Explainyouranswer.
11.3 Construct aB
+
-treeforthe following setofkeyvalues:
(2,3,5, 7,11, 17,19, 23,29, 31)
Assumethatthetreeisinitiallyemptyandvaluesareaddedinascending
order.ConstructB
+
-treesforthecaseswherethenumber ofpointersthat
will?t inone nodeisas follows:
a. Four
b. Six
c. Eight
11.4 For each B
+
-tree of Practice Exercise 11.3, show the form of the tree after
eachof thefollowing seriesofoperations:
a. Insert9.
b. Insert10.
c. Insert8.
d. Delete23.
e. Delete19.
11.5 Consider the modi?ed redistribution scheme for B
+
-trees described on
page501. What isthe expectedheight of thetreeas a function ofn?
11.6 Suppose that we are using extendable hashing on a ?le that contains
recordswiththefollowing search-keyvalues:
2,3,5, 7,11, 17,19, 23,29, 31
Show the extendable hash structure for this ?le if the hash function is
h(x) = xmod8andbucketscanholdthreerecords.
11.7 ShowhowtheextendablehashstructureofPracticeExercise11.6changes
as theresultofeach ofthe following steps:
a. Delete11.
b. Delete31.
PracticeExercises 533
c. Insert1.
d. Insert15.
11.8 GivepseudocodeforaB
+
-treefunctionfindIterator(),whichislikethe
function find(), except that it returns an iterator object, as described in
Section 11.3.2. Also give pseudocode for the iterator class, including the
variablesinthe iteratorobject, andthenext()method.
11.9 Givepseudocodefordeletionofentriesfromanextendablehashstructure,
including details of when and how to coalesce buckets. Do not bother
about reducingthe sizeofthe bucket addresstable.
11.10 Suggest an ef?cient way to test if the bucket address table in extendable
hashing canbereducedinsize,bystoringanextracount withthebucket
address table. Give details of how the count should be maintained when
buckets are split, coalesced, or deleted. (Note:Reducingthesizeofthe
bucket address table is an expensive operation, and subsequent inserts
may cause the table to grow again. Therefore, it is best not to reduce the
sizeassoonasitispossibletodoso,butinsteaddoitonlyifthenumberof
indexentriesbecomes smallcomparedtothe bucket-address-tablesize.)
11.11 Considertheinstructor relationshown inFigure11.1.
a. Construct a bitmap index on the attribute salary, dividing salary
values into 4 ranges: below 50000, 50000 to below 60000, 60000 to
below 70000, and70000 and above.
b. ConsideraquerythatrequestsallinstructorsintheFinancedepart-
ment with salary of 80000 or more. Outline the steps in answering
thequery,andshowthe?nalandintermediatebitmapsconstructed
toanswer the query.
11.12 WhatwouldtheoccupancyofeachleafnodeofaB
+
-tree be, if index
entriesareinsertedinsortedorder?Explainwhy.
11.13 Supposeyouhavearelationr withn
r
tuplesonwhichasecondaryB
+
-tree
istobeconstructed.
a. GiveaformulaforthecostofbuildingtheB
+
-treeindexbyinserting
one record at a time. Assume each block will hold an average of f
entries,and thatall levelsofthetreeabovetheleafareinmemory.
b. Assuming a random disk access takes 10 milliseconds, what is the
cost of indexconstruction onarelationwith10millionrecords?
c. Write pseudocode for bottom-up construction of a B
+
-tree, which
was outlined in Section 11.4.4. You can assume that a function to
ef?cientlysortalarge?leisavailable.
11.14 WhymighttheleafnodesofaB
+
-tree?leorganizationlosesequentiality?
534 Chapter 11 Indexing andHashing
a. Suggest how the ?le organization may be reorganized to restore
sequentiality.
b. An alternative to reorganization is to allocate leaf pages in units of
n blocks, for some reasonably large n. When the ?rst leaf of a B
+
-
tree is allocated, only one block of an n-block unit is used, and the
remaining pages are free. If a page splits, and its n-block unit has a
free page, that space is used for the new page. If the n-block unit is
full,anothern-blockunitisallocated,andthe?rstn/2leafpagesare
placedinonen-blockunit,andtheremaininginthesecondn-block
unit.For simplicity,assume that therearenodeleteoperations.
i. What is the worst case occupancy of allocated space, assuming
nodeleteoperations,afterthe?rstn-block unit isfull.
ii. Is it possible that leaf nodes allocated to an n-node block unit
are not consecutive, that is, is it possible that two leaf nodes are
allocated to one n-node block, but another leaf node in between
thetwoisallocatedtoadifferentn-node block?
iii. Under the reasonable assumption that buffer space is suf?cient
to store a n-page block, how many seeks would be required for
a leaf-level scan of the B
+
-tree, in the worst case? Compare this
number withthe worst case ifleaf pagesareallocatedablock at
atime.
iv. The technique of redistributing values to siblings to improve
spaceutilizationislikelytobemoreef?cientwhenusedwiththe
aboveallocationscheme forleafblocks. Explainwhy.
Exercises
11.15 When is it preferable to use a dense index rather than a sparse index?
Explainyour answer.
11.16 Whatisthedifferencebetweenaclusteringindexandasecondaryindex?
11.17 For each B
+
-tree of Practice Exercise 11.3, show the steps involved in the
following queries:
a. Findrecordswitha search-keyvalueof 11.
b. Findrecordswithasearch-keyvaluebetween7and 17, inclusive.
11.18 The solution presented in Section 11.3.4 to deal with nonunique search
keys added an extra attribute to the search key. What effect could this
change haveonthe heightoftheB
+
-tree?
11.19 Explain the distinction between closed and open hashing. Discuss the
relativemeritsof eachtechniqueindatabaseapplications.
Exercises 535
11.20 What arethecausesof bucketover?owinahash?le organization? What
can be done to reducethe occurrence of bucket over?ows?
11.21 Why is a hash structure not the best choice for a search key on which
rangequeriesarelikely?
11.22 Suppose there is a relation r(A,B,C), with a B
+
-tree index with search
key(A,B).
a. Whatistheworst-casecostof?ndingrecordssatisfying10 < A< 50
usingthisindex,intermsofthenumber ofrecordsretrievedn
1
and
theheight h ofthe tree?
b. What is the worst-case cost of ?nding records satisfying 10 < A <
50 ?5 < B < 10using this index,intermsofthe number ofrecords
n
2
that satisfythisselection,as wellasn
1
and h de?nedabove?
c. Underwhatconditionsonn
1
andn
2
wouldtheindexbeanef?cient
way of ?nding recordssatisfying10 < A< 50 ?5 < B < 10?
11.23 Suppose you have to create a B
+
-tree index on a large number of names,
wherethemaximumsizeofanamemaybequitelarge(say40characters)
andtheaveragenameisitselflarge(say10characters).Explainhowpre?x
compressioncanbeusedtomaximizetheaveragefanoutofnonleafnodes.
11.24 Suppose a relation is stored in a B
+
-tree ?le organization. Suppose sec-
ondary indices stored record identi?ers that are pointers to records on
disk.
a. What would be the effect on the secondary indices if a node split
happensinthe ?leorganization?
b. What would be the cost of updating all affected records in a sec-
ondary index?
c. How does using the search key of the ?le organization as a logical
recordidenti?ersolvethis problem?
d. Whatistheextracostduetotheuseofsuchlogicalrecordidenti?ers?
11.25 Show how to compute existence bitmaps from other bitmaps. Make sure
thatyourtechniqueworkseveninthepresenceofnullvalues,byusinga
bitmapfor the valuenull.
11.26 Howdoesdataencryptionaffectindexschemes?Inparticular,howmight
itaffect schemesthat attempttostoredatainsortedorder?
11.27 Our descriptionof statichashing assumes that a largecontiguous stretch
of disk blocks can be allocated to a static hash table. Suppose you can
allocate only C contiguous blocks. Suggest how to implement the hash
table,ifitcanbemuchlargerthanC blocks.Accesstoablockshouldstill
beef?cient.
536 Chapter 11 Indexing andHashing
Bibliographical Notes
Discussions of the basic data structures in indexing and hashing can be found
in Cormen et al. [1990]. B-tree indices were ?rst introduced in Bayer [1972] and
Bayer and McCreight [1972]. B
+
-trees are discussed in Comer [1979], Bayer and
Unterauer [1977], and Knuth [1973]. The bibliographical notes in Chapter 15
provide references to research on allowing concurrent accesses and updates on
B
+
-trees. Gray and Reuter [1993] provide a good description of issues in the
implementationofB
+
-trees.
Several alternative tree and treelike search structures have been proposed.
Tries are trees whose structure is based on the “digits” of keys (for example, a
dictionarythumb index,which has oneentryfor eachletter).Suchtreesmay not
be balanced in the sense that B
+
-trees are. Tries are discussed by Ramesh et al.
[1989],Orenstein[1982],Litwin[1981],andFredkin[1960].Relatedworkincludes
the digitalB-treesofLomet[1981].
Knuth[1973]analyzesalargenumberofdifferenthashingtechniques.Several
dynamic hashing schemes exist. Extendable hashing was introduced by Fagin
et al. [1979]. Linear hashing was introduced by Litwin [1978] and Litwin [1980].
A performance comparison with extendable hashing is given by Rathi et al.
[1990]. An alternative given by Ramakrishna and Larson [1989] allows retrieval
in a single disk access at the price of a high overhead for a small fraction of
databasemodi?cations.Partitionedhashingisanextensionofhashingtomultiple
attributes,andiscoveredinRivest[1976],Burkhard[1976],andBurkhard[1979].
Vitter[2001]providesanextensivesurveyofexternal-memorydatastructures
and algorithms.
Bitmapindices,andvariantscalledbit-slicedindicesandprojectionindices,
are described in O’Neil and Quass [1997]. They were ?rst introduced in the IBM
Model204?lemanagerontheAS400platform.Theyprovideverylargespeedups
on certain types of queries, and are today implemented on most database sys-
tems. Research on bitmap indices includes Wu and Buchmann [1998], Chan and
Ioannidis [1998], Chanand Ioannidis[1999], and Johnson [1999].
CHAPTER
12
Query Processing
Queryprocessingreferstotherangeofactivitiesinvolvedinextractingdatafrom
a database. The activities include translation of queries in high-level database
languagesintoexpressionsthatcanbeusedatthephysicallevelofthe?lesystem,
a varietyof query-optimizingtransformations, and actual evaluationof queries.
12.1 Overview
The steps involved in processing a query appear in Figure 12.1. The basic steps
are:
1. Parsing and translation.
2. Optimization.
3. Evaluation.
Before query processing can begin, the system must translate the query into
a usable form. A language such as SQL issuitable for human use, but is ill suited
to be the system’s internal representation of a query. A more useful internal
representationisone basedon the extendedrelationalalgebra.
Thus, the ?rstaction the systemmust takein queryprocessingistotranslate
agivenqueryintoitsinternalform.Thistranslationprocessissimilartothework
performed by the parser of a compiler. In generating the internal form of the
query, the parser checks the syntax of the user’s query, veri?es that the relation
names appearing in the query are names of the relations in the database, and
so on. The system constructs a parse-tree representation of the query, which it
then translates into a relational-algebra expression. If the query was expressed
in terms of a view, the translation phase also replaces all uses of the view by the
relational-algebra expression that de?nes the view.
1
Most compiler texts cover
parsingin detail.
1
For materialized views, the expression de?ning the view has already been evaluated and stored. Therefore, the stored
relationcanbeused,insteadofusesoftheviewbeingreplacedbytheexpressionde?ningtheview.Recursiveviewsare
handled differently, viaa ?xed-pointprocedure,as discussed inSection5.4 and AppendixB.3.6.
537
538 Chapter 12 Query Processing
query
output
query
parser and
translator
evaluation engine
relational-algebra
expression
execution plan
optimizer
data statistics
about data
Figure12.1 Steps in query processing.
Given a query, there are generally a variety of methods for computing the
answer. For example, we have seen that, in SQL, a query could be expressed in
several different ways. Each SQL query can itself be translated into a relational-
algebra expression in one of several ways. Furthermore, the relational-algebra
representationofaqueryspeci?esonlypartiallyhowtoevaluateaquery;thereare
usuallyseveralwaystoevaluaterelational-algebraexpressions.Asanillustration,
consider the query:
select salary
from instructor
where salary < 75000;
This query can be translated into either of the following relational-algebra ex-
pressions:
•   salary <75000
(null
salary
(instructor))
• null
salary
(  salary<75000
(instructor))
Further, we can execute each relational-algebra operation by one of several
different algorithms. For example, to implement the preceding selection, we can
search every tuple in instructor to ?nd tuples with salary less than 75000. If a
B
+
-tree index is available on the attribute salary, we can use the index instead to
locate the tuples.
To specify fully how to evaluate a query, we need not only to provide the
relational-algebraexpression, but also to annotate it with instructions specifying
how to evaluate each operation. Annotations may state the algorithm to be used
12.1 Overview 539
salary
salary < 75000; use index 1
instructor
?
?
Figure12.2 A query-evaluation plan.
foraspeci?coperation,ortheparticul ar index or indices to use. A relational-
algebra operation annotated with instructions on how to evaluate it is called an
evaluation primitive. A sequence of primitive operations that can be used to
evaluateaqueryisaquery-executionplanorquery-evaluationplan.Figure12.2
illustrates an evaluation plan for our example query, in which a particular index
(denoted in the ?gure as “index 1”) is speci?ed for the selection operation. The
query-execution engine takes a query-evaluation plan, executes that plan, and
returns the answers to the query.
The different evaluation plans for a given query can have differentcosts. We
donotexpectuserstowritetheirqueriesinawaythatsuggeststhemostef?cient
evaluationplan.Rather,itistheresponsibilityofthesystemtoconstructaquery-
evaluation plan that minimizes the cost of query evaluation; this task is called
query optimization. Chapter13 describesqueryoptimizationin detail.
Oncethequeryplanischosen,thequeryisevaluatedwiththatplan,andthe
resultof the queryis output.
The sequence of steps already described for processing a query is represen-
tative; not all databases exactly follow those steps. For instance, instead of using
the relational-algebra representation, several databases use an annotated parse-
tree representation based on the structure of the given SQL query. However, the
concepts that we describehere formthe basis ofqueryprocessingin databases.
In order to optimize a query, a query optimizer must know the cost of each
operation.Althoughthe exactcostishardtocompute,since itdependsonmany
parameterssuchasactual memoryavailabletotheoperation,itispossibletoget
a rough estimateof executioncost for each operation.
In this chapter we study how to evaluate individual operations in a query
plan, and how to estimate their cost; we return to query optimization in Chap-
ter 13. Section 12.2 outlines how we measure the cost of a query. Sections 12.3
through 12.6 cover the evaluation of individual relational-algebra operations.
Severaloperationsmaybegroupedtogetherintoapipeline,inwhicheachofthe
operations starts working on its input tuples even as they are being generated
by another operation. In Section 12.7, we examine how to coordinate the execu-
tion of multiple operations in a query evaluation plan, in particular, how to use
pipelinedoperations toavoid writing intermediateresultsto disk.
540 Chapter 12 Query Processing
12.2 MeasuresofQueryCost
There are multiple possible evaluation plans for a query, and it is important to
be able to compare the alternativesin terms of their (estimated)cost, and choose
the best plan. To do so, we must estimate the cost of individual operations, and
combine them to get the cost of a query evaluation plan. Thus, as we study
evaluation algorithms for each operation later in this chapter, we also outline
how toestimatethe cost of the operation.
The cost of query evaluation can be measured in terms of a number of dif-
ferent resources, including disk accesses, CPU time to execute a query, and, in
a distributed or parallel database system, the cost of communication (which we
discusslater, in Chapters18 and 19).
Inlargedatabasesystems,thecosttoaccessdatafromdiskisusuallythemost
important cost, since disk accesses are slow compared to in-memory operations.
Moreover, CPU speeds have been improving much faster than have disk speeds.
Thus, it is likely that the time spent in disk activity will continue to dominate
the total time to execute a query. The CPU time taken for a task is harder to
estimate since it depends on low-level details of the execution code. Although
real-life query optimizers do take CPU costs into account, for simplicity in this
book we ignore CPU costsand use only disk-accesscoststo measurethe cost of a
query-evaluationplan.
We use the number of block transfers from disk and the number of disk seeks
to estimate the cost of a query-evaluation plan. If the disk subsystem takes an
averageof t
T
secondstotransferablockofdata,andhasanaverageblock-access
time(diskseektimeplusrotational latency)of t
S
seconds,thenanoperationthat
transfers b blocks and performs S seeks would take b ? t
T
+ S ? t
S
seconds. The
valuesof t
T
and t
S
mustbecalibratedforthedisksystemused,buttypicalvalues
forhigh-enddiskstodaywouldbe t
S
= 4millisecondsand t
T
= 0.1milliseconds,
assuminga4-kilobyteblocksizeandatransferrateof40megabytespersecond.
2
We can re?ne our cost estimates further by distinguishing block reads from
block writes, since block writes are typically about twice as expensive as reads
(thisisbecausedisksystemsreadsectorsbackaftertheyarewrittentoverifythat
thewritewassuccessful).Forsimplicity,weignorethisdetail,andleaveittoyou
to work out moreprecise cost estimatesforvariousoperations.
Thecostestimateswegivedonotincludethecostofwritingthe?nalresultof
anoperationbacktodisk.Thesearetakenintoaccountseparatelywhererequired.
The costs of all the algorithms that we consider depend on the size of the buffer
in main memory. In the best case, all data can be read into the buffers, and the
disk does not need to be accessed again. In the worst case, we assume that the
buffercanholdonlyafewblocksofdata—approximatelyoneblockperrelation.
When presentingcost estimates,we generallyassume the worst case.
2
Somedatabase systemsperformtestseeksandblocktransferstoestimateaverageseekandblocktransfer costs,aspart
of the software installation process.
12.3 Selection Operation 541
Inaddition,althoughweassumethatdatamustbereadfromdiskinitially,it
ispossiblethatablockthatisaccessedisalreadypresentinthein-memorybuffer.
Again, for simplicity, we ignore this effect; as a result, the actual disk-access cost
duringthe executionof a planmay be lessthan the estimatedcost.
The response time for a query-evaluation plan (that is, the wall-clock time
required to execute the plan), assuming no other activity is going on in the
computer, would account for all these costs, and could be used as a measure
of the cost of the plan. Unfortunately, the response time of a plan is very hard to
estimatewithout actually executingthe plan, for the following reasons:
1. The response time depends on the contents of the buffer when the query
begins execution; this information is not available when the query is opti-
mized,and ishard toaccount forevenifitwere available.
2. Inasystemwithmultipledisks,theresponsetimedependsonhowaccesses
are distributed among disks, which is hard to estimate without detailed
knowledge of datalayout on disk.
Interestingly, a plan may get a better response time at the cost of extra resource
consumption. For example,if a systemhas multipledisks, a plan Athat requires
extra disk reads, but performs the reads in parallel across multiple disks may
?nish faster than another plan B that has fewer disk reads, but from only one
disk. However, if many instances of a query using plan Arun concurrently, the
overallresponsetimemayactuallybemorethanifthesameinstancesareexecuted
using plan B,sinceplanAgeneratesmore load on the disks.
As a result, instead of trying to minimize the response time, optimizers gen-
erallytrytominimizethetotalresourceconsumptionofaqueryplan.Ourmodel
of estimating the total disk access time (including seek and data transfer) is an
exampleof such aresource consumption–based model of querycost.
12.3 Selection Operation
In query processing, the ?le scan is the lowest-level operator to access data. File
scansaresearchalgorithmsthat locate andretrieverecordsthatful?ll aselection
condition. In relational systems, a ?le scan allows an entire relation to be read in
those caseswhere the relationisstoredin a single,dedicated?le.
12.3.1 SelectionsUsingFileScansandIndices
Consider a selection operation on a relation whose tuples are stored together in
one ?le. The most straightforward way of performinga selectionis asfollows:
• A1 (linear search). In a linear search, the system scans each ?le block and
testsallrecordstoseewhethertheysatisfytheselectioncondition.Aninitial
seek is required to access the ?rst block of the ?le. In case blocks of the ?le
542 Chapter 12 Query Processing
are not stored contiguously, extra seeks may be required, but we ignore this
effectforsimplicity.
Although it may be slower than other algorithms for implementing selec-
tion, the linear-search algorithm can be applied to any ?le, regardless of the
orderingofthe?le,ortheavailabilityofindices,orthenatureoftheselection
operation. The other algorithms that we shall study are not applicable in all
cases, but when applicable theyaregenerallyfasterthan linearsearch.
Cost estimates for linear scan, as well as for other selection algorithms, are
shown in Figure 12.3. In the ?gure, we use h
i
to represent the height of the B
+
-
tree.Real-lifeoptimizersusuallyassumethatthe rootofthe treeispresentinthe
in-memory buffer since it is frequently accessed. Some optimizers even assume
that allbut the leaflevelof the treeis presentin memory, since they are accessed
relativelyfrequently,andusuallylessthan1percentofthenodesofaB
+
-tree are
nonleaf nodes. The cost formulae can be modi?edappropriately.
Index structures are referred to as access paths, since they provide a path
through which data can be located and accessed. In Chapter 11, we pointed out
thatitisef?cienttoreadtherecordsofa?leinanordercorrespondingcloselyto
physical order.Recallthat a primary index (alsoreferredto asa clustering index)is
an index that allows the records of a ?le to be read in an order that corresponds
to the physical order in the ?le. An index that is not a primary index is called a
secondary index.
Searchalgorithmsthatuseanindexarereferredtoasindexscans.Weusethe
selection predicate to guide us in the choice of the index to use in processing the
query.Searchalgorithms that use an indexare:
• A2 (primary index, equality on key).Foranequalitycomparisononakey
attributewithaprimaryindex,wecanusetheindextoretrieveasinglerecord
thatsatis?esthecorrespondingequalitycondition.Costestimatesareshown
in Figure12.3.
• A3 (primary index, equality on nonkey). We can retrieve multiple records
by using a primary index when the selection condition speci?es an equality
comparison on a nonkey attribute, A. The only difference from the previous
case is that multiple records may need to be fetched. However, the records
must be stored consecutively in the ?le since the ?le is sorted on the search
key.Costestimatesare shown in Figure 12.3.
• A4 (secondary index, equality). Selections specifying an equality condition
can use a secondary index. This strategy can retrieve a single record if the
equalityconditionisonakey;multiplerecordsmayberetrievediftheindex-
ing ?eldisnot a key.
In the ?rst case, only one record is retrieved. The time cost in this case is
thesameasthatforaprimaryindex(caseA2).
Inthesecondcase,eachrecordmayberesidentonadifferentblock,which
mayresultinone I/Ooperationperretrievedrecord,witheach I/Ooperation
requiring a seek and a block transfer. The worst-case time cost in this case is
12.3 Selection Operation 543
Algorithm Cost Reason
A1 LinearSearch t
S
+ b
r
? t
T
One initial seek plus b
r
block transfers,
where b
r
denotes the number of blocks
inthe ?le.
A1 LinearSearch,
Equalityon
Key
Average
case t
S
+
(b
r
/2) ? t
T
Since at most one record satis?es con-
dition, scan can be terminated as soon
as the required record is found. In the
worst case, b
r
blocks transfers are still
required.
A2 Primary
B
+
-treeIndex,
Equalityon
Key
(h
i
+ 1) ?
(t
T
+t
S
)
(Where h
i
denotes the height of the in-
dex.)Indexlookuptraversestheheight
ofthetr eeplusoneI/O to fetch the
record; each of these I/O operations re-
quiresa seekand a block transfer.
A3 Primary
B
+
-treeIndex,
Equalityon
Nonkey
h
i
? (t
T
+
t
S
) + b ? t
T
One seek for each level of the tree, one
seek for the ?rst block. Here b is the
number of blocks containing records
with the speci?ed search key, all of
which are read. These blocks are leaf
blocks assumed to be stored sequen-
tially (since it is a primary index) and
don’t requireadditionalseeks.
A4 Secondary
B
+
-treeIndex,
Equalityon
Key
(h
i
+ 1) ?
(t
T
+t
S
)
Thiscase issimilartoprimaryindex.
A4 Secondary
B
+
-treeIndex,
Equalityon
Nonkey
(h
i
+ n) ?
(t
T
+t
S
)
(Where n is the number of records
fetched.) Here, cost of index traversal
isthesameasforA3,buteachrecord
maybeonadifferentblock,requiringa
seekperrecord.Costispotentiallyvery
high if nislarge.
A5 Primary
B
+
-treeIndex,
Comparison
h
i
? (t
T
+
t
S
) + b ? t
T
Identical to the case of A3, equality on
nonkey.
A6 Secondary
B
+
-treeIndex,
Comparison
(h
i
+ n) ?
(t
T
+t
S
)
Identical to the case of A4, equality on
nonkey.
Figure12.3 Cost estimates for selection algorithms.
(h
i
+ n) ? (t
S
+ t
T
), where n is the number of records fetched, if each record
is in a different disk block, and the block fetches are randomly ordered. The
worst-case costcouldbecomeevenworsethan thatoflinearsearchifalarge
number of recordsareretrieved.
544 Chapter 12 Query Processing
If the in-memory buffer is large, the block containing the record may
already be in the buffer. It is possible to construct an estimate of the average
or expected cost of the selection by taking into account the probability of the
blockcontainingtherecordalreadybeinginthebuffer.Forlargebuffers,that
estimatewill be much lessthan the worst-case estimate.
In certainalgorithms, includingA2, the use of a B
+
-tree?le organization can
save one access since recordsarestoredat theleaf-levelof thetree.
AsdescribedinSection11.4.2,whenrecordsarestoredinaB
+
-tree?leorgani-
zationorother?leorganizationsthatmayrequirerelocationofrecords,secondary
indices usually do not store pointers to the records.
3
Instead, secondary indices
storethevaluesoftheattributesusedasthesearchkeyinaB
+
-tree?leorganiza-
tion.Accessingarecordthroughsuchasecondaryindexisthenmoreexpensive:
First the secondary index is searched to ?nd the primary index search-key val-
ues, then the primary index is looked up to ?nd the records. The cost formulae
describedforsecondaryindiceshavetobemodi?edappropriatelyifsuchindices
areused.
12.3.2 Selections InvolvingComparisons
Consider a selection of the form   A?v
(r). We can implement the selection either
by using linear search or by using indices in one ofthe following ways:
• A5 (primary index, comparison). A primary ordered index (for example, a
primary B
+
-tree index) can be used when the selection condition is a com-
parison. For comparison conditions of the form A>vor A ? v,aprimary
indexon Acanbeusedtodirecttheretrievaloftuples,asfollows:For A ? v,
we look up the value v in the index to ?nd the ?rst tuple in the ?le that has
avalueofA = v. A ?le scan starting from that tuple up to the end of the ?le
returnsalltuplesthatsatisfythecondition.For A>v,the?lescanstartswith
the ?rst tuple such that A>v . The cost estimate for this case is identical to
that for case A3.
For comparisons of the form A<vor A ? v, an index lookup is not
required. For A<v, we use a simple ?le scan starting from the beginning of
the?le,andcontinuingupto(butnotincluding)the?rsttuplewithattribute
A = v.ThecaseofA ? v is similar, except that the scan continues up to (but
not including) the ?rst tuple with attribute A>v. In either case, the index is
not useful.
• A6 (secondary index, comparison). We can use a secondary ordered index
to guide retrieval for comparison conditions involving <, ?, ?,or>.The
lowest-levelindexblocks arescanned,eitherfromthesmallestvalueupto v
(for < and ?), or from v up tothe maximumvalue (for > and ?).
3
Recall that if B
+
-tree ?le organizations are used to store relations, records may be moved between blocks when leaf
nodes are split or merged, and when records are redistributed.
12.3 Selection Operation 545
Thesecondaryindexprovidespointerstotherecords,buttogettheactual
records we have to fetch the records by using the pointers. This step may
require an I/O operation for each record fetched, since consecutive records
maybeondifferentdiskblocks;asbefore,each I/Ooperationrequiresadisk
seek and a block transfer. If the number of retrieved records is large, using
the secondary index may be even more expensive than using linear search.
Therefore the secondary index should be used only if very few records are
selected.
12.3.3 Implementation ofComplex Selections
So far, we have considered only simple selection conditions of the form AopB,
whereopisanequalityorcomparisonoperation.Wenowconsidermorecomplex
selectionpredicates.
• Conjunction:A conjunctive selection isa selectionofthe form:
    1
?  2
?···?  n
(r)
• Disjunction:A disjunctive selection isa selectionof the form:
    1
?  2
?···?  n
(r)
A disjunctive condition is satis?ed by the union of all records satisfying the
individual,simpleconditions  i
.
• Negation: The result of a selection   ¬  (r)isthesetoftuplesofr for which
the condition   evaluates to false. In the absence of nulls, this set is simply
the setof tuplesin r that are not in    (r).
We can implement a selection operation involving either a conjunction or a
disjunction of simpleconditions by using one of the following algorithms:
• A7 (conjunctive selection using one index). We ?rst determine whether an
access path is available for an attribute in one of the simple conditions. If
one is, one of the selection algorithms A2 through A6 can retrieve records
satisfyingthatcondition.Wecompletetheoperationbytesting,inthememory
buffer, whether or not each retrieved record satis?es the remaining simple
conditions.
Toreducethecost,wechoosea  i
andoneofalgorithmsA1throughA6for
whichthecombinationresultsintheleastcostfor    i
(r).Thecostofalgorithm
A7isgivenby the costof the chosen algorithm.
• A8(conjunctiveselectionusingcompositeindex).Anappropriatecomposite
index (that is, an index on multiple attributes) may be available for some
conjunctive selections. If the selection speci?esan equalitycondition on two
ormoreattributes,andacompositeindexexistsonthesecombinedattribute
546 Chapter 12 Query Processing
?elds, then the index can be searched directly. The type of index determines
which of algorithmsA2, A3, or A4will be used.
• A9 (conjunctive selection by intersection of identi?ers). Another alterna-
tive for implementing conjunctive selection operations involves the use of
record pointers or record identi?ers. This algorithm requires indices with
record pointers, on the ?elds involved in the individual conditions. The al-
gorithm scans each index for pointers to tuples that satisfy an individual
condition. The intersection of all the retrieved pointers is the set of pointers
to tuples that satisfy the conjunctive condition. The algorithm then uses the
pointers to retrieve the actual records. If indices are not available on all the
individual conditions, then the algorithm tests the retrieved records against
the remaining conditions.
ThecostofalgorithmA9isthesumofthecostsoftheindividualindexscans,
plusthecostofretrievingtherecordsintheintersectionoftheretrievedlistsof
pointers.Thiscostcanbereducedbysortingthelistofpointersandretrieving
recordsinthesortedorder.Thereby,(1)allpointerstorecordsinablockcome
together,henceallselectedrecordsintheblockcanberetrievedusingasingle
I/O operation, and (2) blocks are read in sorted order, minimizing disk-arm
movement.Section12.4 describessorting algorithms.
• A10(disjunctiveselectionbyunionofidenti?ers).Ifaccesspathsareavail-
able on all the conditions of a disjunctive selection, each index is scanned
for pointers to tuples that satisfy the individual condition. The union of all
the retrieved pointers yields the set of pointers to all tuples that satisfy the
disjunctivecondition.Wethenusethepointerstoretrievetheactualrecords.
However, if even one of the conditions does not have an access path, we
have to perform a linear scan of the relation to ?nd tuples that satisfy the
condition. Therefore, if there is even one such condition in the disjunct, the
most ef?cient access method is a linear scan, with the disjunctive condition
testedon each tuple duringthe scan.
Theimplementationofselectionswithnegationconditionsislefttoyouasan
exercise(Practice Exercise12.6).
12.4 Sorting
Sortingofdataplaysanimportantroleindatabasesystemsfortworeasons.First,
SQLqueriescanspecifythattheoutputbesorted.Second,andequallyimportant
for query processing, several of the relational operations, such as joins, can be
implemented ef?ciently if the input relations are ?rst sorted. Thus, we discuss
sorting here before discussingthe join operationin Section12.5.
We can sort a relation by building an index on the sort key, and then using
that index to read the relation in sorted order. However, such a process orders
the relation only logically, through an index, rather than physically.Hence,the
reading of tuples in the sorted order may lead to a disk access (disk seek plus
12.4 Sorting 547
blocktransfer)foreachrecord,whichcanbeveryexpensive,sincethenumberof
recordscan be much larger than the number of blocks. For this reason, it may be
desirableto orderthe recordsphysically.
The problem of sorting has been studied extensively, both for relations that
?tentirelyinmainmemoryandforrelationsthatarebiggerthanmemory.Inthe
?rst case, standard sorting techniques such as quick-sort can be used. Here, we
discuss how tohandle the second case.
12.4.1 ExternalSort-MergeAlgorithm
Sorting of relations that do not ?t in memory is called external sorting.The
most commonly used technique for external sorting is the external sort–merge
algorithm.Wedescribetheexternalsort–mergealgorithmnext.Let Mdenotethe
number of blocks in the main-memory buffer available for sorting, that is, the
numberofdiskblockswhosecontentscanbebufferedinavailablemainmemory.
1. In the ?rst stage, a number of sorted runs are created; each run is sorted,
but contains only some of the records of the relation.
i=0;
repeat
read Mblocks ofthe relation,or the rest of the relation,
whichever issmaller;
sort the in-memory part of the relation;
writethe sorteddata torun ?le R
i
;
i = i+1;
untilthe end of the relation
2. In the second stage, the runs are merged. Suppose, for now, that the total
number of runs, N,islessthanM, so that we can allocate one block to
each run and have space left to hold one block of output. The merge stage
operatesasfollows:
read one block of each of the N ?les R
i
into a buffer block in memory;
repeat
choose the ?rst tuple(in sort order)among all buffer blocks;
write the tupleto the output, and deleteit from the buffer block;
if the buffer block of any run R
i
is emptyand notend-of-?le(R
i
)
then read the next block of R
i
into the buffer block;
untilall input buffer blocks are empty
Theoutputofthemergestageisthesortedrelation.Theoutput?leisbuffered
to reduce the number of disk write operations. The preceding merge operation
is a generalization of the two-way merge used by the standard in-memory sort–
mergealgorithm; itmerges N runs, soit iscalled an N-way merge.
548 Chapter 12 Query Processing
g
a   
d   31
c    33
b   14
e   16
r   16
d   21
m    3
p     2
d     7
a   14
a    14
a    19
b    14
c    33
d     7
d    21
d    31
e    16
g    24
m    3
p     2
r    16
a    19
b    14
c    33
d    31
e    16
g    24
a    14
d     7
d    21
m    3
p     2
r    16
a   19
d   31
g   24
b   14
c   33
e   16
d   21
m    3
r    16
a    14
d     7
p     2
initial
relation
create
runs
merge
pass–1
merge
pass–2
runs runs
sorted
output
24
19
Figure12.4 External sorting using sort–merge.
In general, if the relation is much larger than memory, there may be M or
more runs generated in the ?rst stage, and it is not possible to allocate a block
for each run during the merge stage. In this case, the merge operation proceeds
in multiple passes. Since there is enough memory for M ?1 input buffer blocks,
each merge can take M ?1 runs as input.
The initial pass functions in this way: It merges the ?rst M ?1runs(asdesc-
ribed in item 2 above) to get a single run for the next pass. Then, it merges the
next M ? 1 runs similarly, and so on, until it has processed all the initial runs.
At this point, the number of runs has been reduced by a factor of M ? 1. If this
reducednumber ofruns isstillgreaterthan orequal to M,anotherpassismade,
with the runs createdby the ?rst pass as input. Each pass reducesthe number of
runs by a factor of M ?1. The passes repeat as many times as required,until the
number of runs is lessthan M;a ?nal passthen generatesthesortedoutput.
Figure 12.4 illustrates the steps of the external sort–merge for an example
relation. For illustration purposes, we assume that only one tuple ?ts in a block
(f
r
= 1), and we assume that memory holds at most three blocks. During the
mergestage, two blocks are used for input and one for output.
12.4.2 CostAnalysisofExternalSort-Merge
We compute the disk-access cost for the external sort–merge in this way: Let
b
r
denote the number of blocks containing records of relation r. The ?rst stage
12.5 Join Operation 549
reads every block of the relation and writes them out again, giving a total of 2b
r
block transfers. The initial number of runs is null b
r
/Mnull .Sincethenumberofruns
decreases by a factor of M ? 1 in each merge pass, the total number of merge
passes required is null log
M?1
(b
r
/M)null . Each of these passes reads every block of the
relationonceand writesitout once,withtwoexceptions.First,the?nal passcan
produce the sorted output without writing its result to disk. Second, there may
be runs that are not read in or written out during a pass—for example, if there
are M runs to be merged in a pass, M ? 1 are read in and merged, and one run
isnotaccessedduringthepass.Ignoringthe(relativelysmall)savingsduetothe
lattereffect,thetotalnumberofblocktransfersforexternalsortingoftherelation
is:
b
r
(2null log
M?1
(b
r
/M)null 1)
ApplyingthisequationtotheexampleinFigure12.4,wegetatotalof12 ?(4 +1)
= 60 block transfers, as you can verify from the ?gure. Note that the above
numbers donot include the cost of writing out the ?nal result.
We also need to add the disk-seek costs. Run generation requires seeks for
readingdataforeachoftherunsaswellasforwritingtheruns.Duringthemerge
phase, if data are read b
b
blocks at a time from each run (that is, b
b
buffer blocks
are allocated to each run), then each merge pass would require around null b
r
/b
b
null
seeksforreadingdata.
4
Althoughtheoutputiswrittensequentially,ifitisonthe
same disk as the input runs the head may have moved away between writes of
consecutiveblocks.Thuswewouldhavetoaddatotalof2null b
r
/b
b
null seeksforeach
merge pass, except the ?nal pass (since we assume the ?nal result is not written
back todisk).The total number of seeksis then:
2null b
r
/Mnullnull b
r
/b
b
null (2null log
M?1
(b
r
/M)null 1)
Applying this equation to the example in Figure 12.4, we get a total of 8 + 12 ?
(2 ?2 ?1) = 44 diskseeksif wesetthenumber of buffer blocks perrun, b
b
to 1.
12.5 JoinOperation
In this section, we study several algorithms for computing the join of relations,
and we analyze theirrespectivecosts.
We use the term equi-join to refer to a join of the form r   r.A=s.B
s,whereA
and B are attributesor setsof attributesof relations r and s,respectively.
We use as a running examplethe expression:
student   takes
4
Tobemoreprecise,sincewereadeachrunseparatelyandmaygetfewerthan b
b
blockswhenreadingtheendofarun,
we mayrequire an extra seekfor eachrun. Weignorethis detail for simplicity.
550 Chapter 12 Query Processing
using the same relation schemas that we used in Chapter 2. We assume the
following information about the two relations:
• Number of records of student: n
student
= 5,000.
• Number of blocks of student: b
student
= 100.
• Number of records of takes: n
takes
= 10,000.
• Number of blocks of takes: b
takes
= 400.
12.5.1 Nested-Loop Join
Figure 12.5 shows a simple algorithm to compute the theta join, r     s,oftwo
relations r and s. This algorithm is called the nested-loop join algorithm, since
it basically consists of a pair of nested for loops. Relation r is called the outer
relation and relation s the innerrelation of the join, since the loop for r encloses
the loop for s. The algorithm uses the notation t
r
· t
s
,wheret
r
and t
s
are tuples;
t
r
·t
s
denotesthetupleconstructedbyconcatenatingtheattributevaluesoftuples
t
r
and t
s
.
Likethelinear?le-scanalgorithmforselection,thenested-loopjoinalgorithm
requires no indices, and it can be used regardless of what the join condition is.
Extendingthealgorithmtocomputethenaturaljoinisstraightforward,sincethe
natural join can be expressed as a theta join followed by elimination of repeated
attributes by a projection. The only change required is an extra step of deleting
repeatedattributesfrom the tuple t
r
· t
s
, before addingitto the result.
The nested-loop join algorithm is expensive, since it examines every pair of
tuples in the two relations. Consider the cost of the nested-loop join algorithm.
The number of pairs of tuples to be considered is n
r
? n
s
,wheren
r
denotes the
numberoftuplesinr,andn
s
denotesthenumberoftuplesins.Foreachrecordin
r,wehavetoperformacompletescanons.Intheworstcase,thebuffercanhold
only one block of each relation, and a total of n
r
? b
s
+ b
r
block transfers would
be required, where b
r
and b
s
denote the number of blocks containing tuples of
r and s, respectively. We need only one seek for each scan on the inner relation
s since it is read sequentially, and a total of b
r
seeks to read r,leadingtoatotal
of n
r
+ b
r
seeks. In the best case, there is enough space for both relations to ?t
simultaneouslyinmemory,soeachblockwouldhavetobereadonlyonce;hence,
only b
r
+b
s
block transfers would be required,along with 2seeks.
for each tuple t
r
in rdobegin
for each tuple t
s
in s dobegin
testpair(t
r
,t
s
) tosee ifthey satisfythe join condition  ifthey do,add t
r
· t
s
to the result;
end
end
Figure12.5 Nested-loop join.
12.5 Join Operation 551
for each block B
r
of r dobegin
for each block B
s
of s dobegin
for each tuple t
r
in B
r
dobegin
for each tuple t
s
in B
s
dobegin
testpair(t
r
,t
s
) to seeif theysatisfythe joincondition
if theydo,add t
r
· t
s
tothe result;
end
end
end
end
Figure12.6 Block nested-loop join.
If one of the relations ?ts entirely in main memory, it is bene?cial to use that
relation as the inner relation, since the inner relation would then be read only
once. Therefore, if s is small enough to ?t in main memory, our strategy requires
onlyatotal b
r
+b
s
blocktransfersand2seeks—thesamecostasthatforthecase
where both relations?t inmemory.
Now consider the natural join of student and takes. Assume for now that we
havenoindiceswhatsoeveroneitherrelation,andthatwearenotwillingtocreate
anyindex.Wecanusethenestedloopstocomputethejoin;assumethat studentis
theouterrelationandtakesistheinnerrelationinthejoin.Wewillhavetoexamine
5000 ?10,000 = 50 ?10
6
pairs of tuples. In the worst case, the number of block
transfersis5000?400+100 =2,000,100,plus5000+100 = 5100seeks.Inthebest-
case scenario, however, we can read both relations only once, and perform the
computation.Thiscomputationrequiresatmost100 +400 = 500blocktransfers,
plus 2 seeks—a signi?cant improvement over the worst-case scenario. If we
had used takes as the relation for the outer loop and student for the inner loop,
the worst-case cost of our ?nal strategy would have been 10,000 ? 100 + 400 =
1,000,400blocktransfers,plus10,400diskseeks.Thenumberofblocktransfersis
signi?cantly less, and although the number of seeks is higher, the overall cost is
reduced,assuming t
S
= 4millisecondsand t
T
= 0.1 milliseconds.
12.5.2 BlockNested-Loop Join
If the buffer is too small to hold either relation entirely in memory, we can still
obtain a major saving in block accesses if we processthe relationson a per-block
basis,ratherthanonaper-tuplebasis.Figure12.6showsblocknested-loopjoin,
which is a variant of the nested-loopjoin where everyblock of the inner relation
ispairedwitheveryblock ofthe outerrelation. Within each pairofblocks, every
tuple in one block is paired with every tuple in the other block, to generate all
pairs of tuples. As before, all pairs of tuples that satisfy the join condition are
addedtothe result.
The primary difference in cost between the block nested-loop join and the
basic nested-loop join is that, in the worst case, each block in the inner relation s
isreadonlyonceforeach blockintheouterrelation,insteadofonceforeach tuple
552 Chapter 12 Query Processing
in the outer relation. Thus, in the worst case, there will be a total of b
r
? b
s
+ b
r
block transfers, where b
r
and b
s
denote the number of blocks containing records
of r and s, respectively.Eachscan oftheinnerrelationrequiresone seek,andthe
scan of the outer relation requires one seek per block, leading to a total of 2 ? b
r
seeks. Clearly, it is more ef?cient to use the smaller relation as the outer relation,
in case neither of the relations ?ts in memory. In the best case, where the inner
relation ?ts in memory, there will be b
r
+ b
s
block transfers and just 2 seeks (we
would choose the smaller relationas the inner relationinthis case).
Now return to our example of computing student   takes,usingtheblock
nested-loopjoinalgorithm. Inthe worstcase, we have toreadeachblock of takes
onceforeachblockof student.Thus,intheworstcase,atotalof100 ?400 +100 =
40,100blocktransfersplus2?100 =200seeksarerequired.Thiscostisasigni?cant
improvementoverthe5000?400+100 =2,000,100blocktransfersplus5100seeks
neededintheworstcaseforthebasicnested-loopjoin.Thebest-casecostremains
the same—namely, 100 +400 = 500 block transfersand 2 seeks.
Theperformanceofthenested-loopandblocknested-loopprocedurescanbe
furtherimproved:
• If the join attributes in a natural join or an equi-join form a key on the inner
relation, then for each outer relation tuple the inner loop can terminate as
soon as the ?rst match isfound.
• Intheblocknested-loopalgorithm,insteadofusingdiskblocksastheblock-
ing unit for the outer relation, we can use the biggest size that can ?t in
memory,whileleavingenoughspaceforthebuffersoftheinnerrelationand
theoutput.Inotherwords,ifmemoryhas Mblocks,wereadin M?2blocks
of the outer relation at a time, and when we read each block of the inner
relationwejoinitwithallthe M ?2blocksoftheouterrelation.Thischange
reduces the number of scans of the inner relation from b
r
to null b
r
/(M ? 2)null ,
where b
r
is the number of blocks of the outer relation. The total cost is then
null b
r
/(M ?2)null b
s
+ b
r
block transfers and 2null b
r
/(M ?2)null seeks.
• Wecanscantheinnerloopalternatelyforwardandbackward.Thisscanning
method orders the requests for disk blocks so that the data remaining in the
buffer from the previous scan can be reused, thus reducing the number of
diskaccessesneeded.
• If an index is available on the inner loop’s join attribute, we can replace
?le scans with more ef?cient index lookups. Section 12.5.3 describes this
optimization.
12.5.3 IndexedNested-Loop Join
In a nested-loop join (Figure 12.5), if an index is available on the inner loop’s
join attribute, index lookups can replace ?le scans. For each tuple t
r
in the outer
relationr,theindexisusedtolookuptuplesinsthatwillsatisfythejoincondition
with tuple t
r
.
12.5 Join Operation 553
This join method is called an indexed nested-loop join;itcanbeusedwith
existingindices,aswellaswithtemporaryindicescreatedforthesolepurposeof
evaluatingthe join.
Lookingup tuplesin s thatwill satisfythe join conditions with agiventuple
t
r
is essentially a selection on s. For example, consider student   takes. Suppose
thatwehavea studenttuplewith ID“00128”.Then,therelevanttuplesin takesare
those that satisfy the selection “ID = 00128”.
Thecostofanindexednested-loopjoincanbecomputedasfollows:Foreach
tuple in the outer relation r, a lookup is performed on the index for s,andthe
relevant tuples are retrieved. In the worst case, there is space in the buffer for
onlyoneblockofr andoneblockoftheindex.Then,b
r
I/Ooperationsareneeded
to read relation r,whereb
r
denotes the number of blocks containing records of
r;eachI/O requires a seek and a block transfer, since the disk head may have
movedin betweeneach I/O.Foreachtupleinr, we perform an index lookup on
s. Then, the time cost of the join can be computed as b
r
(t
T
+ t
S
) + n
r
? c,where
n
r
is the number of records in relation r,andc is the cost of a single selection on
s using the join condition. We have seen in Section 12.3 how to estimate the cost
ofasingleselectionalgorithm(possiblyusingindices);thatestimategivesusthe
value of c.
The cost formula indicates that, if indices are available on both relations r
and s, it is generally most ef?cient to use the one with fewer tuples as the outer
relation.
For example, consider an indexed nested-loop join of student   takes,with
studentastheouterrelation.SupposealsothattakeshasaprimaryB
+
-treeindexon
thejoinattributeID,whichcontains20entriesonaverageineachindexnode.Since
takes has 10,000 tuples, the height of the tree is 4, and one more access is needed
to?ndtheactualdata.Sincen
student
is5000,thetotalcostis100+5000?5 =25,100
diskaccesses,eachofwhichrequiresaseekandablocktransfer.Incontrast,aswe
sawbefore,40,100blocktransfersplus200seekswereneededforablocknested-
loopjoin.Althoughthenumberofblocktransfershasbeenreduced,theseekcost
has actually increased,increasing the total cost since a seekisconsiderablymore
expensive than a block transfer. However, if we had a selection on the student
relation that reduces the number of rows signi?cantly, indexed nested-loop join
could be signi?cantly fasterthan block nested-loop join.
12.5.4 MergeJoin
Themerge-joinalgorithm(alsocalledthesort-merge-joinalgorithm)canbeused
to compute natural joins and equi-joins. Let r(R)ands(S) be the relations whose
natural join is to be computed, and let R ? S denote their common attributes.
Supposethatbothrelationsaresortedontheattributes R ? S.Then,theirjoincan
becomputedbyaprocessmuchlikethemergestageinthemerge–sortalgorithm.
12.5.4.1 Merge-Join Algorithm
Figure 12.7 shows the merge-join algorithm. In the algorithm, JoinAttrs refers to
the attributes in R ? S,andt
r
  t
s
,wheret
r
and t
s
are tuples that have the same
554 Chapter 12 Query Processing
pr :=addressof ?rsttupleof r;
ps :=addressof ?rsttupleof s;
while (ps null null and pr null null) do
begin
t
s
:= tuple towhich ps points;
S
s
:={t
s
};
set pstopointtonexttupleofs;
done := false;
while (not done and ps null null)do
begin
t
s
null
:= tupleto which ps points;
if (t
s
null
[JoinAttrs] = t
s
[JoinAttrs])
then begin
S
s
:= S
s
?{t
s
null
};
set ps topoint to nexttuple ofs;
end
else done := true;
end
t
r
:=tuple to which pr points;
while (pr null null and t
r
[JoinAttrs] < t
s
[JoinAttrs]) do
begin
set pr topoint to nexttuple of r;
t
r
:= tupleto which pr points;
end
while (pr null null and t
r
[JoinAttrs] = t
s
[JoinAttrs]) do
begin
for each t
s
in S
s
do
begin
add t
s
  t
r
to result;
end
set pr topoint to nexttuple of r;
t
r
:= tupleto which pr points;
end
end.
Figure12.7 Merge join.
valuesfor JoinAttrs, denotes the concatenation of the attributesof the tuples, fol-
lowedbyprojectingoutrepeatedattributes.Themerge-joinalgorithmassociates
one pointer with each relation. These pointers point initially to the ?rst tuple of
the respective relations. As the algorithm proceeds, the pointers move through
the relation. A group of tuples of one relation with the same value on the join
attributes is read into S
s
. The algorithm in Figure 12.7 requires that every set of
tuples S
s
?tinmainmemory;wediscussextensionsofthealgorithmtoavoidthis
requirementshortly. Then, the corresponding tuples (if any) of the other relation
are read in, and are processed as they are read.
12.5 Join Operation 555
a3
b1
d8
13 d
f 7
m 5
q 
6
a    A
b    G
c     L
d    N
m   B
a1 a2 a1 a3
pr ps
r
s
Figure12.8 Sorted relations for merge join.
Figure 12.8 shows two relations that are sorted on their join attribute a1. It is
instructive to go through the steps of the merge-join algorithm on the relations
shown in the ?gure.
The merge-joinalgorithmof Figure 12.7 requiresthat each set S
s
of all tuples
withthesamevalueforthejoinattributesmust?tinmainmemory.Thisrequire-
ment can usually be met, even if the relation s is large. If there are some join
attributevaluesforwhich S
s
islargerthanavailablememory,ablocknested-loop
joincanbeperformedforsuchsets S
s
,matchingthemwithcorrespondingblocks
of tuplesin r with the same valuesforthe join attributes.
Ifeitheroftheinputrelationsr and s isnotsortedonthejoinattributes,they
canbesorted?rst,andthenthemerge-joinalgorithmcanbeused.Themerge-join
algorithmcanalsobeeasilyextendedfromnaturaljoinstothemoregeneralcase
of equi-joins.
12.5.4.2 Cost Analysis
Once the relations are in sorted order, tuples with the same value on the join
attributes are in consecutive order.Thereby, each tuple in the sorted order needs
to be read only once, and, as a result, each block is also read only once. Since it
makes only a single pass through both ?les (assuming all sets S
s
?t in memory)
the merge-join method is ef?cient; the number of block transfers is equal to the
sum ofthe number of blocks in both ?les, b
r
+ b
s
.
Assuming that b
b
buffer blocks are allocated to each relation, the number of
disk seeks required would be null b
r
/b
b
nullnull b
s
/b
b
null disk seeks. Since seeks are much
more expensive than data transfer, it makes sense to allocate multiple buffer
blocks to each relation, provided extra memory is available. For example, with
t
T
= 0.1 milliseconds per 4-kilobyte block, and t
S
= 4 milliseconds, the buffer
sizeis400blocks(or1.6megabytes),sotheseektimewouldbe4millisecondsfor
every40millisecondsoftransfertime,inotherwords,seektimewouldbejust10
percentof thetransfertime.
556 Chapter 12 Query Processing
Ifeitheroftheinputrelationsr and s isnotsortedonthejoinattributes,they
must be sorted ?rst; the cost of sorting must then be added to the above costs. If
some some sets S
s
do not ?t inmemory, the cost would increase slightly.
Supposethemerge-joinschemeisappliedtoourexampleofstudent   takes.
ThejoinattributehereisID. Suppose that the relations are already sorted on the
joinattribute ID.Inthiscase,themergejointakesatotalof400 +100 = 500block
transfers.Ifweassumethatintheworstcaseonlyonebufferblockisallocatedto
eachinputrelation(thatis, b
b
= 1),atotalof400+100 = 500seekswouldalsobe
required; in reality b
b
can be set much higher since we need to buffer blocks for
only two relations, and the seekcost would be signi?cantly less.
Suppose the relations are not sorted, and the memory size is the worst case,
only three blocks. The cost isas follows:
1. UsingtheformulaethatwedevelopedinSection12.4,wecanseethatsorting
relation takes requires null log
3?1
(400/3)null 8 merge passes. Sorting of relation
takes then takes 400 ? (2null log
3?1
(400/3)null 1), or 6800, block transfers, with
400 more transfers to write out the result. The number of seeks required is
2?null 400/3null 400 ? (2 ? 8 ? 1) or 6268 seeks for sorting, and 400 seeks for
writing the output, for a total of 6668 seeks, since only one buffer block is
available for each run.
2. Similarly, sorting relation student takes null log
3?1
(100/3)null 6 merge passes
and 100 ? (2null log
3?1
(100/3)null 1), or 1300, block transfers, with 100 more
transfers to write it out. The number of seeksrequiredfor sorting student is
2?null 100/3null 100 ?(2 ?6 ?1) = 1164,and100seeksarerequiredforwriting
the output, for atotal of 1264 seeks.
3. Finally,mergingthetworelationstakes400 +100 = 500blocktransfersand
500 seeks.
Thus, the total cost is 9100 block transfers plus 8932 seeks if the relations are not
sorted,and the memorysize is just 3 blocks.
With a memory size of 25 blocks, and the relations not sorted, the cost of
sorting followed by mergejoin would be asfollows:
1. Sorting the relation takescanbedonewithjustonemergestep,andtakes
a total of just 400 ? (2null log
24
(400/25)null 1) = 1200 block transfers. Similarly,
sorting student takes 300 block transfers. Writing the sorted output to disk
requires 400 + 100 = 500 block transfers, and the merge step requires 500
block transfers to read the data back. Adding up these costs gives a total
cost of 2500 block transfers.
2. Ifweassumethatonlyonebufferblockisallocatedforeachrun,thenumber
ofseeksrequiredinthiscaseis2?null 400/25null400+400 = 832seeksforsorting
takesandwritingthesortedoutputtodisk,andsimilarly2?null 100/25null 100+
100 = 208for student,plus400+100seeksforreadingthesorteddatainthe
merge-joinstep.Addingup these costs givesa total cost of 1640 seeks.
12.5 Join Operation 557
The number of seeks can be signi?cantly reduced by setting aside more
buffer blocks for each run. For example, if 5 buffer blocks are allocated
for each run and for the output from merging the 4 runs of student,the
cost is reduced to 2?null 100/25nullnull 100/5nullnull 100/5null 48 seeks, from 208
seeks. If the merge-join step sets aside 12 blocks each for buffering takes
and student, the number of seeks for the merge-join step goes down to
null 400/12nullnull 100/12null 43, from 500. The total number of seeksis then 251.
Thus, the total cost is 2500 block transfers plus 251 seeks if the relations are not
sorted,and the memorysize is25 blocks.
12.5.4.3 Hybrid Merge Join
It is possible to perform a variation of the merge-join operation on unsorted
tuples, if secondary indices exist on both join attributes. The algorithm scans the
recordsthroughtheindices,resultingintheirbeingretrievedinsortedorder.This
variationpresentsasigni?cantdrawback,however,sincerecordsmaybescattered
throughoutthe?leblocks.Hence,eachtupleaccesscouldinvolveaccessingadisk
block, and that is costly.
To avoid this cost, we can use a hybrid merge-join technique that combines
indices with merge join. Suppose that one of the relations is sorted; the other is
unsorted, but has a secondary B
+
-tree index on the join attributes. The hybrid
merge-join algorithm merges the sorted relation with the leaf entries of the
secondary B
+
-tree index. The result ?le contains tuples from the sorted relation
andaddressesfortuplesofthe unsortedrelation.The result?leisthensortedon
theaddressesoftuplesoftheunsortedrelation,allowingef?cientretrievalofthe
corresponding tuples, in physical storage order, to complete the join. Extensions
of the technique tohandle two unsorted relationsare leftas an exercisefor you.
12.5.5 HashJoin
Likethemerge-joinalgorithm,thehash-joinalgorithmcanbeusedtoimplement
natural joinsand equi-joins.In the hash-join algorithm, a hash function h isused
to partition tuples of both relations. The basic idea is to partition the tuples of
eachoftherelationsintosetsthathavethesamehashvalueonthejoinattributes.
We assume that:
• hisahashfunctionmappingJoinAttrsvaluesto {0, 1,...,n
h
},whereJoinAttrs
denotesthe common attributesof r and s used in the natural join.
• r
0
, r
1
,...,r
n
h
denote partitions of r tuples, each initially empty. Each tuple
t
r
? r isputin partition r
i
,wherei = h(t
r
[JoinAttrs]).
• s
0
,s
1
, ...,s
n
h
denotepartitionsofstuples,eachinitiallyempty.Eachtuplet
s
? s
isputin partition s
i
,wherei = h(t
s
[JoinAttrs]).
558 Chapter 12 Query Processing
0
1
2
3
4
0
1
2
3
4
r
s
.
.
.
.
.
.
.
.
partitions
of r
partitions
of s
Figure12.9 Hash partitioning of relations.
The hash function h should have the “goodness” properties of randomness and
uniformity that we discussed in Chapter 11. Figure 12.9 depicts the partitioning
of the relations.
12.5.5.1 Basics
The idea behind the hash-join algorithm is this: Suppose that an r tuple and an
s tuple satisfy the join condition; then, they have the same value for the join
attributes. If that value is hashed to some value i,ther tuple has to be in r
i
and
the s tuple in s
i
. Therefore, r tuples in r
i
need only to be compared with s tuples
in s
i
; they donot needto be comparedwith s tuplesin any otherpartition.
For example,if d isa tuplein student, c a tuplein takes,andhahashfunction
onthe IDattributesofthetuples,thendand cmustbetestedonlyifh(c) = h(d).If
h(c) null h(d),thencanddmusthavedifferentvaluesforID.However,ifh(c) = h(d),
wemusttestcanddtoseewhetherthevaluesintheirjoinattributesarethesame,
since it is possible that c and d havedifferent iids that have the same hash value.
Figure 12.10 shows the details of the hash-join algorithm to compute the
natural join of relations r and s. As in the merge-join algorithm, t
r
  t
s
denotes
the concatenation of the attributes of tuples t
r
and t
s
, followed by projecting out
repeatedattributes.Afterthepartitioningoftherelations,therestofthehash-join
codeperformsaseparateindexednested-loopjoinoneachofthepartitionpairsi,
for i = 0,...,n
h
.Todoso,it?rstbuildsahashindexoneach s
i
,andthenprobes
(thatis,looksup s
i
)withtuplesfromr
i
.Therelations isthebuildinput,andr is
the probeinput.
Thehashindexon s
i
isbuiltinmemory,sothereisnoneedtoaccessthedisk
to retrieve the tuples. The hash function used to build this hash index must be
differentfromthehashfunction h usedearlier,butisstillappliedtoonlythejoin
12.5 Join Operation 559
/*Partition s */
for each tuple t
s
in s dobegin
i := h(t
s
[JoinAttrs]);
H
s
i
:= H
s
i
? {t
s
};
end
/*Partition r */
for each tuple t
r
in rdobegin
i := h(t
r
[JoinAttrs]);
H
r
i
:= H
r
i
?{t
r
};
end
/*Perform joinon each partition*/
for i :=0 to n
h
dobegin
read H
s
i
and build an in-memoryhash indexon it;
for each tuple t
r
in H
r
i
dobegin
probe the hash index on H
s
i
to locate all tuples t
s
such that t
s
[JoinAttrs] = t
r
[JoinAttrs];
for each matching tuple t
s
in H
s
i
dobegin
add t
r
  t
s
to the result;
end
end
end
Figure12.10 Hash join.
attributes.Inthecourseoftheindexednested-loopjoin,thesystemusesthishash
index to retrieve records that match records in the probe input.
The build and probe phases require only a single pass through both the
buildandprobeinputs.Itisstraightforwardtoextendthehash-joinalgorithmto
compute generalequi-joins.
Thevaluen
h
mustbechosentobelargeenoughsuchthat,foreachi,thetuples
inthepartitions
i
ofthebuildrelation,alongwiththehashindexonthepartition,
?t in memory. It is not necessary for the partitions of the probe relation to ?t in
memory.Clearly,itisbesttousethesmallerinputrelationasthebuildrelation.If
thesizeofthebuildrelationisb
s
blocks,then,foreachofthe n
h
partitionstobeof
sizelessthanorequalto M, n
h
mustbeatleast null b
s
/Mnull .Morepreciselystated,we
havetoaccountfortheextraspaceoccupiedbythehashindexonthepartitionas
well,son
h
shouldbecorrespondinglylarger.Forsimplicity,wesometimesignore
the space requirementof the hash indexin ouranalysis.
12.5.5.2 Recursive Partitioning
If the value of n
h
is greaterthan or equal to the number of blocks of memory, the
relations cannot be partitioned in one pass, since there will not be enough buffer
blocks. Instead, partitioning has to be done in repeated passes. In one pass, the
inputcanbesplitintoatmostasmanypartitionsasthereareblocksavailablefor
useasoutputbuffers.Eachbucketgeneratedbyonepassisseparatelyreadinand
560 Chapter 12 Query Processing
partitionedagaininthe nextpass, tocreatesmallerpartitions.The hash function
usedina passis,of course,differentfromthe one usedin thepreviouspass.The
system repeats this splitting of the input until each partition of the build input
?ts in memory.Such partitioningis calledrecursive partitioning.
Arelationdoesnotneedrecursivepartitioningif M > n
h
+1,orequivalently
M > (b
s
/M) + 1, which simpli?es (approximately) to M >
?
b
s
. For example,
consideramemorysizeof12megabytes,dividedinto4-kilobyteblocks;itwould
contain a total of 3K (3072) blocks. We can use a memory of this size to partition
relationsofsizeupto3K ?3Kblocks,whichis36gigabytes.Similarly,arelationof
size1gigabyterequiresjustover
?
256Kblocks,or2megabytes,toavoidrecursive
partitioning.
12.5.5.3 HandlingofOver?ows
Hash-tableover?owoccurs inpartition i ofthe buildrelation s ifthe hash index
on s
i
is larger than main memory. Hash-table over?ow can occur if there are
many tuples in the build relation with the same values for the join attributes, or
if the hash function does not have the properties of randomness and uniformity.
In either case, some of the partitions will have more tuples than the average,
whereasothers will have fewer; partitioningisthen said to beskewed.
We can handle a small amount of skew by increasing the number of parti-
tions so that the expected size of each partition (including the hash index on the
partition) is somewhat less than the size of memory. The number of partitions
is therefore increased by a small value, called the fudge factor, that is usually
about 20 percent of the number of hash partitions computed as described in
Section12.5.5.
Even if, by using a fudge factor, we are conservative on the sizes of the par-
titions, over?ows can still occur. Hash-table over?ows can be handled by either
over?ow resolutionor over?ow avoidance.Over?owresolutionisperformedduring
the build phase, if a hash-index over?ow is detected. Over?ow resolution pro-
ceeds in this way: If s
i
,foranyi, is found to be too large, it is further partitioned
intosmallerpartitionsbyusingadifferenthashfunction.Similarly,r
i
isalsopar-
titioned using the new hash function, and only tuples in the matching partitions
needtobe joined.
In contrast, over?ow avoidance performs the partitioning carefully, so that
over?ows never occur during the build phase. In over?ow avoidance, the build
relation s is initially partitioned into many small partitions, and then some par-
titions are combined in such a way that each combined partition ?ts in memory.
The probe relationr ispartitionedinthesamewayasthecombinedpartitionson
s,butthesizesofr
i
do not matter.
If a large number of tuples in s have the same value for the join attributes,
theresolutionandavoidancetechniquesmayfailonsomepartitions.Inthatcase,
insteadofcreatinganin-memoryhashindexandusinganested-loopjointojoin
the partitions, we can use other join techniques, such as block nested-loop join,
on those partitions.
12.5 Join Operation 561
12.5.5.4 Cost of Hash Join
We now consider the cost of a hash join. Our analysis assumes that there is no
hash-table over?ow. First, consider the case where recursive partitioning is not
required.
• The partitioning of the two relations r and s calls for a complete reading
of both relations, and a subsequent writing back of them. This operation
requires 2(b
r
+ b
s
) block transfers, where b
r
and b
s
denote the number of
blocks containing records of relations r and s, respectively. The build and
probephasesreadeachofthepartitionsonce,callingforfurther b
r
+b
s
block
transfers.Thenumberofblocksoccupiedbypartitionscouldbeslightlymore
thanb
r
+b
s
,asaresultofpartially?lledblocks.Accessingsuchpartially?lled
blockscanaddanoverheadofatmost2n
h
foreachoftherelations,sinceeach
of the n
h
partitions could have a partially ?lled block that has to be written
and readback. Thus, a hash join isestimatedtorequire:
3(b
r
+ b
s
) +4n
h
block transfers.Theoverhead4n
h
isusuallyquitesmallcomparedto b
r
+b
s
,
and can be ignored.
• Assuming b
b
blocksareallocatedfortheinputbufferandeachoutputbuffer,
partitioningrequiresatotalof2(null b
r
/b
b
nullnull b
s
/b
b
null )seeks.Thebuildandprobe
phases require only one seek for each of the n
h
partitions of each relation,
since each partition can be read sequentially. The hash join thus requires
2(null b
r
/b
b
nullnull b
s
/b
b
null ) +2n
h
seeks.
Now consider the case where recursive partitioning is required. Each pass
reduces the size of each of the partitions by an expected factor of M ? 1; and
passesare repeateduntil eachpartitionisofsize atmost Mblocks.Theexpected
number of passesrequiredfor partitioning s istherefore null log
M?1
(b
s
) ?1null .
• Since,ineachpass,everyblockof s isreadinandwrittenout,thetotalblock
transfers for partitioning of s is 2b
s
null log
M?1
(b
s
) ? 1null . The number of passes
forpartitioningofr isthesameasthenumberofpassesforpartitioningof s,
thereforethejoin isestimatedto require:
2(b
r
+ b
s
)null log
M?1
(b
s
) ?1null b
r
+ b
s
block transfers.
• Again assuming b
b
blocks are allocated for buffering each partition, and
ignoring the relatively small number of seeks during the build and probe
phase, hash join with recursivepartitioning requires:
2(null b
r
/b
b
nullnull b
s
/b
b
null )null log
M?1
(b
s
) ?1null
diskseeks.
562 Chapter 12 Query Processing
Consider, for example, the natural join takes   student. With a memory size
of 20 blocks, the student relation can be partitioned into ?ve partitions, each of
size 20 blocks, which size will ?t into memory. Only one pass is required for
the partitioning. The relation takes is similarly partitioned into ?ve partitions,
each of size 80. Ignoring the cost of writing partially ?lled blocks, the cost is
3(100+400) = 1500blocktransfers.Thereisenoughmemorytoallocate3buffers
fortheinputandeachofthe5outputsduringpartitioning,leadingto2(null 100/3null
null 400/3null ) = 336 seeks.
The hash join can be improved if the main-memory size is large. When the
entirebuild inputcan bekeptinmainmemory, n
h
can be setto0; then, the hash-
joinalgorithmexecutesquickly,withoutpartitioningtherelationsintotemporary
?les, regardless of the probe input’s size. The cost estimate goes down to b
r
+ b
s
block transfers and two seeks.
12.5.5.5 Hybrid Hash Join
Thehybridhash-joinalgorithmperformsanotheroptimization;itisusefulwhen
memorysizesare relativelylarge,but not all ofthe build relation?ts inmemory.
Thepartitioningphaseofthehash-joinalgorithmneedsaminimumofoneblock
ofmemoryasabufferforeachpartitionthatiscreated,andoneblockofmemory
asaninputbuffer.Toreducetheimpactofseeks,alargernumberofblockswould
be used as a buffer; let b
b
denote the number of blocks used as a buffer for the
input and for each partition. Hence, a total of (n
h
+1) ? b
b
blocks of memory are
needed for partitioning the two relations. If memory is larger than (n
h
+ 1) ? b
b
,
wecanusetherestofmemory(M?(n
h
+1)?b
b
blocks)tobufferthe?rstpartition
of the build input (that is, s
0
), so that it will not need to be written out and read
back in. Further, the hash function is designedin such a way that the hash index
on s
0
?ts in M ?(n
h
+1) ? b
b
blocks, in order that, at the end of partitioning of s,
s
0
iscompletelyin memoryand a hash indexcan be built on s
0
.
When the system partitions r it again does not write tuples in r
0
to disk;
instead,asitgeneratesthem,thesystemusesthemtoprobethememory-resident
hash index on s
0
, and to generate output tuples of the join. After they are used
for probing, the tuples can be discarded, so the partition r
0
does not occupy any
memoryspace.Thus,awriteandareadaccesshavebeensavedforeachblockof
both r
0
and s
0
. The system writes out tuples in the other partitions as usual, and
joins them later. The savings of hybrid hash join can be signi?cant if the build
input isonly slightlybiggerthan memory.
Ifthesizeofthebuildrelationis b
s
, n
h
isapproximatelyequalto b
s
/M.Thus,
hybrid hash join is most useful if M >> (b
s
/M) ? b
b
,orM >>
?
b
s
? b
b
,where
the notation >> denotes much larger than. For example,suppose the block size is
4 kilobytes, the build relation size is 5 gigabytes, and b
b
is 20. Then, the hybrid
hash-join algorithm is useful if the size of memory is signi?cantly more than 20
megabytes;memorysizesofgigabytesormorearecommononcomputerstoday.
Ifwedevote1gigabyteforthejoinalgorithm, s
0
wouldbenearly1gigabyte,and
hybrid hash join would be nearly 20 percent cheaper than hash join.
12.6 Other Operations 563
12.5.6 Complex Joins
Nested-loopandblocknested-loopjoinscanbeusedregardlessofthejoincondi-
tions. The other join techniques are more ef?cient than the nested-loop join and
its variants, but can handle only simple join conditions, such as natural joins or
equi-joins. We can implement joins with complex join conditions, such as con-
junctions and disjunctions, by using the ef?cient join techniques, if we apply the
techniquesdevelopedinSection12.3.3for handling complexselections.
Considerthe following join with aconjunctive condition:
r     1
?  2
?···?  n
s
One or more of the join techniques described earlier may be applicable for joins
on the individual conditions r     1
s, r     2
s, r     3
s,andsoon.Wecancompute
the overalljoin by ?rst computing the result of one of these simplerjoins r     i
s;
eachpairoftuplesintheintermediateresultconsistsofonetuplefromr andone
from s.Theresultofthecompletejoinconsistsofthosetuplesintheintermediate
resultthat satisfy the remainingconditions:
  1
?···?  i?1
?  i+1
?···?  n
These conditions can be testedas tuplesin r     i
s are being generated.
Ajoinwhoseconditionisdisjunctivecanbecomputedinthisway.Consider:
r     1
?  2
?···?  n
s
The join can be computed as the union of the recordsin individualjoins r     i
s:
(r     1
s) ?(r     2
s)?···?(r     n
s)
Section12.6 describesalgorithms forcomputing the union ofrelations.
12.6 OtherOperations
Other relational operations and extended relational operations—such as dupli-
cate elimination, projection, set operations, outer join, and aggregation—can be
implementedas outlined inSections 12.6.1through 12.6.5.
12.6.1 DuplicateElimination
We can implement duplicate elimination easily by sorting. Identical tuples will
appear adjacent to each other as a result of sorting, and all but one copy can be
removed.Withexternalsort–merge,duplicatesfoundwhilearunisbeingcreated
canberemovedbeforetheruniswrittentodisk,therebyreducingthenumberof
blocktransfers.Theremainingduplicatescanbeeliminatedduringmerging,and
564 Chapter 12 Query Processing
the ?nal sorted run has no duplicates.The worst-case cost estimatefor duplicate
eliminationisthesame asthe worst-case cost estimatefor sortingofthe relation.
We can also implement duplicate elimination by hashing, as in the hash-join
algorithm. First, the relation is partitioned on the basis of a hash function on the
whole tuple. Then, each partition is read in, and an in-memory hash index is
constructed.Whileconstructingthehashindex,atupleisinsertedonlyifitisnot
alreadypresent.Otherwise,thetupleisdiscarded.Afteralltuplesinthepartition
havebeenprocessed,thetuplesinthehashindexarewrittentotheresult.Thecost
estimate is the same as that for the cost of processing (partitioning and reading
each partition) ofthe build relationin ahash join.
Because of the relatively high cost of duplicate elimination, SQL requires an
explicit request by the user to remove duplicates; otherwise, the duplicates are
retained.
12.6.2 Projection
Wecanimplementprojectioneasilybyperformingprojectiononeachtuple,which
gives a relation that could have duplicate records, and then removing duplicate
records.DuplicatescanbeeliminatedbythemethodsdescribedinSection12.6.1.
If the attributes in the projection list include a key of the relation, no duplicates
willexist;hence,duplicateeliminationisnotrequired.Generalizedprojectioncan
be implementedinthesameway as projection.
12.6.3 SetOperations
We can implement the union, intersection,andset-difference operations by ?rst
sortingbothrelations,andthenscanningoncethrougheachofthesortedrelations
to produce the result. In r ? s, when a concurrent scan of both relations reveals
the same tuple in both ?les, only one of the tuples is retained. The result of r ? s
will contain only those tuples that appear in both relations. We implement set
difference, r ? s, similarly,by retainingtuplesin r only ifthey are absent in s.
For all these operations, only one scan of the two sorted input relations is
required, so the cost is b
r
+ b
s
block transfers if the relations are sorted in the
same order. Assuming a worst case of one block buffer for each relation, a total
of b
r
+b
s
diskseekswouldberequiredinadditionto b
r
+b
s
blocktransfers.The
number of seekscan be reducedby allocating extrabuffer blocks.
If the relations are not sorted initially, the cost of sorting has to be included.
Any sort order can be used in evaluation of set operations, provided that both
inputs have that same sortorder.
Hashing provides another way to implement these set operations. The ?rst
step in each case is to partition the two relations by the same hash function, and
thereby create the partitions r
0
,r
1
,...,r
n
h
and s
0
,s
1
,...,s
n
h
. Depending on the
operation, the systemthen takesthese stepson each partition i = 0,1,...,n
h
:
• r ? s
1. Buildan in-memory hash indexon r
i
.
12.6 Other Operations 565
2. Addthetuplesins
i
tothehashindexonlyiftheyarenotalreadypresent.
3. Addthe tuplesin the hash indexto the result.
• r ? s
1. Buildan in-memoryhash indexon r
i
.
2. For each tuple in s
i
, probe the hash index and output the tuple to the
resultonly ifit isalreadypresentinthe hash index.
• r ? s
1. Buildan in-memoryhash indexon r
i
.
2. Foreachtuplein s
i
, probethehash index,and,ifthe tupleispresentin
thehash index,deleteitfrom thehash index.
3. Addthe tuples remainingin the hash indextothe result.
12.6.4 OuterJoin
Recallthe outer-join operationsdescribedinSection4.1.2.Forexample,thenatural
left outer join takes   student contains the join of takes and student,and,in
addition, for each takes tuple t that has no matching tuple in student (that is,
where ID is not in student), the following tuple t
1
is added to the result. For all
attributes in the schema of takes,tuplet
1
has the same values as tuple t.The
remaining attributes (from the schema of student)oftuplet
1
contain the value
null.
We can implementthe outer-join operationsby using one of two strategies:
1. Compute the corresponding join, and then add further tuples to the join
resultto getthe outer-joinresult.Considerthe leftouter-joinoperationand
two relations: r(R)ands(S). To evaluate r     s,we?rstcomputer     s,
and save that resultas temporaryrelation q
1
. Next,we compute r ? null
R
(q
1
)
toobtainthosetuplesinr thatdonotparticipateinthethetajoin.Wecanuse
anyofthealgorithmsforcomputingthejoins,projection,andsetdifference
described earlier to compute the outer joins. We pad each of these tuples
withnullvaluesforattributesfrom s,andaddittoq
1
togettheresultofthe
outer join.
The right outer-join operation r     s is equivalent to s     r,andcan
therefore be implemented in a symmetric fashion to the left outer join. We
can implement the full outer-join operation r     s by computing the join
r   s, and then adding the extra tuples of both the left and right outer-join
operations, as before.
2. Modify the join algorithms. It is easy to extend the nested-loop join algo-
rithmstocomputetheleftouterjoin:Tuplesintheouterrelationthatdonot
match any tuple in the inner relation are written to the output after being
paddedwithnullvalues.However,itishardtoextendthenested-loopjoin
to compute the full outer join.
566 Chapter 12 Query Processing
Natural outer joins and outer joins with an equi-join condition can be
computedbyextensionsofthemerge-joinandhash-joinalgorithms.Merge
join can be extended to compute the full outer join as follows: When the
mergeofthetworelationsisbeingdone,tuplesineitherrelationthatdonot
match any tuple in the other relationcan be paddedwith nullsand written
to the output. Similarly, we can extend merge join to compute the left and
right outer joins by writing out nonmatching tuples (padded with nulls)
from only one of the relations. Since the relations are sorted, it is easy to
detect whether or not a tuple matches any tuples from the other relation.
For example, when a merge join of takes and student is done, the tuples are
read in sorted order of ID, and it is easy to check, for each tuple, whether
there is a matching tuplein the other.
The cost estimates for implementing outer joins using the merge-join
algorithm are the same as are those for the corresponding join. The only
difference lies in size of the result, and therefore in the block transfers for
writing it out, which we didnot count in our earliercost estimates.
Theextensionofthehash-joinalgorithmtocomputeouterjoinsisleftfor
you to doas an exercise(Exercise12.15).
12.6.5 Aggregation
Recalltheaggregationfunction(operator),discussedinSection3.7.Forexample,
the function
select dept name, avg (salary)
from instructor
groupby dept name;
computesthe averagesalary ineach universitydepartment.
Theaggregationoperationcanbeimplementedinthesamewayasduplicate
elimination.Weuseeithersortingorhashing,justaswedidforduplicateelimina-
tion,butbasedonthegroupingattributes(branch nameintheprecedingexample).
However, instead of eliminating tuples with the same value for the grouping at-
tribute, we gather them into groups, and apply the aggregation operations on
each group togetthe result.
The cost estimate for implementing the aggregation operation is the same as
thecostofduplicateelimination,foraggregatefunctionssuchasmin,max,sum,
count,andavg.
Instead of gathering all the tuples in a group and then applying the aggre-
gationoperations,wecanimplementtheaggregationoperationssum,min,max,
count,andavg on the ?y as the groups are being constructed. For the case of
sum, min,andmax, when two tuples in the same group are found, the system
replaces themby a single tuple containing the sum, min,ormax, respectively,of
the columns being aggregated. For the count operation, it maintains a running
countforeachgroupforwhichatuplehasbeenfound.Finally,weimplementthe
12.7 Evaluation of Expressions 567
avg operation by computing the sum and the count values on the ?y, and ?nally
dividingthesum by thecount togetthe average.
Ifalltuplesoftheresult?tinmemory,boththesort-basedandthehash-based
implementations do not need to write any tuples to disk. As the tuples are read
in, they can be inserted in a sorted tree structure or in a hash index. When we
useon-the-?yaggregationtechniques,onlyonetupleneedstobestoredforeach
of the groups. Hence, the sorted tree structure or hash index ?ts in memory, and
theaggregationcanbeprocessedwithjust b
r
blocktransfers(and1seek)instead
of the 3b
r
transfers (and a worst case of up to 2b
r
seeks) that would be required
otherwise.
12.7 EvaluationofExpressions
So far, we have studied how individual relational operations are carried out.
Nowweconsiderhowtoevaluateanexpressioncontainingmultipleoperations.
The obvious way to evaluate an expression is simply to evaluate one operation
at a time, in an appropriate order. The result of each evaluation is materialized
in a temporary relation for subsequent use. A disadvantage to this approach
is the need to construct the temporary relations, which (unless they are small)
mustbewrittentodisk.Analternativeapproachistoevaluateseveraloperations
simultaneously in a pipeline, with the results of one operation passed on to the
next, without the needto store atemporaryrelation.
In Sections 12.7.1 and 12.7.2, we consider both the materialization approach
and the pipelining approach. We shall see that the costs of these approaches can
differ substantially, but also that there are cases where only the materialization
approach is feasible.
12.7.1 Materialization
It is easiest to understand intuitively how to evaluate an expression by looking
at a pictorial representation of the expression in an operator tree.Considerthe
expression:
null
name
(  building =“Watson”
(department)  instructor)
in Figure12.11.
If we apply the materialization approach, we start from the lowest-level op-
erations in the expression (at the bottom of the tree). In our example, there is
only one such operation: the selectionoperationon department. The inputsto the
lowest-leveloperationsarerelationsinthedatabase.Weexecutetheseoperations
by the algorithms that we studied earlier, and we store the results in temporary
relations. We can use these temporary relations to execute the operations at the
next level up in the tree, where the inputs now are either temporary relations or
relations stored in the database. In our example, the inputs to the join are the in-
structorrelationandthetemporaryrelationcreatedbytheselectionondepartment.
The join can now be evaluated,creating another temporaryrelation.
568 Chapter 12 Query Processing
?
?
name
building = “Watson”
department
instructor
Figure12.11 Pictorial representation of an expression.
Byrepeatingtheprocess,wewilleventuallyevaluatetheoperationattheroot
of the tree, giving the ?nal result of the expression. In our example, we get the
?nal result by executing the projection operation at the root of the tree, using as
input the temporaryrelationcreatedby the join.
Evaluation as just described is called materialized evaluation,sincethere-
sultsofeachintermediateoperationarecreated(materialized)andthenareused
for evaluationof the next-leveloperations.
The cost of a materialized evaluation is not simply the sum of the costs of
the operations involved. When we computed the cost estimates of algorithms,
we ignored the cost of writing the result of the operation to disk. To compute
the cost of evaluating an expression as done here, we have to add the costs of
all the operations, as well as the cost of writing the intermediate results to disk.
We assume that the records of the result accumulate in a buffer, and, when the
buffer is full, they are written to disk. The number of blocks written out, b
r
,can
be estimated as n
r
/f
r
,wheren
r
is the estimated number of tuples in the result
relation r,and f
r
is the blocking factor of the result relation, that is, the number
of records of r that will ?t in a block. In addition to the transfer time, some disk
seeksmayberequired,sincethediskheadmayhavemovedbetweensuccessive
writes. The number of seeks can be estimated as null b
r
/b
b
null where b
b
is the size of
the outputbuffer (measuredinblocks).
Double buffering (using two buffers, with one continuing execution of the
algorithm while the other is being written out) allows the algorithm to execute
morequicklybyperformingCPUactivityinparallelwithI/Oactivity.Thenumber
ofseekscanbereducedbyallocatingextrablockstotheoutputbuffer,andwriting
outmultipleblocksatonce.
12.7.2 Pipelining
We can improve query-evaluation ef?ciency by reducing the number of tem-
porary ?les that are produced. We achieve this reduction by combining several
relational operations into a pipeline of operations, in which the results of one op-
12.7 Evaluation of Expressions 569
eration are passed along to the next operation in the pipeline. Evaluation as just
describediscalled pipelinedevaluation.
For example, consider the expression (null
a1,a2
(r   s)). If materialization were
applied,evaluationwouldinvolvecreatingatemporaryrelationtoholdtheresult
of the join, and then reading back in the result to perform the projection. These
operations can be combined: When the join operation generates a tuple of its
result,itpassesthattupleimmediatelytotheprojectoperationforprocessing.By
combining the join and the projection, we avoid creatingthe intermediateresult,
and instead createthe ?nal result directly.
Creatinga pipelineof operationscan providetwo bene?ts:
1. It eliminates the cost of reading and writing temporary relations, reducing
the cost ofqueryevaluation.
2. It can startgenerating queryresultsquickly,if the root operatorof a query-
evaluation plan is combined in a pipeline with its inputs. This can be quite
useful if the results are displayed to a user as they are generated, since
otherwise there may be a long delaybefore the user seesany query results.
12.7.2.1 Implementation ofPipelining
We can implement a pipeline by constructing a single, complex operation that
combinestheoperationsthatconstitutethepipeline.Althoughthisapproachmay
be feasible for some frequently occurring situations, it is desirable in general to
reusethe code for individualoperationsin the construction of apipeline.
IntheexampleofFigure12.11,allthreeoperationscanbeplacedinapipeline,
which passes the results of the selection to the join as they are generated. In
turn, it passes the results of the join to the projection as they are generated. The
memoryrequirementsarelow,sinceresultsofanoperationarenotstoredforlong.
However, as a result of pipelining, the inputs to the operations are not available
all at once for processing.
Pipelinescan be executedin either oftwo ways:
1. Inademand-drivenpipeline,thesystemmakesrepeatedrequestsfortuples
from the operation at the top of the pipeline. Each time that an operation
receives a request for tuples, it computes the next tuple (or tuples) to be
returned, and then returns that tuple. If the inputs of the operation are not
pipelined, the next tuple(s) to be returned can be computed from the input
relations,whilethesystemkeepstrackofwhathasbeenreturnedsofar.Ifit
hassomepipelinedinputs,theoperationalsomakesrequestsfortuplesfrom
itspipelinedinputs.Usingthetuplesreceivedfromitspipelinedinputs,the
operation computestuplesfor itsoutput, and passesthemup toitsparent.
2. In a producer-driven pipeline, operations do not wait for requests to pro-
duce tuples, but instead generate the tuples eagerly.Eachoperationina
producer-drivenpipelineismodeledasaseparateprocessorthreadwithin
570 Chapter 12 Query Processing
the system that takes a stream of tuples from its pipelined inputs and gen-
eratesa streamof tuplesforitsoutput.
We describe below how demand-driven and producer-driven pipelines can be
implemented.
Each operation in a demand-driven pipeline can be implemented as an iter-
ator that provides the following functions: open(), next(), and close(). After a call
to open(), each call to next() returns the next output tuple of the operation. The
implementationoftheoperationinturncallsopen()andnext()onitsinputs,toget
its input tuples when required. The function close() tells an iterator that no more
tuples are required. The iterator maintains the state of its execution in between
calls, so that successive next() requests receive successive result tuples.
For example, for an iterator implementing the select operation using linear
search, the open() operation starts a ?le scan, and the iterator’s state records the
pointtowhichthe?lehasbeenscanned.Whenthenext()functioniscalled,the?le
scan continues from after the previous point; when the next tuple satisfying the
selectionisfoundbyscanningthe?le,thetupleisreturnedafterstoringthepoint
where it was found in the iterator state. A merge-join iterator’s open() operation
would open its inputs, and if they are not already sorted, it would also sort the
inputs. On calls to next(), it would return the next pair of matching tuples. The
state information would consist of up to where each input had been scanned.
Details of the implementation of iterators are left for you to complete in Practice
Exercise12.7.
Producer-drivenpipelines,ontheotherhand,areimplementedinadifferent
manner. For each pair of adjacent operations in a producer-driven pipeline, the
system creates a buffer to hold tuples being passed from one operation to the
next. The processes or threads corresponding to different operations execute
concurrently. Each operation at the bottom of a pipeline continually generates
output tuples, and puts them in its output buffer, until the buffer is full. An
operation at any other level of a pipeline generates output tuples when it gets
inputtuplesfromlowerdowninthepipeline,untilitsoutputbufferisfull.Once
the operation uses a tuple from a pipelined input, it removes the tuple from its
input buffer. In either case, once the output buffer is full, the operation waits
until its parent operation removes tuples from the buffer, so that the buffer has
spaceformoretuples.Atthispoint,theoperationgeneratesmoretuples,untilthe
buffer is full again. The operation repeats this process until all the output tuples
havebeengenerated.
It is necessary for the system to switch between operations only when an
outputbufferisfull,oraninputbufferisemptyandmoreinputtuplesareneeded
to generate any more output tuples. In a parallel-processing system, operations
in a pipelinemaybe runconcurrently on distinctprocessors(see Chapter18).
Using producer-driven pipelining can be thought of as pushing data up
an operation tree from below, whereas using demand-driven pipelining can be
thought of as pulling data up an operation tree from the top. Whereas tuples
are generated eagerly in producer-driven pipelining, they are generated lazily,
on demand, in demand-driven pipelining. Demand-driven pipelining is used
12.7 Evaluation of Expressions 571
more commonly than producer-driven pipelining, because it is easier to imple-
ment. However, producer-drivenpipelining is very useful in parallel processing
systems.
12.7.2.2 Evaluation AlgorithmsforPipelining
Someoperations,suchassorting,areinherentlyblockingoperations,thatis,they
maynotbeabletooutputanyresultsuntilalltuplesfromtheirinputshavebeen
examined.
5
Other operations, such as join, are not inherently blocking, but speci?c eval-
uation algorithms may be blocking. For example, the hash-join algorithm is a
blockingoperation,sinceitrequiresbothitsinputstobefullyretrievedandparti-
tioned,beforeitoutputsanytuples.Ontheotherhand,theindexednestedloops
join algorithm can output result tuples as it gets tuples for the outer relation. It
is therefore said to be pipelined on itsouter (left-hand side)relation, although it
is blocking on its indexed (right-hand side) input, since the index must be fully
constructed before the indexednested-loopjoin algorithmcan execute.
Hybrid hash join can be viewed as partially pipelined on the probe relation,
since it can output tuples from the ?rst partition as tuples are received for the
probe relation. However, tuples that are not in the ?rst partition will be output
only after the entire pipelined input relation is received. Hybrid hash join thus
providespipelinedevaluationonitsprobeinputifthebuildinput?tsentirelyin
memory,ornearlypipelinedevaluationifmostofthebuildinput?tsinmemory.
If both inputs are sorted on the join attribute, and the join condition is an
equi-join,merge joincan be used,with both itsinputs pipelined.
However, in the more common case that the two inputs that we desire to
pipeline into the join are not already sorted, another alternative is the double-
pipelinedjointechnique,showninFigure12.12.Thealgorithmassumesthatthe
inputtuplesforbothinputrelations,r ands,arepipelined.Tuplesmadeavailable
for both relations are queued for processing in a single queue. Special queue
entries, called End
r
and End
s
, which serve as end-of-?le markers, are inserted in
the queue after all tuples from r and s (respectively) have been generated. For
ef?cient evaluation, appropriate indices should be built on the relations r and s.
As tuples are added to r and s, the indices must be kept up to date. When hash
indicesareusedonr ands,theresultantalgorithmiscalledthedouble-pipelined
hash-join technique.
Thedouble-pipelinedjoinalgorithminFigure12.12assumesthatbothinputs
?tinmemory.Incasethetwoinputsarelargerthanmemory,itisstillpossibleto
use the double-pipelined join technique as usual until available memory is full.
Whenavailablememorybecomesfull,r and s tuplesthathavearriveduptothat
pointcanbetreatedasbeinginpartitionr
0
and s
0
,respectively.Tuplesforr and s
that arrive subsequently are assigned to partitions r
1
and s
1
, respectively, which
5
Blocking operations such as sorting may be able to output tuples early if the input is known to satisfy some special
properties such as being sorted, or partially sorted, already. However, in the absence of such information, blocking
operationscannot output tuplesearly.
572 Chapter 12 Query Processing
done
r
:= false;
done
s
:= false;
r := ?;
s := ?;
result := ?;
while notdone
r
ornotdone
s
do
begin
ifqueue isempty,then wait until queue isnot empty;
t :=top entryinqueue;
if t = End
r
then done
r
:= true
else if t = End
s
then done
s
:= true
else if tis frominput r
then
begin
r:= r ?{t};
result := result ? ({t}  s);
end
else /* t isfrominput s */
begin
s:= s ?{t};
result := result ? (r  {t});
end
end
Figure12.12 Double-pipelined join algorithm.
are written to disk, and are not added to the in-memory index. However, tuples
assigned to r
1
and s
1
are used to probe s
0
and r
0
, respectively, before they are
written to disk. Thus, the join of r
1
with s
0
,ands
0
with r
1
, is also carried out in
a pipelined fashion. After r and s have been fully processed, the join of r
1
tuples
withs
1
tuplesmustbecarriedout,tocompletethejoin;anyofthejointechniques
we haveseenearliercan beusedtojoin r
1
with s
1
.
12.8 Summary
• The ?rst action that the system must perform on a query is to translate the
queryintoitsinternalform,which(forrelationaldatabasesystems)isusually
basedontherelationalalgebra.Intheprocessofgeneratingtheinternalform
ofthequery,theparserchecksthesyntaxoftheuser’squery,veri?esthatthe
relationnamesappearinginthequeryarenamesofrelationsinthedatabase,
and so on. If the query was expressedin terms of a view, the parser replaces
all references to the view name with the relational-algebra expression to
computethe view.
• Given a query, there are generally a variety of methods for computing the
answer. Itisthe responsibilityof the queryoptimizertotransform the query
Review Terms 573
as entered by the user into an equivalent query that can be computed more
ef?ciently.Chapter 13 coversqueryoptimization.
• We can process simple selection operations by performing a linear scan, or
by making use of indices. We can handle complex selections by computing
unions and intersectionsof the resultsof simpleselections.
• We can sort relations larger than memory by the external sort–merge algo-
rithm.
• Queriesinvolvinganaturaljoinmaybeprocessedinseveralways,depending
ontheavailabilityofindicesandtheformofphysicalstoragefortherelations.
?
If the join result is almost as large as the Cartesian product of the two
relations, a block nested-loop join strategymay be advantageous.
?
If indicesare available,the indexed nested-loop join can be used.
?
Ifthe relationsaresorted,a merge join maybe desirable.It maybe advan-
tageous to sort a relation prior to join computation (so as to allow use of
the merge-joinstrategy).
?
The hash-join algorithm partitions the relations into several pieces, such
that each piece of one of the relations ?ts in memory. The partitioning is
carriedoutwithahashfunctiononthejoinattributes,sothatcorrespond-
ing pairs of partitions can be joined independently.
• Duplicate elimination, projection, set operations (union, intersection, and
difference),and aggregationcan be done by sortingor by hashing.
• Outer-joinoperationscanbeimplementedbysimpleextensionsofjoinalgo-
rithms.
• Hashing and sorting are dual, in the sense that any operation such as du-
plicate elimination, projection, aggregation, join, and outer join that can be
implementedbyhashingcanalsobeimplementedbysorting,andviceversa;
that is, any operation that can be implementedby sorting can also be imple-
mentedby hashing.
• An expression can be evaluated by means of materialization, where the sys-
temcomputestheresultofeachsubexpressionandstoresitondisk,andthen
usesit to compute the result of the parent expression.
• Pipelininghelpstoavoidwritingtheresultsofmanysubexpressionstodisk,
byusingtheresultsintheparentexpressionevenastheyarebeinggenerated.
ReviewTerms
• Queryprocessing
• Evaluationprimitive
• Query-executionplan
• Query-evaluationplan
• Query-executionengine
• Measuresofquerycost
574 Chapter 12 Query Processing
• Sequential I/O
• Random I/O
• Filescan
• Linearsearch
• Selectionsusing indices
• Access paths
• Indexscans
• Conjunctive selection
• Disjunctive selection
• Compositeindex
• Intersectionof identi?ers
• External sorting
• External sort–merge
• Runs
• N-way merge
• Equi-join
• Nested-loopjoin
• Blocknested-loopjoin
• Indexednested-loopjoin
• Mergejoin
• Sort-mergejoin
• Hybrid mergejoin
• Hash join
?
Build
?
Probe
?
Buildinput
?
Probe input
?
Recursivepartitioning
?
Hash-table over?ow
?
Skew
?
Fudgefactor
?
Over?owresolution
?
Over?owavoidance
• Hybrid hash join
• Operatortree
• Materializedevaluation
• Double buffering
• Pipelinedevaluation
?
Demand-drivenpipeline
(lazy, pulling)
?
Producer-drivenpipeline
(eager,pushing)
?
Iterator
• Double-pipelinedjoin
PracticeExercises
12.1 Assume (for simplicity in this exercise) that only one tuple ?ts in a block
and memory holds at most 3 blocks. Show the runs created on each pass
ofthesort-mergealgorithm,whenappliedtosortthefollowingtupleson
the ?rst attribute: (kangaroo, 17), (wallaby, 21), (emu, 1), (wombat, 13),
(platypus, 3), (lion, 8), (warthog, 4), (zebra, 11), (meerkat, 6), (hyena, 9),
(hornbill, 2), (baboon, 12).
12.2 Consider the bank database of Figure 12.13, where the primary keys are
underlined,and the following SQL query:
select T.branch name
from branch T, branch S
where T.assets > S.assetsand S.branch city = “Brooklyn”
PracticeExercises 575
Write an ef?cient relational-algebra expression that is equivalent to this
query.Justifyyour choice.
12.3 Let relations r
1
(A, B,C)andr
2
(C, D, E) have the following properties: r
1
has20,000tuples,r
2
has45,000tuples,25tuplesofr
1
?tononeblock,and
30tuplesofr
2
?tononeblock.Estimatethenumberofblocktransfersand
seeksrequired,using each of the following join strategiesfor r
1
  r
2
:
a. Nested-loopjoin.
b. Blocknested-loopjoin.
c. Mergejoin.
d. Hash join.
12.4 Theindexednested-loopjoinalgorithmdescribedinSection12.5.3canbe
inef?cientiftheindexisasecondaryindex,and thereare multipletuples
with the same value for the join attributes.Why is it inef?cient? Describe
a way, using sorting, to reduce the cost of retrieving tuples of the inner
relation. Under what conditions would this algorithm be more ef?cient
than hybrid mergejoin?
12.5 Let r and s be relations with no indices, and assume that the relations
are not sorted. Assuming in?nite memory, what is the lowest-cost way
(in terms of I/O operations) to compute r   s? What is the amount of
memoryrequiredfor thisalgorithm?
12.6 Consider the bank database of Figure 12.13, where the primary keys are
underlined. Suppose that a B
+
-tree index on branch city is available on
relation branch,andthatnootherindexisavailable.Listdifferentwaysto
handle the following selectionsthat involvenegation:
a.   ¬(branch city<“Brooklyn”)
(branch)
b.   ¬(branch city=“Brooklyn”)
(branch)
c.   ¬(branch city<“Brooklyn” ? assets<5000)
(branch)
12.7 Write pseudocode for an iterator that implements indexed nested-loop
join, where the outer relation is pipelined. Your pseudocode must de?ne
branch(branch name, branch city, assets)
customer(customer name, customer street, customer city)
loan (loan number, branch name, amount)
borrower(customer name, loan number)
account (account number, branch name, balance)
depositor (customer name, account number)
Figure12.13 Banking database.
576 Chapter 12 Query Processing
the standard iterator functions open(), next(), and close(). Show what state
information the iteratormust maintain betweencalls.
12.8 Designsort-basedandhash-basedalgorithmsforcomputingtherelational
division operation (see Practise Exercises of Chapter 6 for a de?nition of
the divisionoperation).
12.9 What is the effect on the cost of merging runs if the number of buffer
blocks per run is increased, while keeping overall memory available for
buffering runs ?xed?
Exercises
12.10 Supposeyouneedtosortarelationof40gigabytes,with4kilobyteblocks,
using a memory size of 40 megabytes. Suppose the cost of a seek is 5
milliseconds,while the disktransferrate is40 megabytespersecond.
a. Findthecostofsortingtherelation,inseconds,withb
b
=1andwith
b
b
= 100.
b. In each case, how many merge passes are required?
c. Supposea?ashstoragedeviceisusedinsteadofadisk,andithasa
seektime of 1 microsecond, and a transfer rate of 40 megabytes per
second. Recompute the cost of sortingthe relation,in seconds, with
b
b
=1andwithb
b
= 100, in thissetting.
12.11 Consider the following extended relational-algebra operators. Describe
how toimplementeach operationusing sorting, and using hashing.
a. Semijoin (    ): r     s is de?ned as null
R
(r     s), where Ris the set of
attributes in the schema of r; that it it selects those tuples r
i
in r for
which there isa tuple s
j
in s such that r
i
and s
j
satisfypredicate  .
b. Anti-semijoin(
¯
    ):r
¯
    sisde?nedasr ?null
R
(r     s);thatititselects
thosetuplesr
i
inr forwhichthereisnotuple s
j
in s suchthatr
i
and
s
j
satisfypredicate  .
12.12 Whyisitnotdesirabletoforceuserstomakeanexplicitchoiceofaquery-
processing strategy? Are there cases in which it is desirable for users to
be aware of the costs of competing query-processing strategies? Explain
your answer.
12.13 Design a variant of the hybrid merge-join algorithm for the case where
bothrelationsarenotphysicallysorted,butbothhaveasortedsecondary
indexon the joinattributes.
12.14 Estimate the number of block transfers and seeks required by your solu-
tiontoExercise12.13forr
1
  r
2
,wherer
1
andr
2
areasde?nedinPractice
Exercise12.3.
Bibliographical Notes 577
12.15 The hash-join algorithm as described in Section 12.5.5 computes the nat-
uraljoinoftworelations.Describehowtoextendthehash-joinalgorithm
to compute the natural left outer join, the natural right outer join and the
naturalfullouterjoin.(Hint:Keepextrainformationwitheachtupleinthe
hashindex,todetectwhetherany tupleintheproberelationmatchesthe
tuple in the hash index.) Try out your algorithm on the takes and student
relations.
12.16 Pipelining is used to avoid writing intermediate results to disk. Suppose
you need to sort relation r using sort–merge and merge-join the result
with an alreadysortedrelation s.
a. Describe how the output of the sort of r can be pipelined to the
mergejoin without being writtenback to disk.
b. The same idea is applicable even if both inputs to the merge join
are the outputs of sort–merge operations. However, the available
memory has to be shared between the two merge operations (the
merge-join algorithm itself needs very little memory). What is the
effect of having to share memory on the cost of each sort–merge
operation?
12.17 Write pseudocode for an iterator that implements a version of the sort
–merge algorithm where the result of the ?nal merge is pipelined to its
consumers.Yourpseudocodemustde?nethestandarditeratorfunctions
open(), next(), and close(). Show what state information the iterator must
maintain betweencalls.
12.18 Supposeyouhavetocompute
A
G
sum(C)
(r)aswellas
A,B
G
sum(C)
(r).Describe
how tocompute thesetogether using a single sorting of r.
BibliographicalNotes
Aqueryprocessormustparsestatementsinthequerylanguage,andmusttrans-
latethemintoaninternalform.Parsingofquerylanguagesdifferslittlefrompars-
ing of traditional programming languages. Most compiler texts cover the main
parsing techniques, and present optimization from a programming-language
point ofview.
GraefeandMcKenna[1993b]presentsanexcellentsurveyofquery-evaluation
techniques.
Knuth[1973]presentsanexcellentdescriptionofexternalsortingalgorithms,
includinganoptimizationcalledreplacementselection,whichcancreateinitialruns
that are (on the average) twice the size of memory. Nyberg et al. [1995] shows
thatduetopoorprocessor-cachebehavior,replacementselectionperformsworse
thanin-memoryquicksortforrungeneration,negatingthebene?tsofgenerating
longer runs. Nyberg et al. [1995] presents an ef?cient external sorting algorithm
that takesprocessorcache effectsintoaccount. Queryevaluationalgorithmsthat
578 Chapter 12 Query Processing
take cache effects into account have been extensively studied; see, for example,
Harizopoulos and Ailamaki[2004].
Accordingtoperformancestudiesconductedinthemid-1970s,databasesys-
tems of that period used only nested-loop join and merge join. These studies,
including Blasgen and Eswaran [1976], which was related to the development
of System R, determined that either the nested-loop join or merge join nearly
always provided the optimal join method. Hence, these two were the only join
algorithmsimplementedinSystemR.However,BlasgenandEswaran[1976]did
not include an analysis of hash-join algorithms. Today, hash joins are considered
to be highly ef?cient and widelyused.
Hash-join algorithms were initially developedfor parallel database systems.
Hybrid hash join is described in Shapiro [1986]. Zeller and Gray [1990] and
Davison and Graefe [1994] describe hash-join techniques that can adapt to the
availablememory,whichisimportantinsystemswheremultiplequeriesmaybe
running at the same time. Graefe et al. [1998] describes the use of hash joins and
hash teams, which allow pipelining of hash joins by using the same partitioning
for all hash joinsin a pipelinesequence,in the Microsoft SQL Server.
CHAPTER
13
Query Optimization
Queryoptimizationistheprocessofselectingthemostef?cientquery-evaluation
plan from among the many strategies usually possible for processing a given
query, especially if the query is complex. We do not expect users to write their
queries so that they can be processed ef?ciently. Rather, we expect the system to
construct a query-evaluation plan that minimizes the cost of query evaluation.
This iswherequeryoptimizationcomes into play.
One aspect of optimization occurs at the relational-algebra level, where the
system attempts to ?nd an expression that is equivalent to the given expression,
but more ef?cient to execute. Another aspect is selecting a detailed strategy for
processing the query, such as choosing the algorithm to use for executing an
operation,choosing the speci?c indicesto use,and so on.
The difference in cost (in terms of evaluation time) between a good strategy
and a bad strategy is often substantial, and may be several orders of magnitude.
Hence, it is worthwhile for the system to spend a substantial amount of time
on the selection of a good strategy for processing a query, even if the query is
executedonly once.
13.1 Overview
Consider the following relational-algebra expression, for the query “Find the
namesofallinstructorsintheMusicdepartmenttogetherwiththecoursetitleof
allthe coursesthat the instructors teach.”
null
name,title
(  dept name =“Music”
(instructor   (teaches   null
course id,title
(course))))
Notethattheprojectionofcourseon(course id,title)isrequiredsincecourseshares
anattributedept namewithinstructor;ifwedidnotremovethisattributeusingthe
projection, the above expression using natural joins would return only courses
from the Music department, even if some Music department instructors taught
courses inother departments.
The above expression constructs a large intermediate relation, instructor   teaches   null
course id,title
(course). However, we are interested in only a few tuples
579
580 Chapter13 QueryOptimization
name, title
dept_name = Music
course_id, title
instructor
instructor
teaches
teaches
course
course
?
?
?
?
name, title
course_id, title
?
?
dept_name = Music
(a) Initial expression tree                            (b) Transformed expression tree
Figure13.1 Equivalent expressions.
of this relation (those pertaining to instructors in the Music department), and in
only two of the ten attributes of this relation. Since we are concerned with only
those tuples in the instructor relation that pertain to the Music department, we
do not need to consider those tuples that do not have dept name = “Music”.By
reducing the number of tuples of the instructor relation that we need to access,
we reduce the size of the intermediate result. Our query is now represented by
the relational-algebraexpression:
null
name,title
((  dept name =“Music”
(instructor))  (teaches   null
course id,title
(course)))
which is equivalent to our original algebra expression, but which generates
smaller intermediate relations. Figure 13.1 depicts the initial and transformed
expressions.
An evaluation plan de?nes exactly what algorithm should be used for each
operation, and how the execution of the operations should be coordinated. Fig-
ure 13.2 illustrates one possible evaluation plan for the expression from Fig-
ure 13.1(b). As we have seen, several different algorithms can be used for each
relationaloperation,givingrisetoalternativeevaluationplans.Inthe?gure,hash
join has been chosen for one of the join operations, while the other uses merge
join, after sorting the relations on the join attribute, which is ID.Whereedges
are marked as pipelined, the output of the producer is pipelined directly to the
consumer, without being writtenout todisk.
Given a relational-algebra expression, it is the job of the query optimizer to
comeupwithaquery-evaluationplanthatcomputesthesameresultasthegiven
expression,and is the least-costly way of generating the result (or, at least, is not
much costlierthan the least-costlyway).
To ?nd the least-costly query-evaluation plan, the optimizer needs to gener-
atealternativeplansthatproducethesameresultasthegivenexpression,andto
choose the least-costly one. Generation of query-evaluation plans involves three
steps: (1) generating expressions that are logically equivalent to the given ex-
13.1 Overview 581
?
name, title
instructor
?
dept_name = Music
sort
ID
teaches
(sort to remove duplicates)
(hash join)
(merge join)
pipeline
(use index 1)
pipeline
course
?
course_id, title
sort
ID
pipeline
pipeline
Figure13.2 An evaluation plan.
pression,(2)annotating theresultantexpressionsinalternativeways togenerate
alternativequery-evaluationplans,and(3)estimatingthecostofeachevaluation
plan, and choosing the one whose estimatedcost isthe least.
Steps (1), (2), and (3) are interleaved in the query optimizer—some expres-
sions are generated, and annotated to generate evaluation plans, then further
expressions are generated and annotated, and so on. As evaluation plans are
generated, their costs are estimated by using statistical information about the
relations,such as relationsizesandindexdepths.
To implement the ?rst step, the query optimizer must generate expressions
equivalenttoagivenexpression.Itdoessobymeansofequivalencerulesthatspec-
ify how to transform an expression into a logically equivalent one. We describe
theserulesinSection13.2.
InSection13.3wedescribehowtoestimatestatisticsoftheresultsofeachop-
erationinaqueryplan.UsingthesestatisticswiththecostformulaeinChapter12
allows us to estimate the costs of individual operations. The individualcosts are
combinedtodeterminetheestimatedcostofevaluatingagivenrelational-algebra
expression,as outlinedearlierinSection12.7.
In Section 13.4, we describe how to choose a query-evaluation plan. We can
chooseonebasedontheestimatedcostoftheplans.Sincethecostisanestimate,
the selected plan is not necessarily the least-costly plan; however, as long as the
estimatesaregood,theplanislikelytobetheleast-costlyone,ornotmuchmore
costly than it.
Finally,materializedviewshelptospeedupprocessingofcertainqueries.In
Section 13.5, we study how to “maintain” materialized views—that is, to keep
them up-to-date—and how to perform query optimization with materialized
views.
582 Chapter13 QueryOptimization
VIEWINGQUERYEVALUATIONPLANS
Most database systems provide a way to view the evaluation plan chosen to
executeagivenquery.ItisusuallybesttousetheGUIprovidedwiththedatabase
systemtoviewevaluationplans.However,ifyouuseacommandlineinterface,
many databases support variations of a command “explain <query>”,which
displays the execution plan chosen for the speci?ed query <query>.Theexact
syntax varieswith different databases:
• PostgreSQL usesthesyntax shownabove.
• Oracleusesthesyntaxexplainplanfor.However,thecommandstoresthe
resultantplaninatablecalledplan table,insteadofdisplayingit.Thequery
“select*fromtable(dbms xplan.display);”displays the storedplan.
• DB2followsasimilarapproachtoOracle,butrequirestheprogramdb2exfmt
tobeexecutedto display thestoredplan.
• SQL Server requires the command set showplan text on to be executed
before submitting the query; then, when a query is submitted, instead of
executingthequery,theevaluationplan isdisplayed.
The estimated costs for the plan are also displayed along with the plan. It is
worth noting that the costs are usually not in any externally meaningful unit,
suchassecondsorI/Ooperations,butratherinunitsofwhatevercostmodelthe
optimizer uses. Some optimizers such as PostgreSQL display two cost-estimate
numbers;the?rstindicatestheestimatedcostforoutputtingthe?rstresult,and
thesecondindicatestheestimatedcostforoutputting all results.
13.2 TransformationofRelationalExpressions
A query can be expressed in several different ways, with different costs of eval-
uation. In this section, rather than take the relational expression as given, we
consideralternative,equivalentexpressions.
Tworelational-algebraexpressionsaresaidtobeequivalentif,oneverylegal
databaseinstance,thetwoexpressionsgeneratethesamesetoftuples.(Recallthat
alegaldatabaseinstanceisonethatsatis?esalltheintegrityconstraintsspeci?ed
in the database schema.) Note that the order of the tuples is irrelevant; the two
expressionsmaygeneratethetuplesindifferentorders,butwouldbeconsidered
equivalentas long asthe setoftuplesisthe same.
InSQL,theinputsandoutputsaremultisetsoftuples,andthemultisetversion
of the relational algebra (describedin the box in page 238) is usedfor evaluating
SQL queries. Two expressions in the multiset version of the relational algebra are
said to be equivalentif on every legal database the two expressions generate the
same multiset of tuples. The discussion in this chapter is based on the relational
13.2 TransformationofRelationalExpressions 583
?
E
1
E
2
?
E
2
E
1
Rule 5
E
3
E
1
E
2
E
2
E
3
E
1
Rule 6.a
Rule 7.a
If    only has
attributes from E1
E
1
E
2
E
1
E
2
?
?
?
?
?
Figure13.3 Pictorial representation of equivalences.
algebra. We leave extensions to the multiset version of the relational algebra to
you as exercises.
13.2.1 EquivalenceRules
An equivalence rule says that expressions of two forms are equivalent. We can
replaceanexpressionofthe?rstformbyanexpressionofthesecondform,orvice
versa—thatis,wecanreplaceanexpressionofthesecondformbyanexpression
of the ?rst form—since the two expressions generate the same result on any
valid database. The optimizer uses equivalence rules to transform expressions
into otherlogicallyequivalentexpressions.
We now list a number of general equivalence rules on relational-algebra
expressions.SomeoftheequivalenceslistedappearinFigure13.3.Weuse  ,  1
,  2
,
and so on to denote predicates, L
1
, L
2
, L
3
, and so on to denote lists of attributes,
and E,E
1
,E
2
, and so on to denote relational-algebra expressions. A relation
namer issimplyaspecialcaseofarelational-algebraexpression,andcanbeused
wherever E appears.
1. Conjunctive selection operations can be deconstructed into a sequence of
individualselections.Thistransformationis referredtoas a cascadeof  .
    1
?  2
(E) =     1
(    2
(E))
584 Chapter13 QueryOptimization
2. Selectionoperationsarecommutative.
    1
(    2
(E)) =    2
(    1
(E))
3. Onlythe?naloperationsinasequenceofprojectionoperationsareneeded;
the others can be omitted. This transformation can also be referred to as a
cascade of null.
null
L
1
(null
L
2
(...(null
L
n
(E))...)) = null
L
1
(E)
4. Selectionscan be combined with Cartesianproductsand thetajoins.
a.     (E
1
× E
2
) = E
1
    E
2
This expressionisjust thede?nitionofthe thetajoin.
b.     1
(E
1
    2
E
2
) = E
1
    1
?  2
E
2
5. Theta-joinoperationsarecommutative.
E
1
    E
2
= E
2
    E
1
Actually,theorderofattributesdiffersbetweentheleft-handsideandright-
handside,sotheequivalencedoesnotholdiftheorderofattributesistaken
into account. Aprojectionoperationcan be addedto one of the sidesof the
equivalence to appropriately reorder attributes, but for simplicity we omit
the projectionand ignoretheattributeorderinmost of our examples.
Recall that the natural-join operator is simply a special case of the
theta-joinoperator;hence,natural joins arealsocommutative.
6. a. Natural-join operationsareassociative.
(E
1
  E
2
)  E
3
= E
1
  (E
2
  E
3
)
b. Theta joinsare associative in the following manner:
(E
1
    1
E
2
)    2
?  3
E
3
= E
1
    1
?  3
(E
2
    2
E
3
)
where  2
involvesattributesfromonly E
2
and E
3
.Anyofthesecondi-
tions may be empty; hence, it follows that the Cartesian product (×)
operation is also associative. The commutativity and associativity of
joinoperationsareimportantforjoinreorderinginqueryoptimization.
7. The selection operation distributes over the theta-join operation under the
following two conditions:
a. It distributes when all the attributes in selection condition   0
involve
only the attributesof one of theexpressions(say, E
1
)being joined.
    0
(E
1
    E
2
) = (    0
(E
1
))    E
2
13.2 TransformationofRelationalExpressions 585
b. It distributes when selection condition   1
involves only the attributes
of E
1
and  2
involvesonly theattributesof E
2
.
    1
?  2
(E
1
    E
2
) = (    1
(E
1
))    (    2
(E
2
))
8. Theprojectionoperationdistributesoverthetheta-joinoperationunderthe
following conditions.
a. Let L
1
and L
2
be attributes of E
1
and E
2
, respectively. Suppose that
thejoin condition  involvesonly attributesin L
1
? L
2
. Then,
null
L
1
?L
2
(E
1
    E
2
) = (null
L
1
(E
1
))    (null
L
2
(E
2
))
b. Consider a join E
1
    E
2
.LetL
1
and L
2
be sets of attributes from E
1
and E
2
,respectively.Let L
3
beattributesof E
1
thatareinvolvedinjoin
condition   ,butarenotinL
1
? L
2
,andletL
4
be attributes of E
2
that
areinvolvedinjoincondition  ,butarenotinL
1
? L
2
. Then,
null
L
1
?L
2
(E
1
    E
2
) = null
L
1
?L
2
((null
L
1
?L
3
(E
1
))    (null
L
2
?L
4
(E
2
)))
9. The setoperationsunionand intersectionarecommutative.
E
1
? E
2
= E
2
? E
1
E
1
? E
2
= E
2
? E
1
Setdifferenceisnot commutative.
10. Setunionand intersectionareassociative.
(E
1
? E
2
) ? E
3
= E
1
? (E
2
? E
3
)
(E
1
? E
2
) ? E
3
= E
1
? (E
2
? E
3
)
11. The selection operation distributes over the union, intersection, and set-
differenceoperations.
  P
(E
1
? E
2
) =   P
(E
1
) ?   P
(E
2
)
Similarly, the preceding equivalence, with ? replaced with either ? or ?,
also holds.Further:
  P
(E
1
? E
2
) =   P
(E
1
) ? E
2
The preceding equivalence, with ? replaced by ?,alsoholds,butdoesnot
hold if ? isreplacedby ?.
12. The projectionoperationdistributesovertheunion operation.
null
L
(E
1
? E
2
) = (null
L
(E
1
)) ?(null
L
(E
2
))
586 Chapter13 QueryOptimization
This is only a partial list of equivalences. More equivalences involving ex-
tendedrelationaloperators,suchastheouterjoinandaggregation,arediscussed
intheexercises.
13.2.2 ExamplesofTransformations
Wenowillustratetheuseoftheequivalencerules.Weuseouruniversityexample
withthe relationschemas:
instructor(ID,name,dept name,salary)
teaches(ID,course id,sec id,semester,year)
course(course id,title,dept name,credits)
Inour exampleinSection13.1,the expression:
null
name,title
(  dept name =“Music”
(instructor   (teaches   null
course id,title
(course))))
was transformedinto thefollowing expression:
null
name,title
((  dept name =“Music”
(instructor))  (teaches   null
course id,title
(course)))
which is equivalent to our original algebra expression, but generates smaller
intermediate relations. We can carry out this transformation by using rule 7.a.
Remember that the rule merely says that the two expressions are equivalent; it
doesnot say that one is betterthan the other.
Multiple equivalence rules can be used, one after the other, on a query or on
partsof the query.Asanillustration,supposethat we modifyouroriginal query
to restrict attention to instructors who have taught a course in 2009. The new
relational-algebraqueryis:
null
name,title
(  dept name =“Music” ?year = 2009
(instructor   (teaches   null
course id,title
(course))))
Wecannotapplytheselectionpredicatedirectlytotheinstructorrelation,sincethe
predicateinvolvesattributesofboththeinstructorandteachesrelations.However,
we can ?rst apply rule 6.a (associativity of natural join) to transform the join
instructor   (teaches   null
course id,title
(course)) into (instructor   teaches)   null
course id,title
(course):
null
name,title
(  dept name =“Music” ?year = 2009
((instructor   teaches)  null
course id,title
(course)))
Then, usingrule 7.a,we can rewriteour queryas:
13.2 TransformationofRelationalExpressions 587
null
name,title
((  dept name =“Music” ?year = 2009
(instructor   teaches))  null
course id,title
(course))
Let us examine the selection subexpression within this expression. Using
rule1, we can break the selection into two selections, to getthe following subex-
pression:
  dept name =“Music”
(  year = 2009
(instructor   teaches))
Both of the preceding expressions select tuples with dept name = “Music”
and course id = 2009. However, the latter form of the expression provides a new
opportunitytoapplyRule7.a(“performselectionsearly”),resultinginthesubex-
pression:
  dept name =“Music”
(instructor)    year = 2009
(teaches)
Figure 13.4 depicts the initial expression and the ?nal expression after all
these transformations. We could equally well have used rule 7.b to get the ?nal
expressiondirectly,withoutusingrule1tobreaktheselectionintotwoselections.
Infact,rule7.bcanitselfbederivedfromrules1and7.a.
Asetofequivalencerulesissaidtobeminimalifnorulecanbederivedfrom
any combination of the others. The preceding example illustrates that the set of
equivalencerulesinSection13.2.1isnotminimal.Anexpressionequivalenttothe
original expression may be generated in different ways; the number of different
ways of generating an expression increases when we use a nonminimal set of
equivalence rules. Query optimizers therefore use minimal sets of equivalence
rules.
Now considerthe following formof our examplequery:
name, title
name, title
course_id, title
?
?
dept_name = Music
? year = 2009
?
instructor
teaches
course
course_id, title
?
?
dept_name = Music year = 2009
? ?
instructor teaches course
(a) Initial expression tree (b) Tree after multiple transformations
Figure13.4 Multiple transformations.
588 Chapter13 QueryOptimization
null
name,title
((  dept name =“Music”
(instructor)  teaches)  null
course id,title
(course))
When we compute the subexpression:
(  dept name =“Music”
(instructor)  teaches)
we obtain a relationwhose schema is:
(ID,name,dept name,salary,course id,sec id,semester,year)
Wecaneliminateseveralattributesfromtheschemabypushingprojectionsbased
onequivalencerules8.aand8.b.Theonlyattributesthatwemustretainarethose
that either appear in the result of the query or are needed to process subsequent
operations.Byeliminatingunneededattributes,wereducethenumberofcolumns
of the intermediateresult.Thus, we reducethe size of the intermediateresult.In
our example, the only attributes we need from the join of instructor and teaches
arename andcourse id. Therefore, we can modify the expression to:
null
name,title
((null
name,course id
((  dept name =“Music”
(instructor))  teaches)
  null
course id,title
(course))
The projection null
name,course id
reducesthe size of the intermediatejoin results.
13.2.3 JoinOrdering
Agoodorderingofjoinoperationsisimportantforreducingthesizeoftemporary
results; hence, most query optimizers pay a lot of attention to the join order. As
mentioned in Chapter6 and in equivalencerule 6.a, the natural-join operation is
associative.Thus, for allrelationsr
1
, r
2
,andr
3
:
(r
1
  r
2
)  r
3
= r
1
  (r
2
  r
3
)
Although these expressions are equivalent, the costs of computing them may
differ.Consideragain theexpression:
null
name,title
((  dept name =“Music”
(instructor))  teaches   null
course id,title
(course))
We could choose to compute teaches   null
course id,title
(course)?rst,andthentojoin
the resultwith:
  dept name =“Music”
(instructor)
However, teaches   null
course id,title
(course) is likely to be a large relation, since
itcontains one tuplefor everycourse taught.Incontrast:
  dept name =“Music”
(instructor)  teaches
13.2 TransformationofRelationalExpressions 589
is probably a small relation. To see that it is, we note that, since a university
has a large number of departments, it is likely that only a small fraction of
the university instructors are associated with the Music department. Thus, the
precedingexpressionresultsin one tuple for each course taught by an instructor
intheMusicdepartment.Therefore,thetemporaryrelationthatwemuststoreis
smallerthanitwouldhavebeenhadwecomputedteaches   null
course id,title
(course)
?rst.
There are other options to consider for evaluating our query. We do not care
about the orderinwhich attributesappearin ajoin, since itiseasy tochange the
orderbeforedisplayingtheresult.Thus, for allrelationsr
1
andr
2
:
r
1
  r
2
= r
2
  r
1
That is,natural joiniscommutative(equivalencerule5).
Using the associativityand commutativity of the natural join (rules5 and 6),
considerthe following relational-algebraexpression:
(instructor   null
course id,title
(course))  teaches
Note that there are no attributes in common between null
course id,title
(course)and
instructor, so the join is just a Cartesian product. If there are a tuples ininstructor
andbtuplesin null
course id,title
(course),thisCartesianproductgeneratesa ?b tuples,
one for every possible pair of instructor tuple and course (without regard for
whethertheinstructortaughtthecourse).ThisCartesianproductwouldproduce
a very large temporary relation. However, if the user had entered the preceding
expression, we could use the associativity and commutativity of the natural join
to transform thisexpressionto themoreef?cient expression:
(instructor   teaches)  null
course id,title
(course)
13.2.4 EnumerationofEquivalentExpressions
Query optimizers use equivalence rules to systematically generate expressions
equivalent to the given query expression. Conceptually, this can be done as out-
linedinFigure13.5.Theprocessproceedsasfollows.GivenaqueryexpressionE,
the setof equivalentexpressionsEQinitiallycontains only E.Now,eachexpres-
sioninEQismatchedwitheachequivalencerule.Ifanexpression,say E
i
,ofany
subexpression e
i
of E
i
(which could, as a special case, be E
i
itself) matches one
sideofanequivalencerule,theoptimizergeneratesanewexpressionwheree
i
is
transformedtomatchtheothersideoftherule.Theresultantexpressionisadded
toEQ.This processcontinues untilno more new expressionscan be generated.
The preceding process is extremely costly both in space and in time, but
optimizerscan greatlyreduceboththe spaceand timecost, using two keyideas.
1. If we generate an expression E
null
from an expression E
1
by using an equiva-
lenceruleonsubexpressione
i
,thenE
null
and E
1
haveidenticalsubexpressions
590 Chapter13 QueryOptimization
proceduregenAllEquivalent(E)
EQ= {E}
repeat
Match each expression E
i
inEQwitheachequivalencerule R
j
if any subexpressione
i
of E
i
matches one sideof R
j
Createanewexpression E
null
which isidenticalto E
i
,exceptthat
e
i
istransformedto matchthe othersideof R
j
Add E
null
toEQifitisnot alreadypresentinEQ
untilno new expressioncan be addedtoEQ
Figure13.5 Procedure to generate all equivalent expressions.
exceptfore
i
anditstransformation.Evene
i
anditstransformedversionusu-
ally share many identical subexpressions. Expression-representation tech-
niques that allow both expressions to point to shared subexpressions can
reduce the space requirementsigni?cantly.
2. Itisnotalwaysnecessarytogenerateeveryexpressionthatcanbegenerated
withtheequivalencerules.Ifanoptimizertakescostestimatesofevaluation
into account, itmaybeabletoavoidexaminingsomeof theexpressions,as
weshallseeinSection13.4.Wecanreducethetimerequiredforoptimization
by using techniquessuch as these.
We revisittheseissuesinSection13.4.2.
13.3 EstimatingStatisticsofExpressionResults
The cost of an operation depends on the size and other statistics of its inputs.
Given an expression such as a   (b   c) to estimate the cost of joining a with
(b  c), weneedto haveestimatesof statisticssuch as thesizeofb  c.
In this section, we ?rst list some statistics about database relations that are
stored in database-system catalogs, and then show how to use the statistics to
estimatestatisticsonthe resultsof variousrelationaloperations.
One thing that will become clear later in this section is that the estimates
are not very accurate, since they are based on assumptions that may not hold
exactly. A query-evaluation plan that has the lowest estimated execution cost
may therefore not actually have the lowest actual execution cost. However, real-
worldexperiencehasshownthatevenifestimatesarenotprecise,theplanswith
the lowest estimated costs usually have actual execution costs that are either the
lowestactual execution costs,or arecloseto thelowestactual execution costs.
13.3.1 CatalogInformation
The database-system catalog stores the following statistical information about
databaserelations:
13.3 EstimatingStatisticsofExpressionResults 591
• n
r
, thenumber of tuplesintherelationr.
• b
r
, the number of blocks containing tuplesofrelationr.
• l
r
,the size of a tupleof relationr inbytes.
• f
r
, the blocking factor of relationr—that is,the number of tuplesof relation
r that ?t into one block.
• V(A,r),thenumberofdistinctvaluesthatappearintherelationrforattribute
A.Thisvalueisthesameasthesizeof null
A
(r).If Aisakeyforrelationr,V(A,r)
isn
r
.
The last statistic, V(A,r), can also be maintained for sets of attributes, if desired,
insteadofjustforindividualattributes.Thus,givenasetofattributes,A, V(A,r)
isthe sizeof null
A
(r).
If we assume that the tuples of relation r are stored together physically in a
?le,the following equation holds:
b
r
=
  n
r
f
r
  Statisticsaboutindices,such astheheightsofB
+
-treeindicesandnumberofleaf
pagesin the indices, are alsomaintained in the catalog.
Ifwewishtomaintainaccuratestatistics,then,everytimearelationismodi-
?ed, we must also update the statistics. This update incurs a substantial amount
of overhead.Therefore,most systemsdo not update the statistics on everymod-
i?cation. Instead, they update the statistics during periods of light system load.
As a result, the statistics used for choosing a query-processing strategy may not
be completely accurate. However, if not too many updates occur in the intervals
between the updates of the statistics, the statistics will be suf?ciently accurate to
provideagood estimationof therelativecosts of thedifferentplans.
The statistical information noted here is simpli?ed. Real-world optimizers
oftenmaintainfurtherstatisticalinformationtoimprovetheaccuracyoftheircost
estimatesofevaluationplans.Forinstance,mostdatabasesstorethedistribution
of values for each attribute as a histogram: in a histogram the values for the
attributearedividedintoanumberofranges,andwitheachrangethehistogram
associatesthenumberoftupleswhoseattributevalueliesinthatrange.Figure13.6
showsanexampleofahistogramforaninteger-valuedattributethattakesvalues
in the range 1 to25.
Histograms used in database systems usually record the number of distinct
valuesineachrange,inadditiontothenumberoftupleswithattributevaluesin
that range.
Asanexampleofahistogram,therangeofvaluesforanattributeageofare-
lationpersoncouldbedividedinto0–9,10–19,...,90–99(assumingamaximum
ageof99).Witheachrangewestoreacountofthenumberofpersontupleswhose
age values lie in that range, and the number of distinct age values that lie in that
592 Chapter13 QueryOptimization
value
frequency
50
40
30
20
10
1–5 6–10 11–15 16–20 21–25
Figure13.6 Example of histogram.
range.Withoutsuchhistograminformation,anoptimizerwouldhavetoassume
that the distributionof valuesisuniform; that is,each range has the same count.
Ahistogramtakesuponlyalittlespace,sohistogramsonseveraldifferentat-
tributescanbestoredinthesystemcatalog.Thereareseveraltypesofhistograms
used in database systems. For example, an equi-width histogram divides the
range of values into equal-sized ranges, whereas an equi-depth histogram ad-
justs the boundaries of the ranges such that each range has the same number of
values.
13.3.2 SelectionSizeEstimation
The size estimate of the result of a selection operation depends on the selection
predicate.We ?rst considera singleequalitypredicate,thena singlecomparison
predicate,and?nally combinations ofpredicates.
•   A=a
(r): If we assume uniform distribution of values (that is, each value ap-
pears with equal probability), the selection result can be estimated to have
n
r
/V(A,r) tuples, assuming that the value a appears in attribute Aof some
record ofr.Theassumptionthatthevaluea intheselectionappearsinsome
recordisgenerallytrue,andcostestimatesoftenmakeitimplicitly.However,
it is often not realistic to assume that each value appears with equal prob-
ability. The course id attribute in the takes relation is an example where the
assumptionisnotvalid.Itisreasonabletoexpectthatapopularundergradu-
atecoursewillhavemanymorestudentsthanasmallerspecializedgraduate
course. Therefore, certain course id values appear with greater probability
than do others. Despite the fact that the uniform-distribution assumption is
often not correct, it is a reasonable approximation of reality in many cases,
and ithelpsus tokeepour presentationrelativelysimple.
If a histogram is available on attribute A, we can locate the range that
contains the value a, and modify the above-mentioned estimate n
r
/V(A,r)
13.3 EstimatingStatisticsofExpressionResults 593
COMPUTINGANDMAINTAININGSTATISTICS
Conceptually, statistics on relations can be thought of as materialized views,
whichshouldbeautomaticallymaintainedwhenrelationsaremodi?ed.Unfor-
tunately, keeping statistics up-to-date on every insert, delete or update to the
databasecanbeveryexpensive.Ontheotherhand,optimizersgenerallydonot
need exact statistics: an error of a few percent may result in a plan that is not
quite optimal being chosen, but the alternative plan chosen is likely to have a
cost which is within a few percent of the optimal cost. Thus, it is acceptable to
havestatistics that areapproximate.
Databasesystemsreducethecostofgeneratingandmaintainingstatistics,as
outlinedbelow,byexploitingthefact thatstatistics canbeapproximate.
• Statisticsareoftencomputedfromasampleoftheunderlyingdata,instead
of examining the entire collection of data. For example, a fairly accurate
histogram can be computed from a sample of a few thousand tuples, even
onarelationthathasmillions,orhundredsofmillionsofrecords.However,
thesampleusedmustbearandom sample; a sample that is not random
may have an excessive representation of one part of the relation, and can
give misleading results. For example, if we used a sample of instructors to
compute a histogram on salaries, if the sample has an overrepresentation
of lower-paid instructors the histogram would result in wrong estimates.
Databasesystemstodayroutinelyuserandomsamplingtocreatestatistics.
Seethebibliographicnotesforreferencesonsampling.
• Statistics are not maintained on every update to the database. In fact,
some database systems never update statistics automatically. They rely on
database administrators periodically running a command to update statis-
tics. Oracle and PostgreSQL provide an SQL command called analyze that
generates statistics on speci?ed relations, or on all relations. IBM DB2 sup-
ports an equivalent command calledrunstats. See the system manuals for
details. You should be aware that optimizers sometimes choose very bad
plans due to incorrect statistics. Many database systems, such as IBM DB2,
Oracle, and SQL Server, update statistics automatically at certain points of
time. For example, the system can keep approximate track of how many
tuplesthereareinarelationandrecomputestatisticsifthisnumberchanges
signi?cantly. Another approach is to compare estimated cardinalities of a
relationscanwithactualcardinalitieswhenaqueryisexecuted,andifthey
differ signi?cantly, initiate anupdate ofstatistics for thatrelation.
byusing the frequencycount forthatrange insteadofn
r
,andthenumberof
distinctvaluesthat occurs in that range instead of V(A,r).
•   A?v
(r): Consider a selection of the form   A?v
(r). If the actual value used
in the comparison (v) is available at the time of cost estimation, a more
594 Chapter13 QueryOptimization
accurateestimatecanbemade.Thelowestandhighestvalues(min(A,r)and
max(A,r))fortheattributecanbestoredinthecatalog.Assumingthatvalues
are uniformly distributed, we can estimate the number of records that will
satisfythe condition A ? v as0 ifv<min(A,r), asn
r
if v ? max(A,r),and:
n
r
·
v ?min(A,r)
max(A,r) ?min(A,r)
otherwise.
If a histogram is available on attribute A, we can get a more accurate
estimate; we leave the details as an exercise for you. In some cases, such as
whenthequeryispartofastoredprocedure,thevalue vmaynotbeavailable
when the query is optimized. In such cases, we assume that approximately
one-halftherecordswillsatisfythecomparisoncondition.Thatis,weassume
theresulthasn
r
/2tuples;theestimatemaybeveryinaccurate,butisthebest
we can dowithout any furtherinformation.
• Complexselections:
?
Conjunction:Aconjunctive selection is aselectionof theform:
    1
?  2
?···?  n
(r)
Wecanestimatetheresultsizeofsuchaselection:Foreach  i
,weestimate
thesizeoftheselection    i
(r),denotedbys
i
,asdescribedpreviously.Thus,
theprobabilitythatatupleintherelationsatis?esselectioncondition  i
is
s
i
/n
r
.
Theprecedingprobabilityiscalledtheselectivityoftheselection    i
(r).
Assumingthattheconditionsareindependentofeachother,theprobability
that a tuple satis?es all the conditions is simply the product of all these
probabilities.Thus, we estimatethe number of tuplesin the full selection
as:
n
r
?
s
1
?s
2
?···?s
n
n
n
r
?
Disjunction:Adisjunctiveselection isa selectionof theform:
    1
?  2
?···?  n
(r)
A disjunctive condition is satis?ed by the union of all records satisfying
the individual,simpleconditions  i
.
Asbefore,lets
i
/n
r
denotetheprobabilitythatatuplesatis?escondition
  i
.Theprobabilitythatthetuplewillsatisfythedisjunctionisthen1minus
the probabilitythat itwillsatisfynone of the conditions:
1 ?(1 ?
s
1
n
r
) ?(1 ?
s
2
n
r
)?···?(1 ?
s
n
n
r
)
Multiplyingthisvaluebyn
r
givesustheestimatednumberoftuplesthat
satisfy theselection.
13.3 EstimatingStatisticsofExpressionResults 595
?
Negation:Intheabsenceofnulls,theresultofaselection  ¬  (r)issimply
the tuples of r that are not in     (r). We already know how to estimate
the number of tuples in    (r). The number of tuples in  ¬  (r) is therefore
estimatedtoben(r)minus the estimatednumber of tuplesin    (r).
Wecanaccount fornullsbyestimatingthenumberoftuplesforwhich
the condition  would evaluatetounknown, and subtracting that number
from the above estimate, ignoring nulls. Estimating that number would
requireextrastatisticstobe maintainedinthecatalog.
13.3.3 JoinSizeEstimation
In thissection, weseehow to estimatethesizeof the resultof a join.
TheCartesianproductr ×s containsn
r
?n
s
tuples.Eachtupleofr ×s occupies
l
r
+l
s
bytes,from which we can calculate the size of the Cartesianproduct.
Estimating the size of a natural join is somewhat more complicated than
estimating the size of a selection or of a Cartesian product. Let r(R)ands(S)be
relations.
• If R ? S =?—thatis,therelationshavenoattributeincommon—thenr   s
is the sameasr × s, and we can use our estimationtechnique for Cartesian
products.
• If R ? Sis akey for R, then we know that a tuple of s will joinwith at most
one tuple fromr. Therefore, the number of tuples inr   s is no greater than
the number of tuples ins. The case where R ? Sis a key for Sis symmetric
tothecasejustdescribed.If R?Sformsaforeignkeyof S,referencing R,the
number of tuplesinr   s is exactlythesameas thenumber of tuplesins.
• The most dif?cult case is when R ? S is a key for neither R nor S.Inthis
case,weassume,aswedidforselections,thateachvalueappearswithequal
probability. Consider a tuple t ofr, and assume R ? S ={A}.Weestimate
that tuplet produces:
n
s
V(A,s)
tuplesinr   s,sincethisnumberistheaveragenumberoftuplesins witha
given value for the attributes A. Considering all the tuples inr,weestimate
that thereare:
n
r
?n
s
V(A,s)
tuplesinr   s.Observethat,ifwereversetherolesofr ands inthepreceding
estimate,we obtainan estimateof:
n
r
?n
s
V(A,r)
596 Chapter13 QueryOptimization
tuplesinr   s.ThesetwoestimatesdifferifV(A,r) nullV(A,s).Ifthissituation
occurs, there are likely to be dangling tuples that do not participate in the
join.Thus,thelowerofthetwoestimatesisprobablythemoreaccurateone.
The preceding estimate of join size may be too high if the V(A,r)values
for attribute A in r havefewvaluesincommonwiththeV(A,s)valuesfor
attribute A in s. However, this situation is unlikely to happen in the real
world, since dangling tuples either do not exist or constitute only a small
fractionofthetuples,inmostreal-worldrelations.
More important, the preceding estimate dependson the assumption that
each value appears with equal probability. More sophisticated techniques
for size estimation have to be used if this assumption does not hold. For
example, if we have histograms on the join attributes of both relations, and
bothhistogramshavethesameranges,thenwecanusetheaboveestimation
technique within each range, using the number of rows with values in the
range instead of n
r
or n
s
, and the number of distinct values in that range,
instead of V(A,r)orV(A,s). We then add up the size estimates obtained
for each range to get the overall size estimate. We leave the case where
bothrelationshavehistogramsonthejoinattribute,butthehistogramshave
differentranges,asanexerciseforyou.
Wecanestimatethesizeofathetajoinr     s byrewritingthejoinas    (r ×s),
andusingthesizeestimatesforCartesianproductsalongwiththesizeestimates
forselections,whichwesawinSection13.3.2.
Toillustrateallthesewaysofestimatingjoinsizes,considertheexpression:
student   takes
Assumethefollowingcataloginformationaboutthetworelations:
  n
student
= 5000.
  f
student
= 50,which impliesthat b
student
= 5000/50 = 100.
  n
takes
= 10000.
  f
takes
= 25,whichimpliesthat b
takes
= 10000/25 = 400.
  V(ID,takes) = 2500, which implies that only half the students have taken
any course (this is unrealistic, but we use it to show that our size estimates
are correct even in this case), and on average, each student who has taken a
coursehastakenfourcourses.
TheattributeIDintakesisaforeignkeyonstudent,andnullvaluesdonotoccurin
takes.ID,sinceIDispartoftheprimarykeyoftakes;thus,thesizeofstudent   takes
isexactly n
takes
,whichis10000.
Wenowcomputethesizeestimatesfor student   takes withoutusing infor-
mationaboutforeignkeys.Since V(ID,takes) = 2500and V(ID,student) = 5000,
thetwoestimateswegetare5000 ?10000/2500 = 20000and5000 ?10000/5000 =
10000, and we choose the lower one. In this case, the lower of these estimates is
thesameasthatwhichwecomputedearlierfrominformationaboutforeignkeys.
13.3 EstimatingStatisticsofExpressionResults 597
13.3.4 SizeEstimationforOtherOperations
Weoutlinebelowhowtoestimatethesizesoftheresultsofotherrelational-algebra
operations.
• Projection: The estimated size (number of records or number of tuples) of a
projectionoftheform null
A
(r)isV(A,r),sinceprojectioneliminatesduplicates.
• Aggregation:Thesizeof
A
G
F
(r)issimplyV(A,r), since there is one tuple in
A
G
F
(r) for each distinctvalueof A.
• Setoperations:Ifthetwoinputstoasetoperationareselectionsonthesame
relation, we can rewrite the set operation as disjunctions, conjunctions, or
negations. For example,     1
(r) ?     2
(r) can be rewritten as     1
?  2
(r). Simi-
larly, we can rewrite intersections as conjunctions, and we can rewrite set
difference by using negation, so long as the two relations participating in
the set operations are selections on the same relation. We can then use the
estimatesforselectionsinvolvingconjunctions,disjunctions,andnegationin
Section13.3.2.
If the inputs are not selections on the same relation, we estimate the sizes
this way: The estimated size of r ?s is the sum of the sizes of r and s.The
estimated size ofr ?s is the minimum of the sizes ofr and s. The estimated
size ofr ?s is the same size asr. All three estimates may be inaccurate, but
provideupperbounds on thesizes.
• Outer join: The estimated size of r   s is the size of r   splusthesizeof
r;thatofr   s is symmetric, while that of r   s is the size of r   s plus
thesizesofr ands.Allthreeestimatesmaybeinaccurate,butprovideupper
bounds on the sizes.
13.3.5 EstimationofNumberofDistinctValues
For selections, the number of distinct values of an attribute (or set of attributes)
Aintheresultofa selection, V(A,    (r)),canbe estimatedintheseways:
• If the selectioncondition  forces Ato take on a speci?edvalue (e.g., A = 3),
V(A,    (r)) = 1.
• If   forces Ato take on one of a speci?ed set of values (e.g., (A = 1 ? A =
3 ? A = 4)),then V(A,    (r))is settothe number ofspeci?edvalues.
• If the selection condition   is of the form Aopv,whereop is a comparison
operator, V(A,    (r)) is estimated to be V(A,r) ?s,wheres is the selectivity
of theselection.
• Inallothercasesofselections,weassumethatthedistributionof Avaluesis
independent of the distribution of the values on which selection conditions
are speci?ed, and we use an approximate estimate of min(V(A,r),n
    (r)
). A
moreaccurate estimatecan be derivedfor this case using probability theory,
but theabove approximationworks fairlywell.
598 Chapter13 QueryOptimization
For joins, the number of distinctvalues of an attribute(or set of attributes) A
intheresultof ajoin, V(A,r   s), canbe estimatedintheseways:
• Ifallattributesin Aarefromr,V(A,r   s)isestimatedasmin(V(A,r),n
r  s
),
and similarly if all attributes in Aare from s, V(A,r   s)isestimatedtobe
min(V(A,s),n
r  s
).
• If Acontainsattributes A1fromr and A2froms,thenV(A,r   s)isestimated
as:
min(V(A1,r) ?V(A2 ? A1,s),V(A1 ? A2,r) ?V(A2,s),n
r  s
)
Note that some attributes may be in A1aswellasinA2, and A1 ? A2and
A2?A1denote,respectively,attributesin Athatareonlyfromr andattributes
in Athat are only from s. Again, more accurate estimates can be derived by
using probabilitytheory,but theaboveapproximations work fairlywell.
The estimatesof distinct valuesare straightforward for projections: They are
the samein null
A
(r)asinr.Thesameholdsforgroupingattributesofaggregation.
For results of sum, count,andaverage, we can assume, for simplicity, that all
aggregate values are distinct. For min(A)andmax(A), the number of distinct
values can be estimated as min(V(A,r),V(G,r)), where G denotes the grouping
attributes.Weomitdetailsof estimatingdistinctvaluesfor other operations.
13.4 ChoiceofEvaluationPlans
Generation of expressions is only part of the query-optimization process, since
each operation in the expression can be implemented with different algorithms.
An evaluation plan de?nes exactly what algorithm should be used for each op-
eration,and how theexecutionof the operationsshouldbe coordinated.
Givenanevaluationplan,wecanestimateitscostusingstatisticsestimatedby
thetechniquesinSection13.3coupledwithcostestimatesforvariousalgorithms
and evaluationmethodsdescribedinChapter12.
Acost-basedoptimizerexploresthespaceofallquery-evaluationplansthat
are equivalent to the given query, and chooses the one with the least estimated
cost. We have seen how equivalence rules can be used to generate equivalent
plans.However,cost-basedoptimizationwitharbitraryequivalencerulesisfairly
complicated. We ?rst cover a simpler version of cost-based optimization, which
involves only join-order and join algorithm selection, in Section 13.4.1. Later
in Section 13.4.2 we brie?y sketch how a general-purpose optimizer based on
equivalencerulescan be built,without going intodetails.
Exploring the space of all possible plans may be too expensive for complex
queries.Mostoptimizersincludeheuristicstoreducethecostofqueryoptimiza-
tion, at the potential risk of not ?nding the optimal plan. We study some such
heuristicsinSection13.4.3.
13.4 ChoiceofEvaluationPlans 599
13.4.1 Cost-BasedJoinOrderSelection
The most common type of query in SQL consists of a join of a few relations,with
join predicates and selections speci?ed in the where clause. In this section we
considerthe problemofchoosing the optimaljoinorderfor sucha query.
Foracomplexjoinquery,thenumberofdifferentqueryplansthatareequiv-
alentto thequerycan belarge.As anillustration,consider theexpression:
r
1
  r
2
  ···  r
n
where the joins are expressed without any ordering. With n = 3, there are 12
differentjoinorderings:
r
1
  (r
2
  r
3
) r
1
  (r
3
  r
2
)( r
2
  r
3
)  r
1
(r
3
  r
2
)  r
1
r
2
  (r
1
  r
3
) r
2
  (r
3
  r
1
)( r
1
  r
3
)  r
2
(r
3
  r
1
)  r
2
r
3
  (r
1
  r
2
) r
3
  (r
2
  r
1
)( r
1
  r
2
)  r
3
(r
2
  r
1
)  r
3
Ingeneral,withnrelations,thereare(2(n ?1))!/(n ?1)!differentjoinorders.
(We leave the computation of this expression for you to do in Exercise 13.10.)
For joins involving small numbers of relations, this number is acceptable; for
example, with n = 5, the number is 1680. However, as n increases, this number
rises quickly. With n = 7, the number is 665,280; with n = 10, the number is
greaterthan 17.6 billion!
Luckily,itisnotnecessarytogeneratealltheexpressionsequivalenttoagiven
expression.Forexample,supposewewantto?ndthebestjoinorderoftheform:
(r
1
  r
2
  r
3
)  r
4
  r
5
whichrepresentsalljoinorderswherer
1
,r
2
,andr
3
arejoined?rst(insomeorder),
and the result is joined (in some order) withr
4
andr
5
. There are 12 different join
orders for computing r
1
  r
2
  r
3
, and 12 orders for computing the join of this
resultwithr
4
andr
5
.Thus,thereappeartobe144joinorderstoexamine.However,
once we have found the best join order for the subset of relations {r
1
,r
2
,r
3
},we
can use that order for further joins withr
4
andr
5
, and can ignore all costlier join
ordersofr
1
  r
2
  r
3
.Thus,insteadof144choicestoexamine,weneedtoexamine
only 12 +12 choices.
Usingthisidea,wecandevelopadynamic-programmingalgorithmfor?nding
optimaljoinorders.Dynamic-programmingalgorithmsstoreresultsofcomputa-
tions and reusethem,a procedurethat canreduceexecutiontimegreatly.
A recursive procedure implementing the dynamic-programming algorithm
appears in Figure 13.7. The procedure applies selections on individual relations
at the earliest possible point, that is, when the relations are accessed. It is easiest
to understand the procedure assuming that all joins are natural joins, although
theprocedureworksunchangedwithanyjoincondition.Witharbitraryjoincon-
ditions,thejoinoftwosubexpressionsisunderstoodtoincludealljoinconditions
that relateattributesfromthe two subexpressions.
600 Chapter13 QueryOptimization
procedureFindBestPlan(S)
if(bestplan[S].cost null=? )/*bestplan[S] alreadycomputed*/
returnbestplan[S]
if(Scontains only 1relation)
setbestplan[S].plan andbestplan[S].cost based on bestway of accessing S
elseforeach non-empty subset S1ofSsuch that S1 nullS
P1 =FindBestPlan(S1)
P2 =FindBestPlan(S ? S1)
A=bestalgorithm forjoining resultsof P1andP2
cost= P1.cost+ P2.cost+costofA
ifcost <bestplan[S].cost
bestplan[S].cost=cost
bestplan[S].plan = “execute P1.plan;executeP2.plan;
joinresultsof P1andP2usingA”
returnbestplan[S]
Figure13.7 Dynamic-programming algorithm for join order optimization.
Theprocedurestorestheevaluationplansitcomputesinanassociativearray
bestplan, which is indexed by sets of relations. Each element of the associative
arraycontains two components: the costofthe bestplanof S, andtheplan itself.
The value ofbestplan[S].cost isassumedto be initializedto ? ifbestplan[S]has
not yetbeencomputed.
Theprocedure?rstchecksifthebestplanforcomputingthejoinofthegiven
set of relations Shas beencomputed already(and storedin the associative array
bestplan); ifso,itreturnsthe alreadycomputedplan.
If S contains only one relation, the best way of accessing S (taking selections
onS,ifany,intoaccount)isrecordedinbestplan.Thismayinvolveusinganindex
toidentifytuples,andthenfetchingthetuples(oftenreferredtoasanindexscan),
orscanning theentirerelation(oftenreferredtoasarelationscan).
1
Ifthereisany
selection condition on S, other than those ensured by an index scan, a selection
operationisaddedtothe plan,to ensureall selectionson Saresatis?ed.
Otherwise, if S contains more than one relation, the procedure tries every
way of dividing S into two disjoint subsets. For each division, the procedure
recursively ?nds the best plans for each of the two subsets, and then computes
the cost of the overall plan by using that division.
2
The procedure picks the
cheapest plan from among all the alternatives for dividing S into two sets. The
cheapest plan and its cost are stored in the array bestplan, and returned by the
1
If an index contains all the attributes of a relation that are used in a query, it is possible to perform an index-only scan,
whichretrievesthe requiredattribute valuesfromthe index,without fetching actual tuples.
2
Notethatanindexednestedloopsjoinisconsideredforjoining P1andP2,with P2astheinnerrelation,if P2hasonly
a single relation, sayr, and an index is available on the join attributes ofr.PlanP2 may contain an indexed access tor,
based on selection conditions onr. To allow indexed nested loops join to be used, the index lookup using the selection
condition onrwouldbedroppedfromP2; instead, the selection condition would be checked on tuples returned from
the indexonthe joinattributes ofr.
13.4 ChoiceofEvaluationPlans 601
procedure. The time complexity of the procedure can be shown to be O(3
n
)(see
Practice Exercise13.11).
Actually, the order in which tuples are generated by the join of a set of
relationsisalsoimportantfor?ndingthebestoveralljoinorder,sinceitcanaffect
the cost of further joins (for instance, if merge join is used). A particular sort
order of the tuples is said to be aninterestingsortorder if it could be useful for
a later operation. For instance, generating the result ofr
1
  r
2
  r
3
sorted on the
attributes common with r
4
or r
5
may be useful, but generating it sorted on the
attributescommontoonlyr
1
andr
2
isnotuseful.Usingmergejoinforcomputing
r
1
  r
2
  r
3
may be costlier than using some other join technique, but it may
provideanoutputsortedinaninterestingsortorder.
Hence,itisnotsuf?cientto?ndthebestjoinorderforeachsubsetofthesetof
ngivenrelations. Instead,we have to ?nd the best join order for each subset, for
eachinterestingsortorderofthejoinresultforthatsubset.Thenumberofsubsets
of n relations is 2
n
. The number of interesting sort orders is generally not large.
Thus, about 2
n
join expressions need to be stored. The dynamic-programming
algorithm for ?nding the best join order can be easily extended to handle sort
orders.Thecostoftheextendedalgorithmdependsonthenumberofinteresting
orders for each subset of relations; since this number has been found to be small
in practice, the cost remains at O(3
n
).Withn = 10, this number isaround 59,000,
whichismuchbetterthanthe17 .6 billion different join orders. More important,
the storage required is much less than before, since we need to store only one
join order for each interesting sort order of each of 1024 subsets of r
1
,...,r
10
.
Although both numbers still increase rapidly with n, commonly occurring joins
usuallyhave lessthan 10 relations,and can behandledeasily.
13.4.2 Cost-BasedOptimization withEquivalenceRules
Thejoinorderoptimizationtechniquewejustsawhandlesthemostcommonclass
ofqueries,whichperformaninnerjoinofasetofrelations.However,clearlymany
queries use other features, such as aggregation, outer join, and nested queries,
which arenot addressedby joinorderselection.
Many optimizers follow an approach based on using heuristic transforma-
tionstohandleconstructsotherthanjoins,andapplyingthecost-basedjoinorder
selectionalgorithmtosubexpressionsinvolvingonlyjoinsandselections.Details
of such heuristics are for the most part speci?c to individual optimizers, and we
do not cover them. However, heuristic transformations to handle nested queries
arewidelyused,andareconsideredinmoredetailinSection13.4.4.
In this section, however, we outline how to create a general-purpose cost-
based optimizer based on equivalence rules, which can handle a wide variety of
queryconstructs.
Thebene?tofusingequivalencerulesisthatitiseasytoextendtheoptimizer
withnewrulestohandledifferentqueryconstructs.Forexample,nestedqueries
canberepresentedusingextendedrelational-algebraconstructs,andtransforma-
tions of nested queries can be expressed as equivalence rules. We have already
602 Chapter13 QueryOptimization
seen equivalence rules with aggregation operations, and equivalence rules can
also be created forouterjoins.
In Section13.2.4, wesaw howan optimizercould systematicallygenerateall
expressions equivalent to the given query. The procedure for generating equiv-
alent expressions can be modi?ed to generate all possible evaluation plans as
follows: A new class of equivalence rules, called physical equivalence rules,is
addedthatallowsalogicaloperation,suchasajoin,tobetransformedtoaphys-
ical operation, such as a hash join, or a nested-loops join. By adding such rules
to the original set of equivalence rules, the procedure can generate all possible
evaluation plans. The cost estimation techniques we have seen earlier can then
be usedto choose the optimal(that is,the least-cost)plan.
However, the procedure shown in Section 13.2.4 is very expensive, even if
we do not consider generation of evaluation plans. To make the approach work
ef?cientlyrequiresthefollowing:
1. Aspace-ef?cientrepresentationofexpressionsthatavoidsmakingmultiple
copies ofthe samesubexpressionswhenequivalencerulesareapplied.
2. Ef?cient techniques for detecting duplicate derivationsof the same expres-
sion.
3. A form of dynamic programming based onmemoization,whichstoresthe
optimalqueryevaluationplanforasubexpressionwhenitisoptimizedfor
the ?rst time; subsequent requests to optimize the same subexpression are
handled by returning the alreadymemoizedplan.
4. Techniques that avoid generating all possible equivalent plans, by keeping
track of the cheapest plangeneratedfor any subexpression up to any point
oftime,andpruningawayanyplanthatismoreexpensivethanthecheapest
plan found so farfor that subexpression.
Thedetailsaremorecomplexthanwewishtodealwithhere.Thisapproachwas
pioneeredbytheVolcanoresearchproject,andthequeryoptimizerof SQLServer
isbasedonthisapproach.Seethebibliographicalnotesforreferencescontaining
furtherinformation.
13.4.3 HeuristicsinOptimization
Adrawbackofcost-basedoptimizationisthecostofoptimizationitself.Although
thecostofqueryoptimizationcanbereducedbycleveralgorithms,thenumberof
different evaluation plans for a query can be very large, and ?nding the optimal
plan from this set requires a lot of computational effort. Hence, optimizers use
heuristics to reducethe cost ofoptimization.
Anexampleofaheuristicruleisthefollowingrulefortransformingrelational-
algebra queries:
• Performselectionoperationsas earlyas possible.
13.4 ChoiceofEvaluationPlans 603
Aheuristicoptimizerwouldusethisrulewithout?ndingoutwhetherthecostis
reducedbythistransformation.Inthe?rsttransformationexampleinSection13.2,
theselectionoperationwas pushedintoajoin.
Wesaythattheprecedingruleisaheuristicbecauseitusually,butnotalways,
helpstoreducethecost.Foranexampleofwhereitcanresultinanincreaseincost,
consider an expression     (r   s), where the condition   refers to only attributes
in s. The selection can certainly be performed before the join. However, if r is
extremely small compared to s, and if there is an index on the join attributes
of s, but no index on the attributes used by   , then it is probably a bad idea to
perform the selection early. Performing the selection early—that is, directly on
s—would require doing a scan of all tuples in s. It is probably cheaper, in this
case,tocomputethejoinbyusingtheindex,andthentorejecttuplesthatfailthe
selection.
The projection operation, like the selection operation, reduces the size of
relations. Thus, whenever we need to generate a temporary relation, it is advan-
tageous to apply immediately any projections that are possible. This advantage
suggestsacompanion to the “perform selectionsearly”heuristic:
• Performprojectionsearly.
It is usually better to perform selections earlier than projections, since selections
have the potential to reduce the sizes of relations greatly, and selections enable
the use of indices to access tuples. An example similar to the one used for the
selectionheuristicshouldconvinceyouthatthisheuristicdoesnotalwaysreduce
thecost.
Most practical query optimizers have further heuristics to reduce the cost of
optimization. For example, many query optimizers, such as the System R opti-
mizer,
3
do not consider all join orders, but rather restrict the search to particular
kinds of join orders. The System R optimizer considers only those join orders
where the right operand of each join is one of the initial relationsr
1
,...,r
n
.Such
joinordersarecalledleft-deepjoinorders.Left-deepjoinordersareparticularly
convenient for pipelined evaluation, since the right operand is a stored relation,
and thus only one input to each joinis pipelined.
Figure13.8illustratesthedifferencebetweenleft-deepjointreesandnon-left-
deepjointrees.Thetimeittakestoconsiderallleft-deepjoinordersisO(n!),which
is much less than the time to consider all join orders. With the use of dynamic-
programming optimizations,the SystemR optimizercan ?nd the best join order
in time O(n2
n
). Contrast this cost with the O(3
n
) time required to ?nd the best
overalljoinorder.The SystemRoptimizeruses heuristicstopushselections and
projectionsdownthe querytree.
A heuristic approach to reduce the cost of join-order selection, which was
originallyusedinsomeversionsofOracle,worksroughlythisway:Forann-way
join,itconsidersnevaluationplans.Eachplanusesaleft-deepjoinorder,starting
3
System R was one of the ?rst implementations of SQL, and its optimizer pioneered the idea of cost-based join-order
optimization.
604 Chapter13 QueryOptimization
r4 r5
r3
r1 r2
r5
r4
r3
r2 r1
(a) Left-deep join tree (b) Non-left-deep join tree
Figure13.8 Left-deep join trees.
with a different one of the n relations. The heuristic constructs the join order for
each of the n evaluation plans by repeatedly selecting the “best” relation to join
next,onthebasisofarankingoftheavailableaccesspaths.Eithernested-loopor
sort-mergejoin is chosen for each of the joins, depending on the available access
paths. Finally, the heuristic chooses one of the n evaluation plans in a heuristic
manner, on the basis of minimizing the number of nested-loop joins that do not
have an index available on the inner relation and on the number of sort-merge
joins.
Query-optimization approaches that apply heuristic plan choices for some
parts of the query, with cost-based choice based on generation of alternative
access plans on other parts of the query, have been adopted in several systems.
The approach used in System R and in its successor, the Starburst project, is
a hierarchical procedure based on the nested-block concept of SQL.Thecost-
based optimization techniques described here are used for each block of the
query separately. The optimizers in several database products, such as IBM DB2
and Oracle, are based on the above approach, with extensions to handle other
operations such as aggregation. For compound SQL queries (using the ?, ?,or
? operation), the optimizer processes each component separately,and combines
the evaluationplansto form theoverallevaluationplan.
Most optimizers allow a cost budget to be speci?ed for query optimization.
Thesearchfortheoptimalplanisterminatedwhentheoptimizationcostbudget
is exceeded, and the best plan found up to that point is returned. The budget
itself may be set dynamically; for example, if a cheap plan is found for a query,
the budget may be reduced, on the premise that there is no point spending a lot
of time optimizing the query if the best plan found so far is already quite cheap.
On the other hand, if the best plan found so far is expensive, it makes sense to
invest more time in optimization, which could result in a signi?cant reduction
in execution time. To best exploit this idea, optimizers usually ?rst apply cheap
heuristicsto?ndaplan,andthenstartfullcost-basedoptimizationwithabudget
basedon theheuristicallychosenplan.
13.4 ChoiceofEvaluationPlans 605
Many applications execute the same query repeatedly, but with different
values for the constants. For example, a university application may repeatedly
execute a query to ?nd the courses for which a student has registered, but each
timeforadifferentstudentwithadifferentvalueforthestudentID.Asaheuristic,
manyoptimizersoptimizeaqueryonce,withwhatevervalueswereprovidedfor
the constants when the query was ?rst submitted, and cache the query plan.
Whenever the query is executed again, perhaps with new values for constants,
the cached query plan is reused (using new values for the constants, of course).
The optimal plan for the new constants may differ from the optimal plan for the
initial values, but as a heuristic the cached plan is reused.
4
Caching and reuse of
queryplansis referredtoasplancaching.
Even with the use of heuristics, cost-based query optimization imposes a
substantialoverheadonqueryprocessing.However,theaddedcostofcost-based
query optimization is usually more than offset by the saving at query-execution
time,whichisdominatedbyslowdiskaccesses.Thedifferenceinexecutiontime
between a good plan and a bad one may be huge, making query optimization
essential. The achieved saving is magni?ed in those applications that run on
a regular basis, where a query can be optimized once, and the selected query
plan can be used each time the query is executed. Therefore, most commercial
systems include relatively sophisticated optimizers. The bibliographical notes
givereferencestodescriptionsofthequeryoptimizersofactualdatabasesystems.
13.4.4 OptimizingNestedSubqueries**
SQL conceptually treats nested subqueries in the where clause as functions that
take parameters and return either a single value or a set of values (possibly an
empty set). The parameters are the variables from an outer level query that are
usedinthenestedsubquery(thesevariablesarecalledcorrelationvariables).For
instance,supposewehavethefollowingquery,to?ndthenamesofallinstructors
who taught acourse in 2007:
select name
frominstructor
whereexists (select *
fromteaches
whereinstructor.ID =teaches.ID
andteaches.year = 2007);
Conceptually, the subquery can be viewed as a function that takes a parameter
(here,instructor.ID)andreturnsthesetofallcoursestaughtin2007byinstructors
(with the same ID).
4
For the student registration query, the plan would almost certainly be the same for any student ID. But a query that
took a range of student IDs, and returned registration information for all student IDs in that range, would probably
have adifferentoptimal plan ifthe range isvery small than ifthe range islarge.
606 Chapter13 QueryOptimization
SQL evaluates the overall query (conceptually) by computing the Cartesian
product of the relations in the outerfrom clause and then testing the predicates
in thewhere clause for each tuple in the product. In the preceding example, the
predicatetestsiftheresultof thesubqueryevaluationisempty.
Thistechniqueforevaluatingaquerywithanestedsubqueryiscalledcorre-
lated evaluation. Correlated evaluation is not very ef?cient, since the subquery
isseparatelyevaluatedforeachtupleintheouterlevelquery.Alargenumberof
random disk I/O operationsmay result.
SQL optimizers therefore attempt to transform nested subqueries into joins,
wherepossible.Ef?cientjoinalgorithmshelpavoidexpensiverandomI/O.Where
thetransformationisnotpossible,theoptimizerkeepsthesubqueriesasseparate
expressions, optimizes them separately, and then evaluates them by correlated
evaluation.
Asanexampleoftransforminganestedsubqueryintoajoin,thequeryinthe
precedingexamplecanbe rewrittenas:
select name
frominstructor,teaches
whereinstructor.ID =teaches.IDandteaches.year = 2007;
(To properly re?ect SQL semantics, the number of duplicate derivations should
notchangebecauseoftherewriting;therewrittenquerycanbemodi?edtoensure
this property,as weshall seeshortly.)
In the example, the nested subquery was very simple. In general, it may not
bepossibletodirectlymovethenestedsubqueryrelationsintothefromclauseof
the outer query. Instead, we create a temporary relation that contains the results
of the nested query without the selections using correlation variables from the
outerquery,andjointhetemporarytablewiththeouterlevelquery.Forinstance,
a queryofthe form:
select ...
from L
1
where P
1
andexists (select *
from L
2
where P
2
);
where P
2
is aconjunction of simplerpredicates,can berewrittenas:
createtablet
1
as
selectdistinct V
from L
2
where P
1
2
;
select ...
from L
1
,t
1
where P
1
and P
2
2
;
where P
1
2
containspredicatesin P
2
withoutselectionsinvolvingcorrelationvari-
ables, and P
2
2
reintroduces the selections involving correlation variables (with
13.5 MaterializedViews** 607
relations referenced in the predicate appropriately renamed). Here, V contains
all attributes that are used in selections with correlation variables in the nested
subquery.
Inourexample,the originalquerywouldhave beentransformedto:
createtablet
1
as
selectdistinct ID
fromteaches
whereyear = 2007;
select name
frominstructor,t
1
wheret
1
.ID =instructor.ID;
Thequerywerewrotetoillustratecreationofatemporaryrelationcanbeobtained
bysimplifyingtheabovetransformedquery,assumingthenumberofduplicates
of eachtupledoesnot matter.
Theprocessofreplacinganestedquerybyaquerywithajoin(possiblywith
a temporaryrelation)is calleddecorrelation.
Decorrelation is more complicated when the nested subquery uses aggrega-
tion,orwhentheresultofthenestedsubqueryisusedtotestforequality,orwhen
theconditionlinkingthenestedsubquerytotheouterqueryisnotexists,andso
on. We do not attempt to give algorithms for the general case, and instead refer
you torelevantitemsinthebibliographical notes.
Optimizationofcomplexnestedsubqueriesisadif?culttask,asyoucaninfer
from the above discussion, and many optimizers do only a limited amount of
decorrelation.Itisbesttoavoidusingcomplexnestedsubqueries,wherepossible,
sincewecannotbesurethatthequeryoptimizerwillsucceedinconvertingthem
toaform that can beevaluatedef?ciently.
13.5 Materialized Views**
Whenaviewisde?ned,normallythedatabasestoresonlythequeryde?ningthe
view. In contrast, a materialized view is a view whose contents are computed
and stored. Materialized views constitute redundant data, in that their contents
can be inferred from the view de?nition and the rest of the database contents.
However,itismuchcheaperinmanycasestoreadthecontentsofamaterialized
view than to compute the contents of the view by executing the query de?ning
theview.
Materializedviewsare important for improvingperformance in some appli-
cations. Considerthis view,which givesthe totalsalary ineach department:
createviewdepartment total salary(dept name,total salary)as
select dept name,sum(salary)
frominstructor
groupbydept name;
608 Chapter13 QueryOptimization
Supposethetotalsalaryamountatadepartmentisrequiredfrequently.Comput-
ing the view requires reading every instructor tuple pertaining to a department,
and summingupthesalaryamounts, which canbetime-consuming.Incontrast,
iftheviewde?nitionofthetotalsalaryamountwerematerialized,thetotalsalary
amount could be found bylooking up a singletuple in the materializedview.
5
13.5.1 ViewMaintenance
A problem with materialized views is that they must be kept up-to-date when
the data used in the view de?nition changes. For instance, if the salary value of
aninstructorisupdated,thematerializedviewwillbecomeinconsistentwiththe
underlyingdata,anditmustbeupdated.Thetaskofkeepingamaterializedview
up-to-datewith theunderlyingdatais known asviewmaintenance.
Views can be maintained by manually written code: That is, every piece
of code that updates the salary value can be modi?ed to also update the total
salary amount for the corresponding department. However, this approach is
error prone, since it is easy to miss some places where the salary is updated, and
the materializedviewwillthennolongermatch theunderlyingdata.
Another option for maintaining materialized views is to de?ne triggers on
insert,delete,andupdateofeachrelationintheviewde?nition.Thetriggersmust
modifythecontentsofthematerializedview,totakeintoaccountthechangethat
causedthetriggerto?re.Asimplisticwayofdoingsoistocompletelyrecompute
the materializedviewoneveryupdate.
A better option is to modify only the affected parts of the materialized view,
which isknown asincrementalviewmaintenance.Wedescribehowtoperform
incrementalviewmaintenance inSection13.5.2.
Moderndatabase systemsprovidemoredirect supportfor incremental view
maintenance. Database-system programmers no longer need to de?ne triggers
for view maintenance. Instead, once a view is declared to be materialized, the
database system computes the contents of the view and incrementally updates
the contents when the underlyingdatachange.
Mostdatabasesystemsperformimmediateviewmaintenance;thatis,incre-
mentalviewmaintenanceisperformedassoonasanupdateoccurs,aspartofthe
updating transaction. Some database systems also supportdeferredviewmain-
tenance,whereviewmaintenanceisdeferredtoalatertime;forexample,updates
may be collected throughout a day, and materialized views may be updated at
night.Thisapproachreducestheoverheadonupdatetransactions.However,ma-
terializedviewswith deferredviewmaintenance may not be consistent withthe
underlyingrelationson which they are de?ned.
5
The difference may not be all that large for a medium-sized university, but in other settings the difference can be
very large. For example, if the materialized view computed total sales of each product, from a sales relation with tens
of millions of tuples, the difference between computing the aggregate from the underlying data, and looking up the
materialized viewcan be many ordersofmagnitude.
13.5 MaterializedViews** 609
13.5.2 Incremental ViewMaintenance
Tounderstandhowtomaintainmaterializedviewsincrementally,westartoffby
considering individual operations, and then we see how to handle a complete
expression.
The changes to a relation that can cause a materialized view to become out-
of-date are inserts, deletes,and updates. To simplify our description, we replace
updates to a tuple by deletion of the tuple followed by insertion of the updated
tuple. Thus, we need to consider only inserts and deletes. The changes (inserts
and deletes)to arelationor expressionarereferredto as itsdifferential.
13.5.2.1 JoinOperation
Consider the materialized view v = r   s. Suppose we modify r by inserting a
set of tuples denoted by i
r
. If the old value of r is denoted by r
old
,andthenew
valueofr byr
new
,r
new
=r
old
?i
r
.Now,theoldvalueoftheview, v
old
,isgivenby
r
old
  s, and the new value v
new
is given byr
new
  s.Wecanrewriter
new
  s as
(r
old
?i
r
)  s, which we can again rewrite as(r
old
  s) ?(i
r
  s).Inother words:
v
new
= v
old
?(i
r
  s)
Thus,toupdatethematerializedview v,wesimplyneedtoaddthetuplesi
r
  s
totheoldcontentsofthematerializedview.Insertstos arehandledinanexactly
symmetricfashion.
Now suppose r is modi?ed by deleting a set of tuples denoted by d
r
.Using
the same reasoningas above,we get:
v
new
= v
old
?(d
r
  s)
Deletesons arehandledinan exactlysymmetricfashion.
13.5.2.2 SelectionandProjectionOperations
Consider a view v =     (r). If we modifyr by inserting a set of tuplesi
r
, the new
valueof vcanbecomputedas:
v
new
= v
old
?    (i
r
)
Similarly,ifr is modi?edby deletingaset of tuplesd
r
, the new value of v can be
computed as:
v
new
= v
old
?    (d
r
)
Projection is a more dif?cult operation with which to deal. Consider a ma-
terialized view v = null
A
(r). Suppose the relation r is on the schema R = (A,B),
and r contains two tuples (a,2) and (a,3). Then, null
A
(r) has a single tuple (a). If
610 Chapter13 QueryOptimization
wedeletethetuple(a,2)fromr, we cannot deletethetuple(a)fromnull
A
(r):Ifwe
didso, the resultwould be an emptyrelation,whereas in reality null
A
(r) stillhas a
singletuple(a).Thereasonisthatthesametuple(a)isderivedintwoways,and
deletingonetuplefromr removesonlyoneofthewaysofderiving(a);theother
isstillpresent.
Thisreasonalsogivesustheintuitionforsolution:Foreachtupleinaprojec-
tion such as null
A
(r),wewillkeepacount of how many timesitwas derived.
When a set of tuples d
r
is deleted from r,foreachtuplet in d
r
we do the
following: Let t.Adenote the projection of t on the attribute A. We ?nd (t.A)in
the materialized view, and decrease the count stored with it by 1. If the count
becomes 0, (t.A) is deletedfrom thematerializedview.
Handling insertions is relatively straightforward. When a set of tuples i
r
is
insertedintor,foreachtupletini
r
wedothefollowing:If(t.A)isalreadypresent
inthematerializedview,weincreasethecountstoredwithitby1.Ifnot,weadd
(t.A) tothe materializedview,withthe count setto1.
13.5.2.3 AggregationOperations
Aggregationoperations proceedsomewhat likeprojections.The aggregateoper-
ations in SQL arecount,sum,avg,min,andmax:
• count: Consider a materialized view v =
A
G
count(B)
(r), which computes the
count ofthe attribute B,aftergroupingr byattribute A.
When a set of tuples i
r
is inserted into r,foreachtuplet in i
r
we do
the following: We look for the group t.Ain the materialized view. If it is not
present, we add (t.A,1) to the materialized view. If the group t.Ais present,
we add 1 to the count of the group.
When a setof tuplesd
r
isdeletedfromr,foreachtuplet ind
r
we do the
following:Welookforthegroupt.Ainthematerializedview,andsubtract1
from the count for the group. If the count becomes 0, we delete the tuple for
the groupt.Afromthe materializedview.
• sum:Consideramaterializedview v =
A
G
sum(B)
(r).
When a set of tuples i
r
is inserted into r,foreachtuplet in i
r
we do
the following: We look for the group t.Ain the materialized view. If it is not
present, we add (t.A,t.B) to the materialized view; in addition, we store a
countof1associatedwith(t.A,t.B),justaswedidforprojection.Ifthegroup
t.Ais present, we add the value of t.B to the aggregate value for the group,
and add 1to the count of the group.
When a setof tuplesd
r
isdeletedfromr,foreachtuplet ind
r
we do the
following: We look for the group t.Ain the materialized view, and subtract
t.B fromtheaggregatevalueforthegroup.Wealsosubtract1fromthecount
for the group, and if the count becomes 0, we delete the tuple for the group
t.Afrom thematerializedview.
Withoutkeepingtheextracountvalue,wewouldnotbeabletodistinguish
acasewherethesumforagroupis0fromthecasewherethelasttupleina
groupis deleted.
13.5 MaterializedViews** 611
• avg:Consideramaterializedview v =
A
G
avg(B)
(r).
Directlyupdatingthe averageonan insertor deleteis not possible,since
itdependsnotonlyontheoldaverageandthetuplebeinginserted/deleted,
but alsoon the number of tuplesin the group.
Instead, to handle the case of avg, we maintain the sum and count
aggregate values as described earlier, and compute the average as the sum
dividedby the count.
• min,max: Consider a materializedview v =
A
G
min(B)
(r). (The case ofmax is
exactlyequivalent.)
Handling insertions on r is straightforward. Maintaining the aggregate
values min and max on deletions may be more expensive. For example, if
thetuplecorrespondingtotheminimumvalueforagroupisdeletedfromr,
wehavetolookattheothertuplesofr thatareinthesamegroupto?ndthe
newminimumvalue.
13.5.2.4 OtherOperations
The set operation intersection is maintained as follows: Given materialized view
v = r ?s, when a tuple is inserted inr we check if it is present in s,andifsowe
add it to v. If a tuple is deleted from r, we delete it from the intersection if it is
present.Theothersetoperations,unionandsetdifference,arehandledinasimilar
fashion; we leavedetailstoyou.
Outer joins are handled in much the same way as joins, but with some extra
work. In the case of deletion fromr we have to handle tuples in s that no longer
match any tuple in r. In the case of insertion to r, we have to handle tuples in s
that didnot match any tuple inr.Againweleavedetailsto you.
13.5.2.5 HandlingExpressions
Sofarwehaveseenhowtoupdateincrementallytheresultofasingleoperation.
To handle an entire expression, we can derive expressions for computing the
incrementalchangetotheresultofeachsubexpression,startingfromthesmallest
subexpressions.
For example, suppose we wish to incrementally update a materialized view
E
1
  E
2
when a set of tuples i
r
is inserted into relation r. Let us assume r is
used in E
1
alone. Suppose the set of tuples to be inserted into E
1
is given by
expression D
1
. Then the expression D
1
  E
2
givestheset oftuplestobeinserted
into E
1
  E
2
.
Seethe bibliographical notes for further detailson incremental viewmainte-
nance with expressions.
13.5.3 QueryOptimization andMaterializedViews
Query optimization can be performed by treating materialized views just like
regular relations. However, materialized views offer further opportunities for
optimization:
• Rewritingqueriestousematerializedviews:
612 Chapter13 QueryOptimization
Suppose a materialized view v = r   s is available, and a user submits a
query r   s   t. Rewriting the query as v   t may provide a more ef?cient
query plan than optimizing the query as submitted. Thus, it is the job of the
queryoptimizertorecognizewhenamaterializedviewcanbeusedtospeed
upa query.
• Replacing a useofa materializedviewwiththeviewde?nition:
Supposeamaterializedview v =r   s isavailable,butwithoutanyindex
onit,andausersubmitsaquery  A=10
(v). Suppose also that s has an index
on the common attribute B,andr has an indexon attribute A.Thebestplan
forthisquerymaybetoreplacev with r   s, which can lead to the query
plan  A=10
(r)  s;theselectionandjoincanbeperformedef?cientlybyusing
the indices on r.Aand s.B, respectively. In contrast, evaluating the selection
directlyon v mayrequirea fullscanof v,which may be moreexpensive.
The bibliographical notes give pointers to research showing how to ef?ciently
performqueryoptimizationwithmaterializedviews.
13.5.4 MaterializedViewandIndexSelection
Another related optimization problem is that of materialized view selection,
namely, “What is the best set of views to materialize?” This decision must be
made on the basis of the system workload, which is a sequence of queries and
updates that re?ects the typical load on the system. One simple criterion would
be toselectasetof materializedviewsthat minimizesthe overallexecutiontime
oftheworkloadofqueriesandupdates,includingthetimetakentomaintainthe
materializedviews.Databaseadministratorsusuallymodifythiscriteriontotake
into account the importance of differentqueriesandupdates:Fastresponsemay
berequiredforsomequeriesandupdates,butaslowresponsemaybeacceptable
for others.
Indices are just like materialized views, in that they too are derived data,
can speed up queries, and may slow down updates. Thus, the problem ofindex
selection is closely related to that of materialized view selection, although it is
simpler. We examine index and materialized view selection in more detail in
Sections24.1.6 and 24.1.7.
Mostdatabasesystemsprovidetoolstohelpthedatabaseadministratorwith
indexandmaterializedviewselection.Thesetoolsexaminethehistoryofqueries
andupdates,andsuggestindicesandviewstobematerialized.TheMicrosoftSQL
Server Database Tuning Assistant, the IBM DB2 Design Advisor, and the Oracle
SQL Tuning Wizard are examplesof such tools.
13.6 AdvancedTopicsinQueryOptimization**
There are a number of opportunities for optimizing queries, beyond those we
haveseenso far.We examinea fewof theseinthissection.
13.6 AdvancedTopicsinQueryOptimization** 613
13.6.1 Top-K Optimization
Many queriesfetchresultssortedonsomeattributes,and requireonly the top K
results for some K. Sometimes the bound K is speci?ed explicitly. For example,
some databases support alimit K clause which results in only the top K results
being returned by the query. Other databases support alternative ways of speci-
fyingsimilarlimits.Inothercases,thequerymaynotspecifysuchalimit,butthe
optimizermayallow a hint tobe speci?ed,indicating that only the top K results
of thequeryarelikelytoberetrieved,evenifthequerygeneratesmoreresults.
When K is small, a query optimization plan that generates the entire set of
results, then sorts and generates the top K, is very inef?cient since it discards
most of the intermediate results that it computes. Several techniques have been
proposed to optimize such top-K queries. One approach is to use pipelinedplans
thatcangeneratetheresultsinsortedorder.Anotherapproachistoestimatewhat
isthehighestvalueonthesortedattributesthatwillappearinthetop-K output,
and introduce selection predicates that eliminate larger values. If extra tuples
beyond the top-K are generated they are discarded, and if too few tuples are
generated then the selection condition is changed and the query is re-executed.
Seethe bibliographical notesfor referencestowork on top-K optimization.
13.6.2 JoinMinimization
Whenqueriesaregeneratedthroughviews,sometimesmorerelationsarejoined
than are needed for computation of the query. For example, a view v may in-
clude the join of instructor and department,butauseoftheviewv may use only
attributes from instructor. The join attribute dept name of instructor is a foreign
keyreferencingdepartment.Assumingthatinstructor.dept namehasbeendeclared
notnull,thejoinwithdepartment can be dropped, with no impact on the query.
For,undertheaboveassumption,thejoinwithdepartmentdoesnoteliminateany
tuplesfrominstructor,nor doesitresultinextracopiesof anyinstructor tuple.
Dropping a relationfrom a join as above is an exampleof join minimization.
In fact, join minimization can be performed in other situations as well. See the
bibliographical notes forreferenceson joinminimization.
13.6.3 Optimization ofUpdates
Update queries often involve subqueries in the set and where clauses, which
must also be taken into account in optimizing the update. Updates that involve
a selection on the updated column (e.g., give a 10 percent salary raise to all
employees whose salary is ? $100,000) must be handled carefully. If the update
isdonewhiletheselectionisbeingevaluatedbyanindexscan,anupdatedtuple
may be reinserted in the index ahead of the scan and seen again by the scan;
the same employee tuple may then get incorrectly updated multiple times (an
in?nitenumberoftimes,inthiscase).Asimilarproblemalsoariseswithupdates
involvingsubquerieswhose resultisaffectedbythe update.
The problem of an update affecting the execution of a query associated with
the update is known as the Halloween problem (named so because it was ?rst
614 Chapter13 QueryOptimization
recognizedonaHalloweenday,atIBM).Theproblemcanbeavoidedbyexecuting
thequeriesde?ningtheupdate?rst,creatingalistofaffectedtuples,andupdating
the tuples and indices as the last step. However, breaking up the execution plan
insucha fashionincreasestheexecutioncost.Updateplanscanbeoptimizedby
checking ifthe Halloweenproblemcan occur, and ifitcannot occur, updatescan
beperformedwhilethequeryisbeingprocessed,reducingtheupdateoverheads.
For example, the Halloween problem cannot occur if the update does not affect
indexattributes.Evenifitdoes,iftheupdatesdecreasethevalue,whiletheindex
is scanned in increasing order, updated tuples will not be encountered again
during the scan. In such cases, the index can be updatedevenwhile the query is
being executed,reducingthe overallcost.
Updatequeriesthatresultinalargenumberofupdatescanalsobeoptimized
by collecting the updates as a batch, and then applying the batch of updates
separately to each affected index. When applying the batch of updates to an
index, the batch is ?rst sorted in the index order for that index; such sorting can
greatlyreducethe amount of random I/O requiredfor updatingindices.
Such optimizations of updates are implemented in most database systems.
Seethe bibliographical notesfor referencestosuchoptimization.
13.6.4 MultiqueryOptimization andSharedScans
When a batch of queries are submitted together, a query optimizer can poten-
tially exploit common subexpressions between the different queries, evaluating
them once and reusing them where required. Complex queries may in fact have
subexpressions repeated in different parts of the query, which can be similarly
exploited,toreducequeryevaluationcost.Suchoptimizationisknownasmulti-
queryoptimization.
Common subexpression elimination optimizes subexpressions shared by
different expressions in a program, by computing and storing the result, and
reusing it wherever the subexpression occurs. Common subexpression elimina-
tionisastandardoptimizationappliedonarithmeticexpressionsbyprogramming-
languagecompilers.Exploitingcommonsubexpressionsamongevaluationplans
chosen for each of a batch of queries is just as useful in database query evalua-
tion,andisimplementedbysomedatabases.However,multiqueryoptimization
can do even better in some cases: A query typically has more than one evalua-
tion plan, and a judiciously chosen set of query evaluation plans for the queries
may provide for a greater sharing and lesser cost than that afforded by choos-
ing the lowest cost evaluation plan for each query. More details on multiquery
optimizationmaybe found inreferencescitedinthebibliographical notes.
Sharing of relation scans between queries is another limited form of mul-
tiquery optimization that is implemented in some databases. The shared-scan
optimization works as follows: Instead of reading the relation repeatedly from
disk, once for each query that needs to scan a relation, data are read once from
disk, and pipelined to each of the queries. The shared-scan optimization is par-
ticularly useful when multiple queries perform a scan on a single large relation
(typicallya “fact table”).
13.7 Summary 615
13.6.5 ParametricQueryOptimization
Plan caching, which we saw earlier in Section 13.4.3, is used as a heuristic in
many databases. Recall that with plan caching, if a query is invoked with some
constants, the plan chosen by the optimizer is cached, and reused if the query
is submitted again, even if the constants in the query are different. For example,
supposeaquerytakesadepartmentnameasaparameter,andretrievesallcourses
of the department.With plan caching, a plan chosen when the queryis executed
forthe?rsttime,sayfortheMusicdepartment,isreusedifthequeryisexecuted
for any otherdepartment.
Suchreuseofplansbyplancachingisreasonableiftheoptimalqueryplanis
notsigni?cantlyaffectedbytheexactvalueoftheconstantsinthequery.However,
iftheplanisaffectedbythevalueoftheconstants,parametricqueryoptimization
isanalternative.
Inparametric queryoptimization, a query is optimized without being pro-
vided speci?c values for its parameters, for example, dept name in the preceding
example. The optimizer then outputs several plans, each optimal for a different
parameter value. A plan would be output by the optimizer only if it is optimal
for somepossiblevalueof the parameters.The setofalternativeplans outputby
the optimizer are stored. When a query is submitted with speci?c values for its
parameters,insteadofperformingafulloptimization,thecheapestplanfromthe
set of alternativeplans computed earlieris used.Finding the cheapest such plan
usually takes much less time than reoptimization. See the bibliographical notes
for referencesonparametricqueryoptimization.
13.7 Summary
• Given a query, there are generally a variety of methods for computing the
answer.Itistheresponsibilityofthesystemtotransformthequeryasentered
by the user into an equivalent query that can be computed more ef?ciently.
The process of ?nding a good strategy for processing a query is called query
optimization.
• Theevaluationof complexqueriesinvolvesmany accesses todisk.Sincethe
transfer of data from disk is slow relative to the speed of main memory and
the CPU of the computer system, it is worthwhile to allocate a considerable
amount of processingto choose a methodthat minimizesdiskaccesses.
• There are a number of equivalencerules that we can use to transform an ex-
pressionintoanequivalentone.Weusetheserulestogeneratesystematically
allexpressionsequivalenttothegivenquery.
• Each relational-algebra expression represents a particular sequence of op-
erations. The ?rst step in selecting a query-processing strategy is to ?nd a
relational-algebra expression that is equivalent to the given expression and
isestimatedtocost lesstoexecute.
616 Chapter13 QueryOptimization
• The strategy that the database system chooses for evaluating an operation
dependsonthesizeofeachrelationandonthedistributionofvalueswithin
columns. So that they can base the strategy choice on reliable information,
database systems may store statistics for each relation r. These statistics in-
clude:
?
The number of tuplesin therelationr.
?
The sizeof arecord(tuple)of relationrinbytes.
?
Thenumberofdistinctvaluesthatappearintherelationrforaparticular
attribute.
• Most database systems use histograms to store the number of values for
an attribute within each of several ranges of values. Histograms are often
computedusing sampling.
• These statistics allow us to estimate the sizes of the results of various opera-
tions, as well as the cost of executing the operations. Statistical information
about relations is particularly useful when several indices are available to
assist in the processing of a query. The presence of these structures has a
signi?cant in?uence on the choice of a query-processingstrategy.
• Alternativeevaluationplans for each expressioncan be generatedby equiv-
alence rules, and the cheapest plan across all expressions can be chosen.
Several optimization techniques are available to reduce the number of alter-
nativeexpressionsand plansthat needtobegenerated.
• We use heuristics to reduce the number of plans considered, and thereby to
reduce the cost of optimization. Heuristic rules for transforming relational-
algebra queries include “Perform selection operations as early as possible,”
“Perform projectionsearly,”and “AvoidCartesianproducts.”
• Materialized views can be used to speed up query processing. Incremental
view maintenance is needed to ef?ciently update materialized views when
the underlying relations are modi?ed. The differential of an operation can
becomputedbymeansofalgebraicexpressionsinvolvingdifferentialsofthe
inputs of the operation. Other issues related to materialized views include
howtooptimizequeriesbymakinguseofavailablematerializedviews,and
how toselectviewstobe materialized.
• Anumberofadvancedoptimizationtechniqueshavebeenproposedsuchas
top-K optimization,joinminimization, optimizationof updates,multiquery
optimization,and parametricqueryoptimization.
ReviewTerms
• Queryoptimization
• Transformation ofexpressions
• Equivalenceof expressions
PracticeExercises 617
• Equivalencerules
?
Joincommutativity
?
Joinassociativity
• Minimalsetof equivalencerules
• Enumerationof equivalent
expressions
• Statisticsestimation
• Cataloginformation
• Sizeestimation
?
Selection
?
Selectivity
?
Join
• Histograms
• Distinctvalueestimation
• Random sample
• Choice of evaluationplans
• Interactionofevaluation
techniques
• Cost-basedoptimization
• Join-orderoptimization
?
Dynamic-programming
algorithm
?
Left-deepjoinorder
?
Interesting sort order
• Heuristicoptimization
• Plan caching
• Access-planselection
• Correlatedevaluation
• Decorrelation
• Materializedviews
• Materializedviewmaintenance
?
Recomputation
?
Incrementalmaintenance
?
Insertion
?
Deletion
?
Updates
• Queryoptimizationwith
materializedviews
• Indexselection
• Materializedviewselection
• Top-K optimization
• Joinminimization
• Halloweenproblem
• Multiqueryoptimization
PracticeExercises
13.1 Show that the following equivalences hold. Explain how you can apply
themtoimprovetheef?ciency of certainqueries:
a. E
1
    (E
2
? E
3
) = (E
1
    E
2
? E
1
    E
3
).
b.     (
A
G
F
(E)) =
A
G
F
(    (E)),where  usesonly attributesfrom A.
c.     (E
1
  E
2
) =     (E
1
)   E
2
,where  usesonly attributesfrom E
1
.
13.2 For each of the following pairs of expressions, give instances of relations
that show the expressionsare not equivalent.
a. null
A
(R ? S)andnull
A
(R) ? null
A
(S).
b.   B<4
(
A
G
max(B) as B
(R)) and
A
G
max(B) as B
(  B<4
(R)).
618 Chapter13 QueryOptimization
c. In the preceding expressions, if both occurrences of max were re-
placedbyminwouldtheexpressionsbeequivalent?
d. (R   S)   T and R   (S   T)
In other words, the natural left outer join is not associative. (Hint:
Assumethattheschemasofthethreerelationsare R(a,b1),S(a,b2),
and T(a,b3), respectively.)
e.     (E
1
  E
2
)andE
1
      (E
2
),where  usesonlyattributesfrom E
2
.
13.3 SQL allows relationswithduplicates(Chapter3).
a. De?ne versions of the basic relational-algebra operations   , null,×,
  , ?, ?,and? that work on relations with duplicates, in a way
consistent with SQL.
b. Check which of the equivalence rules 1 through 7.b hold for the
multisetversionof therelational-algebrade?nedinpart a.
13.4 Considertherelationsr
1
(A,B,C),r
2
(C,D,E),andr
3
(E,F),withprimary
keysA,C,andE,respectively.Assumethatr
1
has1000tuples,r
2
has1500
tuples, and r
3
has 750 tuples. Estimate the size of r
1
  r
2
  r
3
,andgive
an ef?cient strategyfor computing the join.
13.5 Consider the relations r
1
(A,B,C), r
2
(C,D,E), and r
3
(E,F)ofPractice
Exercise 13.4. Assume that there are no primary keys, except the entire
schema. Let V(C,r
1
) be 900, V(C,r
2
) be 1100, V(E,r
2
) be 50, and V(E,r
3
)
be 100. Assume thatr
1
has 1000 tuples,r
2
has 1500 tuples, andr
3
has 750
tuples.Estimate the size ofr
1
  r
2
  r
3
and give an ef?cient strategy for
computing the join.
13.6 SupposethataB
+
-treeindexonbuildingisavailableonrelationdepartment,
andthatnootherindexisavailable.Whatwouldbethebestwaytohandle
the following selectionsthat involvenegation?
a.   ¬(building <“Watson”)
(department)
b.   ¬(building =“Watson”)
(department)
c.   ¬(building <“Watson” ? budget <50000)
(department)
13.7 Considerthe query:
select *
fromr,s
where upper(r.A)= upper(s.A);
where “upper” is a function that returns its input argument with all low-
ercaselettersreplacedby thecorrespondinguppercaseletters.
a. Findoutwhatplanisgeneratedforthisqueryonthedatabasesystem
you use.
PracticeExercises 619
b. Somedatabasesystemswouldusea(block)nested-loopjoinforthis
query, which can be very inef?cient. Brie?y explain how hash-join
ormerge-joincanbeusedforthisquery.
13.8 Giveconditions underwhich thefollowing expressionsareequivalent
A,B
G
agg(C)
(E
1
  E
2
)a n d(
A
G
agg(C)
(E
1
))  E
2
whereagg denotesany aggregationoperation.How can the abovecondi-
tions berelaxedifagg isone ofmin ormax?
13.9 Considertheissueofinterestingordersinoptimization.Supposeyouare
givenaquerythat computesthenaturaljoinofasetofrelations S. Given
asubsetS1ofS, what are the interestingordersof S1?
13.10 Show that, with n relations, there are (2(n ? 1))!/(n ? 1)! different join
orders.Hint:Acompletebinarytreeisonewhereeveryinternalnodehas
exactly two children. Use the fact that the number of different complete
binary treeswithnleafnodesis:
1
n
  2(n ?1)
(n ?1)
  Ifyouwish,youcanderivetheformulaforthenumberofcompletebinary
trees with n nodes from the formula for the number of binary trees with
nnodes.The number of binary treeswithnnodes is:
1
n +1
  2n
n
  This number is known as theCatalannumber, and its derivation can be
found in any standard textbookon data structuresor algorithms.
13.11 Show that the lowest-cost join order can be computed in time O(3
n
).
Assumethatyoucanstoreandlookupinformationaboutasetofrelations
(such as the optimal join order for the set, and the cost of that join order)
inconstanttime.(Ifyou?ndthisexercisedif?cult,atleastshowthelooser
time bound of O(2
2n
).)
13.12 Show that, if only left-deep join trees are considered, as in the System R
optimizer, the time taken to ?nd the most ef?cient join order is around
n2
n
.Assumethat thereis onlyone interestingsortorder.
13.13 ConsiderthebankdatabaseofFigure13.9,wheretheprimarykeysareun-
derlined.Constructthefollowing SQLqueriesforthisrelationaldatabase.
a. Writeanestedqueryontherelationaccountto?nd, for eachbranch
with name starting with B, all accounts with the maximum balance
atthe branch.
620 Chapter13 QueryOptimization
branch(branch name,branch city,assets)
customer(customer name,customer street,customer city)
loan (loan number,branch name,amount)
borrower (customer name,loan number)
account (account number,branch name,balance)
depositor (customer name,account number)
Figure13.9 Banking database for Exercise 13.13.
b. Rewrite the preceding query, without using a nested subquery; in
otherwords,decorrelatethequery.
c. Give a procedure (similar to that described in Section 13.4.4) for
decorrelatingsuch queries.
13.14 The setversionof thesemijoinoperator  is de?nedas follows:
r    s = null
R
(r     s)
where Risthesetofattributesintheschemaofr.Themultisetversionof
the semijoin operation returns the same set of tuples, but each tuple has
exactlyas many copiesas ithad inr.
ConsiderthenestedquerywesawinSection13.4.4which?ndsthenames
ofallinstructorswhotaughtacoursein2007.Writethequeryinrelational
algebra using the multiset semjoin operation, ensuring that the number
of duplicates of each name is the same as in the SQL query.(The semijoin
operationis widelyusedfordecorrelationof nestedqueries.)
Exercises
13.15 Suppose that a B
+
-tree index on (dept name, building) is available on re-
lation department. What would be the best way to handle the following
selection?
  (building < “Watson”) ? (budget < 55000) ? (dept name = “Music”)
(department)
13.16 Show how to derive the following equivalences by a sequence of trans-
formations using theequivalencerulesinSection13.2.1.
a.     1
?  2
?  3
(E) =    1
(    2
(    3
(E)))
b.     1
?  2
(E
1
    3
E
2
) =     1
(E
1
    3
(    2
(E
2
))), where   2
involves only
attributesfrom E
2
Exercises 621
13.17 Considerthe twoexpressions    (E
1
  E
2
)and    (E
1
  E
2
).
a. Showusinganexamplethatthetwoexpressionsarenot equivalent
ingeneral.
b. Give a simple condition on the predicate   , which if satis?ed will
ensurethat thetwoexpressionsareequivalent.
13.18 A set of equivalence rules is said to be complete if, whenever two expres-
sions are equivalent, one can be derived from the other by a sequence
of uses of the equivalence rules. Is the set of equivalence rules that we
considered in Section 13.2.1 complete? Hint: Consider the equivalence
  3=5
(r)={} .
13.19 Explain how to use a histogram to estimate the size of a selection of the
form  A?v
(r).
13.20 Supposetwo relationsr and s have histograms on attributesr.Aands.A,
respectively,butwithdifferentranges.Suggesthowtousethehistograms
to estimate the size of r   s. Hint: Split the ranges of each histogram
further.
13.21 Considerthe query
select A, B
from r
wherer.B <some (select B
from s
wheres.A =r.A)
Show how to decorrelate the above query using the multiset version of
thesemijoinoperation,de?nedinExercise13.14.
13.22 Describehowtoincrementallymaintaintheresultsofthefollowingoper-
ations,on both insertionsand deletions:
a. Unionand setdifference.
b. Leftouterjoin.
13.23 Give an example of an expression de?ning a materialized view and two
situations (sets of statistics for the input relations and the differentials)
such that incremental view maintenance is better than recomputation in
one situation,and recomputationisbetterintheother situation.
13.24 Supposeyouwanttogetanswerstor   s sortedonanattributeofr,and
want only the top K answers for some relatively small K. Give a good
way of evaluatingthequery:
a. Whenthejoinisonaforeignkeyofr referencings,wheretheforeign
keyattributeisdeclaredtobe not null.
b. When the joinis not on a foreignkey.
622 Chapter13 QueryOptimization
13.25 Consider a relation r(A,B,C), with an index on attribute A.Givean
exampleofaquerythatcanbeansweredbyusingtheindexonly,without
lookingatthetuplesintherelation.(Queryplansthatuseonlytheindex,
without accessing the actual relation,are calledindex-only plans.)
13.26 Supposeyou have anupdatequeryU. Givea simplesuf?cient condition
onUthatwillensurethattheHalloweenproblemcannotoccur,regardless
of the execution planchosen, orthe indicesthat exist.
BibliographicalNotes
The seminal work of Selinger et al. [1979] describes access-path selection in the
System R optimizer, which was one of the earliest relational-query optimizers.
QueryprocessinginStarburst,describedinHaasetal.[1989],formsthebasisfor
queryoptimizationin IBM DB2.
Graefe and McKenna [1993a] describe Volcano, an equivalence-rule–based
queryoptimizerthat,alongwithitssuccessorCascades(Graefe[1995]),formsthe
basisofqueryoptimizationinMicrosoftSQL Server.
Estimation of statistics of query results, such as result size, is addressed by
Ioannidis and Poosala [1995], Poosala et al. [1996], and Ganguly et al. [1996],
among others. Nonuniform distributions of values cause problems for estima-
tion of query size and cost. Cost-estimation techniques that use histograms of
value distributions have been proposed to tackle the problem. Ioannidis and
Christodoulakis [1993], Ioannidis and Poosala [1995], and Poosala et al. [1996]
present results in this area. The use of random sampling for constructing his-
tograms is well known in statistics, but issues in histogram construction in the
context of databasesisdiscussedinChaudhuri etal.[1998].
Klug [1982] was an early work on optimization of relational-algebra ex-
pressions with aggregate functions. Optimization of queries with aggregation
is addressed by Yan and Larson [1995] and Chaudhuri and Shim [1994]. Opti-
mization of queries containing outer joins is described in Rosenthal and Reiner
[1984], Galindo-Legaria and Rosenthal [1992], and Galindo-Legaria [1994]. Opti-
mizationoftop-K queriesisaddressedinCareyandKossmann[1998]andBruno
etal.[2002].
Optimization of nested subqueries is discussed in Kim [1982], Ganski and
Wong [1987], Dayal [1987], Seshadri et al. [1996] and Galindo-Legaria and Joshi
[2001].
Blakeley et al. [1986] describe techniques for maintenance of materialized
views.OptimizationofmaterializedviewmaintenanceplansisdescribedbyVista
[1998]andMistryetal.[2001].Queryoptimizationinthepresenceofmaterialized
views is addressed by Chaudhuri et al. [1995]. Index selection and materialized
viewselectionareaddressedbyRossetal.[1996],andChaudhuriandNarasayya
[1997].
Optimization of top-K queries is addressed in Carey and Kossmann [1998]
and Bruno et al. [2002]. A collection of techniques for join minimization has
BibliographicalNotes 623
been grouped under the name tableau optimization. The notion of a tableau was
introducedbyAhoetal.[1979b]andAhoetal.[1979a],andwasfurtherextended
by Sagivand Yannakakis [1981].
Parametric query-optimizationalgorithms have been proposed by Ioannidis
etal.[1992],Ganguly[1998] and Hulgeriand Sudarshan[2003]. Sellis[1988]was
an early work on multiquery optimization, while Roy et al. [2000] showed how
to integratemulti-queryoptimizationintoa Volcano-based queryoptimizer.
Galindo-Legaria et al. [2004] describes query processing and optimization
fordatabaseupdates,includingoptimizationofindexmaintenance,materialized
viewmaintenanceplansandintegrityconstraintchecking,alongwithtechniques
to handlethe Halloweenproblem.
This page intentionally left blank 
PART
4
TRANSACTION
MANAGEMENT
The term transaction refers to a collection of operations that form a single logical
unit of work. For instance, transfer of money from one account to another is a
transactionconsistingoftwoupdates,onetoeachaccount.
It isimportant that eitherall actions ofatransaction be executedcompletely,
or, in case of some failure, partial effects of each incomplete transaction be un-
done.Thispropertyiscalled atomicity. Further,once atransactionissuccessfully
executed, its effects must persist in the database—a system failure should not
resultinthedatabaseforgettingaboutatransactionthatsuccessfullycompleted.
Thispropertyiscalled durability.
Inadatabasesystemwheremultipletransactionsareexecutingconcurrently,
if updates to shared data are not controlled there is potential for transactions
to see inconsistent intermediate states created by updates of other transactions.
Such a situation can result in erroneous updates to data stored in the database.
Thus, database systems must provide mechanisms to isolate transactions from
the effects of other concurrently executing transactions. This property is called
isolation.
Chapter 14 describes the concept of a transaction in detail, including the
properties of atomicity, durability, isolation, and other properties provided by
thetransactionabstraction. Inparticular,thechaptermakesprecisethenotionof
isolationbymeansofaconceptcalledserializability.
Chapter 15 describes several concurrency-control techniques that help im-
plement the isolation property. Chapter 16 describes the recovery management
component of a database, which implements the atomicity and durability prop-
erties.
Takenasawhole,thetransaction-managementcomponentofadatabasesys-
tem allows application developersto focus on the implementation of individual
transactions,ignoringtheissuesofconcurrency andfaulttolerance.
625
This page intentionally left blank 
CHAPTER
14
Transactions
Often,acollectionofseveraloperationsonthedatabaseappearstobeasingleunit
fromthepointofviewofthedatabaseuser.Forexample,atransferoffundsfrom
acheckingaccounttoasavingsaccountisasingleoperationfromthecustomer’s
standpoint;withinthedatabasesystem,however,itconsistsofseveraloperations.
Clearly, it is essential that all these operations occur, or that, in case of a failure,
none occur. It would be unacceptable if the checking account were debited but
thesavingsaccountnotcredited.
Collections of operations that form a single logical unit of work are called
transactions. A database system must ensure proper execution of transactions
despite failures—either the entire transaction executes, or none of it does. Fur-
thermore, it must manage concurrent execution of transactions in a way that
avoidsthe introductionof inconsistency. Inour funds-transferexample,a trans-
action computing the customer’s total balance might see the checking-account
balancebeforeitisdebitedbythefunds-transfertransaction,butseethesavings
balanceafteritiscredited.Asaresult,itwouldobtainanincorrectresult.
This chapter introducesthe basic concepts of transaction processing. Details
on concurrent transaction processing and recovery from failures are in Chapters
15 and 16, respectively.Furthertopics in transaction processingarediscussedin
Chapter26.
14.1 TransactionConcept
A transactionisa unitofprogramexecutionthataccessesandpossiblyupdates
various data items. Usually, a transaction is initiated by a user program written
inahigh-leveldata-manipulationlanguage(typicallySQL),orprogramminglan-
guage (for example, C++, or Java), with embeddeddatabase accesses in JDBC or
ODBC. A transaction is delimited by statements (or function calls) of the form
begintransactionandendtransaction.Thetransactionconsistsofalloperations
executedbetweenthebegintransactionandendtransaction.
This collection of steps must appear to the user as a single, indivisible unit.
Sinceatransactionisindivisible,iteitherexecutesinitsentiretyornotatall.Thus,
ifatransactionbeginstoexecutebutfailsforwhateverreason,anychangestothe
627
628 Chapter 14 Transactions
databasethatthetransactionmayhavemademustbeundone.Thisrequirement
holdsregardlessofwhetherthetransactionitselffailed(forexample,ifitdivided
byzero),theoperatingsystemcrashed,orthecomputeritselfstoppedoperating.
As we shall see, ensuring that this requirement is met is dif?cult since some
changestothedatabasemaystillbestoredonlyinthemain-memoryvariablesof
the transaction, while others may have been written to the database and stored
ondisk.This “all-or-none”propertyisreferredtoasatomicity.
Furthermore,since atransaction isasingleunit,itsactions cannot appearto
be separated by other database operations not part of the transaction. While we
wishtopresentthisuser-levelimpressionoftransactions,weknowthatrealityis
quite different. Even a single SQL statement involves many separate accesses to
thedatabase,andatransactionmayconsistofseveral SQLstatements.Therefore,
thedatabasesystemmusttakespecialactionstoensurethattransactionsoperate
properlywithout interferencefromconcurrently executingdatabasestatements.
Thispropertyisreferredtoasisolation.
Evenifthesystemensurescorrectexecutionofatransaction,thisserveslittle
purposeifthesystemsubsequentlycrashesand,asaresult,thesystem “forgets”
about the transaction. Thus, a transaction’s actions must persist across crashes.
Thispropertyisreferredtoasdurability.
Becauseoftheabovethreeproperties,transactionsareanidealwayofstruc-
turing interaction with a database. This leads us to impose a requirement on
transactionsthemselves.Atransactionmustpreservedatabaseconsistency—ifa
transaction isrunatomicallyinisolationstartingfromaconsistentdatabase,the
databasemustagainbeconsistentattheendofthetransaction.Thisconsistency
requirementgoesbeyondthedataintegrityconstraintswehaveseenearlier(such
asprimary-keyconstraints, referentialintegrity, check constraints,and the like).
Rather,transactionsareexpectedtogobeyondthattoensurepreservationofthose
application-dependentconsistencyconstraintsthataretoocomplextostateusing
the SQLconstructsfordataintegrity.Howthisisdoneistheresponsibilityofthe
programmerwhocodesatransaction.Thispropertyisreferredtoasconsistency.
To restate the above more concisely, we require that the database system
maintainthefollowingpropertiesofthetransactions:
  Atomicity. Either all operations of the transaction are re?ected properly in
thedatabase,ornoneare.
  Consistency. Execution of a transaction in isolation (that is, with no other
transaction executing concurrently) preserves the consistency of the data-
base.
  Isolation. Even though multiple transactions may execute concurrently, the
systemguaranteesthat,foreverypairoftransactionsT
i
andT
j
,itappearstoT
i
thateither T
j
?nishedexecutionbefore T
i
startedor T
j
startedexecutionafter
T
i
?nished.Thus,eachtransactionisunawareofothertransactionsexecuting
concurrentlyinthesystem.
  Durability. After a transaction completes successfully, the changes it has
madetothedatabasepersist,eveniftherearesystemfailures.
14.2 ASimple TransactionModel 629
These properties are often called the ACID properties; the acronym is derived
fromthe?rstletterofeachofthefourproperties.
As we shall see later, ensuring the isolation property may have a signi?cant
adverse effect on system performance. For this reason, some applications com-
promise on the isolation property. We shall study these compromises after ?rst
studyingthestrictenforcementofthe ACIDproperties.
14.2 ASimpleTransactionModel
BecauseSQLisapowerfulandcomplexlanguage,webeginourstudyoftransac-
tionswithasimpledatabaselanguagethatfocusesonwhendataaremovedfrom
disk to main memory and from main memory to disk. In doing this, we ignore
SQL insert and delete operations,and deferconsideringthem until Section15.8.
The only actual operations on the data are restricted in our simple language to
arithmeticoperations.Laterweshalldiscusstransactionsinarealistic,SQL-based
contextwitharichersetofoperations.Thedataitemsinoursimpli?edmodelcon-
tainasingledatavalue(anumberinourexamples).Eachdataitemisidenti?ed
byaname(typicallyasingleletterinourexamples,thatis, A, B, C,etc.).
We shall illustrate the transaction concept using a simple bank application
consisting of several accounts and a set of transactions that access and update
thoseaccounts.Transactionsaccessdatausingtwooperations:
  read(X), which transfers the data item X from the database to a variable,
also called X, in a buffer in main memory belonging to the transaction that
executedthereadoperation.
  write(X), which transfers the value in the variable X in the main-memory
buffer of the transaction that executed the write to the data item X in the
database.
Itisimportanttoknowifachangetoadataitemappearsonlyinmainmemory
or if it has been written to the database on disk. In a real database system, the
writeoperationdoesnotnecessarilyresultintheimmediateupdateofthedataon
the disk;the write operationmay be temporarilystoredelsewhereand executed
on the disk later. For now, however, we shall assume that the write operation
updatesthedatabaseimmediately.WeshallreturntothissubjectinChapter16.
Let T
i
be a transaction that transfers $50 from account A to account B.This
transactioncanbede?nedas:
T
i
: read(A);
A:= A ?50;
write(A);
read(B);
B:= B+50;
write(B).
630 Chapter 14 Transactions
Let us now consider each of the ACID properties. (For ease of presentation, we
considertheminanorderdifferentfromtheorder A-C-I-D.)
  Consistency: The consistency requirement here is that the sum of A and B
be unchanged by the execution of the transaction. Without the consistency
requirement,moneycouldbecreatedordestroyedbythetransaction!Itcan
be veri?ed easily that, if the database is consistent before an execution of
the transaction, the database remains consistent after the execution of the
transaction.
Ensuring consistency for an individual transaction is the responsibility
of the application programmerwho codesthe transaction. This taskmay be
facilitated by automatic testing of integrity constraints, as we discussed in
Section4.4.
  Atomicity: Supposethat, justbefore the executionof transaction T
i
,theval-
ues of accounts A and B are $1000 and $2000, respectively. Now suppose
that, during the execution of transaction T
i
, a failureoccurs that prevents T
i
from completing itsexecution successfully. Further,supposethat the failure
happened after the write(A) operation but before the write(B)operation.In
this case, the values of accounts A and B re?ected in the database are $950
and$2000.Thesystemdestroyed$50asaresultofthisfailure.Inparticular,
wenotethatthesum A+ Bisnolongerpreserved.
Thus, because of the failure, the state of the system no longer re?ects
a real state of the world that the database is supposed to capture. We term
such astate an inconsistent state. We mustensure that such inconsistencies
arenot visibleinadatabasesystem.Note,however,that thesystemmustat
some point be in an inconsistent state. Even if transaction T
i
is executed to
completion,thereexistsapointatwhichthevalueofaccountAis$950andthe
valueofaccount Bis$2000,whichisclearlyaninconsistentstate.Thisstate,
however, is eventually replaced by the consistent state where the value of
account Ais$950,andthevalueofaccount Bis$2050.Thus,ifthetransaction
neverstartedorwasguaranteedtocomplete,suchaninconsistentstatewould
notbevisibleexceptduringtheexecutionofthetransaction.Thatisthereason
fortheatomicityrequirement:Iftheatomicitypropertyispresent,allactions
ofthetransactionarere?ectedinthedatabase,ornoneare.
The basic idea behind ensuring atomicity is this: The database system
keeps track (on disk) of the old values of any data on which a transaction
performs a write. This information is written to a ?le called the log.Ifthe
transactiondoesnotcompleteitsexecution,thedatabasesystemrestoresthe
old values from the log to make it appear as though the transaction never
executed.We discuss these ideasfurther in Section14.4. Ensuring atomicity
is the responsibility of the database system; speci?cally, it is handled by a
componentofthedatabasecalledtherecoverysystem,whichwedescribein
detailinChapter16.
  Durability:Oncetheexecutionofthetransactioncompletessuccessfully,and
the user who initiated the transaction has been noti?ed that the transfer of
14.2 ASimple TransactionModel 631
fundshastakenplace,itmustbethecasethatnosystemfailurecanresultin
alossofdatacorrespondingtothistransferoffunds.Thedurabilityproperty
guarantees that, once a transaction completes successfully, all the updates
that it carried out on the database persist, even if there is a system failure
afterthetransactioncompletesexecution.
Weassumefornowthatafailureofthecomputersystemmayresultinloss
of data in main memory, but data written to disk are never lost. Protection
against loss of data on disk is discussed in Chapter 16. We can guarantee
durabilitybyensuringthateither:
1. The updates carried out by the transaction have been written to disk
beforethetransactioncompletes.
2. Informationabouttheupdatescarriedoutbythetransactionandwrit-
tentodiskissuf?cienttoenablethedatabasetoreconstructtheupdates
whenthedatabasesystemisrestartedafterthefailure.
Therecoverysystemofthedatabase,describedinChapter16,isresponsible
forensuringdurability,inadditiontoensuringatomicity.
  Isolation: Even if the consistency and atomicity properties are ensured for
each transaction, if several transactions are executed concurrently, their op-
erationsmayinterleaveinsomeundesirableway,resultinginaninconsistent
state.
Forexample,aswesawearlier,thedatabaseistemporarilyinconsistent
while the transaction to transfer funds from A to B is executing, with the
deductedtotalwrittento AandtheincreasedtotalyettobewrittentoB.Ifa
second concurrently running transaction reads Aand B atthisintermediate
pointandcomputes A+B,itwillobserveaninconsistentvalue.Furthermore,
if this second transaction then performs updates on Aand B based on the
inconsistent values that it read, the database may be left in an inconsistent
stateevenafterbothtransactionshavecompleted.
Awaytoavoidtheproblemofconcurrentlyexecutingtransactionsisto
execute transactions serially—that is, one after the other. However, concur-
rent execution of transactions provides signi?cant performance bene?ts, as
weshall seeinSection14.5. Othersolutions have thereforebeendeveloped;
theyallowmultipletransactionstoexecuteconcurrently.
We discusstheproblemscausedbyconcurrently executingtransactions
in Section 14.5. The isolation property of a transaction ensures that the con-
current execution of transactions results in a system state that is equivalent
toastatethatcouldhavebeenobtainedhadthesetransactionsexecutedone
atatimeinsomeorder.Weshalldiscusstheprinciplesofisolationfurtherin
Section 14.6. Ensuring the isolation property is the responsibility of a com-
ponentofthedatabasesystemcalledtheconcurrency-controlsystem,which
wediscusslater,inChapter15.
632 Chapter 14 Transactions
14.3 StorageStructure
To understand how to ensure the atomicity and durability propertiesof a trans-
action,wemustgainabetterunderstandingofhowthevariousdataitemsinthe
databasemaybestoredandaccessed.
InChapter10wesawthatstoragemediacanbedistinguishedbytheirrelative
speed, capacity, and resilience to failure, and classi?ed as volatile storage or
nonvolatilestorage.Wereviewtheseterms,andintroduceanotherclassofstorage,
calledstablestorage.
  Volatile storage. Information residing in volatile storage does not usually
survive system crashes. Examples of such storage are main memory and
cache memory. Access to volatile storage is extremely fast, both because of
thespeedofthememoryaccessitself,andbecauseitispossibletoaccessany
dataiteminvolatilestoragedirectly.
  Nonvolatile storage. Information residing in nonvolatile storage survives
system crashes. Examples of nonvolatile storage include secondary storage
devicessuchasmagneticdiskand?ashstorage,usedforonlinestorage,and
tertiarystorage devicessuch as optical media,and magnetic tapes,used for
archival storage. At the current state of technology, nonvolatile storage is
slowerthanvolatilestorage,particularlyforrandomaccess.Bothsecondary
and tertiary storage devices, however, are susceptible to failure which may
resultinlossofinformation.
  Stable storage. Information residing in stable storage is never lost (never
should be taken with a grain of salt, since theoretically never cannot be
guaranteed—for example, it is possible, although extremely unlikely, that
a black hole may envelop the earth and permanently destroy all data!). Al-
though stable storage is theoretically impossible to obtain, it can be closely
approximated by techniques that make data loss extremely unlikely. To im-
plement stable storage, we replicate the information in several nonvolatile
storagemedia(usuallydisk)withindependentfailuremodes.Updatesmust
bedonewithcaretoensurethatafailureduringanupdatetostablestorage
does not cause a loss of information. Section 16.2.1 discusses stable-storage
implementation.
The distinctions among the various storage types can be less clear in practice
than in our presentation. For example, certain systems, for example some RAID
controllers, provide battery backup, so that some main memory can survive
systemcrashesandpowerfailures.
Foratransactiontobedurable,itschangesneedtobewrittentostablestorage.
Similarly, for a transaction to be atomic, log records need to be written to stable
storagebeforeanychangesaremadetothedatabaseondisk.Clearly,thedegree
to which a system ensures durability and atomicity depends on how stable its
implementationofstablestoragereallyis.Insomecases,asinglecopyondiskis
consideredsuf?cient,butapplicationswhosedataarehighlyvaluableandwhose
14.4 TransactionAtomicityandDurability 633
transactions are highly important require multiple copies, or, in other words, a
closerapproximationoftheidealizedconceptofstablestorage.
14.4 TransactionAtomicityandDurability
As we noted earlier, a transaction may not always complete its execution suc-
cessfully.Suchatransaction istermed aborted.Ifwearetoensuretheatomicity
property,anabortedtransactionmusthavenoeffectonthestateofthedatabase.
Thus, any changes that the aborted transaction made to the database must be
undone. Once the changes caused by an aborted transaction have been undone,
wesaythatthetransactionhasbeenrolledback.Itispartoftheresponsibilityof
therecoveryschemetomanagetransactionaborts.Thisisdonetypicallybymain-
tainingalog.Eachdatabasemodi?cationmadebyatransactionis?rstrecorded
inthelog.Werecordtheidenti?erofthetransactionperformingthemodi?cation,
the identi?er of the data item being modi?ed, and both the old value (prior to
modi?cation)andthenewvalue(aftermodi?cation)ofthedataitem.Onlythen
isthedatabaseitselfmodi?ed.Maintainingalogprovidesthepossibilityofredo-
ingamodi?cationtoensureatomicityanddurabilityaswellasthepossibilityof
undoingamodi?cationtoensureatomicityincaseofafailureduringtransaction
execution.Detailsoflog-basedrecoveryarediscussedinChapter16.
Atransactionthatcompletesitsexecutionsuccessfullyissaidtobe commit-
ted.Acommittedtransactionthathasperformedupdatestransformsthedatabase
intoanewconsistentstate,whichmustpersistevenifthereisasystemfailure.
Once a transaction has committed, we cannot undo its effects by aborting
it. The only way to undo the effects of a committed transaction is to execute a
compensatingtransaction.Forinstance,ifatransactionadded$20toanaccount,
the compensating transaction would subtract $20 from the account. However,it
is not always possible to create such a compensating transaction. Therefore, the
responsibility of writing and executing a compensating transaction is left to the
user,andisnothandledbythedatabasesystem.Chapter26includesadiscussion
ofcompensatingtransactions.
We need to be more precise about what we mean by successful completion
of a transaction. We therefore establish a simple abstract transaction model. A
transactionmustbeinoneofthefollowingstates:
  Active,theinitialstate;thetransactionstaysinthisstatewhileitisexecuting.
  Partiallycommitted,afterthe?nalstatementhasbeenexecuted.
  Failed,afterthediscoverythatnormalexecutioncannolongerproceed.
  Aborted,afterthetransactionhasbeenrolledbackandthedatabasehasbeen
restoredtoitsstatepriortothestartofthetransaction.
  Committed,aftersuccessfulcompletion.
ThestatediagramcorrespondingtoatransactionappearsinFigure14.1.We
say that a transaction has committed only if it has entered the committed state.
634 Chapter 14 Transactions
active
failed
partially
comminulled
comminulled
aborted
Figure14.1 State diagram of a transaction.
Similarly,wesaythatatransactionhasabortedonlyifithasenteredtheaborted
state.Atransactionissaidtohaveterminatedifithaseithercommittedoraborted.
A transaction starts in the activestate. When it ?nishes its?nal statement,it
entersthepartiallycommittedstate.Atthispoint,thetransactionhascompleted
itsexecution,butitisstillpossiblethatitmayhavetobeaborted,sincetheactual
output may still be temporarily residing in main memory, and thus a hardware
failuremayprecludeitssuccessfulcompletion.
Thedatabasesystemthenwritesoutenoughinformationtodiskthat,evenin
theeventofafailure,theupdatesperformedbythetransactioncanbere-created
when the system restarts after the failure. When the last of this information is
writtenout,thetransactionentersthecommittedstate.
Asmentionedearlier,weassumefornowthatfailuresdonotresultinlossof
dataondisk.Chapter16discussestechniquestodealwithlossofdataondisk.
A transaction enters the failed state after the system determines that the
transactioncannolongerproceedwithitsnormalexecution(forexample,because
of hardware or logical errors). Such a transaction must be rolled back. Then, it
enterstheabortedstate.Atthispoint,thesystemhastwooptions:
  It can restart the transaction, but only if the transaction was aborted as a
result of some hardware or software error that was not created through the
internallogicofthetransaction.Arestartedtransactionisconsideredtobea
newtransaction.
  Itcankillthetransaction.Itusuallydoessobecauseofsomeinternallogical
error that can be corrected only by rewriting the application program, or
becausetheinputwasbad,orbecausethedesireddatawerenotfoundinthe
database.
Wemustbecautiouswhendealingwithobservableexternalwrites,suchas
writes to a user’s screen, or sending email. Once such a write has occurred, it
cannot be erased, since it may have been seen external to the database system.
14.5 TransactionIsolation 635
Mostsystemsallowsuchwritestotakeplaceonlyafterthetransactionhasentered
the committed state. One way to implement such a scheme is for the database
system to store any value associated with such external writes temporarily in
a special relation in the database, and to perform the actual writes only after
the transaction enters the committed state. If the system should fail after the
transaction has entered the committed state, but before it could complete the
externalwrites,thedatabasesystemwillcarryouttheexternalwrites(usingthe
datainnonvolatilestorage)whenthesystemisrestarted.
Handling external writes can be more complicated in some situations. For
example,suppose theexternal actionis that of dispensingcash at an automated
tellermachine,andthesystemfailsjustbeforethecashisactuallydispensed(we
assume that cash can be dispensed atomically). It makes no sense to dispense
cash when the system is restarted, since the user may have left the machine. In
such a case a compensating transaction, such as depositing the cash back in the
user’saccount,needstobeexecutedwhenthesystemisrestarted.
As another example, consider a user making a booking over the Web. It is
possible that the database system or the application server crashes just after the
booking transaction commits. It is also possible that the network connection to
the user is lost just after the booking transaction commits. In either case, even
though the transaction has committed, the external write has not taken place.
To handle such situations, the application must be designed such that when the
user connects to the Web application again, she will be able to see whether her
transactionhadsucceededornot.
For certain applications, it may be desirable to allow active transactions to
display data to users, particularly for long-duration transactions that run for
minutes or hours. Unfortunately, we cannot allow such output of observable
data unless we are willing to compromise transaction atomicity. In Chapter 26,
wediscussalternativetransactionmodelsthatsupportlong-duration,interactive
transactions.
14.5 TransactionIsolation
Transaction-processing systems usually allow multiple transactions to run con-
currently. Allowing multiple transactions to update data concurrently causes
several complications with consistency of the data, as we saw earlier. Ensuring
consistency in spite of concurrent execution of transactions requiresextra work;
itisfareasiertoinsistthattransactionsrun serially—thatis,oneatatime,each
startingonlyafterthepreviousonehascompleted.However,therearetwogood
reasonsforallowingconcurrency:
  Improved throughput and resource utilization. A transaction consists of
many steps.Someinvolve I/O activity;othersinvolve CPU activity.The CPU
and the disks in a computer system can operate in parallel. Therefore, I/O
activity can be done in parallel with processing at the CPU. The parallelism
636 Chapter 14 Transactions
of the CPU and the I/O system can therefore be exploited to run multiple
transactions in parallel. While a read or write on behalf of one transaction
is in progress on one disk, another transaction can be running in the CPU,
while another disk may be executing a read or write on behalf of a third
transaction. All of this increases the throughput of the system—that is, the
numberoftransactionsexecutedinagivenamountoftime.Correspondingly,
theprocessoranddiskutilizationalsoincrease;inotherwords,theprocessor
anddiskspendlesstimeidle,ornotperforminganyusefulwork.
  Reduced waiting time. There may be a mix of transactions running on a
system,someshortandsomelong.Iftransactionsrunserially,ashorttrans-
actionmayhavetowaitforaprecedinglongtransactiontocomplete,which
canleadtounpredictabledelaysinrunningatransaction.Ifthetransactions
are operating on different parts of the database, it is better to let them run
concurrently, sharing the CPU cycles and disk accesses among them. Con-
current execution reduces the unpredictable delaysin running transactions.
Moreover, it also reduces the average response time:theaveragetimefora
transactiontobecompletedafterithasbeensubmitted.
Themotivationforusingconcurrentexecutioninadatabaseisessentiallythe
sameasthemotivationforusingmultiprogramminginanoperatingsystem.
Whenseveraltransactionsrunconcurrently,theisolationpropertymaybevi-
olated,resultingindatabaseconsistencybeingdestroyeddespitethecorrectness
of each individual transaction. In this section, we present the concept of sched-
ulestohelpidentifythoseexecutionsthatareguaranteedtoensuretheisolation
propertyandthusdatabaseconsistency.
Thedatabasesystemmustcontroltheinteractionamongtheconcurrenttrans-
actionstopreventthemfromdestroyingtheconsistencyofthedatabase.Itdoesso
throughavarietyofmechanismscalledconcurrency-controlschemes.Westudy
concurrency-control schemesinChapter15;fornow,we focus on theconcept of
correctconcurrentexecution.
Consider again the simpli?ed banking system of Section 14.1, which has
severalaccounts,andasetoftransactionsthataccessandupdatethoseaccounts.
Let T
1
and T
2
betwotransactionsthattransferfundsfromoneaccounttoanother.
Transaction T
1
transfers$50fromaccount Atoaccount B.Itisde?nedas:
T
1
: read(A);
A:= A ?50;
write(A);
read(B);
B:= B+50;
write(B).
Transaction T
2
transfers10percentofthebalancefromaccount Atoaccount B.It
isde?nedas:
14.5 TransactionIsolation 637
TRENDSINCONCURRENCY
Several current trends in the ?eld of computing are giving rise to an increase
in the amount of concurrency possible. As database systems exploit this con-
currency to increase overall system performance, there will necessarily be an
increasingnumberoftransactionsrunconcurrently.
Earlycomputershadonlyoneprocessor.Therefore,therewasneveranyreal
concurrencyinthecomputer.Theonlyconcurrencywasapparentconcurrency
createdbytheoperatingsystemasitsharedtheprocessoramongseveraldistinct
tasksorprocesses.Moderncomputersarelikelytohavemanyprocessors.These
may be truly distinct processors all part of the one computer. However even a
single processormaybeable torunmorethanoneprocessatatimebyhaving
multiple cores.TheIntelCoreDuoprocessorisawell-knownexampleofsucha
multicoreprocessor.
Fordatabasesystemstotakeadvantageofmultipleprocessorsandmultiple
cores,twoapproachesarebeingtaken.Oneisto?ndparallelismwithinasingle
transaction or query. Another is to support a very large number of concurrent
transactions.
Many service providers now use large collections of computers rather than
large mainframe computers to provide their services. They are making this
choicebasedonthelowercostofthisapproach.Aresultofthisisyetafurther
increaseinthedegreeofconcurrencythatcanbesupported.
The bibliographic notes refer to texts that describe these advances in com-
puterarchitectureandparallelcomputing.Chapter18describesalgorithmsfor
buildingparalleldatabasesystems,whichexploitmultipleprocessorsandmul-
tiplecores.
T
2
: read(A);
temp:= A*0.1;
A:= A ? temp;
write(A);
read(B);
B:= B+ temp;
write(B).
Supposethecurrentvaluesofaccounts Aand Bare$1000and$2000,respec-
tively. Suppose also that the two transactions are executed one at a time in the
order T
1
followed by T
2
. This execution sequence appears in Figure 14.2. In the
?gure, the sequence of instruction steps is in chronological order from top to
bottom, with instructions of T
1
appearing in the left column and instructions of
T
2
appearinginthe rightcolumn. The?nal valuesofaccounts Aand B,afterthe
execution in Figure 14.2 takes place, are $855 and $2145, respectively. Thus, the
totalamountofmoneyinaccountsAandB—thatis,thesumA+B—ispreserved
aftertheexecutionofbothtransactions.
638 Chapter 14 Transactions
T
1
T
2
read(A)
A:= A ?50
write(A)
read(B)
B:= B +50
write(B)
commit
read(A)
temp:= A ?0.1
A:= A ? temp
write(A)
read(B)
B:= B + temp
write(B)
commit
Figure14.2 Schedule 1—a serial schedule in which T
1
is followed by T
2
.
Similarly,ifthetransactionsareexecutedoneatatimeintheorderT
2
followed
by T
1
,thenthecorrespondingexecutionsequenceisthatofFigure14.3.Again,as
expected, the sum A + B is preserved, and the ?nal values of accounts A and B
are$850and$2150,respectively.
T
1
T
2
read(A)
temp:= A ?0.1
A:= A ? temp
write(A)
read(B)
B:= B + temp
write(B)
commit
read(A)
A:= A ?50
write(A)
read(B)
B:= B +50
write(B)
commit
Figure14.3 Schedule 2—a serial schedule in which T
2
is followed by T
1
.
14.5 TransactionIsolation 639
Theexecutionsequencesjustdescribedarecalled schedules.Theyrepresent
thechronologicalorderinwhichinstructionsareexecutedinthesystem.Clearly,
ascheduleforasetoftransactionsmustconsistofallinstructionsofthosetrans-
actions, and must preserve the order in which the instructions appear in each
individual transaction. For example, in transaction T
1
, the instruction write(A)
must appear before the instruction read(B), in any valid schedule. Note that we
include in our schedules the commit operation to indicate that the transaction
has entered the committed state. In the following discussion, we shall refer to
the?rstexecutionsequence(T
1
followedby T
2
)asschedule1,and tothe second
executionsequence(T
2
followedby T
1
)asschedule2.
These schedules are serial: Each serial schedule consists of a sequence of
instructions from various transactions, where the instructions belonging to one
single transaction appear together in that schedule. Recalling a well-known for-
mula from combinatorics, we note that, for a set of n transactions, there exist n
factorial(n!)differentvalidserialschedules.
When the database system executes several transactions concurrently, the
corresponding schedule no longer needs to be serial. If two transactions are
running concurrently, the operating system may execute one transaction for a
little while, then perform a context switch, execute the second transaction for
sometime,andthenswitchbacktothe?rsttransactionforsometime,andsoon.
Withmultipletransactions,the CPUtimeissharedamongallthetransactions.
Severalexecutionsequencesarepossible,sincethevariousinstructionsfrom
bothtransactionsmaynowbeinterleaved.Ingeneral,itisnotpossibletopredict
exactly how many instructions of a transaction will be executed before the CPU
switchestoanothertransaction.
1
Returning to our previous example, suppose that the two transactions are
executed concurrently. One possible schedule appears in Figure 14.4. After this
executiontakesplace,wearriveatthesamestateastheoneinwhichthetransac-
tionsareexecutedseriallyintheorder T
1
followedby T
2
.ThesumA+Bisindeed
preserved.
Not all concurrent executions result in a correct state. To illustrate, consider
the schedule of Figure 14.5. After the execution of this schedule, we arrive at a
statewherethe?nalvaluesofaccounts Aand Bare$950and$2100,respectively.
This?nal stateisan inconsistent state,since wehave gained$50 intheprocessof
theconcurrentexecution.Indeed,thesumA+Bisnotpreservedbytheexecution
ofthetwotransactions.
Ifcontrolofconcurrentexecutionisleftentirelytotheoperatingsystem,many
possibleschedules,includingonesthatleavethedatabaseinaninconsistentstate,
suchastheonejustdescribed,arepossible.Itisthejobofthedatabasesystemto
ensure that any schedule that is executed will leave the database in a consistent
state.Theconcurrency-controlcomponentofthedatabasesystemcarriesoutthis
task.
1
The number of possible schedules for a set of n transactions is very large. There are n! different serial schedules.
Consideringallthepossiblewaysthatstepsoftransactionsmightbeinterleaved,thetotalnumberofpossibleschedules
ismuchlargerthan n!.
640 Chapter 14 Transactions
T
1
T
2
read(A)
A:= A ?50
write(A)
read(A)
temp:= A ?0.1
A:= A ? temp
write(A)
read(B)
B:= B +50
write(B)
commit
read(B)
B:= B + temp
write(B)
commit
Figure14.4 Schedule 3—a concurrent schedule equivalent to schedule 1.
We can ensure consistency of the database under concurrent execution by
makingsurethatanyschedulethatisexecutedhasthesameeffectasaschedule
thatcouldhaveoccurredwithoutanyconcurrentexecution.Thatis,theschedule
should, in some sense, be equivalent to a serial schedule. Such schedules are
calledserializableschedules.
T
1
T
2
read(A)
A:= A ?50
read(A)
temp:= A ?0.1
A:= A ? temp
write(A)
read(B)
write(A)
read(B)
B:= B +50
write(B)
commit
B:= B + temp
write(B)
commit
Figure14.5 Schedule 4—a concurrent schedule resulting in an inconsistent state.
14.6 Serializability 641
14.6 Serializability
Beforewecanconsiderhowtheconcurrency-controlcomponentofthedatabase
systemcanensureserializability,weconsiderhowtodeterminewhenaschedule
isserializable.Certainly,serialschedulesareserializable,butifstepsofmultiple
transactionsareinterleaved,itishardertodeterminewhetherascheduleisseri-
alizable.Sincetransactionsareprograms,itisdif?culttodetermineexactlywhat
operationsatransactionperformsandhowoperationsofvarioustransactionsin-
teract.Forthisreason,weshallnotconsiderthevarioustypesofoperationsthata
transactioncanperformonadataitem,butinsteadconsideronlytwooperations:
read and write. We assume that, between a read(Q) instruction and a write(Q)
instructiononadataitem Q,atransactionmayperformanarbitrarysequenceof
operationsonthecopyof Qthatisresidinginthelocalbufferofthetransaction.
Inthismodel,theonlysigni?cantoperationsofatransaction,fromascheduling
point of view, are its read and write instructions. Commit operations, though
relevant,arenotconsidereduntilSection14.7.Wethereforemayshowonlyread
andwriteinstructionsinschedules,aswedoforschedule3inFigure14.6.
Inthissection,wediscussdifferentformsofscheduleequivalence,butfocus
onaparticularformcalledcon?ictserializability.
Letusconsideraschedule Sinwhichtherearetwoconsecutiveinstructions,
I and J,oftransactionsT
i
and T
j
,respectively(i nullj).If I and J refertodifferent
dataitems,thenwecanswap I and J withoutaffectingtheresultsofanyinstruc-
tionintheschedule.However,if I and J refer to the same data item Q,thenthe
orderofthetwostepsmaymatter.Sincewearedealingwithonlyreadandwrite
instructions,therearefourcasesthatweneedtoconsider:
1. I = read(Q), J = read(Q). The order of I and J does not matter, since the
samevalueof Qisreadby T
i
and T
j
,regardlessoftheorder.
2. I =read(Q), J =write(Q).If I comesbefore J,thenT
i
doesnotreadthevalue
of Qthatiswrittenby T
j
ininstruction J.IfJ comesbefore I,thenT
i
reads
thevalueof Qthatiswrittenby T
j
.Thus,theorderofI and J matters.
T
1
T
2
read(A)
write(A)
read(A)
write(A)
read(B)
write(B)
read(B)
write(B)
Figure14.6 Schedule 3—showing only the read and write instructions.
642 Chapter 14 Transactions
T
1
T
2
read(A)
write(A)
read(A)
read(B)
write(A)
write(B)
read(B)
write(B)
Figure14.7 Schedule 5—schedule 3 after swapping of a pair of instructions.
3. I = write(Q), J = read(Q). The order of I and J matters for reasons similar
tothoseofthepreviouscase.
4. I = write(Q), J = write(Q). Since both instructions are write operations, the
orderoftheseinstructionsdoesnotaffecteither T
i
or T
j
.However,thevalue
obtained by the next read(Q) instruction of S is affected,since the result of
only the latter of the two write instructions is preserved in the database. If
there is no other write(Q) instruction after I and J in S, then the order of I
and J directly affects the ?nal value of Q in the database state that results
fromschedule S.
Thus,onlyinthecasewhereboth I and J are readinstructionsdoestherelative
orderoftheirexecutionnotmatter.
We say that I and J con?ict if they are operations by different transactions
onthesamedataitem,andatleastoneoftheseinstructionsisawriteoperation.
Toillustratetheconceptofcon?ictinginstructions,weconsiderschedule3in
Figure 14.6. The write(A) instruction of T
1
con?icts with the read(A) instruction
of T
2
. However, the write(A) instruction of T
2
does not con?ict with the read(B)
instructionof T
1
,becausethetwoinstructionsaccessdifferentdataitems.
T
1
T
2
read(A)
write(A)
read(B)
write(B)
read(A)
write(A)
read(B)
write(B)
Figure14.8 Schedule 6—a serial schedule that is equivalent to schedule 3.
14.6 Serializability 643
T
3
T
4
read(Q)
write(Q)
write(Q)
Figure14.9 Schedule 7.
Let I and J be consecutive instructions of a schedule S.IfI and J are in-
structionsofdifferenttransactionsand I and J donotcon?ict,thenwecanswap
theorderof I and J toproduceanewschedule S
null
. Sisequivalentto S
null
,sinceall
instructionsappearinthesameorderinbothschedulesexceptfor I and J,whose
orderdoesnotmatter.
Sincethewrite(A)instructionofT
2
inschedule3ofFigure14.6doesnotcon?ict
withthe read(B)instructionof T
1
,wecanswaptheseinstructionstogeneratean
equivalent schedule, schedule 5, in Figure 14.7. Regardless of the initial system
state,schedules3and5bothproducethesame?nalsystemstate.
Wecontinuetoswapnoncon?ictinginstructions:
  Swaptheread(B)instructionof T
1
withtheread(A)instructionof T
2
.
  Swapthewrite(B)instructionof T
1
withthewrite(A)instructionof T
2
.
  Swapthewrite(B)instructionof T
1
withtheread(A)instructionof T
2
.
The ?nal result of these swaps, schedule 6 of Figure 14.8, is a serial schedule.
Note that schedule 6 is exactly the same as schedule 1, but it shows only the
read and write instructions. Thus, we have shown that schedule 3 is equivalent
toaserialschedule.Thisequivalenceimpliesthat,regardlessoftheinitialsystem
state,schedule3willproducethesame?nalstateaswillsomeserialschedule.
If a schedule S can be transformed into a schedule S
null
by a seriesof swaps of
noncon?ictinginstructions,wesaythat Sand S
null
arecon?ictequivalent.
2
Not all serial schedules are con?ict equivalent to each other. For example,
schedules1and2arenotcon?ictequivalent.
The concept of con?ict equivalence leads to the concept of con?ict serializ-
ability.Wesaythataschedule Sis con?ict serializableifitiscon?ict equivalent
to a serial schedule. Thus, schedule 3 is con?ict serializable, since it is con?ict
equivalenttotheserialschedule1.
Finally, consider schedule 7 of Figure 14.9; it consists of only the signi?cant
operations (that is, the read and write)oftransactionsT
3
and T
4
.Thisschedule
is not con?ict serializable, since it is not equivalent to either the serial schedule
<T
3
,T
4
>ortheserialschedule<T
4
,T
3
>.
2
Weusethe term con?ict equivalenttodistinguishthe way wehavejust de?nedequivalencefromotherde?nitionsthat
weshalldiscusslateroninthissection.
644 Chapter 14 Transactions
(a) (b)
T
1
T
2
T
2
T
1
Figure14.10 Precedence graph for (a) schedule 1 and (b) schedule 2.
We now present a simple and ef?cient method for determiningcon?ict seri-
alizability of a schedule. Consider a schedule S.Weconstructadirectedgraph,
calledaprecedencegraph,fromS.Thisgraphconsistsofapair G=( V, E),where
V is aset of verticesand E is a set of edges.The set of verticesconsists of all the
transactions participating in the schedule. The set of edges consists of all edges
T
i
? T
j
forwhichoneofthreeconditionsholds:
1. T
i
executeswrite(Q)beforeT
j
executesread(Q).
2. T
i
executesread(Q)beforeT
j
executeswrite(Q).
3. T
i
executeswrite(Q)beforeT
j
executeswrite(Q).
Ifanedge T
i
? T
j
existsintheprecedencegraph,then,inanyserialschedule S
null
equivalentto S, T
i
mustappearbefore T
j
.
For example, the precedence graph for schedule 1 in Figure 14.10a contains
the single edge T
1
? T
2
, since all the instructions of T
1
are executed before the
?rst instruction of T
2
is executed. Similarly, Figure 14.10b shows the precedence
graphforschedule2withthesingleedge T
2
? T
1
,sincealltheinstructionsof T
2
areexecutedbeforethe?rstinstructionofT
1
isexecuted.
The precedence graph for schedule 4 appears in Figure 14.11. It contains
theedge T
1
? T
2
,becauseT
1
executesread(A)beforeT
2
executeswrite(A).Italso
containstheedgeT
2
?T
1
,becauseT
2
executesread(B)beforeT
1
executeswrite(B).
IftheprecedencegraphforShasacycle,thenscheduleSisnotcon?ictserial-
izable.Ifthegraphcontainsnocycles,thentheschedule Siscon?ictserializable.
Aserializabilityorderofthetransactionscanbeobtainedby?ndingalinear
order consistent with the partial order of the precedence graph. This process is
calledtopologicalsorting.Thereare,ingeneral,severalpossiblelinearordersthat
T
1
T
2
Figure14.11 Precedence graph for schedule 4.
14.6 Serializability 645
(b) (c)
(a)
T
m
T
k
T
k
T
k
T
j
T
i
T
m
T
j
T
i
T
m
T
i
T
j
Figure14.12 Illustration of topological sorting.
canbeobtainedthroughatopologicalsort.Forexample,thegraphofFigure14.12a
hasthetwoacceptablelinearorderingsshowninFigures14.12band14.12c.
Thus, to test for con?ict serializability, we need to construct the precedence
graphandtoinvokeacycle-detectionalgorithm.Cycle-detectionalgorithmscan
be found in standardtextbooks on algorithms.Cycle-detectionalgorithms, such
asthosebasedondepth-?rstsearch,requireontheorderof n
2
operations,where
nisthenumberofverticesinthegraph(thatis,thenumberoftransactions).
Returning to our previous examples, note that the precedence graphs for
schedules 1 and 2 (Figure 14.10) indeed do not contain cycles. The precedence
graphforschedule4(Figure14.11),ontheotherhand,containsacycle,indicating
thatthisscheduleisnotcon?ictserializable.
Itispossibletohavetwoschedulesthatproducethesameoutcome,butthat
arenotcon?ictequivalent.Forexample,considertransaction T
5
,whichtransfers
$10 from account B to account A. Let schedule 8 be as de?ned in Figure 14.13.
Weclaimthatschedule8isnotcon?ictequivalenttotheserialschedule<T
1
,T
5
>,
since, in schedule 8, the write(B) instruction of T
5
con?icts with the read(B)in-
struction of T
1
. This creates an edge T
5
? T
1
in the precedence graph. Similarly,
we seethat the write(A)instructionof T
1
con?icts withthe readinstructionof T
5
646 Chapter 14 Transactions
T
1
T
5
read(A)
A:= A ?50
write(A)
read(B)
B:= B ?10
write(B)
read(B)
B:= B +50
write(B)
read(A)
A:= A +10
write(A)
Figure14.13 Schedule 8.
creatinganedge T
1
? T
5
. Thisshows that the precedencegraph has acycle and
that schedule 8 is not serializable. However, the ?nal values of accounts A and
B after the execution of either schedule 8 or the serial schedule<T
1
,T
5
> are the
same—$960and$2040,respectively.
Wecanseefromthisexamplethatthereareless-stringentde?nitionsofsched-
uleequivalencethancon?ictequivalence.Forthesystemtodeterminethatsched-
ule8producesthesameoutcomeastheserialschedule<T
1
,T
5
>,itmustanalyze
thecomputationperformedby T
1
and T
5
, rather than just the readand writeop-
erations. In general, such analysis is hard to implement and is computationally
expensive.Inourexample,the?nalresultisthesameasthatofaserialschedule
becauseofthemathematicalfactthatadditionandsubtractionarecommutative.
While this may be easy to see in our simple example, the general case is not so
easy since a transaction may be expressed as a complex SQL statement, a Java
programwith JDBCcalls,etc.
However,thereareotherde?nitionsofscheduleequivalencebasedpurelyon
thereadandwriteoperations.Onesuchde?nitionis view equivalence,ade?nition
that leads to the concept of view serializability. View serializability is not used in
practiceduetoitshighdegreeofcomputationalcomplexity.
3
Wethereforedefer
discussionof viewserializabilitytoChapter15,but,for completeness,note here
thattheexampleofschedule8isnotviewserializable.
14.7 TransactionIsolationandAtomicity
So far, we have studied schedules while assuming implicitly that there are no
transaction failures. We now address the effect of transaction failures during
concurrentexecution.
3
Testing for view serializability has been proven to be NP-complete, which means that it is virtually certain that no
ef?cienttestforviewserializabilityexists.
14.7 TransactionIsolationandAtomicity 647
T
6
T
7
read(A)
write(A)
read(A)
commit
read(B)
Figure14.14 Schedule 9, a nonrecoverable schedule.
If a transaction T
i
fails, for whatever reason, we need to undo the effect of
this transaction to ensure the atomicity property of the transaction. In a system
thatallowsconcurrentexecution,theatomicitypropertyrequiresthatanytrans-
action T
j
that is dependent on T
i
(that is, T
j
has read data written by T
i
)isalso
aborted. To achieve this, we need to place restrictions on the type of schedules
permittedinthesystem.
Inthefollowingtwosubsections,weaddresstheissueofwhatschedulesare
acceptable from the viewpoint of recovery from transaction failure.We describe
inChapter15howtoensurethatonlysuchacceptableschedulesaregenerated.
14.7.1 RecoverableSchedules
Consider the partial schedule 9 in Figure 14.14, in which T
7
is a transaction that
performs only one instruction: read(A). We call this a partial schedule because
wehavenotincludedacommitorabortoperationfor T
6
.NoticethatT
7
commits
immediately after executing the read(A) instruction. Thus, T
7
commits while T
6
isstillintheactivestate.Nowsupposethat T
6
failsbeforeitcommits. T
7
hasread
the value of data item A written by T
6
. Therefore, we say that T
7
is dependent
on T
6
.Becauseofthis,wemustabortT
7
to ensure atomicity. However, T
7
has
alreadycommitted and cannot be aborted. Thus, we have a situation where itis
impossibletorecovercorrectlyfromthefailureof T
6
.
Schedule9isanexampleofanonrecoverableschedule.Arecoverableschedule
isonewhere,foreachpairoftransactions T
i
and T
j
suchthat T
j
readsadataitem
previously written by T
i
, the commit operation of T
i
appears before the commit
operation of T
j
. For the exampleof schedule 9 to be recoverable, T
7
would have
todelaycommittinguntilafter T
6
commits.
14.7.2 Cascadeless Schedules
Evenifascheduleisrecoverable,torecovercorrectlyfromthefailureofatrans-
action T
i
, we may have to roll back severaltransactions. Such situations occur if
transactionshavereaddatawrittenby T
i
.Asanillustration,considerthepartial
scheduleofFigure14.15.Transaction T
8
writesavalueofAthatisreadbytransac-
tion T
9
.TransactionT
9
writesavalueof Athatisreadbytransaction T
10
.Suppose
that,atthispoint, T
8
fails. T
8
mustberolledback.Since T
9
isdependenton T
8
, T
9
must be rolled back. Since T
10
is dependent on T
9
, T
10
must be rolled back. This
648 Chapter 14 Transactions
T
8
T
9
T
10
read(A)
read(B)
write(A)
read(A)
write(A)
read(A)
abort
Figure14.15 Schedule 10.
phenomenon,inwhichasingletransactionfailureleadstoaseriesoftransaction
rollbacks,iscalledcascadingrollback.
Cascadingrollbackisundesirable,sinceitleadstotheundoingofasigni?cant
amountofwork.Itisdesirabletorestricttheschedulestothosewherecascading
rollbackscannotoccur.Suchschedulesarecalled cascadelessschedules.Formally,
acascadeless scheduleisonewhere,foreachpairoftransactions T
i
and T
j
such
that T
j
reads a data item previously written by T
i
, the commit operation of T
i
appearsbeforethereadoperationof T
j
.Itiseasytoverifythateverycascadeless
scheduleisalsorecoverable.
14.8 TransactionIsolationLevels
Serializabilityisausefulconceptbecauseitallowsprogrammerstoignoreissues
relatedtoconcurrency when theycodetransactions. If everytransaction has the
property that it maintains database consistency if executed alone, then serial-
izability ensures that concurrent executions maintain consistency. However, the
protocols required to ensure serializability may allow too little concurrency for
certain applications. In these cases, weaker levels of consistency are used. The
use of weaker levels of consistency places additional burdens on programmers
forensuringdatabasecorrectness.
TheSQLstandardalsoallowsatransactiontospecifythatitmaybeexecutedin
suchawaythatitbecomesnonserializablewithrespecttoothertransactions.For
instance, a transaction may operate at the isolation level of read uncommitted,
which permits the transaction to read a data item even if it was written by a
transaction that has not been committed. SQL provides such features for the
bene?t of long transactions whose results do not need to be precise. If these
transactions were to execute in a serializable fashion, they could interfere with
othertransactions,causingtheothers’executiontobedelayed.
Theisolationlevelsspeci?edbythe SQLstandardareasfollows:
  Serializable usually ensures serializable execution. However, as we shall
explain shortly, some database systems implement this isolation level in a
mannerthatmay,incertaincases,allownonserializableexecutions.
14.8 TransactionIsolationLevels 649
  Repeatablereadallowsonlycommitteddatatobereadandfurtherrequires
that,betweentworeadsofadataitembyatransaction,noothertransaction
is allowed to update it. However, the transaction may not be serializable
withrespecttoothertransactions.Forinstance,whenitissearchingfordata
satisfyingsomeconditions,atransactionmay?ndsomeofthedatainserted
byacommittedtransaction,butmaynot?ndotherdatainsertedbythesame
transaction.
  Readcommittedallowsonlycommitteddatatoberead,butdoesnotrequire
repeatablereads.Forinstance,betweentworeadsofadataitembythetrans-
action,anothertransactionmayhaveupdatedthedataitemandcommitted.
  Read uncommitted allows uncommitted data to be read. It is the lowest
isolationlevelallowedby SQL.
All the isolation levels above additionally disallow dirty writes,thatis,they
disallowwritestoadataitemthathasalreadybeenwrittenbyanothertransaction
thathasnotyetcommittedoraborted.
Manydatabasesystemsrun,bydefault,attheread-committedisolationlevel.
In SQL,itispossibletosettheisolationlevelexplicitly,ratherthanacceptingthe
system’s default setting. For example, the statement “set transaction isolation
level serializable;” sets the isolation level to serializable; any of the other isola-
tion levels may be speci?ed instead. The above syntax is supported by Oracle,
PostgreSQLandSQLServer;DB2usesthesyntax “changeisolationlevel,”withits
ownabbreviationsforisolationlevels.
Changing of the isolation level must be done as the ?rst statement of a
transaction. Further,automaticcommitofindividualstatementsmustbe turned
off, if it is on by default; API functions, such as the JDBC method Connec-
tion.setAutoCommit(false) which we saw in Section 5.1.1.7, can be used to do
so.Further,in JDBCthemethod Connection.setTransactionIsolation(int level)can
beusedtosettheisolationlevel;seethe JDBCmanualsfordetails.
Anapplicationdesignermaydecidetoacceptaweakerisolationlevelinorder
toimprovesystemperformance.AsweshallseeinSection14.9andChapter15,
ensuring serializabilitymay force a transaction to wait for other transactions or,
in some cases, to abort because the transaction can no longer be executed as
partofaserializableexecution.While itmayseemshortsightedtoriskdatabase
consistencyforperformance,thistrade-offmakessenseifwecanbesurethatthe
inconsistencythatmayoccurisnotrelevanttotheapplication.
There are many means of implementing isolation levels. As long as the im-
plementation ensures serializability, the designer of a database application or a
userofanapplicationdoesnotneedtoknowthedetailsofsuchimplementations,
except perhaps for dealing with performance issues. Unfortunately, even if the
isolationlevelissettoserializable,somedatabasesystemsactuallyimplementa
weakerlevelofisolation,whichdoesnotruleouteverypossiblenonserializable
execution; we revisit this issue in Section 14.9. If weaker levels of isolation are
used, either explicitly or implicitly, the application designer has to be aware of
somedetailsoftheimplementation,toavoidorminimizethechanceofinconsis-
tencyduetolackofserializability.
650 Chapter 14 Transactions
SERIALIZABILITYINTHEREALWORLD
Serializableschedulesaretheidealwaytoensureconsistency,butinourday-
to-daylives,wedon’timposesuchstringentrequirements.AWebsiteoffering
goods for sale may list an item as being in stock, yet by the time a user selects
the item and goes through the checkout process, that item might no longer be
available. Viewed from a database perspective, this would be a nonrepeatable
read.
As another example, consider seat selection for air travel. Assume that a
traveler has already booked an itinerary and now is selecting seats for each
?ight.ManyairlineWebsitesallowtheusertostepthroughthevarious?ights
andchooseaseat,afterwhichtheuserisaskedtocon?rmtheselection.Itcould
be that other travelers are selecting seats or changing their seat selections for
the same ?ights at the same time. The seat availability that the traveler was
shownisthusactuallychanging,butthetravelerisshownasnapshotoftheseat
availabilityasofwhenthetravelerstartedtheseatselectionprocess.
Even if two travelers are selecting seats at the same time, most likely they
will select different seats, and if so there would be no real con?ict. However,
the transactions are not serializable, since each traveler has read data that was
subsequentlyupdatedbytheothertraveler,leadingtoacycleintheprecedence
graph.Iftwotravelersperformingseatselectionconcurrentlyactuallyselected
the same seat, one of them would not be able to get the seat they selected;
however,thesituationcouldbeeasilyresolvedbyaskingthetravelertoperform
theselectionagain,withupdatedseatavailabilityinformation.
It is possible to enforce serializability by allowing only one traveler to do
seat selection for a particular ?ight at a time. However, doing so could cause
signi?cant delays as travelers would have to wait for their ?ight to become
available for seat selection; in particular a traveler who takes a long time to
makeachoicecouldcauseseriousproblemsforothertravelers.Instead,anysuch
transactionistypicallybrokenupintoapartthatrequiresuserinteraction,and
apartthatrunsexclusivelyonthedatabase.Intheexampleabove,thedatabase
transaction would check if the seats chosen by the user are still available, and
ifsoupdatetheseatselectioninthedatabase.Serializabilityisensuredonlyfor
thetransactionsthatrunonthedatabase,withoutuserinteraction.
14.9 ImplementationofIsolationLevels
So far, we have seen what properties a schedule must have if it is to leave the
database in a consistent state and allow transaction failures to be handled in a
safemanner.
There are various concurrency-control policies that we can use to ensure
that,evenwhenmultipletransactionsareexecutedconcurrently,onlyacceptable
schedules are generated, regardless of how the operating system time-shares
resources(suchas CPUtime)amongthetransactions.
14.9 Implementation ofIsolationLevels 651
As a trivial example of a concurrency-control policy, consider this: A trans-
actionacquiresalockontheentiredatabasebeforeitstartsandreleasesthelock
after it has committed. While a transaction holds a lock, no other transaction is
allowedtoacquirethelock,andallmustthereforewaitforthelocktobereleased.
Asaresultofthelockingpolicy,onlyonetransactioncanexecuteatatime.There-
fore,onlyserialschedulesaregenerated.Thesearetriviallyserializable,anditis
easytoverifythattheyarerecoverableandcascadelessaswell.
A concurrency-control policy such as this one leads to poor performance,
sinceitforcestransactionstowaitforprecedingtransactionsto?nishbeforethey
canstart.Inotherwords,itprovidesapoordegreeofconcurrency(indeed,nocon-
currency at all). As we saw in Section 14.5, concurrent execution has substantial
performancebene?ts.
The goal of concurrency-control policies is to provide a high degree of con-
currency, while ensuring that all schedules that can be generated are con?ict or
viewserializable,recoverable,andcascadeless.
Herewe providean overviewof how some of most important concurrency-
controlmechanismswork,andwedeferthedetailstoChapter15.
14.9.1 Locking
Insteadoflockingtheentiredatabase,atransactioncould,instead,lockonlythose
data items that it accesses. Under such a policy, the transaction must hold locks
long enough to ensure serializability,but for a period short enough not to harm
performanceexcessively.Complicatingmattersare SQLstatementslikethosewe
saw in Section 14.10, where the data items accessed depend on a where clause.
InChapter15,wepresentthetwo-phaselockingprotocol,asimple,widelyused
technique that ensures serializability. Stated simply, two-phase locking requires
atransactiontohavetwophases,onewhereitacquireslocksbutdoesnotrelease
any,andasecondphasewherethetransactionreleaseslocksbutdoesnotacquire
any.(Inpractice,locksareusuallyreleasedonlywhenthetransactioncompletes
itsexecutionandhasbeeneithercommittedoraborted.)
Furtherimprovementstolockingresultifwehavetwokindsoflocks:shared
and exclusive. Shared locks are used for data that the transaction reads and
exclusive locks are used for those it writes. Many transactions can hold shared
locks on the same data item at the same time, but a transaction is allowed an
exclusivelockonadataitemonlyifnoothertransactionholdsanylock(regardless
of whether shared or exclusive) on the data item. This use of two modes of
locksalongwithtwo-phaselockingallowsconcurrentreadingofdatawhilestill
ensuringserializability.
14.9.2 Timestamps
Anothercategoryoftechniquesfortheimplementationof isolationassignseach
transactionatimestamp,typicallywhenitbegins.Foreachdataitem,thesystem
keepstwotimestamps.Thereadtimestampofadataitemholdsthelargest(that
is, the most recent) timestamp of those transactions that read the data item.
The write timestamp of a data item holds the timestamp of the transaction that
652 Chapter 14 Transactions
wrote the current value of the data item. Timestamps are used to ensure that
transactionsaccesseachdataiteminorderofthetransactions’timestampsiftheir
accessescon?ict.Whenthisisnotpossibl e, offending transactions are aborted
andrestartedwithanewtimestamp.
14.9.3 MultipleVersionsandSnapshotIsolation
By maintaining more than one version of a data item, it is possible to allow a
transaction to read an old version of a data item rather than a newer version
written by an uncommitted transaction or by a transaction that should come
later in the serialization order. There are a variety of multiversion concurrency-
control techniques. One in particular, called snapshot isolation,iswidelyused
inpractice.
In snapshot isolation, we can imagine that each transaction is given its own
version, or snapshot, of the database when it begins.
4
It reads data from this
privateversionandisthusisolatedfromtheupdatesmadebyothertransactions.
If the transaction updates the database, that update appears only in its own
version, not in the actual database itself. Information about these updates is
savedsothattheupdatescanbeappliedtothe “real”databaseifthetransaction
commits.
Whenatransaction T entersthepartiallycommittedstate,itthenproceedsto
thecommittedstateonlyifnootherconcurrenttransactionhasmodi?eddatathat
T intendstoupdate.Transactionsthat,asaresult,cannotcommitabortinstead.
Snapshot isolation ensures that attempts to read data never need to wait
(unlikelocking).Read-onlytransactionscannotbeaborted;onlythosethatmodify
data run a slight risk of aborting. Since each transaction reads its own version
or snapshot of the database, reading data does not cause subsequent update
attempts by other transactions to wait (unlike locking). Since most transactions
are read-only (and most others read more data than they update),this is often a
majorsourceofperformanceimprovementascomparedtolocking.
The problem with snapshot isolation is that, paradoxically, it provides too
much isolation. Consider two transactions T and T
null
. In a serializable execution,
either T sees all the updates made by T
null
or T
null
sees all the updates made by
T, because one must follow the other in the serialization order. Under snapshot
isolation,therearecaseswhere neithertransaction seesthe updatesoftheother.
Thisisasituationthatcannotoccurinaserializableexecution.Inmany(indeed,
most) cases, the data accesses by the two transactions do not con?ict and there
isnoproblem.However,if T readssomedataitemthat T
null
updatesand T
null
reads
some data item that T updates, it is possible that both transactions fail to read
the update made by the other. The result, as we shall see in Chapter 15, may
be an inconsistent database state that, of course, could not be obtained in any
serializableexecution.
4
Of course, in reality, the entire database is not copied. Multiple versions are kept only of those data items that are
changed.
14.10 TransactionsasSQLStatements 653
Oracle, PostgreSQL,andSQL Server offer the option of snapshot isolation.
OracleandPostgreSQLimplementtheserializableisolationlevelusingsnapshot
isolation. As a result, their implementation of serializability can, in exceptional
circumstances, result in a nonserializable execution being allowed. SQL Server
instead includes an additional isolation level beyond the standard ones, called
snapshot,tooffertheoptionofsnapshotisolation.
14.10 TransactionsasSQLStatements
InSection4.3,wepresentedthe SQLsyntaxforspecifyingthebeginningandend
of transactions. Now that we have seen some of the issues in ensuring the ACID
properties for transactions, we are ready to consider how those properties are
ensured when transactions are speci?ed as a sequence of SQL statements rather
than the restricted model of simple reads and writes that we considered up to
thispoint.
Inoursimplemodel,weassumedasetofdataitemsexists.Whileoursimple
model allowed data-item values to be changed, it did not allow data items to
be created or deleted. In SQL, however, insert statements create new data and
delete statements delete data. These two statements are, in effect, write opera-
tions, since they change the database, but their interactions with the actions of
other transactions are different from what we saw in our simple model. As an
example,considerthefollowing SQLqueryonouruniversitydatabasethat?nds
allinstructorswhoearnmorethan$90,000.
select ID, name
from instructor
where salary>90000;
Using our sample instructor relation (Appendix A.3), we ?nd that only Ein-
stein and Brandt satisfy the condition. Now assume that around the same time
we are running our query, another user inserts a new instructor named “James”
whosesalaryis$100,000.
insertinto instructorvalues(’11111’,’James’,’Marketing’,100000);
Theresultofourquerywillbedifferentdependingonwhetherthisinsertcomes
before or after our query is run. In a concurrent execution of these transactions,
it is intuitively clear that they con?ict, but this is a con?ict not captured by our
simplemodel.Thissituationisreferredtoasthephantomphenomenon,because
acon?ictmayexiston “phantom”data.
Oursimplemodeloftransactions requiredthatoperationsoperateonaspe-
ci?cdataitemgivenasanargumenttotheoperation.Inoursimplemodel,wecan
lookatthereadandwritestepstoseewhichdataitemsarereferenced.Butinan
SQL statement,thespeci?cdataitems(tuples)referencedmaybedeterminedby
a where clause predicate.So the same transaction, if run more than once, might
654 Chapter 14 Transactions
reference different data items each time it is run if the values in the database
changebetweenruns.
One way of dealing with the above problem is to recognize that it is not
suf?cient for concurrency control to consider only the tuples that are accessed
byatransaction; theinformationusedto?ndthetuplesthatareaccessedbythe
transaction mustalsobe consideredforthe purposeof concurrency control.The
informationusedto?nd tuplescouldbe updatedbyan insertionordeletion,or
inthecaseofanindex,evenbyanupdatetoasearch-keyattribute.Forexample,
iflockingisusedforconcurrencycontrol,thedatastructuresthattrackthetuples
inarelation,aswellasindexstructures,mustbeappropriatelylocked.However,
such locking can lead to poor concurrency in some situations; index-locking
protocolswhich maximizeconcurrency, while ensuringserializabilityinspiteof
inserts,deletes,andpredicatesinqueries,arediscussedinSection15.8.3.
Letusconsideragainthequery:
select ID, name
from instructor
where salary>90000;
andthefollowing SQLupdate:
update instructor
set salary= salary*0.9
where name =’Wu’;
Wenowfaceaninterestingsituationindeterminingwhetherourquerycon?icts
with the update statement. If our query reads the entire instructor relation, then
it reads the tuple with Wu’s data and con?icts with the update. However, if an
index were available that allowed our query direct access to those tuples with
salary>90000,thenourquerywouldnothaveaccessedWu’sdataatallbecause
Wu’ssalaryisinitially$90,000inourexampleinstructorrelation,andreducesto
$81,000aftertheupdate.
However, using the above approach, it would appear that the existence of
a con?ict depends on a low-level query processing decision by the system that
is unrelated to a user-level view of the meaning of the two SQL statements! An
alternative approach to concurrency control treats an insert, delete or update as
con?ictingwithapredicateonarelation,ifitcouldaffectthesetoftuplesselected
byapredicate.Inourexamplequeryabove,thepredicateis“salary>90000”,and
an update of Wu’s salary from $90,000 to a value greater than $90,000, or an
updateof Einstein’s salary from a value greaterthat $90,000 to a value lessthan
orequalto$90,000,wouldcon?ictwiththispredicate.Lockingbasedonthisidea
iscalledpredicatelocking;howeverpredicatelockingisexpensive,andnotused
inpractice.
14.11 Summary 655
14.11 Summary
  Atransactionisaunitofprogramexecutionthataccessesandpossiblyupdates
variousdataitems.Understandingtheconceptofatransactioniscriticalfor
understanding and implementing updates of data in a database in such a
waythatconcurrentexecutionsandfailuresofvariousformsdonotresultin
thedatabasebecominginconsistent.
  TransactionsarerequiredtohavetheACIDproperties:atomicity,consistency,
isolation,anddurability.
?
Atomicity ensures that either all the effects of a transaction are re?ected
inthedatabase,ornone are;afailurecannotleavethedatabaseinastate
whereatransactionispartiallyexecuted.
?
Consistency ensures that, if the database is initially consistent, the ex-
ecution of the transaction (by itself) leaves the database in a consistent
state.
?
Isolation ensures that concurrently executing transactions are isolated
from one another, so that each has the impression that no other trans-
actionisexecutingconcurrentlywithit.
?
Durabilityensuresthat,onceatransactionhasbeencommitted,thattrans-
action’supdatesdonotgetlost,evenifthereisasystemfailure.
  Concurrent execution of transactions improves throughput of transactions
andsystemutilization,andalsoreduceswaitingtimeoftransactions.
  The various types of storage in a computer are volatile storage, nonvolatile
storage,andstablestorage.Datainvolatilestorage,suchas in RAM,arelost
when the computer crashes. Data in nonvolatile storage, such as disk, are
notlostwhenthecomputercrashes,butmayoccasionallybelostbecauseof
failuressuchasdiskcrashes.Datainstablestorageareneverlost.
  Stablestoragethatmustbeaccessibleonlineisapproximatedwithmirrored
disks,orotherformsofRAID,whichprovideredundantdatastorage.Of?ine,
orarchival,stablestoragemayconsistofmultipletapecopiesofdatastored
inphysicallysecurelocations.
  Whenseveraltransactions executeconcurrentlyonthedatabase,theconsis-
tency of data may no longer be preserved. It is therefore necessary for the
systemtocontroltheinteractionamongtheconcurrenttransactions.
?
Since a transaction is a unit that preservesconsistency, a serial execution
oftransactionsguaranteesthatconsistencyispreserved.
?
A schedule captures the key actions of transactions that affect concurrent
execution, such as read and write operations, while abstracting away in-
ternaldetailsoftheexecutionofthetransaction.
656 Chapter 14 Transactions
?
Werequirethatanyscheduleproducedbyconcurrentprocessingofaset
oftransactionswillhaveaneffectequivalenttoascheduleproducedwhen
thesetransactionsarerunseriallyinsomeorder.
?
Asystemthatguaranteesthispropertyissaidtoensure serializability.
?
Thereareseveraldifferentnotionsofequivalenceleadingtotheconcepts
of con?ict serializabilityand view serializability.
  Serializabilityofschedulesgeneratedbyconcurrentlyexecutingtransactions
can be ensured through one of a variety of mechanisms called concurrency-
controlpolicies.
  We can test a given schedule for con?ict serializability by constructing a
precedence graph for the schedule, and by searching for absence of cycles in
thegraph.However,therearemoreef?cientconcurrency-controlpoliciesfor
ensuringserializability.
  Schedules must be recoverable, to make sure that if transaction a sees the
effectsoftransaction b,andb thenaborts,then a alsogetsaborted.
  Schedulesshouldpreferablybecascadeless,sothattheabortofatransaction
does not result in cascading aborts of other transactions. Cascadelessness is
ensuredbyallowingtransactionstoonlyreadcommitteddata.
  Theconcurrency-control–managementcomponentofthedatabaseisrespon-
sible for handling the concurrency-control policies. Chapter 15 describes
concurrency-controlpolicies.
ReviewTerms
  Transaction
  ACIDproperties
?
Atomicity
?
Consistency
?
Isolation
?
Durability
  Inconsistentstate
  Storagetypes
?
Volatilestorage
?
Nonvolatilestorage
?
Stablestorage
  Concurrencycontrolsystem
  Recoverysystem
  Transactionstate
?
Active
?
Partiallycommitted
?
Failed
?
Aborted
?
Committed
?
Terminated
  Transaction
?
Restart
?
Kill
PracticeExercises 657
  Observableexternalwrites
  Concurrentexecutions
  Serialexecution
  Schedules
  Con?ictofoperations
  Con?ictequivalence
  Con?ictserializability
  Serializabilitytesting
  Precedencegraph
  Serializabilityorder
  Recoverableschedules
  Cascadingrollback
  Cascadelessschedules
  Concurrency-control
  Locking
  Multipleversions
  Snapshotisolation
PracticeExercises
14.1 Suppose that there is a database system that never fails. Is a recovery
managerrequiredforthissystem?
14.2 Considera?lesystemsuchastheoneonyourfavoriteoperatingsystem.
a. Whatarethestepsinvolvedincreationanddeletionof?les,andin
writingdatatoa?le?
b. Explain how the issues of atomicity and durability are relevant to
thecreationanddeletionof?lesandtowritingdatato?les.
14.3 Database-system implementers have paid much more attention to the
ACID properties than have ?le-system implementers. Why might this be
thecase?
14.4 Justify the following statement: Concurrent execution of transactions is
more important when data must be fetched from (slow) disk or when
transactionsarelong,andislessimportantwhendataareinmemoryand
transactionsareveryshort.
14.5 Sinceeverycon?ict-serializablescheduleisviewserializable,whydowe
emphasizecon?ictserializabilityratherthanviewserializability?
14.6 Consider the precedence graph of Figure 14.16. Is the corresponding
schedulecon?ictserializable?Explainyouranswer.
14.7 Whatisacascadelessschedule?Why iscascadelessnessofschedulesde-
sirable? Are there any circumstances under which it would be desirable
toallownoncascadelessschedules?Explainyouranswer.
14.8 The lost update anomaly is said to occur if a transaction T
j
reads a data
item,thenanothertransaction T
k
writesthedataitem(possiblybasedona
previousread),afterwhichT
j
writesthedataitem.Theupdateperformed
by T
k
hasbeenlost,sincetheupdatedoneby T
j
ignoredthevaluewritten
by T
k
.
658 Chapter 14 Transactions
T
1
T
4
T
5
T
3
T
2
Figure14.16 Precedence graph for Practice Exercise 14.6.
a. Giveanexampleofascheduleshowingthelostupdateanomaly.
b. Give an example schedule to show that the lost update anomaly is
possiblewiththereadcommittedisolationlevel.
c. Explain why the lost update anomaly is not possible with the re-
peatablereadisolationlevel.
14.9 Consider a database for a bank where the database system uses snap-
shot isolation. Describe a particular scenario in which a nonserializable
executionoccursthatwouldpresentaproblemforthebank.
14.10 Considera databasefor anairlinewherethedatabasesystemusessnap-
shot isolation. Describe a particular scenario in which a nonserializable
execution occurs, but the airline may be willing to accept it in order to
gainbetteroverallperformance.
14.11 The de?nition of a schedule assumes that operations can be totally or-
dered by time. Consider a database system that runs on a system with
multiple processors, where it is not always possible to establish an ex-
act ordering between operations that executed on different processors.
However,operationsonadataitemcanbetotallyordered.
Doestheabovesituationcauseanyproblemforthede?nitionofcon?ict
serializability?Explainyouranswer.
Exercises
14.12 Listthe ACIDproperties.Explaintheusefulnessofeach.
14.13 During its execution, a transaction passes through several states, until it
?nally commits or aborts. List all possible sequences of states through
Exercises 659
which a transaction may pass. Explain why each state transition may
occur.
14.14 Explain the distinction between the terms serial schedule and serializable
schedule.
14.15 Considerthefollowingtwotransactions:
T
13
: read(A);
read(B);
if A = 0then B:= B+1;
write(B).
T
14
: read(B);
read(A);
if B = 0then A:= A+1;
write(A).
Lettheconsistencyrequirementbe A = 0 ? B = 0,with A = B = 0
theinitialvalues.
a. Show that every serial execution involving these two transactions
preservestheconsistencyofthedatabase.
b. Showaconcurrentexecutionof T
13
and T
14
thatproducesanonseri-
alizableschedule.
c. Isthereaconcurrentexecutionof T
13
and T
14
thatproducesaserial-
izableschedule?
14.16 Give an example of a serializable schedule with two transactions such
that the order in which the transactions commit is different from the
serializationorder.
14.17 Whatisarecoverableschedule?Whyisrecoverabilityofschedulesdesir-
able? Are there any circumstances under which it would be desirable to
allownonrecoverableschedules?Explainyouranswer.
14.18 Why do database systems support concurrent execution of transactions,
inspiteoftheextraprogrammingeffortneededtoensurethatconcurrent
executiondoesnotcauseanyproblems?
14.19 Explain why the read-committed isolation level ensures that schedules
arecascade-free.
14.20 For each of the following isolation levels, give an example of a schedule
thatrespectsthespeci?edlevelofisolation,butisnotserializable:
a. Readuncommitted
b. Readcommitted
c. Repeatableread
660 Chapter 14 Transactions
14.21 Suppose that in addition to the operations read and write, we allow an
operation pred read(r, P), which readsall tuplesinrelation r that satisfy
predicate P.
a. Give an example of a schedule using the pred read operation that
exhibitsthephantomphenomenon,andisnonserializableasaresult.
b. Give an example of a schedule where one transaction uses the
pred read operation on relation r and another concurrent transac-
tions deletes a tuple from r, but the schedule does not exhibit a
phantomcon?ict.(Todoso,youhavetogivetheschemaofrelation
r,andshowtheattributevaluesofthedeletedtuple.)
BibliographicalNotes
Gray and Reuter [1993] provides detailed textbook coverage of transaction-
processing concepts, techniques and implementation details, including concur-
rencycontrolandrecoveryissues.BernsteinandNewcomer[1997]providestext-
bookcoverageofvariousaspectsoftransactionprocessing.
The concept of serializability was formalized by Eswaran et al. [1976] in
connectiontoworkonconcurrencycontrolforSystemR.
References covering speci?c aspects of transaction processing, such as con-
currencycontrolandrecovery,arecitedinChapters15,16,and26.
CHAPTER
15
Concurrency Control
We saw in Chapter 14 that one of the fundamental properties of a transaction is
isolation. When several transactions execute concurrently in the database, how-
ever, the isolation property may no longer be preserved. To ensure that it is,
the system must control the interaction among the concurrent transactions; this
control is achieved through one of a variety of mechanisms called concurrency-
control schemes. In Chapter 26, we discuss concurrency-control schemes that
admitnonserializableschedules.Inthischapter,weconsiderthemanagementof
concurrently executing transactions, and we ignore failures. In Chapter 16, we
shall seehow the systemcan recoverfromfailures.
As we shall see, there are a variety of concurrency-control schemes. No one
scheme is clearly the best; each one has advantages. In practice, the most fre-
quentlyusedschemesare two-phase locking and snapshot isolation.
15.1 Lock-BasedProtocols
Onewaytoensureisolationistorequirethatdataitemsbeaccessedinamutually
exclusive manner; that is, while one transaction is accessing a data item, no
other transaction can modify that data item. The most common method used to
implementthisrequirementis toallow atransaction toaccess a dataitemonly if
it is currently holding alock on that item. We introduced the concept of locking
in Section14.9.
15.1.1 Locks
There are various modes in which a data item may be locked. In this section, we
restrictour attention totwo modes:
1. Shared.IfatransactionT
i
has obtained ashared-modelock(denotedbyS)
on item Q,thenT
i
can read,but cannot write, Q.
2. Exclusive.IfatransactionT
i
hasobtainedanexclusive-modelock(denoted
by X) on item Q,thenT
i
can both readand write Q.
661
662 Chapter15 ConcurrencyControl
S X
S true false
X false false
Figure15.1 Lock-compatibility matrix comp.
We require that every transaction request a lock in an appropriate mode
on data item Q, depending on the types of operations that it will perform on
Q. The transaction makes the request to the concurrency-control manager. The
transaction can proceed with the operation only after the concurrency-control
manager grants the lock to the transaction. The use of these two lock modes
allowsmultipletransactionstoreadadataitembutlimitswriteaccesstojustone
transaction ata time.
To state this more generally, given a set of lock modes, we can de?ne a
compatibilityfunction on them as follows: Let A and B represent arbitrary lock
modes. Suppose that a transaction T
i
requests a lock of mode A on item Q on
which transaction T
j
(T
i
null T
j
) currently holds a lock of mode B.IftransactionT
i
can be granted a lock on Q immediately, in spite of the presence of the mode B
lock, then we say mode A is compatible with mode B.Suchafunctioncanbe
representedconvenientlybyamatrix.Thecompatibilityrelationbetweenthetwo
modes of locking discussed in this section appears in the matrix comp of Figure
15.1. An element comp(A, B) of the matrix has the value true if and only if mode
A iscompatible with mode B.
Notethatsharedmodeiscompatiblewithsharedmode,butnotwithexclusive
mode. At any time, several shared-mode locks can be held simultaneously (by
different transactions) on a particular data item. A subsequent exclusive-mode
lock requesthas to wait until the currentlyheld shared-modelocks are released.
A transaction requests a shared lock on data item Q by executing the lock-
S(Q) instruction. Similarly, a transaction requests an exclusive lock through the
lock-X(Q) instruction. A transaction can unlock a data item Q by the unlock(Q)
instruction.
Toaccessadataitem,transactionT
i
must?rstlockthatitem.Ifthedataitemis
alreadylockedbyanothertransactioninanincompatiblemode,theconcurrency-
controlmanagerwillnotgrantthelockuntilallincompatiblelocksheldbyother
transactions have been released. Thus, T
i
is made to wait until all incompatible
locks heldby othertransactions havebeenreleased.
Transaction T
i
mayunlockadataitemthatithadlockedatsomeearlierpoint.
Notethat atransaction musthold alockon adataitemaslongasitaccessesthat
item. Moreover, it is not necessarily desirable for a transaction to unlock a data
item immediately after its ?nal access of that data item, since serializability may
not be ensured.
As an illustration, consider again the banking example that we introduced
in Chapter 14. Let A and B be two accounts that are accessed by transactions T
1
15.1 Lock-BasedProtocols 663
T
1
: lock-X(B);
read(B);
B := B ? 50;
write(B);
unlock(B);
lock-X(A);
read(A);
A:= A + 50;
write(A);
unlock(A).
Figure15.2 Transaction T
1
.
and T
2
.T ransactionT
1
transfers $50 from account B to account A (Figure 15.2).
Transaction T
2
displays the total amount of money in accounts A and B—that is,
the sum A+ B (Figure15.3).
Suppose that the values of accounts A and B are $100 and $200, respectively.
If these two transactions are executed serially, either in the order T
1
, T
2
or the
order T
2
, T
1
, then transaction T
2
will display the value $300. If, however, these
transactionsareexecutedconcurrently,thenschedule1,inFigure15.4,ispossible.
In this case, transaction T
2
displays $250, which is incorrect. The reason for this
mistake is that the transaction T
1
unlocked data item B too early, as a result of
which T
2
saw an inconsistent state.
The schedule shows the actions executed by the transactions, as well as the
points at which the concurrency-control manager grants the locks. The transac-
tion making a lock request cannot execute its next action until the concurrency-
control manager grants the lock. Hence, the lock must be granted in the interval
oftimebetweenthelock-requestoperationand thefollowingactionofthetrans-
action. Exactly when within this interval the lock is granted is not important;
we can safely assume that the lock is granted just before the following action
of the transaction. We shall therefore drop the column depicting the actions of
the concurrency-control manager from all schedules depicted in the rest of the
chapter. We letyou inferwhen locks are granted.
T
2
: lock-S(A);
read(A);
unlock(A);
lock-S(B);
read(B);
unlock(B);
display(A + B).
Figure15.3 Transaction T
2
.
664 Chapter15 ConcurrencyControl
T
1
T
2
concurreny-control manager
lock-X(B)
grant-X(B, T
1
)
read(B)
B := B ?50
write(B)
unlock(B)
lock-S(A)
grant-S(A, T
2
)
read(A)
unlock(A)
lock-S(B)
grant-S(B, T
2
)
read(B)
unlock(B)
display(A+B )
lock-X(A)
grant-X(A, T
1
)
read(A)
A:= A ?50
write(A)
unlock(A)
Figure15.4 Schedule 1.
Suppose now that unlocking is delayed to the end of the transaction. Trans-
action T
3
corresponds to T
1
with unlocking delayed (Figure 15.5). Transaction T
4
correspondsto T
2
with unlocking delayed(Figure 15.6).
Youshouldverifythat thesequenceofreadsandwritesinschedule1, which
lead to an incorrect total of $250 being displayed, is no longer possible with T
3
T
3
: lock-X(B);
read(B);
B := B ? 50;
write(B);
lock-X(A);
read(A);
A := A + 50;
write(A);
unlock(B);
unlock(A).
Figure15.5 Transaction T
3
(transaction T
1
with unlocking delayed).
15.1 Lock-BasedProtocols 665
T
4
: lock-S(A);
read(A);
lock-S(B);
read(B);
display(A + B);
unlock(A);
unlock(B).
Figure15.6 Transaction T
4
(transaction T
2
with unlocking delayed).
and T
4
. Other schedules are possible. T
4
will not print out an inconsistent result
in any of them; we shall seewhy later.
Unfortunately, locking can lead to an undesirable situation. Consider the
partial schedule of Figure 15.7 for T
3
and T
4
.SinceT
3
is holding an exclusive-
mode lock on B and T
4
is requesting a shared-mode lock on B, T
4
is waiting for
T
3
to unlock B. Similarly, since T
4
is holding a shared-mode lock on A and T
3
is
requestinganexclusive-modelockonA,T
3
iswaitingforT
4
tounlockA.Thus,we
have arrivedat a statewhere neitherof thesetransactions can everproceedwith
itsnormalexecution.Thissituationiscalleddeadlock.Whendeadlockoccurs,the
system must roll back one of the two transactions. Once a transaction has been
rolled back, the data items that were locked by that transaction are unlocked.
These data items are then available to the other transaction, which can continue
with its execution. We shall return to theissueofdeadlockhandlinginSection
15.2.
If we do not use locking, or if we unlock data items too soon after reading
or writing them, we may get inconsistent states. On the other hand, if we do
not unlock a data item before requesting a lock on another data item, deadlocks
may occur. There are ways to avoid deadlock in some situations, as we shall see
in Section 15.1.5. However, in general, deadlocks are a necessary evil associated
with locking, if we want to avoid inconsistent states. Deadlocks are de?nitely
T
3
T
4
lock-X(B)
read(B)
B := B ? 50
write(B)
lock-S(A)
read(A)
lock-S(B)
lock-X(A)
Figure15.7 Schedule 2.
666 Chapter15 ConcurrencyControl
preferableto inconsistent states, since theycan be handled by rollingback trans-
actions,whereasinconsistentstatesmayleadtoreal-worldproblemsthatcannot
be handled by the database system.
Weshallrequirethateachtransactioninthesystemfollowasetofrules,called
alockingprotocol,indicatingwhenatransactionmaylockandunlockeachofthe
dataitems.Lockingprotocolsrestrictthenumberofpossibleschedules.Thesetof
allsuchschedulesisapropersubsetofallpossibleserializableschedules.Weshall
present several locking protocols that allow only con?ict-serializable schedules,
and therebyensure isolation. Beforedoingso, we introduce some terminology.
Let {T
0
, T
1
,...,T
n
} be a set of transactions participating in a schedule S.We
say that T
i
precedes T
j
in S, written T
i
? T
j
,ifthereexistsadataitemQ such
that T
i
has held lock mode A on Q,andT
j
has held lock mode B on Q later, and
comp(A,B)=false.IfT
i
? T
j
,thenthatprecedenceimpliesthatinanyequivalent
serialschedule, T
i
mustappearbefore T
j
.Observethatthisgraphissimilartothe
precedence graph that we used in Section 14.6 to test for con?ict serializability.
Con?icts betweeninstructions correspond to noncompatibility of lock modes.
We say that a schedule S is legal under a given locking protocol if S is a
possible schedule for a set of transactions that follows the rules of the locking
protocol.Wesaythatalockingprotocolensurescon?ictserializabilityifandonly
ifalllegalschedulesarecon?ictserializable;inotherwords,foralllegalschedules
the associated ? relationisacyclic.
15.1.2 GrantingofLocks
When a transaction requests a lock on a data item in a particular mode, and no
othertransactionhasalockonthesamedataiteminacon?icting mode,thelock
can be granted. However, care must be taken to avoid the following scenario.
Suppose a transaction T
2
has a shared-mode lock on a data item, and another
transaction T
1
requests an exclusive-mode lock on the data item. Clearly, T
1
has
to wait for T
2
to release the shared-mode lock. Meanwhile, a transaction T
3
may
requestashared-modelockonthesamedataitem.Thelockrequestiscompatible
with the lock granted to T
2
,soT
3
may be granted the shared-mode lock. At this
point T
2
may release the lock, but still T
1
has to wait for T
3
to ?nish. But again,
there may be a new transaction T
4
that requestsa shared-mode lock on the same
data item, and is granted the lock before T
3
releases it. In fact, it is possible that
there is a sequence of transactions that each requests a shared-mode lock on the
data item, and each transaction releases the lock a short while after it is granted,
but T
1
never gets the exclusive-mode lock on the data item. The transaction T
1
may nevermake progress,and issaid to bestarved.
We can avoid starvation of transactions by granting locks in the following
manner: When a transaction T
i
requests a lock on a data item Q in a particular
mode M, the concurrency-control manager grants the lock providedthat:
1. There is no other transaction holding a lock on Q in a mode that con?icts
with M.
15.1 Lock-BasedProtocols 667
2. There is no other transaction that is waiting for a lock on Q and that made
itslock requestbefore T
i
.
Thus, alock requestwillnevergetblockedby a lockrequestthat ismadelater.
15.1.3 TheTwo-PhaseLocking Protocol
One protocol that ensures serializability is thetwo-phaselockingprotocol.This
protocol requires that each transaction issue lock and unlock requests in two
phases:
1. Growing phase. A transaction may obtain locks, but may not release any
lock.
2. Shrinkingphase. A transaction may release locks, but may not obtain any
new locks.
Initially, a transaction is in the growing phase. The transaction acquires locks as
needed.Once the transaction releasesalock, it entersthe shrinking phase,and it
can issueno more lock requests.
For example,transactions T
3
and T
4
are two phase. On the other hand, trans-
actions T
1
and T
2
arenottwophase.Notethattheunlockinstructionsdonotneed
toappearattheendofthetransaction.Forexample,inthecaseoftransaction T
3
,
we could move the unlock(B) instruction to just after the lock-X(A) instruction,
and stillretainthe two-phase locking property.
Wecanshowthatthetwo-phaselockingprotocolensurescon?ictserializabil-
ity.Consideranytransaction.Thepointintheschedulewherethetransactionhas
obtained its ?nal lock (the end of its growing phase) is called the lock point of
thetransaction.Now,transactionscanbeorderedaccordingtotheirlockpoints—
this ordering is, in fact, a serializability ordering for the transactions. We leave
the proof as an exerciseforyou to do(see Practice Exercise15.1).
Two-phase locking does not ensure freedom from deadlock. Observe that
transactions T
3
and T
4
are two phase, but, in schedule 2 (Figure 15.7), they are
deadlocked.
Recall from Section 14.7.2 that, in addition to being serializable, schedules
should be cascadeless. Cascading rollback may occur under two-phase locking.
As an illustration, consider the partial schedule of Figure 15.8. Each transaction
observes the two-phase locking protocol, but the failure of T
5
after the read(A)
stepof T
7
leadsto cascading rollback of T
6
and T
7
.
Cascading rollbacks can be avoided by a modi?cation of two-phase locking
calledthestricttwo-phaselockingprotocol. This protocol requires not only that
lockingbetwophase,butalsothatallexclusive-modelockstakenbyatransaction
be held until that transaction commits. This requirement ensures that any data
written by an uncommitted transaction are locked in exclusive mode until the
transaction commits, preventingany other transaction from readingthe data.
Another variant of two-phase locking is the rigorous two-phase locking
protocol, which requires that all locks be held until the transaction commits.
668 Chapter15 ConcurrencyControl
T
5
T
6
T
7
lock-X(A)
read(A)
lock-S(B)
read(B)
write(A)
unlock(A)
lock-X(A)
read(A)
write(A)
unlock(A)
lock-S(A)
read(A)
Figure15.8 Partial schedule under two-phase locking.
We can easily verify that, with rigorous two-phase locking, transactions can be
serializedinthe orderin which theycommit.
Considerthefollowingtwotransactions,forwhichwehaveshownonlysome
of the signi?cant readand write operations:
T
8
: read(a
1
);
read(a
2
);
...
read(a
n
);
write(a
1
).
T
9
: read(a
1
);
read(a
2
);
display(a
1
+ a
2
).
Ifweemploythetwo-phaselockingprotocol,thenT
8
mustlocka
1
inexclusive
mode. Therefore, any concurrent execution of both transactions amounts to a
serial execution. Notice, however, that T
8
needs an exclusive lock on a
1
only at
the end of its execution, when it writes a
1
.Thus,ifT
8
could initially lock a
1
in
shared mode, and then could later change the lock to exclusive mode, we could
get more concurrency, since T
8
and T
9
could access a
1
and a
2
simultaneously.
This observation leads us to a re?nement of the basic two-phase locking
protocol, in whichlockconversions are allowed. We shall provide a mechanism
for upgrading a shared lock to an exclusive lock, and downgrading an exclusive
lock to a shared lock. We denote conversion from shared to exclusive modes by
upgrade,andfromexclusivetosharedbydowngrade.Lockconversioncannotbe
allowed arbitrarily. Rather, upgrading can take place in only the growing phase,
whereasdowngrading can take place in only the shrinking phase.
15.1 Lock-BasedProtocols 669
T
8
T
9
lock-S(a
1
)
lock-S(a
1
)
lock-S(a
2
)
lock-S(a
2
)
lock-S(a
3
)
lock-S(a
4
)
unlock(a
1
)
unlock(a
2
)
lock-S(a
n
)
upgrade(a
1
)
Figure15.9 Incomplete schedule with a lock conversion.
Returningtoourexample,transactions T
8
and T
9
canrunconcurrentlyunder
the re?ned two-phase locking protocol, as shown in the incomplete schedule of
Figure 15.9, where only some of the locking instructions are shown.
Note that a transaction attempting to upgrade a lock on an item Q may
be forced to wait. This enforced wait occurs if Q is currently locked by another
transaction inshared mode.
Just like the basic two-phase locking protocol, two-phase locking with lock
conversiongeneratesonlycon?ict-serializableschedules,andtransactionscanbe
serializedbytheirlockpoints.Further,ifexclusivelocksarehelduntiltheendof
the transaction, the schedules are cascadeless.
For a set of transactions, there may be con?ict-serializable schedules that
cannot be obtained through the two-phase locking protocol. However, to obtain
con?ict-serializableschedulesthroughnon-two-phaselockingprotocols,weneed
either to have additional information about the transactions or to impose some
structureororderingonthesetofdataitemsinthedatabase.Weshallseeexamples
when we considerother locking protocolslaterin thischapter.
Strict two-phase locking and rigorous two-phase locking (with lock conver-
sions) are used extensivelyin commercial database systems.
A simple but widely used scheme automatically generates the appropriate
lock and unlock instructions for a transaction, on the basis of read and write
requestsfromthe transaction:
• When a transaction T
i
issues a read(Q) operation, the system issues a lock-
S(Q)instruction followed by the read(Q)instruction.
• When T
i
issues a write(Q) operation, the system checks to see whether T
i
already holds a shared lock on Q. If it does, then the system issues an up-
grade(Q) instruction, followed by the write(Q) instruction. Otherwise, the
systemissuesa lock-X(Q)instruction, followed by the write(Q)instruction.
• All locks obtained by a transaction are unlocked after that transaction com-
mitsoraborts.
670 Chapter15 ConcurrencyControl
15.1.4 Implementation ofLocking
A lock manager can be implemented as a process that receives messages from
transactions and sends messages in reply. The lock-manager process replies to
lock-request messages with lock-grant messages, or with messages requesting
rollback of the transaction (in case of deadlocks). Unlock messages require only
an acknowledgment in response, but may result in a grant message to another
waiting transaction.
Thelockmanagerusesthisdatastructure:Foreachdataitemthatiscurrently
locked, it maintains a linked list of records, one for each request, in the order in
which the requests arrived. It uses a hash table, indexed on the name of a data
item, to ?nd the linked list (if any) for a data item; this table is called the lock
table.Eachrecordofthelinkedlistforadataitemnoteswhichtransactionmade
therequest,andwhatlockmodeitrequested.Therecordalsonotesiftherequest
has currently beengranted.
Figure 15.10 shows an example of a lock table. The table contains locks for
?ve different data items, I4, I7, I23, I44, and I912. The lock table uses over?ow
chaining, so there is a linked list of data items for each entry in the lock table.
Thereisalsoalistoftransactionsthathavebeengrantedlocks,orarewaitingfor
locks,foreachofthedataitems.Grantedlocksaretherectangles?lledinadarker
shade,whilewaitingrequestsaretherectangles?lledinalightershade.Wehave
omittedthelockmodetokeepthe?guresimple.Itcanbeseen,forexample,that
T23 has been granted locks on I912and I7, and is waiting for a lockon I4.
Although the ?gure does not show it, the lock table should also maintain an
index on transaction identi?ers, so that it is possible to determine ef?ciently the
set of locks heldby a giventransaction.
The lock manager processesrequeststhis way:
• Whenalockrequestmessagearrives,itaddsarecordtotheendofthelinked
list for the data item, if the linked list is present. Otherwise it creates a new
linkedlist, containing only the record for the request.
It always grantsa lock requeston a data itemthat is not currentlylocked.
But if the transaction requests a lock on an item on which a lock is currently
held,thelockmanagergrantstherequestonlyifitiscompatiblewiththelocks
that are currently held, and all earlier requests have been granted already.
Otherwise the requesthas to wait.
• When the lock manager receives an unlock message from a transaction, it
deletes the record for that data item in the linked list corresponding to that
transaction.Itteststherecordthatfollows,ifany,asdescribedintheprevious
paragraph,toseeifthatrequestcannowbegranted.Ifitcan,thelockmanager
grants that request, and processes the record following it, if any, similarly,
and so on.
• If a transaction aborts, the lock manager deletes any waiting request made
by the transaction. Once the database system has taken appropriate actions
to undo the transaction (see Section 16.3), it releases all locks held by the
aborted transaction.
15.1 Lock-Based Protocols 671
granted
waiting
T8
144
T1 T23
14
T23
17 123
T23 T1 T8 T2
1912
Figure 15.10 Lock table.
This algorithm guarantees freedom from starvation for lock requests, since
a request can never be granted while a request received earlier is waiting to be
granted. We study how to detect and handle deadlocks later, in Section 15.2.2.
Section 17.2.1 describes an alternative implementation—one that uses shared
memoryinsteadofmessagepassingforlockrequest/grant.
15.1.5 Graph-Based Protocols
AsnotedinSection15.1.3,ifwewishtodevelopprotocolsthatarenottwophase,
weneedadditionalinformationonhoweachtransactionwillaccessthedatabase.
There are various models that can give us the additional information, each dif-
fering in the amount of information provided.The simplestmodel requiresthat
we have prior knowledge about the order in which the database items will be
accessed.Givensuchinformation,itispossibletoconstructlockingprotocolsthat
arenottwophase,butthat,nevertheless,ensurecon?ictserializability.
Toacquiresuchpriorknowledge,weimposeapartialordering?ontheset
D = {d
1
, d
2
,...,d
h
} of all data items. If d
i
? d
j
, then any transaction accessing
672 Chapter15 ConcurrencyControl
both d
i
and d
j
must access d
i
before accessing d
j
. This partial ordering may be
the result of either the logical or the physical organization of the data, or it may
be imposedsolelyfor the purposeof concurrency control.
The partial ordering implies that the setD may now be viewed as a directed
acyclic graph, called adatabasegraph. In this section, for the sake of simplicity,
we will restrict our attention to only those graphs that are rooted trees. We shall
presentasimpleprotocol,calledthetreeprotocol,whichisrestrictedtoemployonly
exclusivelocks.Referencestoother,morecomplex,graph-basedlockingprotocols
are in the bibliographical notes.
In the tree protocol, the only lock instruction allowed is lock-X.Eachtrans-
action T
i
can lock a data item at most once, and must observe the following
rules:
1. The ?rst lock by T
i
may be on any dataitem.
2. Subsequently, a data item Q can be locked by T
i
only if the parent of Q is
currently lockedby T
i
.
3. Data itemsmaybe unlocked at any time.
4. A data item that has been locked and unlocked by T
i
cannot subsequently
be relockedby T
i
.
All schedulesthatare legal underthe treeprotocol are con?ict serializable.
To illustrate this protocol, consider the database graph of Figure 15.11. The
following four transactions follow the treeprotocol on this graph. We show only
the lock and unlock instructions:
A
C B
F
E
I
H
J
D
G
Figure15.11 Tree-structured database graph.
15.1 Lock-BasedProtocols 673
T
10
: lock-X(B); lock-X(E); lock-X(D); unlock(B); unlock(E); lock-X(G);
unlock(D); unlock(G).
T
11
: lock-X(D); lock-X(H); unlock(D); unlock(H).
T
12
: lock-X(B); lock-X(E); unlock(E); unlock(B).
T
13
: lock-X(D); lock-X(H); unlock(D); unlock(H).
Onepossiblescheduleinwhichthesefou r transactions participated appears
inFigure15.12.Notethat,duringitsexecution,transaction T
10
holdslocksontwo
disjoint subtrees.
Observe that the schedule of Figure 15.12 is con?ict serializable. It can be
shownnotonlythatthetreeprotocolensurescon?ictserializability,butalsothat
this protocol ensuresfreedomfrom deadlock.
The tree protocol in Figure 15.12 does not ensure recoverability and cas-
cadelessness. To ensure recoverability and cascadelessness, the protocol can be
modi?ed tonot permit releaseof exclusivelocks until the end of the transaction.
Holding exclusive locks until the end of the transaction reduces concurrency.
Here is an alternative that improves concurrency, but ensures only recoverabil-
ity: For each data item with an uncommitted write, we record which transaction
performed the last write to the data item. Whenever a transaction T
i
performs a
readof an uncommitted dataitem,we recordacommitdependencyof T
i
on the
T
10
T
11
T
12
T
13
lock-X(B)
lock-X(D)
lock-X(H)
unlock(D)
lock-X(E)
lock-X(D)
unlock(B)
unlock(E)
lock-X(B)
lock-X(E)
unlock(H)
lock-X(G)
unlock(D)
lock-X(D)
lock-X(H)
unlock(D)
unlock(H)
unlock(E)
unlock(B)
unlock(G)
Figure15.12 Serializable schedule under the tree protocol.
674 Chapter15 ConcurrencyControl
transaction that performed the last write to the data item. Transaction T
i
is then
not permitted to commit until the commit of all transactions on which it has a
commit dependency.If any of these transactions aborts, T
i
mustalso be aborted.
The tree-locking protocol has an advantage over the two-phase locking pro-
tocol in that, unlike two-phase locking, it is deadlock-free, so no rollbacks are
required. The tree-locking protocol has another advantage over the two-phase
locking protocol in that unlocking may occur earlier. Earlier unlocking may lead
to shorter waiting times,and toan increase inconcurrency.
However,theprotocolhasthedisadvantagethat,insomecases,atransaction
mayhavetolockdataitemsthatitdoesnotaccess.Forexample,atransactionthat
needstoaccessdataitemsAandJinthedatabasegraphofFigure15.11mustlock
not only A and J,butalsodataitemsB, D,andH. This additional locking results
in increased locking overhead, the possibility of additional waiting time, and a
potentialdecreaseinconcurrency.Further,withoutpriorknowledgeofwhatdata
items will need to be locked, transactions will have to lock the root of the tree,
and that can reduce concurrency greatly.
For a set of transactions, there may be con?ict-serializable schedules that
cannotbeobtainedthroughthetreeprotocol.Indeed,thereareschedulespossible
underthetwo-phaselockingprotocolthatarenotpossibleunderthetreeprotocol,
and vice versa.Examplesof such schedulesare exploredinthe exercises.
15.2 DeadlockHandling
Asystemisinadeadlockstateifthereexistsasetoftransactionssuchthat every
transaction inthesetiswaitingforanothertransaction intheset.Moreprecisely,
there exists a set of waiting transactions {T
0
, T
1
,...,T
n
} such that T
0
is waiting
for a data item that T
1
holds, and T
1
is waiting for a data item that T
2
holds, and
...,andT
n?1
is waiting for a data itemthat T
n
holds, and T
n
is waiting for a data
itemthatT
0
holds.Noneofthetransactionscanmakeprogressinsuchasituation.
The only remedy to this undesirable situation is for the system to invoke
some drastic action, such as rolling back some of the transactions involved in
thedeadlock.Rollbackofatransactionmaybepartial:That is,atransactionmay
be rolled back to the point where it obtained a lock whose release resolves the
deadlock.
There are two principal methods for dealing with the deadlock problem. We
canuseadeadlockpreventionprotocoltoensurethatthesystemwillneverenter
adeadlockstate.Alternatively,wecanallowthesystemtoenteradeadlockstate,
and then try to recover by using a deadlock detection and deadlock recovery
scheme.Asweshallsee,bothmethodsmayresultintransactionrollback.Preven-
tion is commonly used if the probability that the system would enter a deadlock
stateisrelativelyhigh; otherwise,detectionand recoveryaremoreef?cient.
Notethatadetectionandrecoveryschemerequiresoverheadthatincludesnot
onlytherun-timecostofmaintainingthenecessaryinformationandofexecuting
the detection algorithm, but also the potential losses inherent in recovery from a
deadlock.
15.2 DeadlockHandling 675
15.2.1 Deadlock Prevention
Therearetwoapproachestodeadlockprevention.Oneapproachensuresthatno
cyclic waits can occur by ordering the requests for locks, or requiring all locks
to be acquired together. The other approach is closer to deadlock recovery, and
performs transaction rollback instead of waiting for a lock, whenever the wait
could potentiallyresultina deadlock.
The simplest scheme under the ?rst approach requires that each transaction
locks all its data items before it begins execution. Moreover, either all are locked
inonestepornonearelocked.Therearetwomaindisadvantagestothisprotocol:
(1)itisoftenhard topredict,beforethe transactionbegins,whatdataitemsneed
to be locked; (2) data-item utilization may be very low, since many of the data
itemsmaybe locked but unused for along time.
Another approach for preventing deadlocks is to impose an ordering of all
data items, and to require that a transaction lock data items only in a sequence
consistent withthe ordering.Wehaveseenone such schemeinthetreeprotocol,
which usesa partial orderingof dataitems.
Avariationofthisapproachistouseatotalorderofdataitems,inconjunction
withtwo-phaselocking.Onceatransactionhaslockedaparticularitem,itcannot
requestlocksonitemsthatprecedethatitemintheordering.Thisschemeiseasy
toimplement,aslongasthe setofdataitemsaccessed byatransaction isknown
whenthetransactionstartsexecution.Thereisnoneedtochange theunderlying
concurrency-control system if two-phase locking is used: All that is needed is to
ensure that locks are requested in the right order.
The second approach for preventing deadlocks is to use preemption and
transaction rollbacks. In preemption, when a transaction T
j
requests a lock that
transaction T
i
holds, the lock granted to T
i
may be preempted by rolling back
of T
i
, and granting of the lock to T
j
. To control the preemption, we assign a
uniquetimestamp,basedonacounteroronthesystemclock,toeachtransaction
when it begins. The system uses these timestamps only to decide whether a
transactionshouldwaitorrollback.Lockingisstillusedforconcurrencycontrol.
If a transaction is rolled back, it retains its old timestamp when restarted. Two
differentdeadlock-preventionschemes usingtimestampshavebeenproposed:
1. The wait–die scheme is a nonpreemptive technique. When transaction T
i
requestsadataitemcurrentlyheldbyT
j
, T
i
isallowedtowaitonly ifithas
a timestamp smaller than that of T
j
(that is, T
i
is older than T
j
). Otherwise,
T
i
isrolledback (dies).
Forexample,supposethattransactions T
14
, T
15
,andT
16
havetimestamps
5, 10, and 15, respectively. If T
14
requestsadataitemheldbyT
15
,thenT
14
willwait.If T
24
requestsadataitemheldby T
15
,thenT
16
willberolledback.
2. The wound–wait scheme is a preemptive technique. It is a counterpart to
the wait–die scheme. When transaction T
i
requestsadataitemcurrently
heldby T
j
, T
i
isallowedtowaitonlyifithasatimestamplargerthanthatof
T
j
(thatis, T
i
isyoungerthan T
j
).Otherwise, T
j
isrolledback(T
j
iswounded
by T
i
).
676 Chapter15 ConcurrencyControl
Returning to our example, with transactions T
14
, T
15
,andT
16
,ifT
14
requestsadataitemheldbyT
15
,thenthedataitemwillbepreemptedfrom
T
15
,andT
15
will be rolled back. If T
16
requests a data item held by T
15
,then
T
16
will wait.
The major problem with both of these schemes is that unnecessary rollbacks
may occur.
Another simpleapproach to deadlock prevention is based onlocktimeouts.
In this approach, a transaction that has requested a lock waits for at most a
speci?ed amount of time. If the lock has not been granted within that time, the
transaction is said to time out, and it rolls itself back and restarts. If there was
in fact a deadlock, one or more transactions involved in the deadlock will time
out and roll back, allowing the others to proceed. This scheme falls somewhere
between deadlock prevention, where a deadlock will never occur, and deadlock
detectionand recovery,which Section15.2.2 discusses.
The timeout scheme is particularly easy to implement, and works well if
transactionsareshortandiflongwaitsarelikelytobeduetodeadlocks.However,
in general it is hard to decide how long a transaction must wait before timing
out. Too long a wait resultsin unnecessary delaysonce a deadlockhas occurred.
Too short a wait results in transaction rollback even when there is no deadlock,
leading to wasted resources. Starvation is also a possibility with this scheme.
Hence, the timeout-basedscheme has limitedapplicability.
15.2.2 Deadlock Detection andRecovery
If a system does not employ some protocol that ensures deadlock freedom, then
a detection and recovery scheme must be used. An algorithm that examines the
state of the system is invoked periodically to determine whether a deadlock has
occurred.Ifonehas, thenthesystemmustattempttorecoverfromthedeadlock.
To doso, the systemmust:
• Maintain information about the current allocation of data items to transac-
tions, as well as any outstanding dataitemrequests.
• Provide an algorithm that uses this information to determine whether the
systemhas entereda deadlockstate.
• Recover from the deadlock when the detection algorithm determines that a
deadlockexists.
In this section, we elaborate on theseissues.
15.2.2.1 DeadlockDetection
Deadlocks can be described precisely in terms of a directed graph called await-
forgraph.ThisgraphconsistsofapairG=(V,E),whereVisasetofverticesand
Eisasetofedges.Thesetofverticesconsistsofallthetransactionsinthesystem.
Eachelementintheset Eofedgesisanorderedpair T
i
? T
j
.IfT
i
? T
j
isin E,
15.2 DeadlockHandling 677
T
18
T
20
T
17
T
19
Figure15.13 Wait-for graph with no cycle.
then there is a directed edge from transaction T
i
to T
j
, implying that transaction
T
i
iswaiting for transaction T
j
to releaseadata itemthat itneeds.
When transaction T
i
requestsadataitemcurrentlybeingheldby transaction
T
j
,thentheedge T
i
? T
j
isinsertedinthewait-forgraph.Thisedgeisremoved
only when transaction T
j
isno longer holding a dataitemneededby transaction
T
i
.
A deadlock exists in the system if and only if the wait-for graph contains a
cycle. Each transaction involved in the cycle is said to be deadlocked. To detect
deadlocks, the system needs to maintain the wait-for graph, and periodically to
invoke an algorithm that searchesfora cycle in the graph.
Toillustratetheseconcepts,considerthewait-forgraphinFigure15.13,which
depictsthe following situation:
• Transaction T
17
is waiting for transactions T
18
and T
19
.
• Transaction T
19
iswaiting for transaction T
18
.
• Transaction T
18
iswaiting for transaction T
20
.
Since the graphhas no cycle, the systemisnot in a deadlockstate.
Supposenow that transaction T
20
isrequestinganitemheldby T
19
.Theedge
T
20
? T
19
is added to the wait-for graph, resulting in the new system state in
Figure 15.14. This time,the graph contains the cycle:
T
18
? T
20
? T
19
? T
18
implyingthat transactions T
18
, T
19
,andT
20
are alldeadlocked.
Consequently, the question arises: When should we invoke the detection
algorithm? The answer dependson two factors:
1. How often doesa deadlockoccur?
2. How many transactions will be affectedby the deadlock?
If deadlocks occur frequently, then the detection algorithm should be in-
voked more frequently. Data items allocated to deadlocked transactions will be
678 Chapter15 ConcurrencyControl
T
18
T
20
T
17
T
19
Figure15.14 Wait-for graph with a cycle.
unavailable to other transactions until the deadlock can be broken. In addition,
the number of cycles in the graph may also grow. In the worst case, we would
invoke the detection algorithm every time a request for allocation could not be
granted immediately.
15.2.2.2 RecoveryfromDeadlock
When a detection algorithm determines that a deadlock exists, the system must
recoverfromthedeadlock.Themostcommonsolutionistorollbackoneormore
transactions tobreak the deadlock.Three actions need to be taken:
1. Selectionofavictim.Givenasetofdeadlockedtransactions,wemustdeter-
mine which transaction (or transactions) to roll back to break the deadlock.
We should roll back those transactions that will incur the minimum cost.
Unfortunately,thetermminimumcostisnotapreciseone.Manyfactorsmay
determinethe cost of a rollback, including:
a. How long the transaction has computed, and how much longer the
transaction will compute before itcompletesitsdesignatedtask.
b. How many dataitemsthe transaction has used.
c. How many more dataitemsthe transaction needsfor it tocomplete.
d. How many transactions willbe involvedinthe rollback.
2. Rollback.Oncewehavedecidedthataparticulartransactionmustberolled
back, we must determinehow far this transaction should be rolledback.
The simplest solution is atotalrollback: Abort the transaction and then
restart it. However, it is more effective to roll back the transaction only as
far as necessary to break the deadlock. Such partial rollback requires the
systemtomaintainadditionalinformationaboutthestateofalltherunning
transactions.Speci?cally,thesequenceoflockrequests/grantsandupdates
performedbythetransaction needstoberecorded.Thedeadlockdetection
mechanism should decide which locks the selected transaction needs to
release in order to break the deadlock. The selected transaction must be
rolled back to the point where it obtained the ?rst of these locks, undoing
allactionsittookafterthatpoint.Therecoverymechanismmustbecapable
15.3 MultipleGranularity 679
ofperformingsuchpartialrollbacks.Furthermore,thetransactionsmustbe
capableofresumingexecutionafterapartialrollback.Seethebibliographical
notes for relevant references.
3. Starvation.Inasystemwheretheselectionofvictimsisbasedprimarilyon
cost factors, it may happen that the same transaction is always picked as
a victim. As a result, this transaction never completes its designated task,
thusthereisstarvation.Wemustensurethat atransaction canbepickedas
a victim only a (small) ?nite number of times. The most common solution
isto include the number ofrollbacks in the cost factor.
15.3 MultipleGranularity
In the concurrency-control schemes described thus far, we have used each indi-
vidualdata itemas the unit on which synchronization isperformed.
Therearecircumstances,however,whereitwouldbeadvantageoustogroup
severaldata items,and to treatthemas one individualsynchronization unit. For
example, if a transaction T
i
needs to access the entire database, and a locking
protocol is used, then T
i
must lock each item in the database. Clearly, executing
these locks is time-consuming. It would be better if T
i
could issue a single lock
request to lock the entire database. On the other hand, if transaction T
j
needs to
accessonlyafewdataitems,itshouldnotberequiredtolocktheentiredatabase,
since otherwise concurrency is lost.
Whatisneededisamechanismtoallowthesystemtode?nemultiplelevelsof
granularity.Thisisdonebyallowingdataitemstobeofvarioussizesandde?ning
ahierarchyofdatagranularities,where thesmall granularitiesare nestedwithin
larger ones. Such a hierarchy can be represented graphically as a tree. Note that
the tree that we describe here is signi?cantly different from that used by the
tree protocol (Section 15.1.5). A nonleaf node of the multiple-granularity tree
represents the data associated with its descendants. In the tree protocol, each
node is an independentdataitem.
As an illustration, consider the tree of Figure 15.15, which consists of four
levels of nodes. The highest level represents the entire database. Below it are
nodes of type area; the database consists of exactly these areas. Each area in turn
has nodes of type ?le as its children. Each area contains exactly those ?les that
areitschildnodes.No?leisinmorethanonearea.Finally,each?lehasnodesof
type record. As before, the ?le consists of exactly those records that are its child
nodes, and no recordcan be presentin more than one ?le.
Each node in the tree can be locked individually. As we did in the two-
phase locking protocol, we shall use shared and exclusive lock modes. When a
transaction locks a node, in eithershared or exclusive mode,the transaction also
has implicitly locked all the descendants of that node in the same lock mode.
For example, if transaction T
i
gets an explicit lock on ?le F
c
of Figure 15.15,
in exclusive mode, then it has an implicit lock in exclusive mode on all the
records belonging to that ?le. It does not need to lock the individual records of
F
c
explicitly.
680 Chapter15 ConcurrencyControl
r
a
1
r
a
2
r
a
n
r
b
1
r
b
k
r
c
1
r
c
m
F
a
F
b
F
c
A
1
A
2
DB
Figure15.15 Granularity hierarchy.
Suppose that transaction T
j
wishes to lock record r
b
6
of ?le F
b
.SinceT
i
has
locked F
b
explicitly, it follows that r
b
6
is also locked (implicitly). But, when T
j
issues a lock request for r
b
6
, r
b
6
is not explicitly locked! How does the system
determine whether T
j
can lock r
b
6
? T
j
must traverse the tree from the root to
record r
b
6
. If any node in that path is locked in an incompatible mode, then T
j
must be delayed.
Supposenowthattransaction T
k
wishestolocktheentiredatabase.Todoso,
it simply must lock the root of the hierarchy. Note, however, that T
k
should not
succeed in locking the root node, since T
i
is currently holding a lock on part of
the tree (speci?cally, on ?le F
b
). But how does the system determine if the root
nodecanbelocked?Onepossibilityisforittosearchtheentiretree.Thissolution,
however, defeatsthe whole purpose of the multiple-granularitylocking scheme.
A more ef?cient way to gain this knowledge is to introduce a new class of lock
modes, called intention lock modes. If a node is locked in an intention mode,
explicitlocking is done at a lower levelof the tree (that is, at a ?ner granularity).
Intention locks are put on all the ancestors of a node before that node is locked
explicitly.Thus,atransactiondoesnotneedtosearchtheentiretreetodetermine
whetheritcanlockanodesuccessfully.Atransactionwishingtolockanode—say,
Q—must traversea path in the treefrom the root to Q.Whiletraversingthe tree,
the transaction locks the various nodesin an intention mode.
There is an intention mode associated with shared mode, and there is one
with exclusive mode. If a node is locked inintention-shared(IS)mode, explicit
locking is being done at a lower level of the tree, but with only shared-mode
locks.Similarly,ifanodeislockedinintention-exclusive(IX)mode,thenexplicit
locking is being done at a lower level, with exclusive-mode or shared-mode
locks. Finally, if a node is locked insharedandintention-exclusive(SIX)mode,
the subtree rooted by that node is locked explicitly in shared mode, and that
explicit locking is being done at a lower level with exclusive-mode locks. The
compatibilityfunction for these lock modesisin Figure 15.16.
15.3 MultipleGranularity 681
IS IX S SIX X
IS true true true true false
IX true true false false false
S true false true false false
SIX true false false false false
X false false false false false
Figure15.16 Compatibility matrix.
The multiple-granularity locking protocol uses these lock modes to ensure
serializability.Itrequiresthatatransaction T
i
thatattemptstolockanodeQmust
follow these rules:
1. Transaction T
i
mustobservethelock-compatibilityfunctionofFigure15.16.
2. Transaction T
i
mustlocktherootofthetree?rst,andcanlockitinanymode.
3. Transaction T
i
can lock a node Q inSor IS mode only if T
i
currentlyhas the
parent of Q lockedineither IX or IS mode.
4. Transaction T
i
can lock a node Q in X, SIX,orIX mode only if T
i
currently
has the parent of Q lockedineither IX or SIX mode.
5. Transaction T
i
can lock a node only if T
i
has not previously unlocked any
node (that is, T
i
istwo phase).
6. Transaction T
i
can unlock a node Q only if T
i
currently has none of the
childrenof Q locked.
Observethat the multiple-granularityprotocol requiresthat locks be acquired in
top-down (root-to-leaf) order, whereas locks must be released in bottom-up (leaf-
to-root) order.
As an illustration of the protocol, consider the tree of Figure 15.15 and these
transactions:
• Supposethattransaction T
21
readsrecordr
a
2
in?le F
a
.Then, T
21
needstolock
the database, area A
1
,andF
a
in IS mode (and in that order), and ?nally to
lock r
a
2
inSmode.
• Suppose that transaction T
22
modi?es record r
a
9
in ?le F
a
. Then, T
22
needs
to lock the database, area A
1
, and ?le F
a
(and in that order) in IX mode, and
?nally to lockr
a
9
inX mode.
• Suppose that transaction T
23
reads all the records in ?le F
a
. Then, T
23
needs
tolockthedatabaseandarea A
1
(andinthatorder)in ISmode,and?nallyto
lock F
a
inSmode.
682 Chapter15 ConcurrencyControl
• Suppose that transaction T
24
reads the entire database. It can do so after
locking the database inS mode.
We note that transactions T
21
, T
23
,andT
24
can access the database concurrently.
Transaction T
22
can executeconcurrently with T
21
, but not with either T
23
or T
24
.
Thisprotocolenhancesconcurrency andreduceslockoverhead.Itisparticu-
larlyuseful in applications that include amix of:
• Shorttransactions that access only a fewdataitems.
• Long transactions that produce reportsfrom an entire ?le or setof ?les.
There is a similar locking protocol that is applicable to database systems in
which data granularities are organized in the form of a directed acyclic graph.
Seethebibliographicalnotesforadditionalreferences.Deadlockispossibleinthe
multiple-granularity protocol, as it is in the two-phase locking protocol. There
aretechniquestoreducedeadlockfrequencyinthemultiple-granularityprotocol,
and also to eliminate deadlock entirely. These techniques are referenced in the
bibliographical notes.
15.4 Timestamp-BasedProtocols
The locking protocols that we have described thus far determine the order be-
tween every pair of con?icting transactions at execution time by the ?rst lock
that both members of the pair request that involves incompatible modes. An-
other method for determining the serializability order is to select an ordering
amongtransactions inadvance.Themostcommonmethodfordoingsoistouse
a timestamp-ordering scheme.
15.4.1 Timestamps
With each transaction T
i
in the system, we associate a unique ?xed timestamp,
denotedbyTS(T
i
).Thistimestampisassignedbythe databasesystembeforethe
transaction T
i
starts execution. If a transaction T
i
has been assigned timestamp
TS(T
i
), and a new transaction T
j
enters the system, then TS(T
i
) < TS(T
j
). There
are two simplemethodsfor implementingthis scheme:
1. Usethevalueofthes
¯
ystem clock as the timestamp; that is, a transaction’s
timestampisequaltothevalueoftheclockwhenthetransactionentersthe
system.
2. Use a logical counter that is incremented after a new timestamp has been
assigned; that is, a transaction’s timestamp is equal to the value of the
counter when the transaction enters the system.
15.4 Timestamp-BasedProtocols 683
The timestamps of the transactions determine the serializability order. Thus,
if TS(T
i
) < TS(T
j
), then the system must ensure that the produced schedule is
equivalenttoaserialscheduleinwhichtransaction T
i
appearsbeforetransaction
T
j
.
Toimplementthisscheme,weassociatewitheachdataitemQtwotimestamp
values:
• W-timestamp(Q) denotes the largest timestamp of any transaction that exe-
cuted write(Q)successfully.
• R-timestamp(Q) denotes the largest timestamp of any transaction that exe-
cuted read(Q)successfully.
These timestamps are updated whenever a new read(Q)orwrite(Q) instruction
isexecuted.
15.4.2 TheTimestamp-Ordering Protocol
The timestamp-ordering protocol ensures that any con?icting read and write
operationsare executedin timestamporder.This protocol operatesas follows:
1. Suppose that transaction T
i
issues read(Q).
a. If TS(T
i
) < W-timestamp(Q), then T
i
needs to read a value of Q that
wasalreadyoverwritten.Hence,the readoperationisrejected,and T
i
isrolledback.
b. If TS(T
i
) ? W-timestamp(Q), then the read operation is executed, and
R-timestamp(Q)issettothemaximumofR-timestamp(Q)andTS(T
i
).
2. Suppose that transaction T
i
issues write(Q).
a. If TS(T
i
) < R-timestamp(Q), then the value of Q that T
i
is producing
wasneededpreviously,andthesystemassumedthatthatvaluewould
never be produced. Hence, the system rejects the write operation and
rolls T
i
back.
b. IfTS(T
i
)< W-timestamp(Q),then T
i
isattemptingtowriteanobsolete
value of Q. Hence, the system rejects this write operation and rolls T
i
back.
c. Otherwise, the system executes the write operation and sets W-time-
stamp(Q)toTS(T
i
).
If a transaction T
i
is rolled back by the concurrency-control scheme as result of
issuanceofeitherareadorwriteoperation,thesystemassignsitanewtimestamp
and restartsit.
To illustrate this protocol, we consider transactions T
25
and T
26
.Transaction
T
25
displaysthe contents of accounts A and B:
684 Chapter15 ConcurrencyControl
T
25
: read(B);
read(A);
display(A+ B).
Transaction T
26
transfers $50 from account B to account A, and then displays the
contents of both:
T
26
: read(B);
B := B ? 50;
write(B);
read(A);
A:= A+ 50;
write(A);
display(A+ B).
In presenting schedules under the timestamp protocol, we shall assume that a
transactionisassignedatimestampimmediatelybeforeits?rstinstruction.Thus,
inschedule3ofFigure15.17,TS(T
25
)<TS(T
26
),andthescheduleispossibleunder
the timestampprotocol.
Wenotethat theprecedingexecutioncanalsobeproducedbythetwo-phase
locking protocol. There are, however, schedules that are possible under the two-
phase locking protocol, but are not possible under the timestamp protocol, and
viceversa(seeExercise15.29).
The timestamp-ordering protocol ensures con?ict serializability. This is be-
cause con?icting operationsare processedin timestamporder.
Theprotocolensuresfreedomfromdeadlock,sincenotransactioneverwaits.
However, there is a possibility of starvation of long transactions if a sequence of
con?icting short transactions causes repeated restarting of the long transaction.
If a transaction is suffering from repeated restarts, con?icting transactions need
to be temporarilyblocked toenable the transaction to?nish.
T
25
T
26
read(B)
read(B)
B := B ? 50
write(B)
read(A)
read(A)
display(A+B)
A:= A + 50
write(A)
display(A+B )
Figure15.17 Schedule 3.
15.4 Timestamp-BasedProtocols 685
Theprotocolcangenerateschedulesthatarenotrecoverable.However,itcan
be extendedto maketheschedulesrecoverable,inone of severalways:
• Recoverability and cascadelessness can be ensured by performing all writes
together at the end of the transaction. The writes must be atomic in the
followingsense:Whilethewritesareinprogress,notransactionispermitted
toaccess any ofthe dataitemsthathave beenwritten.
• Recoverabilityandcascadelessnesscanalsobeguaranteedbyusingalimited
form of locking, whereby reads of uncommitted items are postponed until
the transaction that updatedthe itemcommits(see Exercise15.30).
• Recoverabilityalonecanbeensuredbytrackinguncommittedwrites,andal-
lowingatransactionT
i
tocommitonlyafterthecommitofanytransactionthat
wrote a value that T
i
read.Commit dependencies,outlined in Section15.1.5,
can be used forthis purpose.
15.4.3 Thomas’ WriteRule
We now present a modi?cation to the timestamp-ordering protocol that allows
greater potential concurrency than does the protocol of Section 15.4.2. Let us
consider schedule 4 of Figure 15.18, and apply the timestamp-orderingprotocol.
Since T
27
starts before T
28
, we shall assume that TS(T
27
) < TS(T
28
). The read(Q)
operation of T
27
succeeds, as does the write(Q) operation of T
28
.WhenT
27
at-
tempts its write(Q) operation, we ?nd that TS(T
27
) < W-timestamp(Q), since W-
timestamp(Q)=TS( T
28
). Thus, the write(Q)byT
27
is rejected and transaction T
27
must be rolledback.
Although the rollback of T
27
is requiredby the timestamp-orderingprotocol,
itisunnecessary.Since T
28
hasalreadywritten Q,thevaluethat T
27
isattempting
to write is one that will never need to be read. Any transaction T
i
with TS(T
i
) <
TS(T
28
)thatattemptsaread(Q)willberolledback,sinceTS(T
i
)<W-timestamp(Q).
Anytransaction T
j
withTS(T
j
)>TS(T
28
)mustreadthevalueof Qwrittenby T
28
,
rather than the value that T
27
isattemptingtowrite.
Thisobservationleadstoamodi?edversionofthetimestamp-orderingproto-
colinwhichobsoletewriteoperationscanbeignoredundercertaincircumstances.
The protocol rules for read operations remain unchanged. The protocol rules for
write operations, however, are slightly different from the timestamp-ordering
protocol of Section15.4.2.
T
27
T
28
read(Q)
write(Q)
write(Q)
Figure15.18 Schedule 4.
686 Chapter15 ConcurrencyControl
The modi?cation to the timestamp-ordering protocol, called Thomas’ write
rule, isthis: Suppose that transaction T
i
issues write(Q).
1. If TS(T
i
) < R-timestamp(Q), then the value of Q that T
i
is producing was
previouslyneeded,andithadbeenassumedthatthevaluewouldneverbe
produced.Hence, the systemrejectsthe write operation and rolls T
i
back.
2. IfTS(T
i
)<W-timestamp(Q),then T
i
isattemptingtowriteanobsoletevalue
of Q.Hence,thiswrite operationcan be ignored.
3. Otherwise,thesystemexecutesthewriteoperationandsetsW-timestamp(Q)
to TS(T
i
).
The difference between these rules and those of Section 15.4.2 lies in the
second rule. The timestamp-ordering protocol requires that T
i
be rolled back if
T
i
issues write(Q)andTS( T
i
) < W-timestamp(Q). However, here, in those cases
where TS(T
i
) ? R-timestamp(Q),we ignore the obsolete write.
Byignoringthewrite,Thomas’writeruleallowsschedulesthatarenotcon?ict
serializable but are nevertheless correct. Those non-con?ict-serializable sched-
ulesallowedsatisfythede?nitionofviewserializableschedules(seeexamplebox).
Thomas’ write rule makes use of view serializability by, in effect, deleting ob-
solete write operations from the transactions that issue them. This modi?cation
of transactions makes it possible to generate serializable schedules that would
not be possibleunderthe otherprotocols presentedin thischapter. For example,
schedule4ofFigure15.18isnotcon?ictserializableand,thus,isnotpossibleun-
der the two-phase locking protocol, the tree protocol, or the timestamp-ordering
protocol. Under Thomas’ write rule, the write(Q) operation of T
27
would be ig-
nored. The result is a schedule that is view equivalent to the serial schedule <T
27
,
T
28
>.
15.5 Validation-Based Protocols
In cases where a majority of transactions are read-only transactions, the rate of
con?icts among transactions may be low. Thus, many of these transactions, if
executed without the supervision of a concurrency-control scheme, would nev-
ertheless leave the system in a consistent state. A concurrency-control scheme
imposes overhead of code execution and possible delay of transactions. It may
be better to use an alternative scheme that imposes less overhead. A dif?culty
in reducing the overhead is that we do not know in advance which transactions
will be involved in a con?ict. To gain that knowledge, we need a scheme for
monitoring the system.
The validation protocol requires that each transaction T
i
executes in two or
threedifferentphasesinitslifetime,dependingonwhetheritisaread-onlyoran
update transaction. The phases are,in order:
15.5 Validation-BasedProtocols 687
VIEWSERIALIZABILITY
There is another form of equivalence that is less stringent than con?ict equiv-
alence, but that, like con?ict equivalence, is based on only the read and write
operationsoftransactions.
Consider two schedules S and S
null
, where the same set of transactions partici-
pates in both schedules. The schedules S and S
null
are said to beviewequivalent
if threeconditions aremet:
1. Foreachdataitem Q,iftransactionT
i
readstheinitialvalueof Qinschedule S,
then transaction T
i
must, in schedule S
null
, also readthe initial value of Q.
2. ForeachdataitemQ,iftransactionT
i
executesread(Q)inscheduleS,andifthat
value was produced by a write(Q) operation executed by transaction T
j
,then
the read(Q)operationoftransaction T
i
must,inschedule S
null
,alsoreadthevalue
of Qthat was producedby the same write(Q) operationof transaction T
j
.
3. For each data item Q, the transaction (if any) that performs the ?nal write(Q)
operationinscheduleSmustperformthe?nalwrite(Q)operationinscheduleS
null
.
Conditions 1 and 2 ensure that each transaction reads the same values in both
schedulesand,therefore,performsthesamecomputation.Condition3,coupled
with conditions 1 and 2, ensures that both schedules result in the same ?nal
system state.
The concept of view equivalence leads to the concept of view serializability.
We say that a schedule S is view serializable if it is view equivalent to a serial
schedule.
Asanillustration,supposethatweaugmentschedule4with transaction T
29
,
and obtain the following view serializable (schedule 5):
T
27
T
28
T
29
read(Q)
write(Q)
write(Q)
write(Q)
Indeed,schedule5isviewequivalenttotheserialschedule<T
27
,T
28
,T
29
>,since
the one read(Q) instruction reads the initial value of Q in both schedules and
T
29
performsthe?nal writeof Qin both schedules.
Every con?ict-serializable schedule is also view serializable, but there are
view-serializable schedulesthatarenotcon?ictserializable.Indeed,schedule5
is notcon?ictserializable, sinceeverypair ofconsecutiveinstructionscon?icts,
and, thus, no swapping ofinstructions is possible.
Observe that, in schedule 5, transactions T
28
and T
29
perform write(Q)oper-
ations without having performed a read(Q) operation. Writes of this sort are
calledblind writes. Blind writes appear in any view-serializable schedule that
is notcon?ictserializable.
688 Chapter15 ConcurrencyControl
1. Readphase. During this phase, the system executes transaction T
i
.Itreads
the values of the various data items and stores them in variables local to
T
i
.Itperformsallwrite operations on temporary local variables, without
updatesof the actual database.
2. Validationphase.Thevalidationtest(describedbelow)isappliedtotrans-
action T
i
. This determines whether T
i
is allowed to proceed to the write
phase without causing a violation of serializability.If a transaction fails the
validationtest,the systemaborts the transaction.
3. Writephase.Ifthevalidationtestsucceedsfortransaction T
i
,thetemporary
localvariablesthatholdtheresultsofany writeoperationsperformedby T
i
are copied to the database. Read-only transactions omit thisphase.
Eachtransactionmustgothroughthephasesintheordershown.However,phases
of concurrently executingtransactions can be interleaved.
To perform the validation test, we need to know when the various phases of
transactions took place. We shall, therefore, associate three different timestamps
with each transaction T
i
:
1. Start(T
i
),thetimewhenT
i
starteditsexecution.
2. Validation(T
i
), the time when T
i
?nished its read phase and started its
validationphase.
3. Finish(T
i
),thetimewhenT
i
?nished itswritephase.
We determinethe serializabilityorder by the timestamp-orderingtechnique,
using the value of the timestamp Validation(T
i
). Thus, the value TS(T
i
) = Valida-
tion(T
i
)and,ifTS( T
j
) < TS(T
k
), then any produced schedule must be equivalent
to a serial schedule in which transaction T
j
appears before transaction T
k
.The
reason we have chosen Validation(T
i
), rather than Start(T
i
), as the timestamp of
transaction T
i
is that we can expect faster response time provided that con?ict
rates among transactions are indeedlow.
Thevalidationtestfortransaction T
i
requiresthat,foralltransactions T
k
with
TS(T
k
)< TS(T
i
), one ofthe following two conditions must hold:
1. Finish(T
k
)< Start(T
i
). Since T
k
completesitsexecution before T
i
started,the
serializabilityorderisindeedmaintained.
2. ThesetofdataitemswrittenbyT
k
doesnotintersectwiththesetofdataitems
read by T
i
,andT
k
completes its write phase before T
i
starts its validation
phase (Start(T
i
) < Finish(T
k
) < Validation(T
i
)). This condition ensures that
thewritesof T
k
and T
i
donotoverlap.Sincethewritesof T
k
donotaffectthe
read of T
i
, and since T
i
cannot affect the read of T
k
, the serializability order
is indeedmaintained.
15.6 MultiversionSchemes 689
T
25
T
26
read(B)
read(B)
B := B ? 50
read(A)
A:= A + 50
read(A)
< validate>
display(A+B )
<validate>
write(B)
write(A)
Figure15.19 Schedule 6, a schedule produced by using validation.
Asanillustration,consideragaintransactionsT
25
andT
26
.SupposethatTS(T
25
)
< TS(T
26
). Then, the validation phase succeeds in the schedule 6 in Figure 15.19.
Notethatthewritestotheactualvariablesareperformedonlyafterthevalidation
phase of T
26
.Thus,T
25
reads the old values of B and A, and this schedule is
serializable.
The validation scheme automatically guards against cascading rollbacks,
since the actual writes take place only after the transaction issuing the write
has committed.However,there isa possibilityofstarvation oflong transactions,
due toa sequenceof con?icting short transactions that cause repeatedrestarts of
the long transaction. To avoid starvation, con?icting transactions must be tem-
porarilyblocked, toenable the long transaction to?nish.
This validation scheme is called the optimistic concurrency-control scheme
since transactions execute optimistically, assuming they will be able to ?nish
executionandvalidateattheend.Incontrast,lockingandtimestamporderingare
pessimisticin that they force a waitor a rollback whenevera con?ict is detected,
eventhough there isa chance that the schedule may be con?ict serializable.
15.6 MultiversionSchemes
The concurrency-control schemes discussed thus far ensure serializability by ei-
ther delaying an operation or aborting the transaction that issued the operation.
For example, a read operation may be delayed because the appropriate value
has not been written yet; or it may be rejected (that is, the issuing transaction
mustbeaborted)becausethevaluethatitwassupposedtoreadhasalreadybeen
overwritten. These dif?culties could be avoided if old copies of each data item
were keptin a system.
In multiversion concurrency-control schemes, each write(Q)operationcre-
ates a new version of Q. When a transaction issues a read(Q)operation,the
690 Chapter15 ConcurrencyControl
concurrency-control manager selects one of the versions of Q to be read. The
concurrency-controlschememustensurethattheversiontobereadisselectedin
a manner that ensures serializability. It is also crucial, for performance reasons,
that a transaction be able to determine easily and quickly which version of the
dataitemshould be read.
15.6.1 MultiversionTimestamp Ordering
The timestamp-ordering protocol can be extended to a multiversion protocol.
With each transaction T
i
in the system, we associate a unique static timestamp,
denoted by TS(T
i
). The database system assigns this timestamp before the trans-
action startsexecution, asdescribedin Section15.4.
WitheachdataitemQ,asequenceofversions<Q
1
, Q
2
,...,Q
m
>isassociated.
Each version Q
k
contains three data ?elds:
• Contentisthe value of version Q
k
.
• W-timestamp(Q
k
) is the timestamp of the transaction that created version
Q
k
.
• R-timestamp(Q
k
)isthelargesttimestampofanytransactionthatsuccessfully
readversion Q
k
.
A transaction—say, T
i
—creates a new version Q
k
of data item Q by issuing
a write(Q) operation. The content ?eld of the version holds the value written by
T
i
.ThesysteminitializestheW-timestampandR-timestamptoTS(T
i
).Itupdates
the R-timestamp value of Q
k
whenever a transaction T
j
reads the content of Q
k
,
and R-timestamp(Q
k
)< TS(T
j
).
The multiversion timestamp-ordering scheme presented next ensures seri-
alizability. The scheme operates as follows: Suppose that transaction T
i
issues
a read(Q)orwrite(Q)operation.LetQ
k
denote the version of Q whose write
timestampis the largestwritetimestamplessthan or equalto TS(T
i
).
1. If transaction T
i
issues a read(Q), then the value returned is the content of
version Q
k
.
2. If transaction T
i
issues write(Q), and if TS(T
i
) < R-timestamp(Q
k
), then
the system rolls back transaction T
i
. On the other hand, if TS(T
i
) = W-
timestamp(Q
k
),thesystemoverwritesthecontentsofQ
k
;otherwise(ifTS(T
i
)
> R-timestamp(Q
k
)), itcreatesa new versionof Q.
Thejusti?cationforrule1isclear.Atransactionreadsthemostrecentversion
that comes before it in time. The second rule forces a transaction to abort if it
is “too late” in doing a write. More precisely, if T
i
attempts to write a version
that some other transaction would have read,then we cannot allow that write to
succeed.
Versions that are no longer needed are removed according to the following
rule:Supposethattherearetwoversions, Q
k
and Q
j
,ofadataitem,andthatboth
15.6 MultiversionSchemes 691
versionshaveaW-timestamplessthanthetimestampoftheoldesttransactionin
thesystem.Then,theolderofthetwoversions Q
k
and Q
j
willnotbeusedagain,
and can be deleted.
Themultiversiontimestamp-orderingschemehasthedesirablepropertythat
areadrequestneverfailsandisnevermadetowait.Intypicaldatabasesystems,
where reading is a more frequent operation than is writing, this advantage may
be ofmajor practical signi?cance.
Thescheme,however,suffersfromtwoundesirableproperties.First,theread-
ingofadataitemalsorequirestheupdatingoftheR-timestamp?eld,resultingin
two potentialdiskaccesses, ratherthan one. Second,the con?icts betweentrans-
actionsareresolvedthroughrollbacks,ratherthanthroughwaits.Thisalternative
maybeexpensive.Section15.6.2describesanalgorithmtoalleviatethisproblem.
This multiversion timestamp-ordering scheme does not ensure recoverabil-
ity and cascadelessness. It can be extended in the same manner as the basic
timestamp-orderingscheme, to make itrecoverableand cascadeless.
15.6.2 MultiversionTwo-PhaseLocking
The multiversion two-phase locking protocol attempts to combine the advan-
tages of multiversion concurrency control with the advantages of two-phase
locking. This protocol differentiatesbetweenread-onlytransactions andupdate
transactions.
Update transactions perform rigorous two-phase locking; that is, they hold
all locks up to the end of the transaction. Thus, they can be serialized according
to their commit order. Each version of a data item has a single timestamp. The
timestampinthiscaseisnotarealclock-basedtimestamp,butratherisacounter,
whichwewillcallthets-counter,thatisincrementedduringcommitprocessing.
The database system assigns read-only transactions a timestamp by read-
ing the current value of ts-counter before they start execution; they follow the
multiversion timestamp-ordering protocol for performing reads. Thus, when a
read-onlytransaction T
i
issuesaread(Q),thevaluereturnedisthecontentsofthe
versionwhose timestampisthe largesttimestamplessthan orequal to TS(T
i
).
When an update transaction reads an item, it gets a shared lock on the item,
and reads the latest version of that item. When an update transaction wants to
write an item, it ?rst gets an exclusive lock on the item, and then creates a new
version of the data item. The write is performed on the new version, and the
timestamp of the new version is initially set to a value ?, a value greater than
that of any possible timestamp.
When the update transaction T
i
completes its actions, it carries out commit
processing: First, T
i
sets the timestamp on every version it has created to 1 more
thanthevalueofts-counter;then,T
i
incrementsts-counterby1.Onlyoneupdate
transaction isallowed toperformcommit processingat atime.
As a result, read-only transactions that start after T
i
increments ts-counter
will see the values updated by T
i
, whereas those that start before T
i
increments
ts-counterwillseethevaluebeforetheupdatesbyT
i
. In either case, read-only
692 Chapter15 ConcurrencyControl
transactions never need to wait for locks. Multiversion two-phase locking also
ensuresthat schedulesarerecoverableandcascadeless.
Versionsaredeletedinamannerlikethatofmultiversiontimestampordering.
Supposetherearetwoversions, Q
k
and Q
j
,ofadataitem,andthatbothversions
have a timestamp less than or equal to the timestamp of the oldest read-only
transaction inthesystem.Then,the olderofthetwoversions Q
k
and Q
j
willnot
be usedagainand can bedeleted.
15.7 SnapshotIsolation
Snapshot isolation is a particular type of concurrency-control scheme that has
gained wide acceptance in commercial and open-source systems,including Ora-
cle,PostgreSQL,andSQLServer.WeintroducedsnapshotisolationinSection14.9.3.
Here,we take amore detailedlookinto how itworks.
Conceptually,snapshotisolationinvolvesgivingatransactiona“snapshot”of
thedatabaseatthetimewhenitbeginsitsexecution.Itthenoperatesonthatsnap-
shot in complete isolation from concurrent transactions. The data values in the
snapshotconsistonlyofvalueswrittenbycommittedtransactions.Thisisolation
isidealforread-onlytransactionssincetheyneverwaitandareneverabortedby
theconcurrencymanager.Transactionsthatupdatethedatabasemust,ofcourse,
interact with potentially con?icting concurrent update transactions before up-
dates are actually placed in the database. Updates are kept in the transaction’s
privateworkspace untilthe transactionsuccessfullycommits,at which pointthe
updates are written to the database. When a transaction T is allowed to commit,
the transition of T to the committed state and the writing of all of the updates
madeby T tothedatabasemustbedoneasanatomicactionsothatanysnapshot
created for another transaction either includes all updates by transaction T or
none of them.
15.7.1 ValidationSteps forUpdate Transactions
Decidingwhetherornottoallowanupdatetransactiontocommitrequiressome
care. Potentially, two transactions running concurrently might both update the
samedataitem.Sincethesetwotransactionsoperateinisolationusingtheirown
private snapshots, neither transaction sees the update made by the other. If both
transactions are allowed to write to the database, the ?rst update written will
be overwritten by the second. The result is a lost update. Clearly, this must be
prevented.Therearetwovariantsofsnapshotisolation,bothofwhichpreventlost
updates.Theyarecalled?rstcommitterwinsand?rstupdaterwins.Bothapproaches
arebasedontestingthetransactionagainstconcurrenttransactions.Atransaction
issaidtobeconcurrentwith T ifitwasactiveorpartiallycommittedatanypoint
fromthestartofT uptoandincludingthetimewhenthistestisbeingperformed.
Under ?rst committer wins,whenatransactionT enters the partially com-
mittedstate, the following actions are taken in an atomic action:
15.7 SnapshotIsolation 693
• AtestismadetoseeifanytransactionthatwasconcurrentwithT hasalready
writtenanupdatetothedatabaseforsomedataitemthat T intendstowrite.
• Ifsome such transaction isfound, then T aborts.
• If no such transaction is found, then T commits and its updates are written
tothe database.
Thisapproachiscalled“?rstcommitterwins”becauseiftransactionscon?ict,the
?rst one to be tested using the above rule succeeds in writing its updates, while
the subsequent ones are forced to abort. Details of how to implement the above
testsare addressedinExercise15.19.
Under?rst updaterwins the system uses a locking mechanism that applies
only to updates (reads are unaffected by this, since they do not obtain locks).
When a transaction T
i
attempts to update a data item, it requests a write lock on
that data item. If the lock is not held by a concurrent transaction, the following
stepsare takenafterthe lockis acquired:
• Ifthe itemhas been updatedby any concurrent transaction, then T
i
aborts.
• Otherwise T
i
mayproceedwithitsexecutionincludingpossiblycommitting.
If, however, some other concurrent transaction T
j
already holds a write lock on
that dataitem,then T
i
cannot proceed and the following rulesare followed:
• T
i
waitsuntil T
j
aborts or commits.
?
If T
j
aborts, then the lock is released and T
i
canobtainthelock.Afterthe
lock is acquired, the check for an update by a concurrent transaction is
performed as described earlier: T
i
aborts if a concurrent transaction had
updatedthe dataitem,and proceedswithitsexecutionotherwise.
?
If T
j
commits, then T
i
must abort.
Locks are releasedwhen the transaction commits or aborts.
This approach is called “?rst updater wins” because if transactions con?ict,
the?rstonetoobtainthelockistheonethatispermittedtocommitandperform
its update. Those that attempt the update later abort unless the ?rst updater
subsequentlyabortsfor some otherreason. (Asan alternativeto waitingto seeif
the ?rst updater T
j
aborts, a subsequent updater T
i
can be aborted as soon as it
?nds that the write lock it wishes to obtain is held by T
j
.)
15.7.2 SerializabilityIssues
Snapshot isolation is attractive in practice because the overhead is low and no
aborts occur unless two concurrent transactions update the same dataitem.
Thereis,however,oneseriousproblemwiththesnapshotisolationschemeas
we have presented it, and as it is implemented in practice: snapshot isolation does
notensureserializability.ThisistrueeveninOracle,whichusessnapshotisolation
694 Chapter15 ConcurrencyControl
astheimplementationfortheserializableisolationlevel!Next,wegiveexamples
ofpossiblenonserializableexecutionsundersnapshotisolationandshowhowto
deal with them.
1. Suppose that we have two concurrent transactions T
i
and T
j
and two data
items A and B. Suppose that T
i
reads A and B,thenupdatesB, while T
j
reads Aand B,thenupdatesA.Forsimplicity,weassumetherearenoother
concurrent transactions. Since T
i
and T
j
are concurrent, neither transaction
seestheupdatebytheotherinitssnapshot.But,sincetheyupdatedifferent
data items, both are allowed to commit regardless of whether the system
usesthe ?rst-update-winspolicy orthe ?rst-committer-winspolicy.
However, the precedence graph has a cycle. There is an edge in the
precedence graph from T
i
to T
j
because T
i
reads the value of Athat existed
before T
j
writes A.Thereisalsoanedgeintheprecedencegraphfrom T
j
to
T
i
because T
j
readsthevalueof B thatexistedbefore T
i
writes B.Sincethere
is acycle in the precedencegraph, the resultis anonserializable schedule.
This situation, where each of a pair of transactions has read data that
is written by the other, but there is no data written by both transactions, is
referred to as write skew. As a concrete example of write skew, consider
a banking scenario. Suppose that the bank enforces the integrity constraint
that the sum of the balances in the checking and the savings account of a
customermustnotbenegative.Supposethecheckingandsavingsbalances
foracustomerare$100 and $200, respectively.Supposethat transaction T
36
withdraws$200fromthecheckingaccount,afterverifyingtheintegritycon-
straint by reading both balances. Suppose that concurrently transaction T
37
withdraws$200fromthesavingsaccount,againafterverifyingtheintegrity
constraint. Since each of the transactions checks the integrity constraint on
its own snapshot, if they run concurrently each will believethat the sum of
thebalancesafterthewithdrawalis$100,andthereforeitswithdrawaldoes
not violate the constraint. Since the two transactions update different data
items, they do not have any update con?ict, and under snapshot isolation
both of themcan commit.
Unfortunately, in the ?nal state after both T
36
and T
37
have committed,
the sum of the balances is $-100, violating the integrity constraint. Such a
violation could neverhave occurred in any serialexecutionof T
36
and T
37
.
It is worth noting that integrity constraints that are enforced by the
database,suchasprimary-keyandforeign-keyconstraints,cannotbecheck-
ed on a snapshot; otherwise it would be possible for two concurrent trans-
actions to insert two tuples with the same primary key value, or for a
transaction to insert a foreign key value that is concurrently deleted from
the referenced table. Instead, the database system must check these con-
straintsonthecurrentstateofthedatabase,aspartofvalidationatthetime
of commit.
2. Forthenextexample,weshallconsidertwoconcurrentupdatetransactions
thatdonotthemselvespresentanyproblemasregardsserializabilityunless
15.7 SnapshotIsolation 695
aread-onlytransactionhappenstoshowupatjusttherighttimetocausea
problem.
SupposethatwehavetwoconcurrenttransactionsT
i
and T
j
andtwodata
items Aand B.SupposethatT
i
reads B andthenupdates B,whileT
j
reads A
and B,thenupdatesA.Runningthesetwotransactionsconcurrentlycauses
noproblem.SinceT
i
accessesonlydataitem B,therearenocon?ictsondata
item A and therefore there is no cycle in the precedence graph. The only
edgeintheprecedencegraphistheedgefromT
j
to T
i
because T
j
reads the
value of B that existedbefore T
i
writes B.
However,letussupposethat T
i
commitswhile T
j
isstillactive.Suppose
that, after T
i
commits but before T
j
commits, a new read-only transaction
T
k
enters the system and T
k
reads both Aand B. Its snapshot includes the
update by T
i
because T
i
has already committed. However, since T
j
has not
committed, its update has not yet been written to the database and is not
includedinthesnapshot seenby T
k
.
Consider the edges that are added to the precedence graph on account
of T
k
. There is an edge in the precedence graph from T
i
to T
k
because T
i
writes the value of B that existed before T
k
reads B. There is an edge in the
precedencegraphfrom T
k
to T
j
because T
k
reads the value of Athatexisted
before T
j
writes A. That leads to a cycle in the precedence graph, showing
that the resultingschedule isnonserializable.
The above anomalies may not be as troublesome as they ?rst appear. Recall
that the reason for serializability is to ensure that, despite concurrent execution
of transactions, database consistency is preserved. Since consistency is the goal,
we can accept the potential for nonserializable executions if we are sure that
those nonserializable executions that mightoccurwillnotleadtoinconsistency .
The second example above is a problem only if the application that submits the
read-only transaction (T
k
) cares about seeing updates to Aand B out of order. In
that example, we did not specify the database consistency constraints that each
transaction expects to hold. If we are dealing with a ?nancial database, it might
be a very serious matter for T
k
to read updates out of proper serial order. On the
otherhand,if Aand B areenrollmentsintwosectionsofthesamecourse,then T
k
may not demand perfect serialization and we may know from our applications
that update rates are low enough that any inaccuracy in what T
k
reads is not
signi?cant.
Thefactthatthedatabasemustcheckintegrityconstraintsatthetimeofcom-
mit, and not on a snapshot, also helps avoid inconsistencies in some situations.
Some?nancialapplicationscreateconsecutivesequencenumbers,forexampleto
number bills, by taking the maximum current bill number and adding 1 to the
value to get a new bill number. If two such transactions run concurrently, each
would see the same set of bills in its snapshot, and each would create a new bill
with the same number. Both transactions pass the validation tests for snapshot
isolation,sincetheydonotupdateanytupleincommon.However,theexecution
is not serializable; the resultant database state cannot be obtained by any serial
696 Chapter15 ConcurrencyControl
executionofthetwotransactions.Creatingtwobillswiththesamenumbercould
have seriouslegal implications.
The above problem is an example of the phantom phenomenon, since the
insert performed by each transaction con?icts with the read performed by the
othertransactionto?ndthemaximumbillnumber,butthecon?ictisnotdetected
by snapshot isolation.
1
Luckily,inmostsuchapplicationsthebillnumberwouldhavebeendeclared
asaprimarykey,andthedatabasesystemwoulddetecttheprimarykeyviolation
outside the snapshot, and roll back one of the two transactions.
2
An application developer can guard against certain snapshot anomalies by
appendingaforupdateclause to the SQL selectqueryas illustratedbelow:
select *
from instructor
where ID = 22222
forupdate;
Adding the for update clause causes the system to treat data that are read as if
theyhad beenupdatedforpurposesof concurrency control. Inour?rstexample
of write skew, if the for update clause is appended to the select queries that
read the account balances, only one of the two concurrent transactions would be
allowedto commit since itappearsthat both transactions have updatedboth the
checking and savings balances.
In our second example of nonserializable execution, if the author of transac-
tion T
k
wishedtoavoidthisanomaly,theforupdateclausecouldbeappendedto
theselectquery,eventhoughthereisinfactnoupdate.Inourexample,if T
k
used
selectforupdate, it would be treated as if it had updated Aand B when it read
them.Theresultwouldbethateither T
k
or T
j
wouldbeaborted,andretriedlater
asanewtransaction.Thiswouldleadtoaserializableexecution.Inthisexample,
the queries in the other two transactions do not need theforupdate clause to be
added; unnecessary use of the forupdate clause can cause signi?cant reduction
in concurrency.
Formal methods exist (see the bibliographical notes) to determine whether a
given mix of transactions runs the risk of nonserializable execution under snap-
shot isolation, andtodecideonwhat con?icts tointroduce(using theforupdate
clause, for example), to ensure serializability. Of course, such methods can work
only if we know in advance what transactions are being executed. In some ap-
plications, all transactions are from a predetermined set of transactions making
this analysis possible. However, if the application allows unrestricted, ad-hoc
transactions, then no such analysis ispossible.
1
The SQL standard uses the term phantom problem to refer to non-repeatable predicate reads, leading some to claim
thatsnapshotisolationavoidsthephantomproblem;however,suchaclaimisnotvalidunderourde?nitionofphantom
con?ict.
2
Theproblemofduplicatebillnumbersactuallyoccurredseveraltimesina?nancialapplicationinI.I.T.Bombay,where
(forreasonstoocomplextodiscusshere)thebillnumberwasnotaprimarykey,andwasdetectedby?nancialauditors.
15.8 InsertOperations,DeleteOperations,andPredicateReads 697
Ofthethreewidelyusedsystemsthatsupportsnapshotisolation, SQLServer
offers the option of a serializable isolation level that truly ensures serializability
alongwithasnapshotisolationlevelthatprovidestheperformanceadvantagesof
snapshot isolation (along with the potential for the anomalies discussed above).
In Oracle and PostgreSQL,theserializable isolation level offers only snapshot iso-
lation.
15.8 InsertOperations,DeleteOperations,andPredicateReads
Until now, we have restricted our attention to read and write operations. This
restriction limits transactions to data items already in the database. Some trans-
actionsrequirenotonlyaccesstoexistingdataitems,butalsotheabilitytocreate
new data items. Others require the ability to delete data items. To examine how
suchtransactionsaffectconcurrencycontrol,weintroducetheseadditionaloper-
ations:
• delete(Q)deletesdata item Q fromthe database.
• insert(Q)insertsanewdataitemQintothedatabaseandassigns Qaninitial
value.
An attempt by a transaction T
i
to perform a read(Q) operation after Q has been
deletedresultsin a logical errorin T
i
. Likewise,an attemptby a transaction T
i
to
perform a read(Q)operationbeforeQ has been inserted results in a logical error
in T
i
. It isalsoa logical errorto attempttodeletea nonexistent dataitem.
15.8.1 Deletion
Tounderstandhowthepresenceofdeleteinstructionsaffectsconcurrencycontrol,
we must decide when adelete instruction con?icts with another instruction. Let
I
i
and I
j
be instructions of T
i
and T
j
, respectively, that appear in schedule S in
consecutiveorder.Let I
i
=delete(Q).We considerseveralinstructions I
j
.
• I
j
= read(Q). I
i
and I
j
con?ict. If I
i
comes before I
j
, T
j
will have a logical
error.If I
j
comes before I
i
, T
j
can executethe readoperationsuccessfully.
• I
j
= write(Q). I
i
and I
j
con?ict. If I
i
comes before I
j
, T
j
will have a logical
error.If I
j
comes before I
i
, T
j
can executethe write operationsuccessfully.
• I
j
= delete(Q). I
i
and I
j
con?ict. If I
i
comes before I
j
, T
i
will have a logical
error.If I
j
comes before I
i
, T
i
will have a logical error.
• I
j
=insert(Q). I
i
and I
j
con?ict. Supposethatdataitem Qdidnotexistprior
totheexecutionof I
i
and I
j
.Then,if I
i
comesbefore I
j
,alogicalerrorresults
forT
i
.IfI
j
comesbefore I
i
,thennologicalerrorresults.Likewise,ifQexisted
prior to the execution of I
i
and I
j
, then a logical error results if I
j
comes
before I
i
,butnototherwise.
698 Chapter15 ConcurrencyControl
We can conclude the following:
• Underthetwo-phaselockingprotocol,anexclusivelockisrequiredonadata
itembeforethatitemcan bedeleted.
• Underthetimestamp-orderingprotocol,atestsimilartothatforawritemust
be performed.Supposethat transaction T
i
issuesdelete(Q).
?
If TS(T
i
) < R-timestamp(Q), then the value of Q that T
i
was to delete has
already been read by a transaction T
j
with TS(T
j
) > TS(T
i
). Hence, the
delete operationis rejected,and T
i
is rolledback.
?
IfTS(T
i
)<W-timestamp(Q),thenatransactionT
j
withTS(T
j
)>TS(T
i
)has
written Q.Hence,thisdelete operationisrejected,and T
i
isrolledback.
?
Otherwise, thedelete isexecuted.
15.8.2 Insertion
Wehavealreadyseenthataninsert(Q)operationcon?ictswithadelete(Q)opera-
tion.Similarly,insert(Q)con?ictswitharead(Q)operationorawrite(Q)operation;
no read or write can be performedon a data itembefore itexists.
Sinceaninsert(Q)assignsavaluetodataitemQ,aninsertistreatedsimilarly
to a write for concurrency-control purposes:
• Underthetwo-phaselockingprotocol,if T
i
performsaninsert(Q)operation,
T
i
isgivenan exclusivelock on the newlycreateddataitem Q.
• Underthetimestamp-orderingprotocol,ifT
i
performsaninsert(Q)operation,
the valuesR-timestamp(Q)andW-timestamp( Q)aresettoTS(T
i
).
15.8.3 PredicateReads andThePhantom Phenomenon
Considertransaction T
30
that executesthe following SQL queryon the university
database:
selectcount(*)
from instructor
where dept name= ’Physics’ ;
Transaction T
30
requiresaccesstoalltuplesofthe instructorrelationpertainingto
the Physics department.
Let T
31
be a transaction thatexecutesthe following SQL insertion:
insertinto instructor
values (11111,’Feynman’, ’Physics’, 94000);
Let S be a schedule involving T
30
and T
31
. We expect there to be potential for
a con?ict for the following reasons:
15.8 InsertOperations,DeleteOperations,andPredicateReads 699
• If T
30
uses the tuple newly inserted by T
31
in computing count(*), then T
30
reads a value written by T
31
. Thus, in a serial schedule equivalent to S, T
31
must come before T
30
.
• If T
30
does not use the tuple newly inserted by T
31
in computing count(*),
thenina serialscheduleequivalentto S, T
30
must come before T
31
.
The second of these two cases is curious. T
30
and T
31
do not access any tuple
in common, yet they con?ict with each other! In effect, T
30
and T
31
con?ict on
a phantom tuple. If concurrency control is performed at the tuple granularity,
this con?ict would go undetected. As a result, the system could fail to prevent a
nonserializable schedule. Thisproblemis calledthephantomphenomenon.
In addition to the phantom problem, we also need to deal with the situation
we saw in Section 14.10, where a transaction used an index to ?nd only tuples
with dept name = “Physics”, and as a result did not read any tuples with other
department names. If another transaction updates one of these tuples, changing
its department name to Physics, a problem equivalent to the phantom problem
occurs.Bothproblemsarerootedinpredicatereads,andhaveacommonsolution.
To prevent the above problems, we allow transaction T
30
to prevent other
transactions from creating new tuples in the instructor relation with dept name =
“Physics”,andfromupdatingthedepartmentnameofanexistinginstructortuple
to Physics.
To?ndall instructortupleswith dept name= “Physics”, T
30
mustsearcheither
the whole instructor relation, or at least an index on the relation. Up to now, we
have assumed implicitly that the only data items accessed by a transaction are
tuples.However, T
30
is an example of a transaction that reads information about
what tuples are in a relation, and T
31
is an example of a transaction that updates
that information.
Clearly, it is not suf?cient merely to lock the tuples that are accessed; the
informationusedto?ndthetuplesthatareaccessedbythetransactionmustalso
be locked.
Lockingofinformationusedto?ndtuplescanbeimplementedbyassociating
a data item with the relation; the data item represents the information used to
?ndthetuplesintherelation.Transactions,suchas T
30
,thatreadtheinformation
about what tuples are in a relation would then have to lock the data item corre-
spondingtotherelationinsharedmode.Transactions,suchasT
31
,thatupdatethe
information about what tuplesare in arelation would have to lock the data item
in exclusive mode. Thus, T
30
and T
31
would con?ict on a real data item, rather
than on a phantom. Similarly, transactions that use an index to retrieve tuples
mustlock the indexitself.
Do not confuse the locking of an entire relation, as in multiple-granularity
locking,withthelockingofthedataitemcorrespondingtotherelation.Bylocking
thedataitem,atransactiononlypreventsothertransactionsfromupdatinginfor-
mationaboutwhattuplesareintherelation.Lockingisstillrequiredontuples.A
transactionthatdirectlyaccessesatuplecanbegrantedalockonthetupleseven
700 Chapter15 ConcurrencyControl
when another transaction has an exclusive lock on the data item corresponding
to the relationitself.
Themajordisadvantageoflockingadataitemcorrespondingtotherelation,
orlockinganentireindex,isthelowdegreeofconcurrency—twotransactionsthat
insertdifferenttuplesinto a relationarepreventedfrom executingconcurrently.
Abettersolutionisanindex-lockingtechniquethatavoidslockingthewhole
index.Anytransactionthatinsertsatupleintoarelationmustinsertinformation
into every index maintained on the relation. We eliminate the phantom phe-
nomenon by imposing a locking protocol for indices. For simplicity we shall
consideronly B
+
-tree indices.
Aswe saw in Chapter11, everysearch-keyvalueisassociated with an index
leaf node. A query will usually use one or more indices to access a relation. An
insertmustinsertthenewtupleinallindicesontherelation.Inourexample,we
assume that there is an index on instructor for dept name. Then, T
31
must modify
the leaf containing the key “Physics”.IfT
30
reads the same leaf node to locate all
tuplespertainingtothePhysicsdepartment,then T
30
and T
31
con?ict onthat leaf
node.
Theindex-lockingprotocoltakesadvantageoftheavailabilityofindicesona
relation,byturninginstancesofthephantomphenomenonintocon?ictsonlocks
on indexleafnodes. The protocol operatesasfollows:
• Everyrelationmusthave atleastone index.
• AtransactionT
i
can access tuples of a relation only after ?rst ?nding them
through one or more of the indices on the relation. For the purpose of the
index-locking protocol, a relation scan is treated as a scan through all the
leavesof one of the indices.
• AtransactionT
i
that performs a lookup (whether a range lookup or a point
lookup)mustacquireasharedlockonalltheindexleafnodesthatitaccesses.
• AtransactionT
i
may not insert, delete, or update a tuple t
i
in a relation
r without updating all indices on r. The transaction must obtain exclusive
locks on all index leaf nodes that are affected by the insertion, deletion, or
update. For insertion and deletion, the leaf nodes affected are those that
contain (after insertion) or contained (before deletion) the search-key value
of the tuple. For updates, the leaf nodes affected are those that (before the
modi?cation)containedtheoldvalueofthesearchkey,andnodesthat(after
the modi?cation) contain the new value ofthe search key.
• Locks are obtained on tuplesasusual.
• The rulesof the two-phase locking protocol mustbe observed.
Notethattheindex-lockingprotocoldoesnotaddressconcurrencycontrolon
internal nodes of an index; techniques for concurrency control on indices, which
minimize lock con?icts, are presentedin Section15.10.
Locking an index leaf node prevents any update to the node, even if the
update did not actually con?ict with the predicate. A variant called key-value
15.9 WeakLevelsofConsistencyinPractice 701
locking, which minimizes such false lock con?icts, is presented in Section 15.10
as part of indexconcurrency control.
As noted in Section 14.10, it would appear that the existence of a con?ict
between transactions depends on a low-level query-processing decision by the
system that is unrelated to a user-level view of the meaning of the two transac-
tions. An alternative approach to concurrency control acquires shared locks on
predicates in a query, such as the predicate “salary > 90000” on the instructor
relation. Inserts and deletes of the relation must then be checked to see if they
satisfythepredicate;iftheydo,thereisalockcon?ict,forcingtheinsertordelete
to wait till the predicate lock is released. For updates, both the initial value and
the ?nal value of the tuple must be checked against the predicate. Such con?ict-
ing inserts, deletes and updates affect the set of tuples selected by the predicate,
and cannot be allowed to execute concurrently with the query that acquired the
(shared) predicate lock. We call the above protocolpredicatelocking;
3
predicate
locking is not used in practice since it is more expensive to implement than the
index-lockingprotocol, and doesnot givesigni?cant additional bene?ts.
Variants of the predicate-locking technique can be used for eliminating the
phantom phenomenon under the other concurrency-control protocols presented
in this chapter. However, many database systems, such as PostgreSQL (as of ver-
sion 8.1) and (to the best of our knowledge) Oracle (as of version 10g) do not
implementindexlockingorpredicatelocking, andarevulnerabletononserializ-
abilitydue to phantom problemsevenif the isolation levelis settoserializable.
15.9 WeakLevelsofConsistencyinPractice
In Section 14.5, we discussed the isolation levels speci?ed by the SQL standard:
serializable, repeatable read, read committed, and read uncommitted. In this
section, we ?rst brie?y outline some older terminology relating to consistency
levels weaker than serializability and relate it to the SQL standard levels. We
then discuss the issue of concurrency control for transactions that involve user
interaction, an issuethat we brie?ydiscussedearlierin Section14.8.
15.9.1 Degree-TwoConsistency
Thepurposeofdegree-twoconsistencyistoavoidcascadingabortswithoutnec-
essarilyensuring serializability.The locking protocol for degree-twoconsistency
uses the same two lock modes that we used for the two-phase locking protocol:
shared (S)and exclusive(X). A transaction must hold the appropriatelock mode
when it accessesa dataitem,but two-phase behavior isnot required.
In contrast to the situation in two-phase locking, S-locks may be released
at any time, and locks may be acquired at any time. Exclusive locks, however,
3
The term predicate locking was used for a version of the protocol that used shared and exclusive locks on predicates,
andwasthusmorecomplicated.Theversionwepresenthere,withonlysharedlocksonpredicates,isalsoreferredtoas
precisionlocking.
702 Chapter15 ConcurrencyControl
T
32
T
33
lock-S(Q)
read(Q)
unlock(Q)
lock-X(Q)
read(Q)
write(Q)
unlock(Q)
lock-S(Q)
read(Q)
unlock(Q)
Figure15.20 Nonserializable schedule with degree-two consistency.
cannotbereleaseduntilthetransactioneithercommitsoraborts.Serializabilityis
not ensured by this protocol. Indeed, a transaction may read the same data item
twiceand obtaindifferentresults.InFigure15.20, T
32
reads the value of Qbefore
and afterthat valueis writtenby T
33
.
Clearly, reads are not repeatable, but since exclusive locks are held until
transactioncommit,notransactioncanreadanuncommittedvalue.Thus,degree-
twoconsistencyisoneparticularimplementationoftheread-committedisolation
level.
15.9.2 CursorStability
Cursorstability is a form of degree-twoconsistency designed for programs that
iterate over tuples of a relation by using cursors. Instead of locking the entire
relation, cursor stabilityensuresthat:
• Thetuplethatiscurrentlybeingprocessedbytheiterationislockedinshared
mode.
• Anymodi?edtuplesarelockedinexclusivemodeuntilthetransactioncom-
mits.
These rules ensure that degree-twoconsistency is obtained. Two-phase lock-
ing is not required. Serializability is not guaranteed. Cursor stability is used in
practice on heavily accessed relations as a means of increasing concurrency and
improving system performance. Applications that use cursor stability must be
coded in a way that ensures database consistency despite the possibility of non-
serializable schedules. Thus, the use of cursor stability is limited to specialized
situations with simpleconsistency constraints.
15.9.3 Concurrency Control AcrossUserInteractions
Concurrency-control protocols usually consider transactions that do not involve
user interaction. Consider the airline seat selection example from Section 14.8,
15.9 WeakLevelsofConsistencyinPractice 703
which involved user interaction. Suppose we treat all the steps from when the
seat availability is initially shown to the user, till the seat selection is con?rmed,
as asingle transaction.
If two-phase locking is used, the entire set of seats on a ?ight would be
locked in shared mode till the user has completed the seat selection, and no
other transaction would be able to update the seat allocation information in this
period. Clearly such locking would be a very bad idea since a user may take
a long time to make a selection, or even just abandon the transaction without
explicitlycancellingit.Timestampprotocolsorvalidationcouldbeusedinstead,
which avoid the problem of locking, but both these protocols would abort the
transaction for a user A if any other user B has updated the seat allocation
information,eveniftheseatselectedby B doesnotcon?ictwiththeseatselected
byuser A.Snapshotisolationisagoodoptioninthissituation,sinceitwouldnot
abort the transaction of user Aas long as B didnot selectthe same seatas A.
However, snapshot isolation requires the database to remember information
aboutupdatesperformedbyatransactionevenafterithascommitted,aslongas
anyotherconcurrenttransactionisstillactive,whichcanbeproblematicforlong
duration transactions.
Anotheroptionistosplitatransactionthatinvolvesuserinteractionintotwo
or more transactions, such that no transaction spans a user interaction. If our
seat selection transaction is split thus, the ?rst transaction would read the seat
availability, while the second transaction would complete the allocation of the
selected seat. If the second transaction is written carelessly, it could assign the
selectedseattothe user,without checking ifthe seat was meanwhile assigned to
someotheruser,resultinginalost-updateproblem.Toavoidtheproblem,aswe
outlinedinSection14.8,thesecondtransactionshouldperformtheseatallocation
only ifthe seatwas not meanwhile assigned to some otheruser.
The above idea has been generalized in an alternative concurrency control
scheme, which uses version numbers stored in tuples to avoid lost updates. The
schema of each relation is altered by adding an extra version number attribute,
which is initialized to 0 when the tuple is created. When a transaction reads (for
the ?rst time)a tuple that it intends to update,it remembersthe versionnumber
ofthattuple.Thereadisperformedasastand-alonetransactiononthedatabase,
andhenceanylocksthatmaybeobtainedarereleasedimmediately.Updatesare
done locally, and copied to the database as part of commit processing, using the
followingstepswhichareexecutedatomically(thatis,aspartofasingledatabase
transaction):
• Foreachupdatedtuple,thetransactionchecksifthecurrentversionnumber
is the same as the version number of the tuple when it was ?rst read by the
transaction.
1. If the version numbers match, the update is performed on the tuple in
the database,and itsversionnumber isincrementedby 1.
2. Iftheversionnumbersdonot match, thetransaction isaborted,rolling
back allthe updatesitperformed.
704 Chapter15 ConcurrencyControl
If the version number check succeeds for all updated tuples, the transaction
commits.Itisworthnotingthatatimestampcouldbeusedinsteadoftheversion
number, without impacting the scheme in any way.
Observe the close similarity between the above scheme and snapshot isola-
tion. The version number check implements the ?rst-committer-wins rule used
in snapshot isolation, and can be used even if the transaction was active for
a very long time. However, unlike snapshot isolation, the reads performed by
transaction may not correspond to a snapshot of the database; and unlike the
validation-basedprotocol, readsperformedby the transaction are not validated.
Werefertotheaboveschemeasoptimisticconcurrencycontrolwithoutread
validation. Optimistic concurrency control without read validation provides a
weak level of serializability, and does not ensure serializability. A variant of this
schemeusesversionnumberstovalidatereadsatthetimeofcommit,inaddition
to validating writes, to ensure that the tuples read by the transaction were not
updatedsubsequenttotheinitialread;thisschemeisequivalenttotheoptimistic
concurrency-control scheme which we saw earlier.
Theaboveschemehasbeenwidelyusedbyapplicationdeveloperstohandle
transactions that involve user interaction. An attractive feature of the scheme is
thatitcanbeimplementedeasilyontopofadatabasesystem.Thevalidationand
updatestepsperformedaspartofcommitprocessingarethenexecutedasasingle
transactioninthedatabase,usingtheconcurrency-controlschemeofthedatabase
to ensure atomicity for commit processing. The above scheme is also used by
the Hibernate object-relational mapping system (Section 9.4.2), and other object-
relational mapping systems, where it is referred to as optimistic concurrency
control(eventhoughreadsarenotvalidatedbydefault).Transactionsthatinvolve
user interaction are calledconversations in Hibernate to differentiatethem from
regular transactions validation using version numbers is very useful for such
transactions.Object-relationalmappingsystemsalsocachedatabasetuplesinthe
formofobjectsinmemory,andexecutetransactionsonthecachedobjects;updates
on the objects are converted into updates on the database when the transaction
commits.Datamayremainincacheforalongtime,andiftransactionsupdatesuch
cached data, there is a risk of lost updates. Hibernate and other object-relational
mapping systems therefore perform the version number checks transparently as
part of commit processing. (Hibernate allows programmers to bypass the cache
and executetransactions directlyon the database, ifserializabilityisdesired.)
15.10 ConcurrencyinIndexStructures**
It is possible to treat access to index structures like any other database struc-
ture,andtoapplytheconcurrency-controltechniquesdiscussedearlier.However,
since indices are accessed frequently, they would become a point of great lock
contention, leading to a low degree of concurrency. Luckily, indices do not have
to be treated like other database structures. It is perfectly acceptable for a trans-
actiontoperformalookuponanindextwice,andto?ndthatthestructureofthe
indexhaschangedinbetween,aslongastheindexlookupreturnsthecorrectset
15.10 ConcurrencyinIndexStructures** 705
of tuples. Thus, it is acceptable to have nonserializable concurrent access to an
index,as long as the accuracy of the indexismaintained.
We outline two techniques for managing concurrent access to B
+
-trees. The
bibliographicalnotesreferenceothertechniquesforB
+
-trees,aswellastechniques
for otherindexstructures.
ThetechniquesthatwepresentforconcurrencycontrolonB
+
-treesarebased
onlocking, but neithertwo-phase lockingnorthe treeprotocol isemployed.The
algorithmsforlookup,insertion,and deletionarethose usedinChapter11,with
only minor modi?cations.
The ?rst technique is calledthecrabbingprotocol:
• When searching for a key value, the crabbing protocol ?rst locks the root
node in shared mode. When traversing down the tree, it acquires a shared
lockonthechildnodetobetraversedfurther.Afteracquiringthelockonthe
childnode,itreleasesthelockontheparentnode.Itrepeatsthisprocessuntil
itreachesa leafnode.
• When inserting or deleting a key value, the crabbing protocol takes these
actions:
?
It follows the same protocol as for searching until it reaches the desired
leafnode. Up to this point, it obtains (and releases)only shared locks.
?
It locks the leaf node in exclusive mode and inserts or deletes the key
value.
?
If it needs to split a node or coalesce it with its siblings, or redistribute
keyvaluesbetweensiblings,thecrabbingprotocollockstheparentofthe
node in exclusive mode. After performing these actions, it releases the
locks on the node and siblings.
If the parent requires splitting, coalescing, or redistribution of key
values,theprotocolretainsthelockontheparent,andsplitting,coalescing,
or redistribution propagates further in the same manner. Otherwise, it
releases the lock on the parent.
The protocol gets its name from the way in which crabs advance by moving
sideways, moving the legs on one side, then the legs on the other, and so on
alternately. The progress of locking while the protocol both goes down the tree
and goes back up (in case of splits, coalescing, or redistribution) proceeds in a
similarcrab-like manner.
Once a particular operation releases a lock on a node, other operations can
access that node. There is a possibility of deadlocks between search operations
coming down the tree, and splits, coalescing, or redistribution propagating up
the tree. The system can easily handle such deadlocks by restarting the search
operationfrom the root, afterreleasingthe locks held by the operation.
Thesecondtechniqueachievesevenmoreconcurrency,avoidingevenholding
the lock on one node while acquiring the lock on another node, by using a
modi?ed version of B
+
-trees called B-link trees; B-link trees require that every
706 Chapter15 ConcurrencyControl
node(includinginternalnodes,notjusttheleaves)maintainapointertoitsright
sibling.Thispointerisrequiredbecausealookupthatoccurswhileanodeisbeing
split may have to search not only that node but also that node’s right sibling (if
one exists). We shall illustrate this technique with an example later, but we ?rst
present the modi?edproceduresof theB-link-treelockingprotocol.
• Lookup.EachnodeoftheB
+
-treemustbelockedinsharedmodebeforeitis
accessed. A lock on a nonleaf node is released before any lock on any other
nodeintheB
+
-treeisrequested.Ifasplitoccursconcurrentlywithalookup,
thedesiredsearch-keyvaluemaynolongerappearwithintherangeofvalues
representedbyanodeaccessedduringlookup.Insuchacase,thesearch-key
valueisintherangerepresentedbyasiblingnode,whichthesystemlocates
by following the pointer to the right sibling. However, the system locks leaf
nodes following the two-phase locking protocol, as Section 15.8.3 describes,
to avoidthe phantom phenomenon.
• Insertionanddeletion.Thesystemfollowstherulesforlookuptolocatethe
leaf node into which it will make the insertion or deletion. It upgrades the
shared-modelockonthisnodetoexclusivemode,andperformstheinsertion
ordeletion.Itlocksleafnodesaffectedbyinsertionordeletionfollowingthe
two-phaselockingprotocol,asSection15.8.3describes,toavoidthephantom
phenomenon.
• Split. If the transaction splits a node, it creates a new node according to the
algorithm of Section 11.3 and makes it the right sibling of the original node.
The right-sibling pointers of both the original node and the new node are
set.Followingthis,the transaction releasesthe exclusivelockon theoriginal
node (provided it is an internal node; leaf nodes are locked in two-phase
manner), and then requests an exclusive lock on the parent, so that it can
insertapointertothenewnode.(Thereisnoneedtolockorunlock thenew
node.)
• Coalescence. If a node has too few search-key values after a deletion, the
nodewithwhichitwillbecoalescedmustbelockedinexclusivemode.Once
the transaction has coalesced these two nodes, it requests an exclusive lock
on the parent so that the deleted node can be removed. At this point, the
transactionreleasesthelocksonthecoalescednodes.Unlesstheparentnode
mustbe coalesced also, itslock isreleased.
Observe this important fact: An insertion or deletionmay lock a node, unlock it,
and subsequently relock it. Furthermore, a lookup that runs concurrently with
a split or coalescence operation may ?nd that the desired search key has been
movedto the right-sibling node by the splitor coalescence operation.
As an illustration, consider the B-linktree in Figure 15.21. Assume that there
are two concurrent operationson this B-linktree:
1. Insert “Chemistry”.
2. Look up “Comp. Sci.”
15.10 ConcurrencyinIndexStructures** 707
History
Elec.Eng.
Biology Comp.Sci. Elec.Eng. Finance History
Music
Music Physics
Figure15.21 B-link tree for department ?le with n = 3.
Let us assume that the insertion operation begins ?rst. It does a lookup on
“Chemistry”,and?ndsthatthenodeintowhich“Chemistry”shouldbeinsertedis
full.Itthereforeconvertsitssharedlockonthenodetoexclusivemode,andcreates
anewnode.Theoriginalnodenowcontainsthesearch-keyvalues“Biology”and
“Chemistry”. The new node contains the search-key value “Comp. Sci.”
Now assume that a context switch occurs that results in control passing to
the lookup operation. This lookup operation accesses the root, and follows the
pointertotheleftchildoftheroot.Itthenaccessesthatnode,andobtainsapointer
to the left child. This left-child node originally contained the search-key values
“Biology” and “Comp. Sci.”. Since this node is currently locked by the insertion
operation in exclusive mode, the lookup operation must wait. Note that, at this
point, the lookup operation holdsno locks at all!
Theinsertionoperationnowunlockstheleafnodeandrelocksitsparent,this
time in exclusive mode. It completes the insertion, leaving the B-link tree as in
Figure 15.22. The lookup operation proceeds. However, it is holding a pointer
to an incorrect leaf node. It therefore follows the right-sibling pointer to locate
the next node. If this node, too, turns out to be incorrect, the lookup follows that
node’s right-sibling pointer. It can be shown that, if a lookup holds a pointer
to an incorrect node, then, by following right-sibling pointers, the lookup must
eventuallyreachthecorrect node.
Lookupandinsertionoperationscannotleadtodeadlock.Coalescingofnodes
duringdeletioncancauseinconsistencies,sincealookupmayhavereadapointer
to a deleted node from its parent, before the parent node was updated, and may
History
Elec.Eng.
Biology . c e l E y r t s i m e h C Eng. Finance Comp.Sci.
Music
Music Physics
Comp.Sci.
History
Figure15.22 Insertion of “Chemistry” into the B-link tree of Figure 15.21.
708 Chapter15 ConcurrencyControl
then try to access the deleted node. The lookup would then have to restart from
the root. Leaving nodes uncoalesced avoids such inconsistencies. This solution
results in nodes that contain too few search-key values and that violate some
properties of B
+
-trees. In most databases, however, insertions are more frequent
than deletions, so it is likely that nodes that have too few search-key values will
gain additionalvaluesrelativelyquickly.
Instead of locking index leaf nodes in a two-phase manner, some index
concurrency-control schemes use key-value locking on individual key values,
allowingotherkeyvaluestobeinsertedordeletedfromthesameleaf.Key-value
locking thus provides increased concurrency. Using key-value locking na¨ ?vely,
however, would allow the phantom phenomenon to occur; to prevent the phan-
tom phenomenon, the next-key locking technique is used. In this technique,
every index lookup must lock not only the keys found within the range (or the
single key, in case of a point lookup) but also the next-key value—that is, the
key value just greater than the last key value that was within the range. Also,
every insert must lock not only the value that is inserted, but also the next-key
value. Thus, if a transaction attempts to insert a value that was within the range
oftheindexlookupofanothertransaction,thetwotransactionswouldcon?icton
the keyvalue nextto the insertedkeyvalue.Similarly,deletesmustalso lock the
next-keyvaluetothevaluebeingdeleted,toensurethatcon?ictswithsubsequent
range lookupsof otherqueriesaredetected.
15.11 Summary
• When several transactions execute concurrently in the database, the consis-
tency of data may no longer be preserved. It is necessary for the system to
controltheinteractionamongtheconcurrenttransactions,andthiscontrolis
achieved through one of a variety of mechanisms called concurrency-control
schemes.
• Toensureserializability,wecanusevariousconcurrency-controlschemes.All
these schemes either delay an operation or abort the transaction that issued
the operation. The most common ones are locking protocols, timestamp-
orderingschemes,validationtechniques, and multiversionschemes.
• Alockingprotocolisasetofrulesthatstatewhenatransactionmaylockand
unlock each ofthe dataitemsinthe database.
• The two-phase locking protocol allows a transaction to lock anew dataitem
only if that transaction has not yet unlocked any data item. The protocol
ensures serializability, but not deadlock freedom. In the absence of informa-
tion concerning the manner in which data items are accessed,the two-phase
locking protocol isboth necessary and suf?cient for ensuring serializability.
• The strict two-phase locking protocol permitsreleaseofexclusivelocks only
attheendoftransaction,inordertoensurerecoverabilityandcascadelessness
15.11 Summary 709
of the resulting schedules. The rigorous two-phase locking protocol releases
all locks only at the end of the transaction.
• Graph-based locking protocols impose restrictions on the order in which
items are accessed, and can thereby ensure serializability without requiring
theuseoftwo-phaselocking,andcanadditionallyensuredeadlockfreedom.
• Various locking protocols do not guard against deadlocks. One way to pre-
vent deadlock is to use an ordering of data items, and to request locks in a
sequenceconsistent with the ordering.
• Another way to prevent deadlock is to use preemption and transaction roll-
backs. To control the preemption, we assign a unique timestamp to each
transaction. The system uses these timestamps to decide whether a transac-
tion should wait or roll back. If a transaction is rolled back, it retains its old
timestampwhenrestarted.Thewound–waitschemeisapreemptivescheme.
• If deadlocks are not prevented, the system must deal with them by using
a deadlock detection and recovery scheme. To do so, the system constructs
a wait-for graph. A system is in a deadlock state if and only if the wait-for
graph contains a cycle. When the deadlock detection algorithm determines
thatadeadlockexists,thesystemmustrecoverfromthedeadlock.Itdoesso
by rolling back one or more transactions to break the deadlock.
• There are circumstances where it would be advantageous to group several
data items, and to treat them as one aggregate data item for purposes of
working, resulting in multiple levels of granularity. We allow data items of
varioussizes,andde?neahierarchyofdataitems,wherethesmallitemsare
nestedwithinlargerones.Suchahierarchycanberepresentedgraphicallyas
atree.Locksareacquiredinroot-to-leaforder;theyarereleasedinleaf-to-root
order.The protocol ensuresserializability,but not freedomfrom deadlock.
• Atimestamp-orderingschemeensuresserializabilitybyselectinganordering
in advance between every pair of transactions. A unique ?xed timestamp
is associated with each transaction in the system. The timestamps of the
transactions determine the serializability order. Thus, if the timestamp of
transactionT
i
issmallerthanthetimestampoftransactionT
j
,thenthescheme
ensuresthattheproducedscheduleisequivalenttoaserialscheduleinwhich
transaction T
i
appears before transaction T
j
. It does so by rolling back a
transaction wheneversuch an orderis violated.
• A validation scheme is an appropriate concurrency-control method in cases
whereamajorityoftransactionsareread-onlytransactions,andthustherate
of con?icts among these transactions is low. A unique ?xed timestamp is
associated with each transaction in the system. The serializability order is
determinedbythetimestampofthetransaction.Atransactioninthisscheme
is never delayed. It must, however, pass a validation test to complete. If it
doesnot passthe validationtest,the systemrollsitback to itsinitial state.
710 Chapter15 ConcurrencyControl
• Amultiversionconcurrency-controlschemeisbasedonthecreationofanew
versionofadataitemforeachtransactionthatwritesthatitem.Whenaread
operation is issued, the system selects one of the versions to be read. The
concurrency-control schemeensuresthattheversiontobereadisselectedin
amanner thatensuresserializability,byusingtimestamps.Areadoperation
always succeeds.
?
In multiversion timestamp ordering, a write operation may result in the
rollback of the transaction.
?
In multiversion two-phase locking, write operations may result in a lock
wait or, possibly, in deadlock.
• Snapshot isolation is a multiversion concurrency-control protocol based on
validation, which, unlike multiversion two-phase locking, does not require
transactions to be declared as read-only or update. Snapshot isolation does
notguaranteeserializability,butisneverthelesssupportedbymanydatabase
systems.
• A delete operation may be performed only if the transaction deleting the
tuple has an exclusive lock on the tuple to be deleted. A transaction that
insertsa new tupleinto the database isgivenan exclusivelock on the tuple.
• Insertions can lead to the phantom phenomenon, in which an insertion log-
ically con?icts with a query even though the two transactions may access
no tuple in common. Such con?ict cannot be detected if locking is done
only on tuples accessed by the transactions. Locking is required on the data
used to ?nd the tuples in the relation. The index-locking technique solves
this problem by requiring locks on certain index nodes. These locks ensure
that all con?icting transactions con?ict on a real data item, rather than on a
phantom.
• Weaklevelsofconsistencyareusedinsomeapplicationswhereconsistencyof
query results is not critical, and using serializability would result in queries
adversely affecting transaction processing. Degree-two consistency is one
such weaker level of consistency; cursor stability is a special case of degree-
two consistency, and is widelyused.
• Concurrency control is a challenging task for transactions that span user
interactions. Applications often implement a scheme based on validation
of writes using version numbers stored in tuples; this scheme provides a
weaklevelofserializability,andcanbeimplementedattheapplicationlevel
without modi?cations to the database.
• Special concurrency-control techniques can be developed for special data
structures. Often, special techniques are applied in B
+
-trees to allow greater
concurrency. These techniques allow nonserializable access to the B
+
-tree,
but they ensurethat the B
+
-treestructureiscorrect,and ensurethat accesses
to the database itselfare serializable.
ReviewTerms 711
ReviewTerms
• Concurrency control
• Locktypes
?
Shared-mode(S)lock
?
Exclusive-mode(X) lock
• Lock
?
Compatibility
?
Request
?
Wait
?
Grant
• Deadlock
• Starvation
• Locking protocol
• Legalschedule
• Two-phase locking protocol
?
Growing phase
?
Shrinking phase
?
Lock point
?
Strict two-phase locking
?
Rigorous two-phase locking
• Lock conversion
?
Upgrade
?
Downgrade
• Graph-based protocols
?
Tree protocol
?
Commitdependency
• Deadlockhandling
?
Prevention
?
Detection
?
Recovery
• Deadlockprevention
?
Orderedlocking
?
Preemptionof locks
?
Wait–diescheme
?
Wound–wait scheme
?
Timeout-basedschemes
• Deadlockdetection
?
Wait-forgraph
• Deadlockrecovery
?
Total rollback
?
Partial rollback
• Multiplegranularity
?
Explicitlocks
?
Implicitlocks
?
Intention locks
• Intention lock modes
?
Intention-shared (IS)
?
Intention-exclusive(IX)
?
Sharedand intention-
exclusive(SIX)
• Multiple-granularitylocking
protocol
• Timestamp
?
Systemclock
?
Logical counter
?
W-timestamp(Q)
?
R-timestamp(Q)
• Timestamp-orderingprotocol
?
Thomas’ write rule
• Validation-based protocols
?
Read phase
712 Chapter15 ConcurrencyControl
?
Validationphase
?
Write phase
?
Validationtest
• Multiversiontimestampordering
• Multiversiontwo-phase locking
?
Read-only transactions
?
Updatetransactions
• Snapshot isolation
?
Lostupdate
?
Firstcommitterwins
?
Firstupdaterwins
?
Writeskew
?
Selectfor update
• Insertand deleteoperations
• Phantom phenomenon
• Index-locking protocol
• Predicate locking
• Weak levelsof consistency
?
Degree-twoconsistency
?
Cursorstability
• Optimistic concurrency control
without readvalidation
• Conversations
• Concurrency in indices
?
Crabbing
?
B-linktrees
?
B-link-treelocking protocol
?
Next-keylocking
PracticeExercises
15.1 Show that the two-phase locking protocol ensures con?ict serializability,
and that transactions can be serializedaccording to their lock points.
15.2 Considerthe following two transactions:
T
34
: read(A);
read(B);
if A = 0then B := B+1;
write(B).
T
35
: read(B);
read(A);
if B = 0then A:= A+1;
write(A).
Add lock and unlock instructions to transactions T
31
and T
32
,sothat
they observe the two-phase locking protocol. Can the execution of these
transactions resultin a deadlock?
15.3 Whatbene?tdoesrigoroustwo-phaselockingprovide?Howdoesitcom-
pare with other forms of two-phase locking?
PracticeExercises 713
15.4 Consider a database organized in the form of a rooted tree. Suppose that
weinsertadummyvertexbetweeneachpairofvertices.Showthat,ifwe
follow the tree protocol on the new tree, we get better concurrency than
ifwe follow the treeprotocol on the original tree.
15.5 Showbyexamplethatthereareschedulespossibleunderthetreeprotocol
thatarenotpossibleunderthetwo-phaselockingprotocol,andviceversa.
15.6 Consider the following extension to the tree-locking protocol, which al-
lows both shared and exclusivelocks:
• A transaction can be either a read-only transaction, in which case it
canrequestonlysharedlocks,oranupdatetransaction,inwhichcase
itcan requestonly exclusivelocks.
• Eachtransactionmustfollowtherulesofthetreeprotocol.Read-only
transactionsmaylockanydataitem?rst,whereasupdatetransactions
must lock the root ?rst.
Show that the protocol ensuresserializabilityand deadlockfreedom.
15.7 Consider the following graph-based locking protocol, which allows only
exclusive lock modes, and which operates on data graphs that are in the
form ofa rooted directedacyclic graph.
• A transaction can lock any vertex?rst.
• To lock any other vertex, the transaction must be holding a lock on
the majorityof the parentsof that vertex.
Show that the protocol ensuresserializabilityand deadlockfreedom.
15.8 Consider the following graph-based locking protocol, which allows only
exclusive lock modes and which operates on data graphs that are in the
form ofa rooted directedacyclic graph.
• A transaction can lock any vertex?rst.
• To lock any other vertex, the transaction must have visited all the
parents of that vertex and must be holding a lock on one of the
parents of the vertex.
Show that the protocol ensuresserializabilityand deadlockfreedom.
15.9 Locking is not done explicitly in persistent programming languages.
Rather, objects (or the corresponding pages) must be locked when the
objects are accessed. Most modern operating systems allow the user to
set access protections (no access, read, write) on pages, and memory ac-
cessthatviolatetheaccessprotectionsresultinaprotectionviolation(see
the Unix mprotect command, for example). Describe how the access-
protection mechanism can be used for page-level locking in a persistent
programminglanguage.
714 Chapter15 ConcurrencyControl
S X I
S true false false
X false false false
I false false true
Figure15.23 Lock-compatibility matrix.
15.10 Consideradatabasesystemthatincludesanatomicincrementoperation,
in addition to the read and write operations. Let V be the value of data
item X.Theoperation
increment(X)byC
setsthevalueofXtoV+Cinanatomicstep.ThevalueofXisnotavailable
tothetransactionunlessthelatterexecutesa read(X).Figure15.23shows
a lock-compatibility matrix for three lock modes: share mode, exclusive
mode,and incrementation mode.
a. Show that, if all transactions lock the data that they access in the
correspondingmode,thentwo-phaselockingensuresserializability.
b. Show that the inclusion of increment mode locks allows for in-
creased concurrency. (Hint: Consider check-clearing transactions in
our bank example.)
15.11 Intimestampordering,W-timestamp(Q)denotesthelargesttimestampof
anytransactionthatexecutedwrite(Q)successfully.Supposethat,instead,
wede?nedittobethetimestampofthemostrecenttransactiontoexecute
write(Q)successfully.Wouldthischangeinwordingmakeanydifference?
Explainyour answer.
15.12 Useofmultiple-granularitylockingmayrequiremoreorfewerlocksthan
an equivalent system with a single lock granularity. Provide examples of
bothsituations,andcomparetherelativeamountofconcurrencyallowed.
15.13 Considerthevalidation-basedconcurrency-controlschemeofSection15.5.
Show that by choosing Validation(T
i
), rather than Start(T
i
), as the time-
stampoftransaction T
i
,wecanexpectbetterresponsetime,providedthat
con?ict ratesamong transactions are indeedlow.
15.14 For each of the following protocols, describe aspects of practical applica-
tions that would lead you to suggest using the protocol, and aspects that
would suggestnot usingthe protocol:
• Two-phase locking.
• Two-phase locking with multiple-granularitylocking.
PracticeExercises 715
• The tree protocol.
• Timestampordering.
• Validation.
• Multiversiontimestampordering.
• Multiversiontwo-phase locking.
15.15 Explain why the following technique for transaction execution may pro-
vide better performance than just using strict two-phase locking: First
execute the transaction without acquiring any locks and without per-
forminganywritestothedatabaseasinthevalidation-basedtechniques,
but unlike the validation techniques do not perform either validation or
writes on the database. Instead, rerun the transaction using strict two-
phase locking. (Hint: Considerwaitsfor disk I/O.)
15.16 Considerthetimestamp-orderingprotocol,andtwotransactions,onethat
writes two data items p and q, and another that reads the same two data
items. Give a schedule whereby the timestamp test for a write operation
fails and causes the ?rst transaction to be restarted, in turn causing a
cascading abort of the other transaction. Show how this could result in
starvation of both transactions. (Such a situation, where two or more
processescarryoutactions,but areunable tocompletetheirtaskbecause
of interaction with the otherprocesses, iscalledalivelock.)
15.17 Deviseatimestamp-basedprotocolthatavoidsthephantomphenomenon.
15.18 SupposethatweusethetreeprotocolofSection15.1.5tomanageconcur-
rent access to a B
+
-tree. Since a split may occur on an insert that affects
the root, it appears that an insert operation cannot release any locks un-
til it has completed the entire operation. Under what circumstances is it
possible toreleasea lock earlier?
15.19 The snapshot isolation protocol uses a validation step which, before per-
forming a write of a data item by transaction T, checks if a transaction
concurrent with T has alreadywrittenthe dataitem.
a. A straightforward implementation uses a start timestamp and a
commit timestampforeach transaction, inadditiontoan update set,
that is the set of data items updated by the transaction. Explain
how to perform validation for the ?rst-committer-wins scheme by
using the transaction timestamps along with the update sets. You
may assume that validation and other commit processing steps are
executedserially,that isfor one transaction at a time,
b. Explainhowthevalidationstepcanbeimplementedaspartofcom-
mit processing for the ?rst-committer-wins scheme, using a mod-
i?cation of the above scheme, where instead of using update sets,
eachdataitemhasawritetimestampassociatedwithit.Again,you
716 Chapter15 ConcurrencyControl
may assume that validation and other commit processing steps are
executedserially.
c. The ?rst-updater-wins scheme can be implemented using times-
tamps as described above, except that validation is done imme-
diately after acquiring an exclusive lock, instead of being done at
commit time.
i. Explain how to assign write timestamps to data items to imple-
ment the ?rst-updater-winsscheme.
ii. Show that as a result of locking, if the validation is repeated at
commit time the result would not change.
iii. Explain why there is no need to perform validation and other
commitprocessingstepsseriallyin this case.
Exercises
15.20 Whatbene?tdoesstricttwo-phaselockingprovide?Whatdisadvantages
result?
15.21 Most implementations of database systems use strict two-phase locking.
Suggestthree reasons forthe popularityof this protocol.
15.22 Consider a variant of the tree protocol called the forest protocol. The
databaseisorganizedasaforestofrootedtrees.Eachtransaction T
i
must
follow the following rules:
• The ?rst lock in each treemay be on any dataitem.
• Thesecond,andallsubsequent,locksinatreemayberequestedonly
ifthe parentof the requestednode iscurrently locked.
• Data itemsmay be unlocked atany time.
• A data item may not be relocked by T
i
after it has been unlocked by
T
i
.
Show that the forestprotocol does not ensure serializability.
15.23 Underwhatconditionsisitlessexpensivetoavoiddeadlockthantoallow
deadlockstooccur and thento detectthem?
15.24 If deadlock is avoided by deadlock-avoidance schemes, is starvation still
possible?Explainyour answer.
15.25 In multiple-granularity locking, what is the difference between implicit
and explicitlocking?
15.26 AlthoughSIXmodeisusefulinmultiple-granularitylocking,anexclusive
and intention-shared (XIS) mode isof no use. Why is ituseless?
Exercises 717
15.27 The multiple-granularity protocol rules specify that a transaction T
i
can
lock a node Q in S or IS mode only if T
i
currently has the parent of Q
lockedineitherIXor ISmode.Giventhat SIXandSlocksarestrongerthan
IX or IS locks, why does the protocol not allow locking a node in S or IS
modeiftheparentislockedineitherSIX or S mode?
15.28 Whenatransactionisrolledbackundertimestampordering,itisassigned
a new timestamp.Why can it not simplykeepitsold timestamp?
15.29 Showthatthereareschedulesthatarepossibleunderthetwo-phaselock-
ingprotocol, butarenot possibleunderthetimestampprotocol,and vice
versa.
15.30 Under a modi?ed version of the timestamp protocol, we require that a
commitbitbetestedtoseewhetherareadrequestmustwait.Explainhow
thecommitbitcanpreventcascadingabort.Whyisthistestnotnecessary
for write requests?
15.31 As discussed in Exercise 15.19, snapshot isolation can be implemented
using a form of timestamp validation. However, unlike the multiversion
timestamp-ordering scheme, which guarantees serializability, snapshot
isolationdoesnotguaranteeserializability.Explainwhatisthekeydiffer-
ence between the protocols that resultsin thisdifference.
15.32 Outlinethekeysimilaritiesanddifferencesbetweenthetimestampbased
implementation of the ?rst-committer-wins version of snapshot isola-
tion, describedin Exercise15.19, and the optimistic-concurrency-control-
without-read-validationscheme, describedin Section15.9.3.
15.33 Explain the phantom phenomenon. Why may this phenomenon lead to
anincorrectconcurrentexecutiondespitetheuseofthetwo-phaselocking
protocol?
15.34 Explainthereasonfortheuseofdegree-twoconsistency.Whatdisadvan-
tagesdoesthisapproachhave?
15.35 Give example schedules to show that with key-value locking, if any of
lookup, insert, or delete do not lock the next-key value, the phantom
phenomenon couldgo undetected.
15.36 Many transactions update a common item (e.g., the cash balance at a
branch), and private items (e.g., individual account balances). Explain
how you can increase concurrency (and throughput) by ordering the op-
erations of the transaction.
15.37 Consider the following locking protocol: All items are numbered, and
once an item is unlocked, only higher-numbered items may be locked.
Locks may be released at any time. Only X-locks are used. Show by an
examplethat this protocol doesnot guarantee serializability.
718 Chapter15 ConcurrencyControl
BibliographicalNotes
Gray and Reuter [1993] provides detailed textbook coverage of transaction-
processing concepts, including concurrency-control concepts and implementa-
tiondetails.BernsteinandNewcomer[1997] providestextbookcoverageofvari-
ous aspects of transaction processingincluding concurrency control.
The two-phase locking protocol was introduced by Eswaran et al. [1976].
The tree-locking protocol is from Silberschatz and Kedem [1980]. Other non-
two-phase locking protocols that operate on more general graphs are described
in Yannakakis et al. [1979], Kedem and Silberschatz [1983], and Buckley and
Silberschatz[1985].Korth[1983]exploresvariouslockmodesthatcanbeobtained
from the basic shared and exclusivelock modes.
Practice Exercise 15.4 is from Buckley and Silberschatz [1984]. Practice Ex-
ercise 15.6 is from Kedem and Silberschatz [1983]. Practice Exercise 15.7 is from
Kedem and Silberschatz [1979]. Practice Exercise 15.8 is from Yannakakis et al.
[1979]. Practice Exercise15.10 isfromKorth [1983].
The locking protocol for multiple-granularity data items is from Gray et al.
[1975].AdetaileddescriptionispresentedbyGrayetal.[1976].KedemandSilber-
schatz [1983] formalizes multiple-granularity locking for an arbitrary collection
of lock modes (allowing for more semantics than simply read and write). This
approachincludesaclassoflockmodescalledupdatemodestodealwithlockcon-
version. Carey [1983] extends the multiple-granularity idea to timestamp-based
concurrency control. An extension of the protocol to ensure deadlockfreedomis
presentedbyKorth [1982].
The timestamp-based concurrency-control scheme is from Reed [1983]. A
timestampalgorithmthatdoesnotrequireanyrollbacktoensureserializabilityis
presentedbyBuckleyandSilberschatz[1983].Thevalidationconcurrency-control
scheme is fromKung and Robinson [1981].
MultiversiontimestamporderwasintroducedinReed[1983].Amultiversion
tree-lockingalgorithmappearsin Silberschatz[1982].
Degree-twoconsistencywasintroducedinGrayetal.[1975].Thelevelsofcon-
sistency—or isolation—offered in SQL are explained and critiqued in Berenson
etal.[1995].Manycommercialdatabasesystemsuseversion-basedapproachesin
combination with locking. PostgreSQL,Oracle, and SQL Serverallsupportforms
of the snapshot isolation protocol mentioned in Section 15.6.2. Details can be
found in Chapters27, 28, and 30, respectively.
It should be noted that on PostgreSQL (as of version 8.1.4) and Oracle (as
of version 10g), setting the isolation level to serializable results in the use of
snapshot isolation, which does not guarantee serializability. Fekete et al. [2005]
describes how to ensure serializable executions under snapshot isolation, by
rewriting certain transactions to introduce con?icts; these con?icts ensure that
thetransactionscannotrunconcurrentlyundersnapshotisolation;Jorwekaretal.
[2007] describes an approach, that given a set of (parametrized) transactions
running under snapshot isolation, can check if the transactions are vulnerability
to nonserializability,
BibliographicalNotes 719
Concurrency in B
+
-trees was studied by Bayer and Schkolnick [1977] and
Johnson and Shasha [1993]. The techniques presented in Section 15.10 are based
on Kung and Lehman [1980] and Lehman and Yao [1981]. The technique of key-
valuelockingusedin ARIESprovidesforveryhighconcurrencyonB
+
-treeaccess
and is described in Mohan [1990a] and Mohan and Narang [1992]. Ellis [1987]
presentsa concurrency-control technique for linear hashing.
This page intentionally left blank 
CHAPTER
16
Recovery System
A computer system, like any other device, is subject to failure from a variety
of causes: disk crash, power outage, software error, a ?re in the machine room,
even sabotage. In any failure, information may be lost. Therefore, the database
system must take actions in advance to ensure that the atomicity and durability
properties of transactions, introduced in Chapter 14, are preserved. An integral
part of a database system is a recovery scheme that can restore the database to
the consistent state that existed before the failure. The recovery scheme must
also provide high availability; that is, it must minimize the time for which the
databaseis not usableafterafailure.
16.1 Failure Classi?cation
Therearevarioustypesoffailurethatmayoccurinasystem,eachofwhichneeds
to be dealt with in a different manner. In this chapter, we shall consider only the
following typesoffailure:
• Transactionfailure.Therearetwotypesoferrorsthatmaycauseatransaction
tofail:
?
Logical error. The transaction can no longer continue with its normal
executionbecauseofsomeinternalcondition,suchasbadinput,datanot
found, over?ow,orresourcelimitexceeded.
?
System error. The system has entered an undesirable state (for example,
deadlock), as a result of which a transaction cannot continue with its
normal execution. The transaction, however, can be reexecuted at a later
time.
• System crash. There is a hardware malfunction, or a bug in the database
softwareortheoperatingsystem,thatcausesthelossofthecontentofvolatile
storage,andbringstransactionprocessingtoahalt.Thecontentofnonvolatile
storageremainsintact, andis not corrupted.
721
722 Chapter 16 RecoverySystem
The assumption that hardware errors and bugs in the software bring the
systemtoahalt,butdonotcorruptthenonvolatilestoragecontents,isknown
asthefail-stopassumption.Well-designedsystemshavenumerousinternal
checks,atthehardwareandthesoftwarelevel,thatbringthesystemtoahalt
whenthereisan error.Hence,thefail-stop assumptionis areasonable one.
• Diskfailure.Adiskblocklosesitscontentasaresultofeitheraheadcrashor
failureduringadata-transferoperation.Copiesofthedataonotherdisks,or
archivalbackupsontertiarymedia,suchas DVDortapes,areusedtorecover
fromthe failure.
Todeterminehow thesystemshould recoverfrom failures,we needtoiden-
tifythefailuremodesofthosedevicesusedforstoringdata.Next,wemust
consider how these failure modes affect the contents of the database. We can
then propose algorithms to ensure database consistency and transaction atomic-
ity despite failures. These algorithms, known as recovery algorithms, have two
parts:
1. Actions taken during normal transaction processing to ensure that enough
information existstoallowrecoveryfromfailures.
2. Actions taken after a failure to recover the database contents to a state that
ensuresdatabaseconsistency, transaction atomicity,and durability.
16.2 Storage
As we saw in Chapter 10, the various data items in the database may be stored
and accessed in a number of different storage media. In Section 14.3, we saw
that storage media can be distinguished by their relative speed, capacity, and
resiliencetofailure.Weidenti?edthreecategoriesofstorage:
• Volatilestorage
• Nonvolatilestorage
• Stablestorage
Stable storage or, more accurately, an approximation thereof, plays a critical role
inrecoveryalgorithms.
16.2.1 Stable-Storage Implementation
To implement stable storage, we need to replicate the needed information in
severalnonvolatilestoragemedia(usuallydisk)withindependentfailuremodes,
andtoupdatetheinformationinacontrolledmannertoensurethatfailureduring
datatransfer doesnot damagethe neededinformation.
16.2 Storage 723
Recall (from Chapter 10) that RAID systems guarantee that the failure of a
singledisk(evenduringdatatransfer)willnotresultinlossofdata.Thesimplest
and fastest form of RAID is the mirrored disk, which keeps two copies of each
block,onseparatedisks.OtherformsofRAIDofferlowercosts,butattheexpense
of lowerperformance.
RAID systems,however,cannot guardagainst dataloss duetodisasterssuch
as?resor?ooding.Manysystemsstorearchivalbackupsoftapesoffsitetoguard
againstsuchdisasters.However,sincetapescannotbecarriedoffsitecontinually,
updatessincethemostrecenttimethattapeswerecarriedoffsitecouldbelostin
such a disaster. More secure systems keep a copy of each block of stable storage
at a remote site, writing it out over a computer network, in addition to storing
theblockonalocaldisksystem.Sincetheblocksareoutputtoaremotesystemas
andwhentheyareoutputtolocalstorage,onceanoutputoperationiscomplete,
the output is not lost, even in the event of a disaster such as a ?re or ?ood. We
studysuch remote backup systemsinSection16.9.
In the remainder of this section, we discuss how storage media can be pro-
tectedfromfailureduringdatatransfer.Blocktransferbetweenmemoryanddisk
storagecan resultin:
• Successfulcompletion.Thetransferredinformationarrivedsafelyatitsdes-
tination.
• Partialfailure.Afailureoccurredinthemidstoftransfer,andthedestination
block has incorrect information.
• Total failure. The failure occurred suf?ciently early during the transfer that
thedestinationblock remains intact.
We require that, if a data-transfer failure occurs, the system detects it and
invokes a recovery procedure to restore the block to a consistent state. To do so,
the system must maintain two physical blocks for each logical database block;
in the case of mirrored disks, both blocks are at the same location; in the case of
remotebackup,oneoftheblocksislocal,whereastheotherisataremotesite.An
output operationisexecutedas follows:
1. Writethe informationonto the ?rstphysical block.
2. Whenthe?rstwritecompletessuccessfully,writethesameinformationonto
the secondphysical block.
3. Theoutputiscompletedonlyafterthesecondwritecompletessuccessfully.
If the system fails while blocks are being written, it is possible that the two
copiesofablockareinconsistentwitheachother.Duringrecovery,foreachblock,
the systemwould needtoexaminetwo copiesof the blocks. If both arethe same
and no detectable error exists, then no further actions are necessary. (Recall that
errorsinadiskblock,such asapartialwritetotheblock, aredetectedby storing
a checksum with each block.) If the system detects an error in one block, then it
724 Chapter 16 RecoverySystem
replaces its content with the content of the other block. If both blocks contain no
detectableerror,buttheydifferincontent,thenthesystemreplacesthecontentof
the?rstblockwiththevalueofthesecond. This recovery procedureensures that
awritetostablestorageeithersucceedscompletely(thatis,updatesallcopies)or
resultsinno change.
The requirement of comparing every corresponding pair of blocks during
recoveryisexpensivetomeet.Wecanreducethecostgreatlybykeepingtrackof
block writes that are in progress, using a small amount of nonvolatile RAM.On
recovery,only blocks forwhich writeswereinprogressneedtobe compared.
Theprotocolsforwritingoutablocktoaremotesitearesimilartotheprotocols
for writing blocks to a mirrored disk system, which we examined in Chapter 10,
and particularlyinPractice Exercise10.3.
We can extend this procedure easily to allow the use of an arbitrarily large
number of copies of each block of stable storage. Although a large number of
copies reduces the probability of a failure to even lower than two copies do, it is
usually reasonabletosimulatestablestoragewithonly twocopies.
16.2.2 Data Access
AswesawinChapter10,thedatabasesystemresidespermanentlyonnonvolatile
storage(usuallydisks)andonlypartsofthedatabaseareinmemoryatanytime.
1
The database is partitioned into ?xed-length storage units called blocks.Blocks
aretheunitsofdatatransfertoandfromdisk,andmaycontainseveraldataitems.
We shall assume that nodataitemspans two or moreblocks. This assumptionis
realisticformost data-processingapplications,suchas abank or auniversity.
Transactions input information from the disk to main memory, and then
output the information back onto the disk. The input and output operations are
done in block units. The blocks residing on the disk are referred to as physical
blocks;theblocksresidingtemporarilyinmainmemoryarereferredtoasbuffer
blocks. The area of memory where blocks reside temporarily is called the disk
buffer.
Block movements between disk and main memory are initiated through the
following twooperations:
1. input(B)transfersthephysical block B tomainmemory.
2. output(B) transfers the buffer block B to the disk, and replaces the appro-
priatephysical block there.
Figure16.1illustratesthis scheme.
Conceptually, each transaction T
i
has a private work area in which copies of
data items accessed and updated by T
i
are kept. The system creates this work
areawhenthetransactionisinitiated;thesystemremovesitwhenthetransaction
1
There is a special category of database system, called main-memory database systems, where the entire database can be
loadedinto memory at once.Weconsider such systems inSection26.4.
16.2 Storage 725
A
B
input(A)
output(B)
B
mainmemory
disk
Figure 16.1 Block storage operations.
eithercommitsoraborts.Eachdataitem Xkeptintheworkareaoftransaction T
i
isdenotedby x
i
.TransactionT
i
interactswiththedatabasesystembytransferring
datatoandfromitsworkareatothesystembuffer.Wetransferdatabythesetwo
operations:
1. read(X) assigns the value of data item X to the local variable x
i
.Itexecutes
this operationas follows:
a. If block B
X
on which X resides is not in main memory, it issues
input(B
X
).
b. Itassigns to x
i
thevalueof X fromthe bufferblock.
2. write(X) assigns the value of local variable x
i
to data item X in the buffer
block. Itexecutesthisoperationasfollows:
a. If block B
X
on which X resides is not in main memory, it issues
input(B
X
).
b. Itassigns thevalueof x
i
to X inbuffer B
X
.
Note that both operations may require the transfer of a block from disk to main
memory. They do not, however, speci?cally require the transfer of a block from
mainmemorytodisk.
A buffer block is eventually written out to the disk either because the buffer
manager needs the memory space for other purposes or because the database
systemwishestore?ectthechangetoBonthedisk.Weshallsaythatthedatabase
systemperformsaforce-outputofbuffer B ifitissuesanoutput(B).
When a transaction needs to access a data item X for the ?rst time, it must
execute read(X). The system then performs all updates to X on x
i
.Atanypoint
duringitsexecutionatransactionmayexecutewrite(X)tore?ectthechangeto X
inthedatabase itself;write(X) mustcertainlybe doneafterthe?nal writeto X.
726 Chapter 16 RecoverySystem
Theoutput(B
X
)operationforthebufferblock B
X
onwhich Xresidesdoesnot
needtotakeeffectimmediatelyafterwrite(X)isexecuted,sincetheblock B
X
may
containotherdataitemsthatarestillbeingaccessed.Thus,theactualoutputmay
takeplacelater.Noticethat,ifthesystemcrashesafterthewrite(X)operationwas
executedbutbeforeoutput(B
X
)wasexecuted,thenewvalueofXisneverwritten
to disk and, thus, is lost. As we shall see shortly, the database system executes
extraactionstoensurethatupdatesperformedbycommittedtransactionsarenot
lostevenifthereisasystemcrash.
16.3 Recovery and Atomicity
Consideragainoursimpli?edbanking systemandatransaction T
i
thattransfers
$50 from account A to account B, with initial values of A and B being $1000 and
$2000,respectively.Supposethatasystemcrashhasoccurredduringtheexecution
ofT
i
,afteroutput(B
A
)hastakenplace,butbeforeoutput(B
B
)wasexecuted,where
B
A
and B
B
denote the buffer blocks on which Aand B reside. Since the memory
contents werelost,we donot knowthe fateof thetransaction.
When the system restarts, the value of A would be $950, while that of B
would be $2000, which is clearly inconsistent with the atomicity requirement
for transaction T
i
. Unfortunately, there is no way to ?nd out by examining the
database state what blocks had beenoutput, and what had not, beforethe crash.
It is possible that the transaction completed, updating the database on stable
storage from an initial state with the values of A and B being $1000 and $1950;
it is also possible that the transaction did not affect the stable storage at all, and
the values of A and B were $950 and $2000 initially; or that the updated B was
outputbutnottheupdated A;orthattheupdated Awasoutputbuttheupdated
Bwas not.
Our goal is to perform either all or no database modi?cations made by T
i
.
However,if T
i
performedmultipledatabasemodi?cations,severaloutputopera-
tions maybe required,and afailuremay occur aftersomeof thesemodi?cations
havebeenmade,butbeforeallofthemaremade.
Toachieveourgoal of atomicity,wemust ?rstoutput tostable storageinfor-
mationdescribingthemodi?cations,withoutmodifyingthedatabaseitself.Aswe
shallsee,thisinformationcanhelpusensurethatallmodi?cationsperformedby
committed transactions are re?ected in the database (perhaps during the course
of recovery actions after a crash). This information can also help us ensure that
no modi?cations madeby an abortedtransactionpersistinthe database.
16.3.1 Log Records
The most widely used structure for recording database modi?cations is the log.
The log is a sequence of log records, recording all the update activities in the
database.
Thereareseveraltypesoflogrecords.Anupdatelogrecorddescribesasingle
databasewrite.Ithas these?elds:
16.3 RecoveryandAtomicity 727
SHADOW COPIESANDSHADOW PAGING
In the shadow-copy scheme, a transaction that wants to update the database
?rst creates a complete copy of the database. All updates are done on the new
databasecopy,leavingtheoriginalcopy,theshadowcopy,untouched.Ifatany
pointthetransactionhastobeaborted,thesystemmerelydeletesthenewcopy.
The old copy of the database has not been affected. The current copy of the
database isidenti?ed bya pointer,calleddb-pointer,which isstoredondisk.
If the transaction partially commits (that is, executes its ?nal statement) it
is committed as follows: First, the operating system is asked to make sure that
all pages of the new copy of the database have been written out to disk. (Unix
systems use the fsync command for this purpose.) After the operating system
has written all the pages to disk, the database system updates the pointer db-
pointer to point to the new copy of the database; the new copy then becomes
the current copy of the database. The old copy of the database is then deleted.
The transaction is said to have been committed at the point where the updated
db-pointeriswritten todisk.
Theimplementationactuallydependsonthewritetodb-pointerbeingatomic;
that is, either all its bytes are written or none of its bytes are written. Disk sys-
temsprovideatomicupdatestoentireblocks,oratleasttoadisksector.Inother
words, the disk system guarantees that it will update db-pointer atomically, as
long as we make sure that db-pointer lies entirely in a single sector, which we
canensurebystoringdb-pointerat thebeginningofa block.
Shadow copy schemes are commonly used by text editors (saving the ?le is
equivalenttotransactioncommit,whilequittingwithoutsavingthe?leisequiv-
alenttotransactionabort).Shadowcopyingcanbeusedforsmalldatabases,but
copying a large database would be extremely expensive. A variant of shadow-
copying,calledshadow-paging, reducescopyingasfollows: theschemeuses a
pagetablecontainingpointerstoallpages;thepagetableitselfandallupdated
pages are copied to a new location. Any page which is not updated by a trans-
action is not copied, but instead the new page table just stores a pointer to the
originalpage.Whenatransactioncommits,itatomicallyupdatesthepointerto
thepagetable,whichactsasdb-pointer,to pointtothenewcopy.
Shadow paging unfortunately does not work well with concurrent transac-
tionsandisnotwidely usedindatabases.
• Transaction identi?er, which is the unique identi?er of the transaction that
performedthewriteoperation.
• Data-item identi?er, which is the unique identi?er of the data item written.
Typically, it is the location on disk of the data item, consisting of the block
identi?er of the block on which the data item resides, and an offset within
theblock.
• Oldvalue,which is thevalueofthe dataitempriortothe write.
728 Chapter 16 RecoverySystem
• Newvalue,which is thevaluethatthe dataitemwillhaveafterthe write.
Werepresentanupdatelogrecordas<T
i
, X
j
, V
1
, V
2
>,indicatingthattransaction
T
i
has performed a write on data item X
j
. X
j
had value V
1
before the write, and
has value V
2
after the write. Other special log records exist to record signi?cant
events during transaction processing, such as the start of a transaction and the
commit orabort ofa transaction. Among the typesof logrecordsare:
• <T
i
start>.TransactionT
i
has started.
• <T
i
commit>.TransactionT
i
has committed.
• <T
i
abort>.TransactionT
i
has aborted.
We shallintroduceseveralother typesoflogrecordslater.
Whenever a transaction performs a write, it is essential that the log record
for that write be created and added to the log, before the database is modi?ed.
Oncealogrecordexists,wecanoutputthemodi?cationtothedatabaseifthatis
desirable. Also, we have the ability to undo a modi?cation that has already been
output tothe database.Weundoitby using theold-value?eldinlogrecords.
For log records to be useful for recovery from system and disk failures, the
log must reside in stable storage. For now, we assume that every log record is
written to the end of the log on stable storage as soon as it is created. In Section
16.5, we shall see when it is safe to relax this requirement so as to reduce the
overheadimposedbylogging.Observethatthelogcontainsacompleterecordof
alldatabaseactivity.Asaresult,thevolumeofdatastoredinthelogmaybecome
unreasonably large. In Section 16.3.6, we shall show when it is safe to erase log
information.
16.3.2 Database Modi?cation
As we noted earlier, a transaction creates a log record prior to modifying the
database.Thelogrecordsallowthesystemtoundochangesmadebyatransaction
in the event that the transaction must be aborted; they allow the system also to
redo changes made by a transaction if the transaction has committed but the
system crashed before those changes could be stored in the database on disk. In
order for us to understand the role of these log records in recovery, we need to
considerthe stepsatransaction takesinmodifyingadataitem:
1. Thetransactionperformssomecomputationsinitsownprivatepartofmain
memory.
2. The transaction modi?es the data block in the disk buffer in main memory
holding thedataitem.
3. Thedatabasesystemexecutestheoutputoperationthatwritesthedatablock
todisk.
16.3 RecoveryandAtomicity 729
We say a transaction modi?es the database if it performs an update on a disk
buffer, or on the disk itself; updates to the private part of main memory do not
count as database modi?cations. If a transaction does not modify the database
until it has committed, it is said to use the deferred-modi?cation technique. If
databasemodi?cationsoccurwhilethetransactionisstillactive,thetransactionis
saidtousetheimmediate-modi?cationtechnique.Deferredmodi?cationhasthe
overhead that transactions need to make local copies of all updated data items;
further, if a transaction reads a data item that it has updated, it must read the
valuefromitslocal copy.
Therecoveryalgorithmswedescribeinthischaptersupportimmediatemod-
i?cation. Asdescribed,theyworkcorrectlyevenwithdeferredmodi?cation,but
can beoptimizedtoreduceoverheadwhen usedwithdeferredmodi?cation; we
leavedetailsas anexercise.
Arecoveryalgorithm musttakeintoaccount avarietyoffactors, including:
• The possibility that a transaction may have committed although some of its
databasemodi?cationsexistonlyinthediskbufferinmainmemoryandnot
inthedatabaseon disk.
• The possibility that a transaction may have modi?ed the database while in
theactivestateand,as aresultof asubsequentfailure,may needtoabort.
Because all database modi?cations must be preceded by the creation of a log
record, the system has available both the old value prior to the modi?cation of
thedataitemandthenewvaluethatistobewrittenforthedataitem.Thisallows
thesystemtoperform undoand redo operationsas appropriate.
• Undo using a log record sets the data item speci?ed in the log record to the
oldvalue.
• Redo using a log record sets the data item speci?ed in the log record to the
new value.
16.3.3 Concurrency Control and Recovery
If the concurrency control scheme allows a data item X that has been modi?ed
by a transaction T
1
to be further modi?ed by another transaction T
2
before T
1
commits,thenundoingtheeffectsof T
1
byrestoringtheoldvalueof X(before T
1
updated X) would alsoundo the effectsof T
2
. Toavoidsuch situations, recovery
algorithmsusuallyrequirethatifadataitemhasbeenmodi?edbyatransaction,
no other transaction can modify the data item until the ?rst transaction commits
or aborts.
This requirement can be ensured by acquiring an exclusive lock on any up-
dated data item and holding the lock until the transaction commits; in other
words, by using strict two-phase locking. Snapshot-isolation and validation-
730 Chapter 16 RecoverySystem
based concurrency-control techniques also acquire exclusive locks on data items
atthetimeofvalidation,beforemodifyingthedataitems,andholdthelocksuntil
the transaction is committed; as a result the above requirement is satis?ed even
by theseconcurrency control protocols.
We discuss later, in Section 16.7, how the above requirement can be relaxed
incertaincases.
Wheneithersnapshot-isolationorvalidationisusedforconcurrencycontrol,
database updates of a transaction are (conceptually) deferred until the transac-
tion is partially committed; the deferred-modi?cation technique is a natural ?t
with these concurrency control schemes. However, it is worth noting that some
implementations of snapshot isolation use immediate modi?cation, but provide
a logical snapshot on demand: when a transaction needs to read an item that
a concurrent transaction has updated, a copy of the (already updated) item is
made, and updates made by concurrent transactions are rolledback on the copy
oftheitem.Similarly,immediatemodi?cationofthedatabaseisanatural?twith
two-phase locking, but deferred modi?cation can also be used with two-phase
locking.
16.3.4 Transaction Commit
Wesaythatatransactionhascommittedwhenitscommitlogrecord,whichisthe
last log record of the transaction, has been output to stable storage; at that point
all earlier log records have already been output to stable storage. Thus, there is
enough information in the log to ensure that even if there is a system crash, the
updates of the transaction can be redone. If a system crash occurs before a log
record < T
i
commit> is output to stable storage, transaction T
i
will be rolled
back.Thus,theoutputoftheblockcontainingthecommitlogrecordisthesingle
atomicactionthat resultsinatransaction gettingcommitted.
2
With most log-based recovery techniques, including the ones we describe in
this chapter, blocks containing the data items modi?ed by a transaction do not
have to be output to stable storage when the transaction commits, but can be
output sometimelater.Wediscussthis issuefurtherinSection16.5.2.
16.3.5 Using the Log to Redo and Undo Transactions
We now provide an overview of how the log can be used to recover from a
systemcrash,andtorollbacktransactionsduringnormaloperation.However,we
postponedetailsoftheproceduresforfailurerecoveryandrollbacktoSection16.4.
Consideroursimpli?edbankingsystem.LetT
0
beatransactionthattransfers
$50 from account Ato account B:
2
The output of a block can be made atomic by techniques for dealing with data-transfer failure, as described earlier in
Section16.2.1.
16.3 RecoveryandAtomicity 731
<T
0
  start>
<T
0
 ,  A,  1000,  950>
<T
0
 ,  B,  2000,  2050>
<T
0
  commit>
<T
1
  start>
<T
1
 ,  C,  700,  600>
<T
1
  commit>
Figure 16.2 Portion of the system log corresponding to T
0
and T
1
.
T
0
: read(A);
A:= A ?50;
write(A);
read(B);
B:= B+50;
write(B).
Let T
1
bea transactionthat withdraws $100 fromaccount C:
T
1
: read(C);
C:= C ? 100;
write(C).
The portion of the log containing the relevant information concerning these two
transactions appearsinFigure16.2.
Figure 16.3 shows one possible order in which the actual outputs took place
inboththedatabasesystemandthelogasaresultoftheexecutionof T
0
and T
1
.
3
Using the log, the system can handle any failure that does not result in the
lossofinformationinnonvolatilestorage.Therecoveryschemeusestworecovery
procedures.Boththeseproceduresmakeuseofthelogto?ndthesetofdataitems
updatedby eachtransaction T
i
,and theirrespectiveoldand newvalues.
• redo(T
i
)setsthevalueofalldataitemsupdatedby transaction T
i
tothenew
values.
The order in which updates are carried out by redo is important; when
recovering from a system crash, if updates to a particular data item are
applied in an order different from the order in which they were applied
originally, the ?nal state of that data item will have a wrong value. Most
recovery algorithms, including the one we describe in Section 16.4, do not
perform redo of each transaction separately; instead they perform a single
scanof thelog,duringwhichredoactions areperformedfor eachlogrecord
asitisencountered.Thisapproachensurestheorderofupdatesispreserved,
3
Noticethatthisordercouldnotbeobtainedusingthedeferred-modi?cationtechnique,becausethedatabaseismodi?ed
by T
0
beforeit commits,and likewisefor T
1
.
732 Chapter 16 RecoverySystem
Log Database
A = 950
B = 2050
C = 600
<T
0
  start>
<T
0
 ,  A,  1000,  950>
<T
0
 ,  B,  2000,  2050>
<T
0
  commit>
<T
1
  start>
<T
1
 ,  C,  700,  600>
<T
1
  commit>
Figure 16.3 State of system log and database corresponding to T
0
and T
1
.
andismoreef?cientsincethelogneedstobereadonlyonceoverall,instead
of once pertransaction.
• undo(T
i
) restoresthe value of all data items updated by transaction T
i
to the
oldvalues.
Inthe recoveryschemethatwe describeinSection16.4:
?
The undo operation not only restores the data items to their old value,
butalsowriteslogrecordstorecordtheupdatesperformedaspartofthe
undo process. These log records are special redo-only log records, since
they donot needtocontain the old-valueof theupdateddataitem.
As with the redo procedure, the order in which undo operations are
performedisimportant;again wepostponedetailstoSection16.4.
?
When the undo operation for transaction T
i
completes, it writes a <T
i
abort> logrecord,indicatingthat the undohas completed.
As we shall see in Section 16.4, the undo(T
i
) procedure is executed
onlyonceforatransaction,ifthetransactionisrolledbackduringnormal
processing or if on recovering from a system crash, neither a commit nor
an abort record is found for transaction T
i
. As a result, every transaction
willeventuallyhaveeitheracommitoranabort record in the log.
After a system crash has occurred, the system consults the log to determine
which transactions need to be redone, and which need to be undone so as to
ensureatomicity.
• Transaction T
i
needs to be undone if the log contains the record <T
i
start>,
butdoesnotcontaineithertherecord<T
i
commit>ortherecord<T
i
abort>.
• Transaction T
i
needstoberedoneifthelogcontainstherecord<T
i
start>and
eithertherecord<T
i
commit>ortherecord<T
i
abort>.Itmayseemstrange
to redo T
i
if the record<T
i
abort> is in the log. To see why this works, note
16.3 RecoveryandAtomicity 733
<T
0
  start>
<T
0
 ,  A,  1000,  950>
<T
0
 ,  B,  2000,  2050>
<T
0
  start>
<T
0
 ,  A,  1000,  950>
<T
0
 ,  B,  2000,  2050>
<T
0
  commit>
<T
1
  start>
<T
1
 ,  C,  700,  600>
<T
0
  start>
<T
0
 ,  A,  1000,  950>
<T
0
 ,  B,  2000,  2050>
<T
0
  commit>
<T
1
  start>
<T
1
 ,  C,  700,  600>
<T
1
  commit>
(a) (b) (c)
Figure 16.4 The same log, shown at three different times.
that if <T
i
abort> is in the log, so are the redo-only records written by the
undooperation.Thus,theendresultwillbetoundo T
i
’smodi?cationsinthis
case. This slight redundancy simpli?es the recovery algorithm and enables
fasteroverallrecoverytime.
As an illustration, return to our banking example, with transaction T
0
and
T
1
executed one after the other in the order T
0
followed by T
1
. Suppose that the
systemcrashesbeforethecompletionofthetransactions.Weshallconsiderthree
cases.The stateof the logsfor eachof thesecasesappearsinFigure16.4.
First,letusassumethat thecrash occurs justafterthelog recordfor thestep:
write(B)
oftransactionT
0
hasbeenwrittentostablestorage(Figure16.4a).Whenthesystem
comes back up, it ?nds the record <T
0
start> in the log, but no corresponding
<T
0
commit> or<T
0
abort> record. Thus, transaction T
0
must be undone, so an
undo(T
0
) is performed. As a result, the values in accounts A and B (on the disk)
arerestoredto$1000 and$2000, respectively.
Next,letusassumethatthecrashcomesjustafterthelogrecordforthestep:
write(C)
of transaction T
1
has been written to stable storage (Figure 16.4b). When the
system comes back up, two recovery actions need to be taken. The operation
undo(T
1
) must be performed, since the record<T
1
start> appears in the log, but
there is no record <T
1
commit> or <T
1
abort>.Theoperationredo(T
0
)mustbe
performed, since the log contains both the record<T
0
start> and the record <T
0
commit>. At the end of the entire recovery procedure, the values of accounts A,
B,andCare$950, $2050, and $700, respectively.
Finally,letus assumethat thecrash occurs justafterthe logrecord:
<T
1
commit>
734 Chapter 16 RecoverySystem
hasbeenwrittentostablestorage(Figure16.4c).Whenthesystemcomesbackup,
both T
0
and T
1
needtoberedone,sincetherecords<T
0
start>and<T
0
commit>
appearinthelog,asdotherecords<T
1
start>and<T
1
commit>.Afterthesystem
performs the recovery procedures redo(T
0
)andredo(T
1
), the values in accounts
A, B,andCare$950,$2050, and $600, respectively.
16.3.6 Checkpoints
When a system crash occurs, we must consult the log to determine those trans-
actionsthatneedtoberedoneandthosethatneedtobeundone.Inprinciple,we
need to search the entire log to determine this information. There are two major
dif?cultieswiththisapproach:
1. The search process is time-consuming.
2. Mostofthetransactionsthat,accordingtoouralgorithm,needtoberedone
have already written their updates into the database. Although redoing
themwillcause noharm,itwillneverthelesscause recoverytotakelonger.
Toreducethesetypesofoverhead,we introducecheckpoints.
We describe below a simple checkpoint scheme that (a) does not permit any
updates to be performed while the checkpoint operation is in progress, and (b)
outputs all modi?ed buffer blocks to disk when the checkpoint is performed.
We discuss later how to modify the checkpointing and recovery procedures to
providemore?exibilityby relaxingboththeserequirements.
Acheckpoint isperformedasfollows:
1. Output onto stable storage all log records currently residing in main mem-
ory.
2. Output tothe diskall modi?edbuffer blocks.
3. Outputontostablestoragealogrecordoftheform<checkpoint L>,where
L isalistof transactions activeat thetimeofthe checkpoint.
Transactions are not allowed to perform any update actions, such as writing
to a buffer block or writing a log record, while a checkpoint is in progress. We
discusshow this requirementcanbe enforced,later,inSection16.5.2.
The presence of a <checkpoint L> record in the log allows the system to
streamlineitsrecoveryprocedure.Consideratransaction T
i
thatcompletedprior
tothecheckpoint.Forsuchatransaction,the<T
i
commit>record(or< T
i
abort>
record) appears in the log before the <checkpoint> record. Any database mod-
i?cations made by T
i
must have been written to the database either prior to the
checkpoint or as part of the checkpoint itself. Thus, at recovery time, there is no
needtoperformaredo operationon T
i
.
Afterasystemcrashhasoccurred,thesystemexaminesthelogto?ndthelast
<checkpoint L> record (this can be done by searching the log backward, from
the endofthe log,untilthe ?rst<checkpoint L> record is found).
16.4 RecoveryAlgorithm 735
Theredoorundooperationsneedtobeappliedonlytotransactionsin L,and
to all transactions that started execution after the <checkpoint L> record was
writtento thelog.Letusdenotethis setof transactions as T.
• For all transactions T
k
in T that have no <T
k
commit> record or <T
k
abort>
record in the log, execute undo(T
k
).
• For all transactions T
k
in T such that either the record <T
k
commit> or the
record<T
k
abort> appearsinthe log,executeredo(T
k
).
Note that we need only examine the part of the log starting with the last check-
pointlogrecordto?ndthesetoftransactions T,andto?ndoutwhetheracommit
orabortrecordoccurs inthe logfor eachtransaction in T.
As an illustration, consider the set of transactions {T
0
, T
1
,...,T
100
}. Suppose
thatthemostrecentcheckpointtookplaceduringtheexecutionoftransaction T
67
and T
69
, while T
68
and all transactions with subscripts lower than 67 completed
before the checkpoint. Thus, only transactions T
67
, T
69
,...,T
100
need to be con-
sidered during the recovery scheme. Each of them needs to be redone if it has
completed (that is, either committed or aborted); otherwise, it was incomplete,
and needstobeundone.
Consider the set of transactions L in a checkpoint log record. For each trans-
action T
i
in L, log records of the transaction that occur prior to the checkpoint
log record may be needed to undo the transaction, in case it does not commit.
However,alllogrecordspriortotheearliestofthe< T
i
start>logrecords,among
transactions T
i
inL,arenotneededoncethecheckpointhascompleted.Theselog
records can be erased whenever the database system needs to reclaim the space
occupiedbytheserecords.
The requirement that transactions must not perform any updates to buffer
blocks or to the log during checkpointing can be bothersome, since transaction
processing has to halt while a checkpoint is in progress. A fuzzy checkpoint
is a checkpoint where transactions are allowed to perform updates even while
buffer blocks arebeing writtenout. Section16.5.4describesfuzzy-checkpointing
schemes. Later in Section 16.8 we describe a checkpoint scheme that is not only
fuzzy,butdoesnotevenrequireallmodi?edbufferblockstobeoutputtodiskat
thetimeofthecheckpoint.
16.4 Recovery Algorithm
Until now, in discussing recovery, we have identi?ed transactions that need to
be redone and those that need to be undone, but we have not given a precise
algorithm for performing these actions. We are now ready to present the full
recovery algorithm using log records for recovery from transaction failure and
a combination of the most recent checkpoint and log records to recover from a
systemcrash.
736 Chapter 16 RecoverySystem
The recovery algorithm described in this section requires that a data item
that has been updated by an uncommitted transaction cannot be modi?ed by
any othertransaction, untilthe?rsttransactionhaseithercommittedoraborted.
Recall that thisrestrictionwas discussedearlier,inSection16.3.3.
16.4.1 Transaction Rollback
First consider transaction rollback during normal operation (that is, not during
recoveryfromasystemcrash).RollbackofatransactionT
i
isperformedasfollows:
1. The log is scanned backward, and for each log record of T
i
of the form
<T
i
, X
j
, V
1
, V
2
> that isfound:
a. The value V
1
iswrittentodataitem X
j
,and
b. Aspecialredo-onlylogrecord<T
i
, X
j
, V
1
>iswrittentothelog,where
V
1
is the value being restored to data item X
j
during the rollback.
These log records are sometimes called compensation log records.
Such records do not need undo information, since we never need to
undo such an undo operation. We shall explain later how they are
used.
2. Once the log record<T
i
start> is found the backward scan is stopped, and
a log record<T
i
abort> iswrittentothelog.
Observe that every update action performed by the transaction or on behalf
ofthetransaction,includingactionstakentorestoredataitemstotheiroldvalue,
have now been recorded in the log. In Section 16.4.2 we shall see why this is a
good idea.
16.4.2 Recovery After a System Crash
Recovery actions, when the database system is restarted after a crash, take place
intwophases:
1. Intheredophase,thesystemreplaysupdatesofalltransactionsbyscanning
the log forward from the last checkpoint. The log records that are replayed
include log records for transactions that were rolled back before system
crash, and those that hadnot committedwhenthe systemcrash occurred.
Thisphasealsodeterminesalltransactionsthatwereincompleteatthetime
ofthecrash,andmustthereforeberolledback.Suchincompletetransactions
wouldeitherhavebeenactiveatthetimeofthecheckpoint,andthuswould
appearinthetransactionlistinthecheckpointrecord,orwouldhavestarted
later;further,suchincompletetransactionswouldhaveneithera<T
i
abort>
nor a<T
i
commit> record in the log.
The speci?cstepstakenwhilescanning the logareas follows:
a. Thelistoftransactionstoberolledback,undo-list,isinitiallysettothe
list L inthe<checkpoint L>log record.
16.4 RecoveryAlgorithm 737
b. Whenever a normal log record of the form <T
i
, X
j
, V
1
, V
2
>,ora
redo-only log record of the form <T
i
, X
j
, V
2
> is encountered, the
operationis redone;that is,thevalue V
2
iswrittentodataitem X
j
.
c. Whenever a log record of the form<T
i
start> is found, T
i
is addedto
undo-list.
d. Whenever a log record of the form <T
i
abort> or <T
i
commit> is
found, T
i
isremovedfrom undo-list.
At the end of the redo phase, undo-list contains the list of all transactions
that areincomplete,thatis,theyneithercommittednor completedrollback
beforethe crash.
2. In theundophase, the system rolls back all transactions in the undo-list. It
performsrollback by scanning thelog backward from theend.
a. Wheneverit?ndsalogrecordbelongingtoatransactionintheundo-
list, it performs undo actions just as if the log record had been found
duringthe rollback ofa failedtransaction.
b. When the system ?nds a <T
i
start> log record for a transaction T
i
in
undo-list, it writes a<T
i
abort> log recordto the log, and removes T
i
from undo-list.
c. Theundophaseterminatesonceundo-listbecomesempty,thatis,the
systemhasfound<T
i
start>logrecordsforalltransactions thatwere
initiallyinundo-list.
Aftertheundophaseofrecoveryterminates,normaltransactionprocessing
can resume.
Observe that the redo phase replays every log record since the most recent
checkpoint record. In other words, this phase of restart recovery repeats all the
update actions that were executed after the checkpoint, and whose log records
reachedthestablelog.Theactionsincludeactionsofincompletetransactionsand
the actions carried out to rollback failed transactions. The actions are repeated
in the same order in which they were originally carried out; hence, this process
is called repeating history. Although it may appear wasteful, repeating history
evenforfailedtransactionssimpli?esrecoveryschemes.
Figure 16.5 shows an example of actions logged during normal operation,
and actions performed during failure recovery. In the log shown in the ?gure,
transaction T
1
had committed, and transaction T
0
had been completely rolled
back,beforethesystemcrashed.Observehowthevalueofdataitem B isrestored
during the rollback of T
0
. Observe also the checkpoint record, with the list of
activetransactions containing T
0
and T
1
.
Whenrecoveringfromacrash,intheredophase,thesystemperformsaredo
of all operations after the last checkpoint record. In this phase, the list undo-list
initially contains T
0
and T
1
; T
1
is removed ?rst when its commit log record is
found, while T
2
is added when its start log record is found. Transaction T
0
is
738 Chapter 16 RecoverySystem
Figure 16.5 Example of logged actions, and actions during recovery.
removed from undo-list when its abort log record is found, leaving only T
2
in
undo-list. The undo phase scans the log backwards from the end, and when it
?ndsalogrecordof T
2
updating A,theoldvalueof Aisrestored,andaredo-only
logrecordwrittentothelog.Whenthestartrecordfor T
2
isfound,anabortrecord
is added for T
2
. Since undo-list contains no more transactions, the undo phase
terminates,completingrecovery.
16.5 Buffer Management
In this section, we consider several subtle details that are essential to the imple-
mentationofacrash-recoveryschemethatensuresdataconsistencyandimposes
a minimalamount ofoverheadoninteractions withthedatabase.
16.5.1 Log-Record Buffering
So far, we have assumed that every log record is output to stable storage at the
timeitiscreated.Thisassumptionimposesahighoverheadonsystemexecution
for several reasons: Typically, output to stable storage is in units of blocks. In
most cases, a log record is much smaller than a block. Thus, the output of each
log record translates to a much larger output at the physical level. Furthermore,
as we saw in Section 16.2.1, the output of a block to stable storage may involve
severaloutputoperations atthe physicallevel.
The cost of outputting a block to stable storage is suf?ciently high that it is
desirabletooutputmultiplelogrecordsatonce.Todoso,wewritelogrecordsto
alogbufferinmainmemory,wheretheystaytemporarilyuntiltheyareoutputto
stable storage. Multiple log records can be gathered in the log buffer and output
16.5 BufferManagement 739
tostablestorageinasingleoutputoperation.Theorderoflogrecordsinthestable
storage must be exactly the same as the order in which they were written to the
logbuffer.
As a result of log buffering, a log record may reside in only main memory
(volatile storage) for a considerable time before it is output to stable storage.
Since such log records are lost if the system crashes, we must impose additional
requirementson therecoverytechniquestoensuretransaction atomicity:
• Transaction T
i
enters the commit state after the<T
i
commit> log record has
beenoutput tostable storage.
• Before the <T
i
commit> log record can be output to stable storage, all log
records pertaining to transaction T
i
musthavebeenoutputtostablestorage.
• Before a block of data in main memory can be output to the database (in
nonvolatile storage), all log records pertaining to data in that block must
havebeenoutputtostablestorage.
Thisruleiscalledthewrite-aheadlogging(WAL)rule.(Strictlyspeaking,
the WAL rule requires only that the undo information in the log has been
output to stable storage, and it permits the redo information to be written
later.Thedifferenceisrelevantinsystemswhereundoinformationandredo
informationarestoredinseparatelogrecords.)
Thethreerulesstatesituationsinwhichcertainlogrecordsmusthavebeenoutput
to stable storage. There is no problem resulting from the output of log records
earlier than necessary. Thus, when the system ?nds it necessary to output a log
record to stable storage, it outputs an entire block of log records, if there are
enough log records in main memory to ?ll a block. If there are insuf?cient log
records to ?ll the block, all log records in main memory are combined into a
partiallyfullblock and areoutputtostablestorage.
Writingthe bufferedlog todiskissometimesreferredtoas alogforce.
16.5.2 Database Buffering
InSection16.2.2,wedescribedtheuseofatwo-levelstoragehierarchy.Thesystem
stores the database in nonvolatile storage (disk), and brings blocks of data into
mainmemoryasneeded.Sincemainmemoryistypicallymuchsmallerthanthe
entire database, it may be necessary to overwrite a block B
1
in main memory
whenanotherblock B
2
needstobebroughtintomemory.If B
1
hasbeenmodi?ed,
B
1
must be output prior to the input of B
2
. As discussed in Section 10.8.1 in
Chapter 10, this storage hierarchy is similar to the standard operating-system
concept of virtual memory.
Onemightexpectthattransactionswouldforce-outputallmodi?edblocksto
disk when they commit. Such a policy is called the force policy. The alternative,
the no-force policy, allows a transaction to commit even if it has modi?ed some
blocks that have not yet been written back to disk. All the recovery algorithms
described in this chapter work correctly even with the no-force policy. The no-
740 Chapter 16 RecoverySystem
force policy allows faster commit of transactions; moreover it allows multiple
updates to accumulate on a block before it is output to stable storage, which can
reduce the number of output operations greatly for frequently updated blocks.
Asaresult,the standardapproach takenby most systemsis theno-force policy.
Similarly, one might expect that blocks modi?ed by a transaction that is still
activeshould not be writtento disk.This policy is calledtheno-steal policy.The
alternative, the steal policy, allows the system to write modi?ed blocks to disk
evenifthetransactionsthatmadethosemodi?cationshavenotallcommitted.As
long as the write-ahead logging rule is followed, all the recovery algorithms we
studyinthechapterworkcorrectlyevenwiththestealpolicy.Further,theno-steal
policy does not work with transactions that perform a large number of updates,
sincethebuffermayget?lledwithupdatedpagesthatcannotbeevictedtodisk,
andthetransactioncannotthenproceed.Asaresult,thestandardapproachtaken
by most systemsisthe stealpolicy.
To illustrate the need for the write-ahead logging requirement, consider our
bankingexamplewithtransactions T
0
and T
1
.Supposethatthestateofthelogis:
<T
0
start>
<T
0
, A, 1000, 950>
andthattransaction T
0
issuesaread(B).AssumethattheblockonwhichBresides
is not in main memory, and that main memory is full. Suppose that the block on
which A resides is chosen to be output to disk. If the system outputs this block
to disk and then a crash occurs, the values in the database for accounts A, B,
and C are $950, $2000, and $700, respectively.This database state is inconsistent.
However,because ofthe WAL requirements,thelogrecord:
<T
0
, A, 1000, 950>
must be output to stable storageprior to output of the block on which A resides.
The system can use the log record during recovery to bring the database back to
aconsistent state.
When a block B
1
is to be output to disk, all log records pertaining to data in
B
1
must be output to stable storage before B
1
is output. It is important that no
writestotheblock B
1
beinprogresswhiletheblockisbeingoutput,sincesucha
writecouldviolatethewrite-aheadloggingrule.Wecanensurethatthereareno
writesinprogressby using aspecialmeans of locking:
• Beforeatransactionperformsawriteonadataitem,itacquiresanexclu-
sive lock on the block in which the data item resides. The lock is released
immediatelyafterthe updatehasbeenperformed.
• Thefollowing sequenceof actions is takenwhena block istobe output:
?
Obtain an exclusive lock on the block, to ensure that no transaction is
performingawriteonthe block.
16.5 BufferManagement 741
?
Output log records to stable storage until all log records pertaining to
block B
1
havebeenoutput.
?
Output block B
1
todisk.
?
Releasethelock once theblock outputhas completed.
Locksonbufferblocksareunrelatedtolocksusedforconcurrency-control of
transactions, and releasing them in a non-two-phase manner does not have any
implications on transaction serializability. These locks, and other similar locks
that areheldforashort duration,areoftenreferredtoaslatches.
Locks on buffer blocks can also be used to ensure that buffer blocks are not
updated,andlogrecordsarenotgenerated,whileacheckpointisinprogress.This
restriction may be enforced by acquiring exclusive locks on all buffer blocks, as
wellasanexclusivelockonthelog,beforethecheckpointoperationisperformed.
Theselocks canbe releasedas soonas thecheckpoint operationhas completed.
Database systems usually have a process that continually cycles through
the buffer blocks, outputting modi?ed buffer blocks back to disk. The above
locking protocol must of course be followed when the blocks are output. As a
result of continuous output of modi?ed blocks, the number of dirty blocks in
the buffer, that is, blocks that have been modi?ed in the buffer but have not
been subsequently output, is minimized. Thus, the number of blocks that have
tobeoutputduringacheckpointisminimized;further,whenablockneedstobe
evicted from the buffer it is likely that there will be a non-dirty block available
foreviction,allowingtheinputtoproceedimmediatelyinsteadofwaitingforan
output tocomplete.
16.5.3 Operating System Role in Buffer Management
Wecan manage thedatabase bufferby using one oftwo approaches:
1. Thedatabasesystemreservespartofmainmemorytoserveasabufferthat
it,ratherthantheoperatingsystem,manages.Thedatabasesystemmanages
data-block transfer inaccordance withtherequirementsinSection16.5.2.
This approach has the drawback of limiting ?exibility in the use of main
memory.Thebuffermustbekeptsmallenoughthatotherapplicationshave
suf?cient main memoryavailable for theirneeds.However,evenwhen the
other applications are not running, the database will not be able to make
use of all the available memory. Likewise, non-database applications may
not use that part of main memory reserved for the database buffer, even if
someof the pagesinthe databasebuffer arenot beingused.
2. The database system implementsits buffer within the virtual memory pro-
videdbytheoperatingsystem.Sincetheoperatingsystemknowsaboutthe
memory requirements of all processes in the system, ideally it should be
in charge of deciding what buffer blocks must be force-output to disk, and
when.But,toensurethewrite-aheadloggingrequirementsinSection16.5.1,
the operating system should not write out the database buffer pages itself,
742 Chapter 16 RecoverySystem
but instead should request the database system to force-output the buffer
blocks.Thedatabasesysteminturnwouldforce-outputthebufferblocksto
the database,afterwritingrelevantlog recordstostablestorage.
Unfortunately, almost all current-generation operating systems retain
complete control of virtual memory. The operating system reserves space
on disk for storing virtual-memory pages that are not currently in main
memory; this space is called swap space. If the operating system decides
to output a block B
x
, that block is output to the swap space on disk, and
thereisnowayforthedatabasesystemtogetcontroloftheoutputofbuffer
blocks.
Therefore, if the database buffer is in virtual memory, transfers between
database ?les and the buffer in virtual memory must be managed by the
databasesystem,whichenforcesthewrite-aheadloggingrequirementsthat
we discussed.
This approach may result in extra output of data to disk. If a block B
x
is output by the operating system, that block is not output to the database.
Instead, it is output to the swap space for the operating system’s virtual
memory. When the database system needs to output B
x
, the operating sys-
temmayneed?rsttoinput B
x
fromitsswapspace.Thus,insteadofasingle
outputof B
x
,theremaybetwooutputsofB
x
(onebytheoperatingsystem,
and one by thedatabase system)and one extrainputof B
x
.
Althoughbothapproachessufferfromsomedrawbacks,oneortheothermust
be chosen unless the operating system is designed to support the requirements
of databaselogging.
16.5.4 Fuzzy Checkpointing
ThecheckpointingtechniquedescribedinSection16.3.6requiresthatallupdates
to the database be temporarily suspended while the checkpoint is in progress.
If the number of pages in the buffer is large, a checkpoint may take a long
time to ?nish, which can result in an unacceptable interruption in processing of
transactions.
To avoidsuch interruptions,the checkpointing technique can be modi?edto
permit updates to start once the checkpoint record has been written, but before
the modi?ed buffer blocks are written to disk. The checkpoint thus generated is
afuzzycheckpoint.
Since pages are output to disk only after the checkpoint record has been
written, it is possible that the system could crash before all pages are written.
Thus,acheckpointondiskmaybeincomplete.Onewaytodealwithincomplete
checkpoints is this: The location in the log of the checkpoint record of the last
completed checkpoint is stored in a ?xed position, last-checkpoint, on disk. The
system does not update this information when it writes the checkpoint record.
Instead, before it writes the checkpoint record, it creates a list of all modi?ed
buffer blocks. The last-checkpoint information is updated only after all buffer
blocksinthe listofmodi?edbufferblockshave beenoutputtodisk.
16.6 FailurewithLossofNonvolatile Storage 743
Even with fuzzy checkpointing, a buffer block must not be updated while
it is being output to disk, although other buffer blocks may be updated concur-
rently.The write-aheadlog protocol must be followedso that (undo) log records
pertainingtoablock areonstablestoragebeforethe block isoutput.
16.6 Failure with Loss of Nonvolatile Storage
Until now, we have considered only the case where a failure results in the loss
of information residing in volatile storage while the content of the nonvolatile
storage remains intact. Although failures in which the content of nonvolatile
storageislostarerare,weneverthelessneedtobepreparedtodealwiththistype
of failure. In this section, we discuss only disk storage. Our discussions apply as
welltoothernonvolatilestoragetypes.
The basic scheme is to dump the entire contents of the database to stable
storageperiodically—say,onceperday.Forexample,wemaydumpthedatabase
tooneormoremagnetictapes.Ifafailureoccursthatresultsinthelossofphysical
databaseblocks, thesystemusesthemostrecentdumpinrestoringthedatabase
to a previous consistent state. Once this restoration has been accomplished, the
system uses the log to bring the database system to the most recent consistent
state.
Oneapproachtodatabasedumpingrequiresthatnotransactionmaybeactive
during the dump procedure, and uses a procedure similar to checkpointing:
1. Output all log records currently residing in main memory onto stable stor-
age.
2. Output allbufferblocks onto thedisk.
3. Copythe contents of thedatabase tostablestorage.
4. Output a log record<dump> onto thestablestorage.
Steps 1, 2, and 4 correspond to the three steps used for checkpoints in Section
16.3.6.
To recover from the loss of nonvolatile storage, the system restores the
database to disk by using the most recent dump. Then, it consults the log and
redoes all the actions since the most recent dump occurred. Notice that no undo
operationsneedtobeexecuted.
Incaseofapartialfailureofnonvolatilestorage,suchasthefailureofasingle
block or a few blocks, only those blocks need to be restored, and redo actions
performedonly forthose blocks.
A dump of the database contents is also referred to as an archival dump,
since we can archive the dumps and use them later to examine old states of the
database.Dumpsof a databaseand checkpointing of buffers aresimilar.
MostdatabasesystemsalsosupportanSQLdump,whichwritesout SQL DDL
statements and SQL insert statements to a ?le, which can then be reexecuted to
744 Chapter 16 RecoverySystem
re-createthedatabase.Suchdumpsareusefulwhenmigratingdatatoadifferent
instance of the database, or to a different version of the database software, since
the physical locations and layout may be differentintheother databaseinstance
or databasesoftware version.
The simple dump procedure described here is costly for the following two
reasons. First, the entire database must be copied to stable storage, resulting in
considerable data transfer. Second, since transaction processing is halted during
the dump procedure, CPU cycles are wasted. Fuzzy dump schemes have been
developed that allow transactions to be active while the dump is in progress.
They are similar to fuzzy-checkpointing schemes; see the bibliographical notes
for moredetails.
16.7 Early Lock Release and Logical Undo Operations
Any index used in processing a transaction, such as a B
+
-tree, can be treated as
normal data, but to increase concurrency, we can use the B
+
-tree concurrency-
control algorithm described in Section 15.10 to allow locks to be released early,
in a non-two-phase manner. As a result of early lock release, it is possible that a
value in a B
+
-tree node is updated by one transaction T
1
, which inserts an entry
(V1, R1), and subsequently by another transaction T
2
, which inserts an entry
(V2, R2)inthesamenode,movingtheentry( V1, R1) even before T
1
completes
execution.
4
Atthispoint,wecannotundotransaction T
1
byreplacingthecontents
of the node with the old value prior to T
1
performing its insert, since that would
also undo the insert performed by T
2
;transactionT
2
may still commit (or may
have already committed). In this example, the only way to undo the effect of
insertionof(V1, R1)istoexecuteacorrespondingdeleteoperation.
In the rest of this section, we see how to extend the recovery algorithm of
Section16.4tosupportearlylock release.
16.7.1 Logical Operations
The insertion and deletion operations are examples of a class of operations that
requirelogicalundooperationssincetheyreleaselocksearly;wecallsuchopera-
tionslogicaloperations.Suchearlylockreleaseisimportantnotonlyforindices,
but also for operations on other system data structures that are accessed and
updated very frequently; examples include data structures that track the blocks
containing records of a relation, the free space in a block, and the free blocks
in a database. If locks were not released early after performing operations on
such data structures, transactions would tend to run serially, affecting system
performance.
The theory of con?ict serializability has been extended to operations, based
on what operations con?ict with what other operations. For example,two insert
4
Recall that an entry consists of a key value and a record identi?er, or a key value and a record in the case of the leaf
levelofaB
+
-tree?leorganization.
16.7 EarlyLockReleaseandLogicalUndoOperations 745
operations on a B
+
-tree do not con?ict if they insert different key values, even if
theybothupdateoverlappingareasofthesameindexpage.However,insertand
deleteoperationscon?ictwithotherinsertanddeleteoperations,aswellaswith
readoperations, ifthey use the samekey value.Seethe bibliographical notes for
references to more information on this topic.
Operationsacquirelower-levellockswhiletheyexecute,butreleasethemwhen
they complete; the corresponding transaction must however retain a higher-level
lock in a two-phase manner to prevent concurrent transactions from executing
con?icting actions. For example, while an insert operation is being performed
on a B
+
-tree page, a short-term lock is obtained on the page, allowing entries in
the page to be shifted during the insert; the short-term lock is released as soon
as the page has been updated. Such early lock release allows a second insert
to execute on the same page. However, each transaction must obtain a lock on
the key values being inserted or deleted, and retain it in a two-phase manner,
to prevent a concurrent transaction from executing a con?icting read, insert or
deleteoperationonthesamekeyvalue.
Oncethelower-levellockisreleased,theoperationcannotbeundonebyusing
theoldvaluesofupdateddataitems,andmustinsteadbeundonebyexecutinga
compensating operation;suchanoperationiscalledalogicalundooperation.It
isimportantthatthelower-levellocksacquiredduringanoperationaresuf?cient
toperformasubsequentlogicalundooftheoperation,forreasonsexplainedlater
inSection16.7.4.
16.7.2 Logical Undo Log Records
To allow logical undo of operations, before an operation is performed to modify
an index, the transaction creates a log record <T
i
, O
j
, operation-begin>,where
O
j
isauniqueidenti?erfortheoperationinstance.
5
Whilethesystemisexecuting
the operation, it creates update log records in the normal fashion for all updates
performedbytheoperation.Thus,theusualold-valueandnew-valueinformation
iswrittenoutasusualforeachupdateperformedbytheoperation;theold-value
information is required in case the transaction needs to be rolled back before
theoperationcompletes.Whentheoperation?nishes,itwritesanoperation-end
log record of the form <T
i
, O
j
, operation-end, U>,wheretheU denotes undo
information.
For example, if the operation inserted an entry in a B
+
-tree, the undo infor-
mationU wouldindicatethatadeletionoperationistobeperformed,andwould
identifytheB
+
-treeandwhatentrytodeletefromthetree.Suchloggingofinfor-
mationaboutoperationsiscalledlogicallogging.Incontrast,loggingofold-value
andnew-valueinformationiscalledphysicallogging,andthecorrespondinglog
records are called physicallogrecords.
Note that in the above scheme, logical logging is used only for undo, not for
redo;redooperationsareperformedexclusivelyusingphysicallogrecord.Thisis
because the state of the database after a systemfailure may re?ect some updates
5
The positioninthe logoftheoperation-begin logrecordcan beused as theunique identi?er.
746 Chapter 16 RecoverySystem
of an operation and not other operations, depending on what buffer blocks had
beenwrittentodiskbeforethefailure.DatastructuressuchasB
+
-treeswouldnot
be in a consistent state, and neither logical redo nor logical undo operations can
beperformedonaninconsistentdatastructure.Toperformlogicalredoorundo,
the database state on disk must be operation consistent, that is, it should not
havepartialeffectsofanyoperation.However,asweshallsee,thephysicalredo
processingintheredophaseoftherecoveryscheme,alongwithundoprocessing
using physical log records ensures that the parts of the database accessed by a
logicalundooperationareinanoperationconsistentstate,beforethelogicalundo
operationisperformed.
An operation is said to be idempotent if executing it several times in a row
gives the same result as executing it once. Operations such as inserting an entry
intoaB
+
-treemaynotbeidempotent,andtherecoveryalgorithmmusttherefore
make sure that an operation that has already been performed is not performed
again. On the other hand, a physical log record is idempotent, since the corre-
spondingdataitemwouldhavethesamevalueregardlessofwhetherthelogged
updateisexecutedone ormultipletimes.
16.7.3 Transaction Rollback With Logical Undo
Whenrollingbackatransaction T
i
,thelogisscannedbackwards,andlogrecords
correspondingto T
i
areprocessedas follows:
1. Physical log records encountered during the scan are handled as described
earlier,exceptthosethatareskippedasdescribedshortly.Incompletelogical
operations are undone using the physical log records generated by the
operation.
2. Completedlogicaloperations,identi?edbyoperation-endrecords,arerolled
backdifferently.Wheneverthesystem?ndsalogrecord<T
i
, O
j
,operation-
end, U>,ittakesspecialactions:
a. It rolls back the operation by using the undo information U in the
log record. It logs the updates performed during the rollback of the
operation just like updates performed when the operation was ?rst
executed.
At the end of the operation rollback, instead of generating a log
record <T
i
, O
j
, operation-end, U>, the database system generates a
log record<T
i
, O
j
,operation-abort>.
b. As the backward scan of the log continues, the system skips all log
recordsoftransactionT
i
untilit?ndsthelogrecord<T
i
, O
j
,operation-
begin>. After it ?nds the operation-begin log record, it processes log
records of transaction T
i
inthenormal manner again.
Observe that the system logs physical undo information for the updates
performed during rollback, instead of using a redo-only compensation log
records.Thisisbecauseacrashmayoccurwhilealogicalundoisinprogress,
16.7 EarlyLockReleaseandLogicalUndoOperations 747
andonrecoverythesystemhastocompletethelogicalundo;todoso,restart
recoverywillundothepartialeffectsoftheearlierundo,usingthephysical
undo information, and thenperformthe logicalundo again.
Observealsothatskippingoverphysicallogrecordswhentheoperation-
end log record is found during rollback ensures that the old values in the
physicallogrecordarenotusedforrollback,once theoperationcompletes.
3. Ifthesystem?ndsarecord<T
i
, O
j
,operation-abort>,itskipsallpreceding
records (including theoperation-endrecord for O
j
)untilit?ndstherecord
<T
i
, O
j
,operation-begin>.
Anoperation-abort log record would be found only if a transaction that
isbeingrolledbackhadbeenpartiallyrolledbackearlier.Recallthatlogical
operations may not be idempotent, and hence a logical undo operation
must not be performed multiple times. These preceding log records must
beskippedtopreventmultiplerollbackofthesameoperation,incasethere
hadbeenacrashduringanearlierrollback,andthetransactionhadalready
beenpartlyrolledback.
4. As before, when the <T
i
start> log record has been found, the transaction
rollback iscomplete,and the systemaddsarecord<T
i
abort> tothelog.
If a failure occurs while a logical operation is in progress, the operation-end
logrecordfortheoperationwillnotbefoundwhenthetransactionisrolledback.
However, for every update performed by the operation, undo information—in
the form of the old value in the physical log records—is available in the log. The
physical log records will be used to roll back the incomplete operation.
Now suppose an operation undo was in progress when the system crash
occurred, which could happen if a transaction was being rolled back when the
crash occurred. Then the physical log records written during operation undo
would be found, and the partial operation undo would itself be undone using
thesephysicallogrecords.Continuinginthebackwardscanofthelog,theoriginal
operation’s operation-end record would then be found, and the operation undo
would be executed again. Rolling back the partial effects of the earlier undo
operationusingthephysicallogrecordsbringsthedatabasetoaconsistentstate,
allowing thelogical undooperationtobeexecutedagain.
Figure 16.6 shows an example of a log generated by two transactions, which
add or subtract a value from a data item. Early lock release on the data item C
by transaction T
0
after operation O
1
completes allows transaction T
1
to update
the data item using O
2
, even before T
0
completes, but necessitates logical undo.
The logical undo operation needs to add or subtract a value from the data item,
insteadof restoringan oldvaluetothedataitem.
The annotations on the ?gure indicate that before an operation completes,
rollback can perform physical undo; after the operation completes and releases
lower-levellocks,theundomustbeperformedbysubtractingoraddingavalue,
instead of restoring the old value. In the example in the ?gure, T
0
rolls back
operation O
1
by adding 100 to C; on the other hand, for data item B, which was
not subject to early lock release, undo is performed physically. Observe that T
1
,
748 Chapter 16 RecoverySystem
Figure 16.6 Transaction rollback with logical undo operations.
whichhadperformedanupdateonC commits, and its update O
2
, which added
200toC andwasperformedbeforetheundoof O
1
,haspersistedeventhough O
1
has been undone.
Figure 16.7 shows an example of recovery from a crash, with logical undo
logging. In this example, operation T
1
was active and executing operation O
4
at the time of checkpoint. In the redo pass, the actions of O
4
that are after the
checkpoint log record are redone. At the time of crash, operation O
5
was being
executed by T
2
, but the operation was not complete. The undo-list contains T
1
and T
2
at the end of the redo pass. During the undo pass, the undo of operation
O
5
is carried out using the old value in the physical log record, setting C to
400;thisoperationisloggedusingaredo-onlylogrecord.Thestartrecordof T
2
is
encounterednext,resultingintheadditionof< T
2
abort>tothelog,andremoval
of T
2
from undo-list.
The next log record encountered is the operation-end record of O
4
;logical
undo is performedfor this operation by adding 300 to C, which is loggedphysi-
cally,andanoperation-abortlogrecordisaddedfor O
4
.Thephysicallogrecords
that were part of O
4
are skipped until the operation-begin log record for O
4
is
encountered. In this example, there are no other intervening log records, but in
general log records from other transactions may be found before we reach the
operation-begin log record; such log records should of course not be skipped
(unless they are part of a completedoperation for the corresponding transaction
and the algorithm skips those records). After the operation-begin log record is
16.7 EarlyLockReleaseandLogicalUndoOperations 749
            was
Figure 16.7 Failure recovery actions with logical undo operations
foundfor O
4
,aphysicallogrecordisfoundfor T
1
,whichisrolledbackphysically.
Finallythestartlogrecordfor T
1
isfound;thisresultsin< T
1
abort>beingadded
to the log, and T
1
being deleted from undo-list. At this point undo-list is empty,
and theundophase iscomplete.
16.7.4 Concurrency Issues in Logical Undo
As mentioned earlier, it is important that the lower-level locks acquired during
an operation are suf?cient to perform a subsequent logical undo of the oper-
ation; otherwise concurrent operations that execute during normal processing
maycauseproblemsintheundo-phase.Forexample,supposethelogicalundoof
operation O
1
oftransaction T
1
cancon?ictatthedataitemlevelwithaconcurrent
operation O
2
oftransaction T
2
,andO
1
completeswhile O
2
doesnot.Assumealso
that neither transaction had committed when the system crashed. The physical
update log records of O
2
may appear before and after the operation-end record
for O
1
,andduringrecoveryupdatesdoneduringthelogicalundoof O
1
mayget
fully or partially overwrittenby old values during the physical undo of O
2
.This
problemcannotoccurif O
1
hadobtainedallthelower-levellocksrequiredforthe
logical undoof O
1
,since then there cannot be such a concurrent O
2
.
750 Chapter 16 RecoverySystem
If both the original operation and its logical undo operation access a single
page (such operations are called physiological operations, and are discussed in
Section16.8),thelockingrequirementaboveismeteasily.Otherwisethedetailsof
the speci?c operation need to be considered when deciding on what lower-level
locks need to be obtained. For example, update operations on a B
+
-tree could
obtain a short-term lock on the root, to ensure that operations execute serially.
See the bibliographical notes for references on B
+
-tree concurrency control and
recovery exploiting logical undo logging. See the bibliographical notes also for
references to an alternative approach, called multi-level recovery, which relaxes
this locking requirement.
16.8 ARIES**
The state of the art in recovery methods is best illustrated by the ARIES recovery
method.TherecoverytechniquethatwedescribedinSection16.4,alongwiththe
logicalundologgingtechniquesdescribedinSection16.7,ismodeledafterARIES,
buthasbeensimpli?edsigni?cantlytobringoutkeyconceptsandmakeiteasier
tounderstand.Incontrast, ARIESusesanumberoftechniquestoreducethetime
taken for recovery, and to reduce the overhead of checkpointing. In particular,
ARIES is able to avoid redoing many logged operations that have already been
appliedandtoreducetheamountofinformationlogged.Thepricepaidisgreater
complexity;the bene?ts areworththeprice.
The major differences between ARIES and the recovery algorithm presented
earlierarethat ARIES:
1. Usesalogsequencenumber(LSN)toidentifylogrecords,andstoresLSNsin
databasepagestoidentifywhichoperationshavebeenappliedtoadatabase
page.
2. Supports physiological redo operations, which are physical in that the af-
fectedpageisphysicallyidenti?ed,but can belogical withinthepage.
Forinstance,thedeletionofarecordfromapagemayresultinmanyother
records in the page being shifted, if a slottedpage structure(Section10.5.2)
is used. With physical redo logging, all bytes of the page affected by the
shiftingofrecordsmustbelogged.Withphysiologicallogging,thedeletion
operation can be logged, resulting in a much smaller log record. Redo of
the deletion operation would delete the record and shift other records as
required.
3. Usesadirtypagetabletominimizeunnecessaryredosduringrecovery.As
mentionedearlier,dirtypagesarethosethathavebeenupdatedinmemory,
and the diskversionisnot up-to-date.
4. Uses a fuzzy-checkpointing scheme that records only information about
dirty pages and associated information and does not even require writing
16.8 ARIES** 751
ofdirtypagestodisk.It?ushesdirtypagesinthebackground,continuously,
insteadofwritingthemduringcheckpoints.
In the rest of this section, we provide an overview of ARIES. The bibliographical
noteslistreferencesthatprovideacompletedescriptionof ARIES.
16.8.1 Data Structures
EachlogrecordinARIEShasalogsequencenumber(LSN)thatuniquelyidenti?es
therecord.Thenumberisconceptuallyju st a logical identi?er whose value is
greaterforlogrecordsthatoccurlaterinthelog.Inpractice,the LSNisgenerated
in such a way that it can also be used to locate the log record on disk. Typically,
ARIESsplitsalogintomultiplelog?les,eachofwhichhasa?lenumber.Whena
log ?le grows to some limit, ARIES appends further log records to a new log ?le;
the new log ?le has a ?le number that is higher by 1 than the previous log ?le.
The LSN thenconsists of a?le number andan offsetwithinthe?le.
Each page alsomaintains an identi?ercalledthePageLSN. Wheneveran up-
dateoperation(whetherphysicalorphysiological)occursonapage,theoperation
stores the LSN of its log record in the PageLSN ?eld of the page. During the redo
phaseofrecovery,anylogrecordswith LSNlessthanorequaltothePageLSNofa
pageshouldnotbeexecutedonthepage,sincetheiractionsarealreadyre?ected
on the page. In combination with a scheme for recording PageLSNsaspartof
checkpointing,whichwepresentlater,ARIEScanavoidevenreadingmanypages
forwhichloggedoperationsarealreadyre?ectedondisk.Thereby,recoverytime
isreducedsigni?cantly.
The PageLSN is essential for ensuring idempotence in the presence of physi-
ological redo operations, since reapplying a physiological redo that has already
beenappliedtoapagecouldcause incorrectchangestoapage.
Pages should not be ?ushed to disk while an update is in progress, since
physiological operations cannot be redone on the partially updated state of the
pageondisk.Therefore, ARIESuseslatchesonbufferpagestopreventthemfrom
being written to disk while they are being updated. It releases the buffer page
latch only after the update is completed, and the log record for the update has
beenwrittentothe log.
Each log record also contains the LSN of the previous log record of the same
transaction.Thisvalue,storedinthePrevLSN?eld,permitslogrecordsofatrans-
action to be fetched backward, without reading the whole log. There are special
redo-only log records generated during transaction rollback, called compensa-
tion log records (CLRs) in ARIES. These serve the same purpose as the redo-only
log records in our earlierrecoveryscheme. In addition CLRs serve the role of the
operation-abort log records in our scheme. The CLRs have an extra ?eld, called
the UndoNextLSN,thatrecordstheLSN of the log that needs to be undone next,
when the transaction is being rolled back. This ?eld serves the same purpose as
the operation identi?er in the operation-abort log record in our earlier recovery
scheme,which helpstoskipoverlogrecordsthathavealreadybeenrolledback.
752 Chapter 16 RecoverySystem
Figure 16.8 Data structures used in ARIES.
The DirtyPageTable contains a list of pages that have been updated in the
databasebuffer.Foreachpage,itstoresthePageLSNanda?eldcalledtheRecLSN,
which helps identify log records that have been applied already to the version
of the page on disk. When a page is inserted into the DirtyPageTable (when it
is ?rst modi?ed in the buffer pool), the value of RecLSN is set to the current
end of log. Whenever the page is ?ushed to disk, the page is removed from the
DirtyPageTable.
A checkpoint log record contains the DirtyPageTable and a list of active
transactions. For each transaction, the checkpoint log record also notes LastLSN,
the LSN of the last log recordwritten by the transaction. A ?xed positionon disk
also notesthe LSN of thelast (complete)checkpoint logrecord.
Figure 16.8 illustrates some of the data structures used in ARIES.Thelog
records shown in the ?gure are pre?xed by their LSN;thesemaynotbeexplicitly
stored,butinferredfromthepositioninthelog,inanactualimplementation.The
dataitemidenti?erinalogrecordisshown intwoparts,forexample4894.1;the
?rst identi?es the page, and the second part identi?es a record within the page
(we assume a slotted page record organization within a page). Note that the log
is shown with newest records on top, since older log records, which are on disk,
are shown lower inthe ?gure.
16.8 ARIES** 753
Eachpage(whetherinthebufferorondisk)hasanassociatedPageLSN?eld.
Youcanverifythatthe LSNforthelastlogrecordthatupdatedpage4894is7567.
By comparing PageLSNs for the pages in the buffer with the PageLSNsforthe
corresponding pages in stable storage, you can observe that the DirtyPageTable
contains entries for all pages in the buffer that have been modi?ed since they
werefetchedfromstablestorage.TheRecLSNentryintheDirtyPageTablere?ects
the LSN at the end of the log when the page was added to DirtyPageTable, and
would begreaterthan orequalto thePageLSNforthatpageonstablestorage.
16.8.2 Recovery Algorithm
ARIES recoversfrom asystemcrashinthreepasses.
• Analysispass:Thispassdetermineswhichtransactionstoundo,whichpages
were dirty at the time of the crash, and the LSN from which the redo pass
shouldstart.
• Redopass: Thispassstarts fromapositiondeterminedduringanalysis, and
performs a redo, repeating history, to bring the database to a state it was in
beforethecrash.
• Undo pass: This pass rolls back all transactions that were incomplete at the
timeof crash.
16.8.2.1 AnalysisPass
The analysis pass ?nds the last complete checkpoint log record,and reads in the
DirtyPageTable from this record. It then sets RedoLSN to the minimum of the
RecLSNs of the pages in the DirtyPageTable. If there are no dirty pages, it sets
RedoLSN to the LSN of the checkpoint log record. The redo pass starts its scan
of the log from RedoLSN. All the log records earlier than this point have already
beenappliedtothedatabasepagesondisk.Theanalysispassinitiallysetsthelist
oftransactionstobeundone,undo-list,tothelistoftransactionsinthecheckpoint
log record. The analysis pass also reads from the checkpoint log record the LSNs
of thelast logrecordforeachtransaction inundo-list.
Theanalysispasscontinuesscanningforwardfromthecheckpoint.Whenever
it?ndsalogrecordforatransactionnotintheundo-list,itaddsthetransactionto
undo-list.Wheneverit?ndsatransactionendlogrecord,itdeletesthetransaction
from undo-list. All transactions left in undo-list at the end of analysis have to be
rolled back later, in the undo pass. The analysis pass also keeps track of the last
recordof eachtransactioninundo-list,which isusedintheundo pass.
TheanalysispassalsoupdatesDirtyPageTablewheneverit?ndsalogrecord
for an update on a page. If the page is not in DirtyPageTable, the analysis pass
adds it to DirtyPageTable, and sets the RecLSNofthepagetotheLSN of the log
record.
754 Chapter 16 RecoverySystem
16.8.2.2 RedoPass
Theredopassrepeatshistorybyreplayingeveryactionthatisnotalreadyre?ected
inthepageondisk.TheredopassscansthelogforwardfromRedoLSN.Whenever
it?nds an updatelog record,ittakesthisaction:
1. If the page is not in DirtyPageTable or the LSN of the update log record is
lessthantheRecLSNofthepageinDirtyPageTable,thentheredopassskips
the log record.
2. Otherwise the redo pass fetches the page from disk, and if the PageLSN is
lessthan the LSN ofthe logrecord,it redoesthelogrecord.
Note that if either of the tests is negative, then the effects of the log record
have already appeared on the page; otherwise the effects of the log record are
not re?ected on the page. Since ARIES allows non-idempotent physiological log
records, a log record should not be redone if its effect is already re?ected on the
page. If the ?rst test is negative, it is not even necessary to fetch the page from
disktocheck itsPageLSN.
16.8.2.3 UndoPassandTransactionRollback
The undo pass is relatively straightforward. It performs a single backward scan
of the log, undoing all transactions in undo-list. The undo pass examines only
logrecordsoftransactionsinundo-list;thelast LSN recorded during the analysis
pass isusedto?nd the lastlogrecordfor eachtransactioninundo-list.
Whenever an update log record is found, it is used to perform an undo
(whetherfortransactionrollbackduringnormalprocessing,orduringtherestart
undopass).TheundopassgeneratesaCLRcontainingtheundoactionperformed
(whichmustbephysiological).ItsetstheUndoNextLSNoftheCLRtothePrevLSN
valueof theupdatelogrecord.
If a CLR is found, its UndoNextLSN value indicates the LSN of the next log
record to be undone for that transaction; later log records for that transaction
have already been rolled back. For log records other than CLRs,thePrevLSN?eld
of the log record indicates the LSN of the next log record to be undone for that
transaction. The next log record to be processed at each stop in the undo pass is
the maximum,across alltransactions inundo-list,of nextlog record LSN.
Figure16.9illustratestherecoveryactionsperformedbyARIES,onanexample
log. We assume that the last completed checkpoint pointer on disk points to the
checkpoint log record with LSN 7568. The PrevLSN values in the log records are
shown using arrows inthe?gure,while theUndoNextLSN valueis shown using
adashedarrowfortheonecompensationlogrecord,with LSN7565,inthe?gure.
The analysis pass would start from LSN 7568, and when it is complete, RedoLSN
would be 7564. Thus, the redo pass must start at the log record with LSN 7564.
Note that this LSN is less than the LSN of the checkpoint log record, since the
ARIES checkpointing algorithm does not ?ush modi?ed pages to stable storage.
The DirtyPageTable at the end of analysis would include pages 4894, 7200 from
16.8 ARIES** 755
Figure 16.9 Recovery actions in ARIES.
thecheckpointlogrecord,and2390whichisupdatedbythelogrecordwith LSN
7570.Attheendoftheanalysispass,thelistoftransactionstobeundoneconsists
of only T
145
inthisexample.
Theredopassfortheaboveexamplestartsfrom LSN7564andperformsredo
of log records whose pages appear in DirtyPageTable. The undo pass needs to
undo only transaction T
145
, and hence starts from its LastLSN value 7567, and
continues backwards untiltherecord< T
145
start>isfound at LSN 7563.
16.8.3 Other Features
Among otherkeyfeaturesthat ARIESprovidesare:
• Nested top actions: ARIES allows the logging of operations that should not
beundoneevenifatransactiongetsrolledback;forexample,ifatransaction
allocates a page to a relation, even if the transaction is rolled back the page
allocation should not be undone since other transactions may have stored
records in the page. Such operations that should not be undone are called
nestedtopactions.Suchoperationscanbemodeledasoperationswhoseundo
actiondoesnothing.InARIES,suchoperationsareimplementedbycreatinga
756 Chapter 16 RecoverySystem
dummy CLR whose UndoNextLSN is set such that transaction rollback skips
thelog recordsgeneratedby the operation.
• Recovery independence: Some pages can be recovered independently from
others,sothattheycanbeusedevenwhileotherpagesarebeingrecovered.If
somepagesofadiskfail,theycanberecoveredwithoutstoppingtransaction
processingonother pages.
• Savepoints: Transactions can record savepoints, and can be rolled back par-
tially,uptoasavepoint.Thiscanbequiteusefulfordeadlockhandling,since
transactions can be rolledback up toapoint that permitsreleaseof required
locks,and thenrestartedfrom that point.
Programmerscan alsouse savepointstoundoa transaction partially,and
thencontinue execution;this approach can beuseful tohandlecertainkinds
oferrorsdetectedduringthetransaction execution.
• Fine-grainedlocking:TheARIESrecoveryalgorithmcanbeusedwithindex
concurrency-control algorithms that permit tuple-level locking on indices,
insteadofpage-levellocking,which improvesconcurrency signi?cantly.
• Recovery optimizations: The DirtyPageTable can be used to prefetch pages
during redo, instead of fetching a page only when the system ?nds a log
recordto beappliedto the page.Out-of-orderredoisalso possible:Redo can
be postponed on a page being fetched from disk, and performed when the
pageisfetched.Meanwhile,otherlog recordscan continue tobeprocessed.
In summary, the ARIES algorithm is a state-of-the-art recovery algorithm,
incorporatingavarietyofoptimizationsdesignedtoimproveconcurrency,reduce
loggingoverhead,and reducerecoverytime.
16.9 Remote Backup Systems
Traditional transaction-processing systems are centralized or client–server sys-
tems.Suchsystemsarevulnerabletoenvironmentaldisasterssuch as?re,?ood-
ing, or earthquakes. Increasingly, there is a need for transaction-processing sys-
temsthatcanfunctioninspiteofsystemfailuresorenvironmentaldisasters.Such
systemsmust providehighavailability; that is, the time for which the system is
unusable must beextremelysmall.
Wecanachievehighavailabilitybyperformingtransactionprocessingatone
site, called the primarysite, and having aremote backupsite where all the data
from the primary site are replicated. The remote backup site is sometimes also
called the secondary site. The remote site must be kept synchronized with the
primarysite,asupdatesareperformedattheprimary.Weachievesynchronization
bysendingalllogrecordsfromprimarysitetotheremotebackupsite.Theremote
backupsitemustbephysicallyseparatedfromtheprimary—forexample,wecan
locate it in a different state—so that a disaster at the primary does not damage
16.9 RemoteBackupSystems 757
log
records
backup network primary
Figure 16.10 Architecture of remote backup system.
the remote backup site. Figure 16.10 shows the architecture of a remote backup
system.
When the primary site fails, the remote backup site takes over processing.
First, however, it performs recovery, using its (perhaps outdated) copy of the
data from the primary, and the log records received from the primary. In effect,
the remote backup site is performing recovery actions that would have been
performedattheprimarysitewhenthelatterrecovered.Standardrecoveryalgo-
rithms,withminormodi?cations,canbeusedforrecoveryattheremotebackup
site.Oncerecoveryhasbeenperformed,theremotebackupsitestartsprocessing
transactions.
Availability is greatly increased over a single-site system, since the system
can recoverevenifalldataattheprimarysitearelost.
Severalissuesmustbe addressedindesigningaremotebackup system:
• Detection of failure. It is important for the remote backup system to detect
when the primary has failed. Failure of communication lines can fool the
remotebackupintobelievingthattheprimaryhasfailed.Toavoidthisprob-
lem, we maintain several communication links with independent modes of
failure between the primary and the remote backup. For example, several
independent network connections, including perhaps a modem connection
overatelephoneline,maybeused.Theseconnectionsmaybebackedupvia
manualinterventionbyoperators,whocancommunicateoverthetelephone
system.
• Transfer of control. When the primary fails, the backup site takes over pro-
cessing and becomes the new primary. When the original primary site re-
covers, it can either play the role of remote backup, or take over the role
of primary site again. In either case, the old primary must receive a log of
updatescarriedout by thebackup sitewhilethe oldprimarywas down.
The simplest way of transferring control is for the old primary to receive
redo logs from the old backup site, and to catch up with the updates by
applyingthemlocally.Theoldprimarycanthenact asaremotebackup site.
If control must be transferred back, the old backup site can pretend to have
failed,resultingintheold primarytaking over.
758 Chapter 16 RecoverySystem
• Time to recover. If the log at the remote backup grows large, recovery will
take a long time. The remote backup site can periodically process the redo
log recordsthat it has receivedand can performa checkpoint, so that earlier
parts of the log can be deleted. The delay before the remote backup takes
overcanbe signi?cantlyreducedasaresult.
A hot-spare con?guration can make takeover by the backup site almost
instantaneous. Inthis con?guration, theremotebackup sitecontinually pro-
cesses redo log records as they arrive, applying the updates locally. As soon
as the failure of the primary is detected, the backup site completes recov-
ery by rolling back incomplete transactions; it is then ready to process new
transactions.
• Time to commit. To ensure that the updates of a committed transaction are
durable, a transaction must not be declared committed until its log records
havereachedthebackupsite.Thisdelaycanresultinalongerwaittocommit
atransaction,andsomesystemsthereforepermitlowerdegreesofdurability.
Thedegreesofdurabilitycan beclassi?edas follows:
?
One-safe.Atransactioncommitsassoonasitscommitlogrecordiswrit-
tentostable storageatthe primarysite.
The problem with this scheme is that the updates of a committed
transaction may not have made it to the backup site, when the backup
sitetakesoverprocessing.Thus,theupdatesmayappeartobelost.When
the primary site recovers, the lost updates cannot be merged in directly,
sincetheupdatesmaycon?ictwithlaterupdatesperformedatthebackup
site. Thus, human intervention may be required to bring the database to
aconsistent state.
?
Two-very-safe. Atransactioncommitsassoonasitscommitlogrecordis
writtentostablestorageat theprimaryand thebackup site.
The problem with this scheme is that transaction processing cannot
proceedifeithertheprimaryorthebackupsiteisdown.Thus,availability
isactuallylessthaninthesingle-sitecase,althoughtheprobabilityofdata
loss ismuch less.
?
Two-safe. This scheme is the same as two-very-safe if both primary and
backup sites are active. If only the primary is active, the transaction is
allowed to commit as soon as its commit log record is written to stable
storageat theprimarysite.
Thisschemeprovidesbetteravailabilitythandoestwo-very-safe,while
avoiding the problem of lost transactions faced by the one-safe scheme.
It results in a slower commit than the one-safe scheme, but the bene?ts
generallyoutweighthecost.
Severalcommercial shared-disk systems providea levelof fault tolerance that is
intermediatebetweencentralizedandremotebackupsystems.Inthesecommer-
cial systems, the failure of a CPU does not result in system failure. Instead, other
CPUs take over, and they carry out recovery. Recovery actions include rollback
16.10 Summary 759
of transactions running on the failed CPU, and recovery of locks held by those
transactions. Since data are on a shared disk, there is no need for transfer of log
records. However, we should safeguard the data from disk failure by using, for
example,a RAIDdiskorganization.
An alternative way of achieving high availability is to use a distributed
database, with data replicated at more than one site. Transactions are then re-
quired to update all replicas of any data item that they update. We study dis-
tributeddatabases,including replication,inChapter19.
16.10 Summary
• A computer system, like any other mechanical or electrical device, is sub-
ject to failure. There are a variety of causes of such failure, including disk
crash, powerfailure,and softwareerrors.Ineachof thesecases,information
concerning thedatabase systemislost.
• Inadditiontosystemfailures,transactions may alsofail forvariousreasons,
suchas violationof integrityconstraints or deadlocks.
• Anintegralpartofadatabasesystemisarecoveryschemethatisresponsible
for the detection of failures and for the restoration of the database to a state
that existedbeforethe occurrence ofthefailure.
• The various types of storage in a computer are volatile storage, nonvolatile
storage, and stable storage. Data in volatile storage, such as in RAM,arelost
when the computer crashes. Data in nonvolatile storage, such as disk, are
not lost when the computer crashes, but may occasionally be lost because of
failuressuch as diskcrashes. Datainstablestorageareneverlost.
• Stable storage that must be accessible online is approximated with mirrored
disks,orotherformsofRAID,whichprovideredundantdatastorage.Of?ine,
or archival, stable storage may consist of multiple tape copies of data stored
inaphysically securelocation.
• Incaseoffailure,thestateofthedatabasesystemmaynolongerbeconsistent;
thatis,itmaynotre?ectastateoftheworldthatthedatabaseissupposedto
capture.Topreserveconsistency,we requirethat eachtransaction beatomic.
It is the responsibility of the recovery scheme to ensure the atomicity and
durabilityproperty.
• Inlog-basedschemes,allupdatesarerecordedon alog, which mustbe kept
in stable storage. A transaction is considered to have committed when its
last log record, which is the commit log record for the transaction, has been
outputtostablestorage.
• Log records contain old values and new values for all updated data items.
Thenewvaluesareusedincasetheupdatesneedtoberedoneafterasystem
crash. The old values are used to roll back the updates of the transaction if
760 Chapter 16 RecoverySystem
the transaction aborts during normal operation, as well as to roll back the
updates of the transaction in case the system crashed before the transaction
committed.
• In the deferred-modi?cationsscheme, during the execution of a transaction,
allthewriteoperationsaredeferreduntilthetransactionhasbeencommitted,
atwhichtimethesystemusestheinformationonthelogassociatedwiththe
transactioninexecutingthedeferredwrites.Withdeferredmodi?cation,log
recordsdonot needtocontain oldvaluesofupdateddataitems.
• Toreducetheoverheadofsearchingthelogandredoingtransactions,wecan
usecheckpointing techniques.
• Modern recovery algorithms are based on the concept of repeating history,
wherebyallactionstakenduringnormaloperation(sincethelastcompleted
checkpoint)arereplayedduringtheredopassofrecovery.Repeatinghistory
restores the system state to what it was at the time the last log record was
output to stable storage before the system crashed. Undo is then performed
from this state, by executing an undo pass that processes log records of
incompletetransactions inreverseorder.
• Undo of an incomplete transaction writes out special redo-only log records,
andanabortlogrecord.Afterthat,thetransactioncanbeconsideredtohave
completed,anditwill not beundone again.
• Transaction processing is based on a storage model in which main memory
holds a log buffer, a database buffer, and a system buffer. The system buffer
holdspagesof systemobject codeand localwork areasof transactions.
• Ef?cient implementation of a recovery scheme requires that the number of
writes to the database and to stable storage be minimized. Log records may
be kept in volatile log buffer initially, but must be written to stable storage
whenone of thefollowing conditions occurs:
?
Before the <T
i
commit> log record may be output to stable storage, all
log records pertaining to transaction T
i
must have been output to stable
storage.
?
Before a block of data in main memory is output to the database (in
nonvolatile storage), all log records pertaining to data in that block must
have beenoutput tostablestorage.
• Modern recovery techniques support high-concurrency locking techniques,
such as those used for B
+
-tree concurrency control. These techniques allow
early release of lower-level locks obtained by operations such as inserts or
deletes,which allows other such operations to be performed by other trans-
actions. After lower-level locks are released, physical undo is not possible,
andinsteadlogicalundo,suchasadeletiontoundoaninsertion,isrequired.
Transactions retain higher-level locks that ensure that concurrent transac-
ReviewTerms 761
tions cannot perform actions that could make logical undo of an operation
impossible.
• Torecoverfromfailuresthatresultinthelossofnonvolatilestorage,wemust
dump the entire contents of the database onto stable storage periodically—
say,onceperday.Ifafailureoccursthatresultsinthelossofphysicaldatabase
blocks, we use the most recent dump in restoring the database to a previous
consistentstate.Oncethisrestorationhasbeenaccomplished,weusethelog
tobring the databasesystemtothe mostrecentconsistent state.
• The ARIESrecoveryschemeisastate-of-the-artschemethatsupportsanum-
ber of features to provide greater concurrency, reduce logging overheads,
andminimizerecoverytime.Itisalsobasedonrepeatinghistory,andallows
logicalundooperations.Thescheme?ushespagesonacontinuousbasisand
does not need to ?ush all pages at the time of a checkpoint. It uses log se-
quencenumbers(LSNs)toimplementavarietyofoptimizationsthatreduce
thetimetakenforrecovery.
• Remotebackupsystemsprovideahighdegreeofavailability,allowingtrans-
action processing to continue even if the primary site is destroyed by a ?re,
?ood, or earthquake. Data and log records from a primary site are continu-
ally backed up to a remote backup site. If the primary site fails, the remote
backupsitetakesovertransactionprocessing,afterexecutingcertainrecovery
actions.
Review Terms
• Recoveryscheme
• Failureclassi?cation
?
Transaction failure
?
Logical error
?
Systemerror
?
Systemcrash
?
Data-transfer failure
• Fail-stopassumption
• Diskfailure
• Storagetypes
?
Volatilestorage
?
Nonvolatilestorage
?
Stablestorage
• Blocks
?
Physical blocks
?
Bufferblocks
• Diskbuffer
• Force-output
• Log-basedrecovery
• Log
• Logrecords
• Updatelog record
• Deferredmodi?cation
• Immediatemodi?cation
• Uncommittedmodi?cations
• Checkpoints
• Recoveryalgorithm
762 Chapter 16 RecoverySystem
• Restartrecovery
• Transaction rollback
• Physical undo
• Physical logging
• Transaction rollback
• Checkpoints
• Restartrecovery
• Redophase
• Undophase
• Repeatinghistory
• Buffermanagement
• Log-record buffering
• Write-aheadlogging(WAL)
• Logforce
• Database buffering
• Latches
• Operatingsystemandbuffer
management
• Fuzzy checkpointing
• Earlylock release
• Logicaloperations
• Logicallogging
• Logicalundo
• Lossof nonvolatilestorage
• Archivaldump
• Fuzzy dump
• ARIES
?
Logsequencenumber (LSN)
?
PageLSN
?
Physiological redo
?
Compensationlog record
(CLR)
?
DirtyPageTable
?
Checkpointlogrecord
?
Analysisphase
?
Redophase
?
Undophase
• Highavailability
• Remotebackup systems
?
Primarysite
?
Remotebackup site
?
Secondarysite
• Detectionoffailure
• Transfer of control
• Timetorecover
• Hot-sparecon?guration
• Timetocommit
?
One-safe
?
Two-very-safe
?
Two-safe
Practice Exercises
16.1 Explain why log records for transactions on the undo-list must be pro-
cessedinreverseorder,whereasredoisperformedinaforwarddirection.
16.2 Explain the purpose of the checkpoint mechanism. How often should
checkpointsbeperformed?Howdoesthefrequencyofcheckpointsaffect:
• Systemperformance whenno failureoccurs?
• Thetimeittakestorecoverfromasystemcrash?
• Thetimeittakestorecoverfromamedia(disk)failure?
PracticeExercises 763
16.3 Some database systems allow the administrator to choose between two
formsoflogging:normallogging,usedtorecoverfromsystemcrashes,and
archival logging,usedtorecoverfrommedia(disk)failure.Whencanalog
record be deleted, in each of these cases, using the recovery algorithm of
Section16.4?
16.4 Describe how to modify the recovery algorithm of Section 16.4 to imple-
ment savepoints, and to perform rollback to a savepoint. (Savepoints are
describedinSection16.8.3.)
16.5 Supposethe deferredmodi?cationtechniqueisusedinadatabase.
a. Is the old-value part of an update log record required any more?
Why or why not?
b. If old values are not stored in update log records, transaction undo
is clearly not feasible. How would the redo-phase of recovery have
tobe modi?edas aresult?
c. Deferredmodi?cationcanbeimplementedbykeepingupdateddata
items in local memory of transactions, and reading data items that
have not been updated directly from the database buffer. Suggest
howtoef?cientlyimplementadataitemread,ensuringthatatrans-
actionseesitsownupdates.
d. Whatproblemwouldarisewiththeabovetechnique,iftransactions
performalargenumber ofupdates?
16.6 Theshadow-pagingschemerequiresthepagetabletobecopied.Suppose
thepagetableisrepresentedasaB
+
-tree.
a. Suggest how to share as many nodes as possible between the new
copy and the shadow-copy of the B
+
-tree, assuming that updates
aremadeonlyto leafentries,withno insertions anddeletions.
b. Even with the above optimization, logging is much cheaper than a
shadow-copy scheme, for transactions that perform small updates.
Explainwhy.
16.7 Supposewe(incorrectly)modifytherecoveryalgorithmofSection16.4to
not log actions taken during transaction rollback. When recovering from
a system crash, transactions that were rolled back earlier would then be
included in undo-list, and rolled back again. Give an example to show
how actions taken during the undo phase of recovery could result in
an incorrect database state. (Hint: Consider a data item updated by an
abortedtransaction, and thenupdatedby atransactionthat commits.)
16.8 Disk space allocated to a ?le as a result of a transaction should not be
released even if the transaction is rolled back. Explain why, and explain
how ARIESensuresthat such actions arenot rolledback.
764 Chapter 16 RecoverySystem
16.9 Supposeatransactiondeletesarecord,andthefreespacegeneratedthus
is allocated to a record inserted by another transaction, even before the
?rst transactioncommits.
a. What problem can occur if the ?rst transaction needs to be rolled
back?
b. Wouldthisproblembeanissueifpage-levellockingisusedinstead
oftuple-levellocking?
c. Suggest how to solve this problem while supporting tuple-level
locking, by logging post-commit actions in special log records, and
executing them after commit. Make sure your scheme ensures that
suchactions areperformedexactlyonce.
16.10 Explain the reasons why recovery of interactive transactions is more dif-
?culttodealwiththanisrecoveryofbatchtransactions.Isthereasimple
way to deal with this dif?culty? (Hint: Consider an automatic teller ma-
chine transaction inwhich cashis withdrawn.)
16.11 Sometimesatransactionhastobeundoneafterithascommittedbecause
it was erroneously executed, for example because of erroneous input by
abank teller.
a. Give an example to show that using the normal transaction undo
mechanismtoundosuchatransactioncouldleadtoaninconsistent
state.
b. One way to handle this situation is to bring the whole database
to a state prior to the commit of the erroneous transaction (called
point-in-timerecovery).Transactionsthatcommittedlaterhavetheir
effectsrolledbackwiththisscheme.
Suggest a modi?cation to the recovery algorithm of Section 16.4
toimplementpoint-in-timerecoveryusingdatabasedumps.
c. Later nonerroneous transactions can be re-executed logically, if the
updates are available in the form of SQL but cannot be re-executed
using theirlogrecords.Why?
Exercises
16.12 Explain the difference between the three storage types—volatile, non-
volatile,andstable—in termsof I/Ocost.
16.13 Stablestoragecannot be implemented.
a. Explainwhy itcannot be.
b. Explainhow databasesystemsdealwiththis problem.
Exercises 765
16.14 Explain how the database may become inconsistent if some log records
pertaining to a block are not output to stable storage before the block is
outputtodisk.
16.15 Outlinethedrawbacksoftheno-stealandforcebuffermanagementpoli-
cies.
16.16 Physiological redo logging can reduce logging overheads signi?cantly,
especiallywithaslottedpagerecordorganization. Explainwhy.
16.17 Explain why logical undo logging is used widely, whereas logical redo
logging(other than physiologicalredologging)israrelyused.
16.18 Consider the log in Figure 16.5. Suppose there is a crash just before the
< T
0
abort>logrecordiswrittenout.Explainwhatwouldhappenduring
recovery.
16.19 Supposethereisatransactionthathasbeenrunningforaverylongtime,
buthasperformedveryfewupdates.
a. What effect would the transaction have on recovery time with the
recovery algorithm of Section 16.4, and with the ARIES recovery
algorithm.
b. Whateffectwouldthetransactionhaveondeletionofoldlogrecords?
16.20 ConsidertheloginFigure16.6.Supposethereisacrashduringrecovery,
justafterbeforetheoperationabortlogrecordiswrittenforoperation O
1
.
Explainwhat would happenwhenthe systemrecoversagain.
16.21 Compare log-based recovery with the shadow-copy scheme in terms of
theiroverheads,forthecasewhendataisbeingaddedtonewlyallocated
diskpages(inotherwords,thereisnooldvaluetoberestoredincasethe
transaction aborts).
16.22 Inthe ARIES recoveryalgorithm:
a. Ifatthebeginningoftheanalysispass,apageisnotinthecheckpoint
dirtypagetable,willweneedto applyany redorecordstoit?Why?
b. WhatisRecLSN,andhowisitusedtominimizeunnecessaryredos?
16.23 Explainthedifferencebetweenasystemcrashand a “disaster.”
16.24 Foreachofthefollowingrequirements,identifythebestchoice ofdegree
of durabilityinaremotebackup system:
a. Data loss must be avoided but some loss of availability may be
tolerated.
b. Transaction commit must be accomplished quickly, evenat the cost
of lossof somecommittedtransactions inadisaster.
c. Ahighdegreeofavailabilityanddurabilityisrequired,butalonger
running timeforthe transactioncommit protocol isacceptable.
766 Chapter 16 RecoverySystem
16.25 The Oracle database system uses undolog recordstoprovideasnapshot
view of the database, under snapshot-isolation. The snapshot view seen
by transaction T
i
re?ects updates of all transactions that had committed
when T
i
started, and the updates of T
i
; updates of all other transactions
arenot visibleto T
i
.
Describeascheme forbuffer handling wherebytransactions aregiven
a snapshot view of pages in the buffer. Include details of how to use the
logtogeneratethesnapshotview.Youcanassumethatoperationsaswell
as theirundo actions affectonly one page.
Bibliographical Notes
Gray and Reuter [1993] is an excellent textbook source of information about
recovery, including interesting implementation and historical details. Bernstein
and Goodman [1981] is an early textbook source of information on concurrency
control and recovery.
An overview of the recovery scheme of System R is presented by Gray et al.
[1981]. Tutorial and survey papers on various recovery techniques for database
systems include Gray [1978], Lindsay et al. [1980], and Verhofstad [1978]. The
conceptsoffuzzycheckpointingandfuzzydumpsaredescribedinLindsayetal.
[1980]. A comprehensive presentation of the principles of recovery is offered by
Haerderand Reuter[1983].
The state-of-the-art in recovery methods is best illustrated by the ARIES re-
coverymethod,describedinMohanetal.[1992]andMohan[1990b].Mohanand
Levine[1992]presentsARIESIM,anextensionofARIEStooptimizeB
+
-treeconcur-
rencycontrolandrecoveryusinglogicalundologging. ARIESanditsvariantsare
used in several database products, including IBM DB2 and Microsoft SQL Server.
RecoveryinOracleis describedinLahirietal.[2001].
SpecializedrecoverytechniquesforindexstructuresaredescribedinMohan
andLevine[1992]andMohan[1993];MohanandNarang[1994]describesrecov-
ery techniques for client–server architectures, while Mohan and Narang [1992]
describesrecoverytechniquesforparallel-databasearchitectures.
A generalized version of the theory of serializability, with short duration
lower-levellocksduringoperations,combinedwithlongerdurationhigher-level
locks, is described by Weikum [1991]. In Section 16.7.3, we saw the requirement
thatanoperationshouldacquirealllower-levellocksthatmaybeneededforthe
logicalundooftheoperation.Thisrequirementcan berelaxedby performingall
physical undo operations ?rst, before perfoming any logical undo operations. A
generalizedversionofthisidea,calledmulti-levelrecovery,presentedinWeikum
etal.[1990],allowsmultiplelevelsoflogicaloperations,withlevel-by-levelundo
passesduringrecovery.
Remote backup algorithms for disaster recovery are presented in King et al.
[1991] andPolyzois and Garcia-Molina [1994].
PART
5
SYSTEMARCHITECTURE
The architecture of a database system is greatly in?uenced by the underlying
computer system on which the database system runs. Database systems can
be centralized, where one server machine executes operations on the database.
Databasesystemscanalsobedesignedtoexploitparallelcomputerarchitectures.
Distributed databases span multiple geographically separated machines.
Chapter 17 ?rst outlines the architectures of database systems running on
server systems, which are used in centralized and client–server architectures.
The various processes that together implement the functionality of a database
are outlined here. The chapter then outlines parallel computer architectures, and
paralleldatabasearchitecturesdesignedfordifferenttypesofparallelcomputers.
Finally,thechapteroutlinesarchitecturalissuesinbuildingadistributeddatabase
system.
Chapter 18 describes how various actions of a database, in particular query
processing, can be implemented to exploit parallel processing.
Chapter 19 presents a number of issues that arise in a distributed database,
and describes how to deal with each issue. The issues include how to store data,
how to ensure atomicity of transactions that execute at multiple sites, how to
performconcurrencycontrol,andhowtoprovidehighavailabilityinthepresence
of failures.aCloud-baseddatastoragesystems,distributedqueryprocessing and
directory systems are also described in this chapter.
767
This page intentionally left blank 
CHAPTER
17
Database-System Architectures
The architecture of a database system is greatly in?uenced by the underlying
computer system on which it runs, in particular by such aspects of computer
architecture as networking, parallelism,and distribution:
• Networkingofcomputersallowssometaskstobeexecutedonaserversystem
and some tasks to be executed on client systems. This division of work has
ledto client–server database systems.
• Parallelprocessingwithinacomputersystemallowsdatabase-systemactivi-
tiestobespeededup,allowingfasterresponsetotransactions,aswellasmore
transactions per second. Queries can be processed in a way that exploits the
parallelismofferedbytheunderlyingcomputersystem.Theneedforparallel
queryprocessing has ledto parallel database systems.
• Distributing data across sites in an organization allows those data to reside
wheretheyaregeneratedormostneeded,butstilltobeaccessiblefromother
sites and from other departments. Keeping multiple copies of the database
acrossdifferentsitesalsoallowslargeorganizationstocontinuetheirdatabase
operationsevenwhenonesiteisaffectedbyanaturaldisaster,suchas?ood,
?re, or earthquake. Distributed database systems handle geographically or ad-
ministrativelydistributeddataspreadacross multipledatabase systems.
We study the architecture of database systems in this chapter, starting with
thetraditionalcentralizedsystems,and coveringclient–server,parallel,anddis-
tributeddatabase systems.
17.1 Centralized andClient–ServerArchitectures
Centralized database systems are those that run on a single computer system
and do not interact with other computer systems. Such database systems span
a range from single-user database systems running on personal computers to
high-performance database systems running on high-end server systems. Client
769
770 Chapter17 Database-SystemArchitectures
–server systems, on the other hand, have functionality split between a server
systemand multipleclientsystems.
17.1.1 Centralized Systems
Amodern,general-purposecomputersystemconsistsofone toafewprocessors
andanumberofdevicecontrollersthatareconnectedthroughacommonbusthat
providesaccess tosharedmemory(Figure17.1). The processorshave localcache
memoriesthatstorelocalcopiesofpartsofthememory,tospeedupaccesstodata.
Each processor may have several independentcores, each of which can execute
a separate instruction stream. Each device controller is in charge of a speci?c
type of device (for example, a disk drive, an audio device, or a video display).
The processors and the device controllers can execute concurrently, competing
for memory access. Cache memory reduces the contention for memory access,
sinceitreducesthenumberoftimesthattheprocessorneedstoaccesstheshared
memory.
Wedistinguishtwowaysinwhichcomputersareused:assingle-usersystems
andasmultiusersystems.Personalcomputersandworkstationsfallintothe?rst
category. A typicalsingle-usersystem isa desktopunit used by a single person,
usuallywithonlyoneprocessorandoneortwoharddisks,andusuallyonlyone
person using the machine at a time. A typical multiuser system, on the other
hand, has more disks and more memory and may have multiple processors. It
servesa largenumber of userswho are connected tothe systemremotely.
Database systems designed for use by single users usually do not provide
many of the facilities that a multiuserdatabase provides.In particular, they may
not support concurrency control, which is not required when only a single user
can generate updates. Provisions for crash recovery in such systems are either
absent or primitive—for example, they may consist of simply making a backup
of the database before any update. Some such systems do not support SQL,and
they provide a simpler query language, such as a variant of QBE. In contrast,
USB controller
keyboard printer mouse monitor
disks
graphics
adapter
disk
controller
memory
CPU
on-line
Figure17.1 A centralized computer system.
17.1 CentralizedandClient–ServerArchitectures 771
database systems designed for multiuser systems support the full transactional
featuresthat we have studiedearlier.
Althoughmostgeneral-purposecomputersystemsinusetodayhavemultiple
processors,theyhavecoarse-granularityparallelism,withonlyafewprocessors
(about two to four, typically), all sharing the main memory. Databases running
on such machines usually do not attempt to partition a single query among the
processors;instead,theyruneachqueryonasingleprocessor,allowingmultiple
queries to run concurrently. Thus, such systems support a higher throughput;
that is, they allow a greater number of transactions to run per second, although
individualtransactions do not run any faster.
Databasesdesignedforsingle-processormachinesalreadyprovidemultitask-
ing, allowing multiple processes to run on the same processor in a time-shared
manner,givingaviewtotheuserofmultipleprocessesrunninginparallel.Thus,
coarse-granularity parallel machines logically appear to be identical to single-
processor machines, and database systems designed for time-shared machines
can be easilyadaptedtorun on them.
In contrast, machines with ?ne-granularity parallelism have a large num-
ber of processors, and database systems running on such machines attempt to
parallelize single tasks (queries, for example) submitted by users. We study the
architecture of paralleldatabase systemsin Section17.3.
Parallelism is emerging as a critical issue in the future design of database
systems.Whereastodaythosecomputersystemswithmulticoreprocessorshave
only a few cores, future processors will have large numbers of cores.
1
As a re-
sult,paralleldatabasesystems,which once werespecializedsystemsrunningon
speciallydesignedhardware, willbecome the norm.
17.1.2 Client–ServerSystems
As personal computers became faster, more powerful, and cheaper, there was
a shift away from the centralized system architecture. Personal computers sup-
planted terminals connected to centralized systems. Correspondingly, personal
computers assumed the user-interface functionality that used to be handled di-
rectly by the centralized systems. As a result, centralized systems today act as
serversystemsthatsatisfyrequestsgeneratedby client systems.Figure17.2shows
the generalstructure of aclient–serversystem.
Functionalityprovidedbydatabasesystemscanbebroadlydividedintotwo
parts—the front end and the back end. The back end manages access structures,
queryevaluationand optimization,concurrency control, and recovery.The front
end of a database system consists of tools such as the SQL user interface, forms
interfaces, report generation tools, and data mining and analysis tools (see Fig-
ure 17.3). The interface between the front end and the back end is through SQL,
orthroughanapplicationprogram.
1
The reasons for this pertain to issues in computer architecture related to heat generation and power consumption.
Rather than make processors signi?cantly faster, computer architects are using advances in chip design to put more
coresonasingle chip,a trend likelyto continue for some time.
772 Chapter17 Database-SystemArchitectures
client client client client
server
network
Figure17.2 General structure of a client–server system.
Standardssuchas ODBCand JDBC,whichwesawinChapter3,weredeveloped
to interface clients with servers. Any client that uses the ODBC or JDBC interface
can connect toany serverthat providestheinterface.
Certain application programs, such as spreadsheets and statistical-analysis
packages, use the client–server interface directly to access data from a back-end
server.Ineffect,theyprovidefront endsspecializedfor particulartasks.
Systemsthatdealwithlargenumbersofusersadoptathree-tierarchitecture,
which we saw earlier in Figure 1.6 (Chapter 1), where the front end is a Web
browser that talks to an application server. The application server, in effect, acts
as a clientto the database server.
Some transaction-processing systems provide atransactionalremoteproce-
dure call interface to connect clients with a server. These calls appear like ordi-
naryprocedurecallstotheprogrammer,butalltheremoteprocedurecallsfroma
clientareenclosedinasingletransactionattheserverend.Thus,ifthetransaction
aborts, theservercan undothe effectsofthe individualremoteprocedurecalls.
17.2 ServerSystem Architectures
Serversystemscanbebroadlycategorizedastransactionserversanddataservers.
SQL user
interface
forms
interface
report
generation
tools
data mining 
and analysis
tools
SQL engine
front end
interface
(SQL  API)
back end
Figure17.3 Front-end and back-end functionality.
17.2 ServerSystemArchitectures 773
• Transaction-serversystems,alsocalledquery-serversystems,provideanin-
terface to which clients can send requests to perform an action, in response
to which they execute the action and send back resultsto the client. Usually,
clientmachinesshiptransactionstotheserversystems,wherethosetransac-
tions are executed, and results are shipped back to clients that are in charge
ofdisplayingthedata.Requestsmaybespeci?edbyusing SQL,orthrougha
specializedapplicationprograminterface.
• Data-server systems allow clients to interact with the servers by making
requests to read or update data, in units such as ?les or pages. For example,
?le servers provide a ?le-system interface where clients can create, update,
read, and delete ?les. Data servers for database systems offer much more
functionality; they support units of data—such as pages, tuples, or objects
—that are smaller than a ?le. They provide indexing facilities for data, and
providetransaction facilitiessothatthe dataareneverleftinaninconsistent
state ifa clientmachine or processfails.
Ofthese,thetransaction-serverarchitectureisbyfarthemorewidelyusedarchi-
tecture.Weshallelaborateonthetransaction-serveranddata-serverarchitectures
in Sections17.2.1and 17.2.2.
17.2.1 Transaction Servers
Atypicaltransaction-serversystemtodayconsistsofmultipleprocessesaccessing
data in shared memory, as in Figure 17.4. The processes that form part of the
database systeminclude:
• Serverprocesses:Theseareprocessesthatreceiveuserqueries(transactions),
executethem,andsendtheresultsback.Thequeriesmaybesubmittedtothe
serverprocessesfromauserinterface,orfromauserprocessrunningembed-
ded SQL,orviaJDBC, ODBC, orsimilarprotocols.Somedatabasesystemsuse
aseparateprocessforeachusersession,andafewuseasingledatabasepro-
cess for all user sessions, but with multiple threads so that multiple queries
can execute concurrently. (A thread is like a process, but multiple threads
execute as part of the same process, and all threads within a process run in
the same virtual-memory space. Multiple threads within a process can exe-
cute concurrently.) Many database systems use a hybrid architecture, with
multipleprocesses,each one running multiplethreads.
• Lockmanagerprocess:Thisprocessimplementslockmanagerfunctionality,
which includeslock grant, lock release,and deadlockdetection.
• Databasewriterprocess:Thereareoneormoreprocessesthatoutputmodi-
?ed bufferblocks back to diskon acontinuous basis.
• Log writer process: This process outputs log records from the log record
buffer to stable storage. Server processes simply add log records to the log
774 Chapter17 Database-SystemArchitectures
lock 
manager
process
lock table log bu?er
shared
memory 
database
 writer
process
log writer
process
checkpoint
process
process
monitor
process
server
process
server
process
user
process
user
process
server
process
user
process
ODBC JDBC
log disks data disks
query plan cache
bu?er pool
Figure17.4 Shared memory and process structure.
record buffer in shared memory, and if a log force is required, they request
the log writerprocesstooutput logrecords.
• Checkpointprocess: This processperformsperiodiccheckpoints.
• Process monitor process: This process monitors other processes, and if any
of them fails, it takes recovery actions for the process, such as aborting any
transaction being executed by the failed process, and then restarting the
process.
Thesharedmemorycontainsallshareddata,suchas:
• Bufferpool.
• Lock table.
• Log buffer, containing log records waiting to be output to the log on stable
storage.
17.2 ServerSystemArchitectures 775
• Cached query plans, which can be reused if the same query is submitted
again.
All database processes can access the data in shared memory. Since multiple
processes may read or perform updates on data structures in shared memory,
theremustbeamechanismtoensurethatadatastructureismodi?edbyat
most one process at a time, and no process is reading a data structure while it is
being written by others. Such mutual exclusion can be implemented by means
of operating system functions called semaphores. Alternative implementations,
with less overhead, use special atomic instructions supported by the computer
hardware; one type of atomic instruction tests a memory location and setsit to 1
atomically. Further implementation details of mutual exclusion can be found in
any standard operating system textbook. The mutual exclusion mechanisms are
also usedto implementlatches.
To avoid the overhead of message passing, in many database systems,
serverprocessesimplementlockingby directlyupdatingthe locktable (which is
in shared memory), instead of sending lock request messages to a lock manager
process. The lock request procedure executes the actions that the lock manager
process would take on getting a lock request. The actions on lock request and
releaseare likethose in Section15.1.4, but with two signi?cant differences:
• Sincemultipleserverprocessesmayaccesssharedmemory,mutualexclusion
mustbe ensuredon the lock table.
• If a lock cannot be obtained immediately because of a lock con?ict, the lock
request code may monitor the lock table to check when the lock has been
granted. The lock release code updates the lock table to note which process
has beengranted the lock.
To avoid repeated checks on the lock table, operating system semaphores
canbeusedbythelockrequestcodetowaitforalockgrantnoti?cation.The
lockreleasecodemustthenusethe semaphoremechanism tonotify waiting
transactions that theirlocks have been granted.
Evenifthesystemhandleslockrequeststhroughsharedmemory,itstillusesthe
lock manager processfordeadlockdetection.
17.2.2 DataServers
Data-serversystemsareusedinlocal-areanetworks,wherethereisahigh-speed
connectionbetweentheclientsandtheserver,theclientmachinesarecomparable
in processing power to the server machine, and the tasks to be executed are
computation intensive. In such an environment, it makes sense to ship data to
client machines, to perform all processing at the client machine (which may
take a while), and then to ship the data back to the server machine. Note that
this architecture requires full back-end functionality at the clients. Data-server
architectureshavebeenparticularlypopularinobject-orienteddatabasesystems
(Chapter22).
776 Chapter17 Database-SystemArchitectures
Interesting issues arise in such an architecture, since the time cost of com-
munication between the client and the server is high compared to that of a local
memoryreference(milliseconds,versuslessthan 100 nanoseconds):
• Page shipping versus item shipping. The unit of communication for data
canbeofcoarsegranularity,suchasapage,or?negranularity,suchasatuple
(or an object, in the context of object-oriented database systems). We use the
termitem toreferto both tuplesand objects.
If the unit of communication is a single item, the overhead of message
passingishighcomparedtotheamountofdatatransmitted.Instead,whenan
itemisrequested,itmakessensealsotosendbackotheritemsthatarelikely
to be used in the near future. Fetching items even before they are requested
iscalledprefetching.Pageshippingcanbeconsideredaformofprefetching
ifmultipleitemsresideonapage,sincealltheitemsinthepageareshipped
when a processdesirestoaccess a single itemin the page.
• Adaptive lock granularity. Locks are usually granted by the server for the
data items that it ships to the client machines. A disadvantage of page ship-
pingisthat clientmachines maybe grantedlocks of toocoarse agranularity
—alockonapageimplicitlylocksallitemscontainedinthepage.Evenifthe
clientisnotaccessingsomeitemsinthepage,ithasimplicitlyacquiredlocks
on all prefetched items. Other client machines that require locks on those
items may be blocked unnecessarily. Techniques for lockde-escalation have
been proposed where the server can request its clients to transfer back locks
on prefetched items. If the client machine does not need a prefetched item,
it can transfer locks on the item back to the server, and the locks can then be
allocated toother clients.
• Data caching. Data that are shipped to a client on behalf of a transaction
can becached at the client, even after the transaction completes, if suf?cient
storage space is available. Successive transactions at the same client may be
able to make use of the cached data. However,cache coherency is an issue:
Even if a transaction ?nds cached data, it must make sure that those data
are up to date, since they may have been updated by a different client after
they were cached. Thus, a message must still be exchanged with the server
to check validityofthe data, and toacquire a lock on the data.
• Lockcaching.Iftheuseofdataismostlypartitionedamongtheclients,with
clientsrarelyrequestingdatathatarealsorequestedbyotherclients,lockscan
alsobecachedattheclientmachine.Supposethataclient?ndsadataitemin
thecache,andthatitalso?ndsthelockrequiredforanaccesstothedataitem
in the cache. Then, the access can proceed without any communication with
the server. However, the server must keep track of cached locks; if a client
requestsalockfromtheserver,theservermustcallbackallcon?ictinglocks
on the data item from any other client machines that have cached the locks.
The task becomes more complicated when machine failures are taken into
account. This technique differs from lock de-escalation in that lock caching
takesplace across transactions; otherwise, the two techniquesare similar.
17.3 ParallelSystems 777
Thebibliographicalreferencesprovidemoreinformationaboutclient–server
database systems.
17.2.3 Cloud-Based Servers
Serversareusuallyownedbytheenterpriseprovidingtheservice,butthereisan
increasingtrendforserviceproviderstorelyatleastinpartuponserversthatare
owned by a “third party”that isneitherthe clientnor the serviceprovider.
One model for using third-party servers is to outsource the entire service
to another company that hosts the service on its own computers using its own
software. This allows the service provider to ignore most details of technology
and focus on the marketingof the service.
Another model for using third-party servers is cloud computing,inwhich
theserviceproviderrunsitsownsoftware,butrunsitoncomputersprovidedby
another company. Under this model, the third party does not provide any of the
application software; it provides only a collection of machines. These machines
arenot“real”machines,butrathersimulatedbysoftwarethatallowsasinglereal
computer to simulateseveralindependentcomputers. Suchsimulatedmachines
are called virtual machines. The service provider runs its software (possibly
including a database system) on these virtual machines. A major advantage of
cloud computing is that the service provider can add machines as needed to
meetdemandandreleasethemattimesoflightload.Thiscanprovetobehighly
cost-effectiveintermsof both moneyand energy.
Athirdmodelusesacloudcomputingserviceasadataserver;such cloud-based
data storage systems are covered in detail in Section 19.9. Database applications
using cloud-based storage may run on the same cloud (that is, the same set
of machines), or on another cloud. The bibliographical references provide more
information about cloud-computing systems.
17.3 ParallelSystems
ParallelsystemsimproveprocessingandI/Ospeedsbyusingmultipleprocessors
anddisksinparallel.Parallelmachinesarebecomingincreasinglycommon,mak-
ing the studyof paralleldatabase systemscorrespondinglymore important. The
drivingforcebehindparalleldatabasesystemsisthedemandsofapplicationsthat
have to query extremely large databases (of the order of terabytes—that is, 10
12
bytes)or that have to processan extremelylarge number of transactions persec-
ond(oftheorderofthousandsoftransactionspersecond).Centralizedandclient
–serverdatabase systemsare not powerful enough to handle such applications.
In parallel processing, many operations are performed simultaneously, as
opposedtoserialprocessing,inwhichthecomputationalstepsareperformedse-
quentially.Acoarse-grainparallelmachineconsistsofasmallnumberofpowerful
processors; a massively parallel or ?ne-grain parallel machine uses thousands
ofsmallerprocessors.Virtuallyallhigh-endmachinestodayoffersomedegreeof
coarse-grain parallelism:atleasttwo orfour processors.Massivelyparallelcom-
778 Chapter17 Database-SystemArchitectures
puterscanbedistinguishedfromthecoarse-grainparallelmachinesbythemuch
largerdegreeofparallelismthattheysupport.Parallelcomputerswithhundreds
of processorsand disksare available commercially.
There are two main measures of performance of a database system: (1)
throughput, the number of tasks that can be completed in a given time inter-
val, and (2)responsetime, the amount of time it takes to complete a single task
from the time it is submitted. A system that processes a large number of small
transactions can improve throughput by processing many transactions in paral-
lel.Asystemthatprocesseslargetransactionscanimproveresponsetimeaswell
as throughput by performingsubtasks of each transaction in parallel.
17.3.1 Speedup and Scaleup
Twoimportantissuesinstudyingparallelismarespeedupandscaleup.Running
agiventaskinlesstimebyincreasingthedegreeofparallelismiscalledspeedup.
Handling largertasks by increasingthe degreeof parallelismiscalledscaleup.
Consider a database application running on a parallel system with a certain
number of processors and disks. Now suppose that we increase the size of the
system by increasing the number of processors, disks, and other components of
the system. The goal is to process the task in time inversely proportional to the
number of processors and disks allocated. Suppose that the execution time of a
task on the larger machine is T
L
, and that the execution time of the same task on
thesmallermachineis T
S
.Thespeedupduetoparallelismisde?nedas T
S
/T
L
.The
parallel system is said to demonstratelinearspeedup if the speedup is N when
the larger system has N times the resources (processors, disk, and so on) of the
smaller system. If the speedup is less than N, the system is said to demonstrate
sublinearspeedup.Figure 17.5illustrateslinearand sublinear speedup.
Scaleuprelatestotheabilitytoprocesslargertasksinthesameamountoftime
byprovidingmoreresources.Let Qbeatask,andlet Q
N
beataskthatis Ntimes
bigger than Q. Suppose that the execution time of task Q on a given machine
linear speedup
sublinear speedup
resources
speed
Figure17.5 Speedup with increasing resources.
17.3 ParallelSystems 779
linear scaleup
sublinear scaleup
problem size
T
S
T
L
Figure17.6 Scaleup with increasing problem size and resources.
M
S
is T
S
, and the execution time of task Q
N
on a parallel machine M
L
,whichis
N times larger than M
S
,isT
L
. The scaleup is then de?ned as T
S
/T
L
. The parallel
system M
L
issaidtodemonstratelinearscaleupontask Qif T
L
= T
S
.IfT
L
> T
S
,
thesystemissaidtodemonstratesublinearscaleup.Figure17.6illustrateslinear
and sublinear scaleups (where the resources increase in proportion to problem
size).Therearetwokindsofscaleupthatarerelevantinparalleldatabasesystems,
dependingon how the size of the taskismeasured:
• Inbatchscaleup,thesizeofthedatabaseincreases,andthetasksarelargejobs
whoseruntimedependsonthesizeofthedatabase.Anexampleofsuchatask
is a scan of a relation whose size is proportional to the size of the database.
Thus,thesizeofthedatabaseisthemeasureofthesizeoftheproblem.Batch
scaleupalsoappliesinscienti?capplications,suchasexecutingaqueryatan
N-times?ner resolutionorperformingan N-timeslongersimulation.
• In transaction scaleup, the rate at which transactions are submitted to the
database increases and the size of the database increases proportionally to
the transaction rate. This kind of scaleup is what is relevant in transaction-
processing systems where the transactions are small updates—for example,
a deposit or withdrawal from an account—and transaction rates grow as
more accounts are created. Such transaction processing is especially well
adapted for parallel execution, since transactions can run concurrently and
independently on separate processors, and each transaction takes roughly
the same amount of time,evenif the database grows.
Scaleupisusuallythemoreimportantmetricformeasuringef?ciencyofpar-
allel database systems. The goal of parallelism in database systems is usually to
make sure that the database system can continue to perform at an acceptable
speed,evenas the size of the database and the number of transactions increases.
Increasing the capacity of the system by increasing the parallelism provides a
smoother path for growth for an enterprise than does replacing a centralized
780 Chapter17 Database-SystemArchitectures
system with a faster machine (even assuming that such a machine exists). How-
ever, we must also look at absolute performance numbers when using scaleup
measures; a machine that scales up linearly may perform worse than a machine
that scales less than linearly, simply because the latter machine is much faster to
startoff with.
Anumberoffactorsworkagainstef?cientparalleloperationandcandiminish
both speedupand scaleup.
• Start-up costs. There is a start-up cost associated with initiating a single
process. In a parallel operation consisting of thousands of processes, the
start-up time may overshadow the actual processing time, affecting speedup
adversely.
• Interference. Since processes executing in a parallel system often access
shared resources, a slowdown may result from the interference of each new
processasitcompeteswithexistingprocessesfor commonly heldresources,
suchasasystembus,orshareddisks,orevenlocks.Bothspeedupandscaleup
are affectedby this phenomenon.
• Skew. By breaking down a single task into a number of parallel steps, we
reduce the size of the average step. Nonetheless, the service time for the
single slowest step will determinethe service time for the task as a whole. It
is often dif?cult to divide a task into exactly equal-sized parts, and the way
thatthesizesaredistributedistherefore skewed.Forexample,ifataskofsize
100 is divided into 10 parts, and the division is skewed, there may be some
tasks of size less than 10 and some tasks of size more than 10; if even one
task happens to be of size 20, the speedup obtained by running the tasks in
parallelisonly ?ve,insteadof tenas wewould havehoped.
17.3.2 Interconnection Networks
Parallel systems consist of a set of components (processors, memory, and disks)
that can communicate with each other via an interconnection network.Fig-
ure 17.7 shows three commonly used typesofinterconnection networks:
• Bus.Allthesystemcomponentscansenddataonandreceivedatafromasin-
glecommunicationbus.ThistypeofinterconnectionisshowninFigure17.7a.
The bus could be an Ethernet or a parallel interconnect. Bus architectures
work well for small numbers of processors. However, they do not scale well
with increasing parallelism, since the bus can handle communication from
only one component at atime.
• Mesh.Thecomponentsarenodesinagrid,andeachcomponentconnectsto
allitsadjacentcomponentsinthegrid.Inatwo-dimensionalmesheachnode
connectstofouradjacentnodes,whileinathree-dimensionalmesheachnode
connects tosix adjacentnodes.Figure17.7b shows atwo-dimensional mesh.
17.3 ParallelSystems 781
110
111 011
101
100
000
(c) hypercube (b) mesh (a) bus
001
010
Figure17.7 Interconnection networks.
Nodes that are not directly connected can communicate with one another
by routing messages via a sequence of intermediate nodes that are directly
connected to one another. The number of communication links grows as the
number of components grows, and the communication capacity of a mesh
thereforescalesbetterwith increasingparallelism.
• Hypercube. The components are numbered in binary, and a component is
connected to another if the binary representations of their numbers differ
in exactly one bit. Thus, each of the n components is connected to log(n)
other components. Figure 17.7c shows a hypercube with eight nodes. In
a hypercube interconnection, a message from a component can reach any
other component by going through at most log(n) links. In contrast, in a
mesh architecture a component may be 2(
?
n ?1) links away from some of
the other components (or
?
n links away, if the mesh interconnection wraps
aroundattheedgesofthegrid).Thuscommunicationdelaysinahypercube
are signi?cantly lower than in a mesh.
17.3.3 ParallelDatabase Architectures
There are several architectural models for parallel machines. Among the most
prominent ones are those in Figure 17.8 (in the ?gure, M denotes memory, P
denotesaprocessor, and disksare shown ascylinders):
• Sharedmemory.Alltheprocessorsshareacommonmemory(Figure17.8a).
• Shareddisk. All the processors share a common set of disks (Figure 17.8b).
Shared-disksystemsaresometimescalledclusters.
• Sharednothing. The processors share neither a common memory nor com-
mon disk(Figure17.8c).
• Hierarchical. This model is a hybrid of the preceding three architectures
(Figure17.8d).
In Sections17.3.3.1through 17.3.3.4,we elaborate on each of these models.
782 Chapter17 Database-SystemArchitectures
P
P
M
P
P
P
M M M
P
P
P
P
P
P
P
P
P
P
P
P
P
P
P
(a) shared memory
P
P
P
P
(c) shared nothing (d) hierarchical
P M
P
P
P
P
(b) shared disk
P M
P M
P M
M
M
M
M P
M
M
Figure17.8 Parallel database architectures.
Techniques used to speed up transaction processing on data-server systems,
such as data and lock caching and lock de-escalation, outlined in Section 17.2.2,
can also be used in shared-disk parallel databases as well as in shared-nothing
parallel databases. In fact, they are very important for ef?cient transaction pro-
cessing insuch systems.
17.3.3.1 SharedMemory
Inashared-memoryarchitecture,theprocessorsanddiskshaveaccesstoacom-
mon memory, typically via a bus or through an interconnection network. The
bene?t of shared memory is extremelyef?cient communication between proces-
sors—data in shared memory can be accessed by any processor without being
moved with software. A processor can send messages to other processors much
fasterbyusingmemorywrites(whichusuallytakelessthanamicrosecond)than
by sending a message through a communication mechanism. The downside of
shared-memory machines is that the architecture is not scalable beyond 32 or 64
processorsbecause the bus or the interconnection network becomesa bottleneck
(since itis sharedby allprocessors).Addingmoreprocessorsdoesnot help after
a point, since the processors will spend most of their time waiting for their turn
on the bus toaccess memory.
Shared-memoryarchitecturesusuallyhavelargememorycachesateachpro-
cessor, so that referencing of the shared memory is avoided whenever possible.
17.3 ParallelSystems 783
However,atleastsomeofthedatawillnotbeinthecache,andaccesseswillhave
to go to the shared memory. Moreover, the caches need to be kept coherent; that
is,ifaprocessorperformsawritetoamemorylocation, the datainthatmemory
location should be either updated at or removed from any processor where the
data are cached. Maintaining cache coherency becomes an increasing overhead
withincreasingnumbersofprocessors.Consequently,shared-memorymachines
are not capable of scaling up beyond a point; current shared-memory machines
cannot supportmore than 64 processors.
17.3.3.2 SharedDisk
In the shared-disk model, all processors can access all disks directly via an in-
terconnectionnetwork,buttheprocessorshaveprivatememories.Therearetwo
advantages of this architecture over a shared-memory architecture. First, since
eachprocessorhasitsownmemory,thememorybusisnotabottleneck.Second,
itoffersacheapwaytoprovideadegreeoffaulttolerance: If a processor (or its
memory) fails, the other processors can take over its tasks, since the database is
resident on disks that are accessible from all processors. We can make the disk
subsystemitselffaulttolerantbyusinga RAIDarchitecture,asdescribedinChap-
ter10. The shared-diskarchitecture has found acceptance in many applications.
The main problem with a shared-disk system is again scalability. Although
the memory bus is no longer a bottleneck, the interconnection to the disk sub-
systemis now a bottleneck; itis particularly so in a situation where the database
makesalargenumberofaccessestodisks.Comparedtoshared-memorysystems,
shared-disk systems can scale to a somewhat larger number of processors, but
communication across processors is slower (up to a few milliseconds in the ab-
senceofspecial-purposehardwareforcommunication),sinceithastogothrough
a communication network.
17.3.3.3 SharedNothing
In a shared-nothing system, each node of the machine consists of a processor,
memory, and one or more disks. The processors at one node may communicate
withanotherprocessoratanothernodebyahigh-speedinterconnectionnetwork.
A node functions as the server for the data on the disk or disks that the node
owns. Since local disk references are serviced by local disks at each processor,
the shared-nothing model overcomes the disadvantage of requiring all I/O to go
throughasingleinterconnectionnetwork;onlyqueries,accessestononlocaldisks,
and result relations pass through the network. Moreover, the interconnection
networksfor shared-nothing systemsareusuallydesignedtobe scalable, so that
their transmission capacity increases as more nodes are added. Consequently,
shared-nothing architectures are more scalable and can easily support a large
number of processors. The main drawbacks of shared-nothing systems are the
costs of communication and of nonlocal disk access, which are higher than in a
shared-memoryorshared-diskarchitecturesincesendingdatainvolvessoftware
interaction atboth ends.
784 Chapter17 Database-SystemArchitectures
17.3.3.4 Hierarchical
The hierarchical architecture combines the characteristics of shared-memory,
shared-disk, and shared-nothing architectures. At the top level, the system con-
sistsofnodesthatareconnectedbyaninterconnectionnetworkanddonotshare
disks or memory with one another. Thus, the top level is a shared-nothing ar-
chitecture. Each node of the system could actually be a shared-memory system
with a few processors. Alternatively, each node could be a shared-disk system,
andeachofthesystemssharingasetofdiskscouldbeashared-memorysystem.
Thus, a system could be built as a hierarchy, with shared-memory architecture
with a few processors at the base, and a shared-nothing architecture at the top,
with possibly a shared-disk architecture in the middle. Figure 17.8d illustrates
a hierarchical architecture with shared-memory nodes connected together in a
shared-nothingarchitecture.Commercialparalleldatabasesystemstodayrunon
severalof these architectures.
Attemptstoreducethecomplexityofprogrammingsuchsystemshaveyielded
distributedvirtual-memoryarchitectures,wherelogicallythereisasingleshared
memory, but physically there are multiple disjoint memory systems; the virtual-
memory-mapping hardware, coupled with system software, allows each pro-
cessor to view the disjoint memories as a single virtual memory. Since access
speedsdiffer,dependingon whether the page is available locally or not, such an
architecture isalso referredto asanonuniformmemoryarchitecture (NUMA).
17.4 DistributedSystems
In a distributed database system, the database is stored on several computers.
The computers in a distributed system communicate with one another through
various communication media, such as high-speed private networks or the In-
ternet.Theydonot share mainmemoryor disks.The computersina distributed
systemmayvaryinsizeandfunction,rangingfromworkstationsuptomainframe
systems.
The computers in a distributed system are referred to by a number of dif-
ferent names, such as sites or nodes, depending on the context in which they
are mentioned. We mainly use the termsite, to emphasize the physical distribu-
tion of these systems. The general structure of a distributed system appears in
Figure 17.9.
The main differences between shared-nothing parallel databases and dis-
tributed databases are that distributed databases are typically geographically
separated, are separately administered, and have a slower interconnection. An-
other major difference is that, in a distributed database system, we differentiate
between local and global transactions. A local transaction is one that accesses
dataonlyfromsiteswherethetransactionwasinitiated.Aglobaltransaction,on
the other hand, is one that either accesses data in a site different from the one at
which the transactionwas initiated,or accessesdatainseveraldifferentsites.
17.4 DistributedSystems 785
site A site C
site B
communication
via network
network
Figure 17.9 A distributed system.
Thereare severalreasonsfor buildingdistributeddatabase systems,including
sharingofdata,autonomy,andavailability.
  Sharingdata. The majoradvantageinbuildinga distributeddatabase system
ist h e provision of an environment where users at one site maybea ble to
access the datare sidingatother sites. Forinstance,ina distributeduniversity
system,where each campus storesdatare latedto thatc ampus, it isp ossible
for a user in one campus toa ccess datainano ther campus. Without this
capability, the transferofstudentrecords fromone campus toano ther campus
would have tore sortto somee xternalmechanism thatwould couplee xisting
systems.
  Autonomy. The primary advantageo fs h aringd ata by means ofd ata dis-
tribution ist h at each sitei s able tore tain a degree ofc ontrol over data that
are storedl ocally. Inacentralized system, the databasea dministrator oft h e
centrals ite controls the database.Inadistributed system, there is a global
databasea dministrator responsible for theen tire system. A part oft h esere -
sponsibilities isd elegatedt o the locald atabasea dministrator for each site.
Depending on the designo ft h e distributedd atabase system, each adminis-
trator mayhavea differentdegreeoflocalautonomy.The possibilityoflocal
autonomyisoftena majoradvantageofdistributeddatabases.
  Availability. Ifone site failsina distributed system, there mainingsitesmay
bea ble to continueo perating. In particular, ifd atai tems are replicated in
severalsites,a transactionneedinga particular datai temmay?ndt hatitem
in any ofs everals ites. Thus, the failure of a site does not necessarily imply
the shutdownoft he system.
786 Chapter17 Database-SystemArchitectures
The failure of one site must be detected by the system, and appropriate
actionmaybeneededtorecoverfromthefailure.Thesystemmustnolonger
use the services of the failed site. Finally, when the failed site recovers or is
repaired, mechanisms must be available to integrate it smoothly back into
the system.
Althoughrecoveryfromfailureismorecomplexindistributedsystemsthan
incentralizedsystems,theabilityofmostofthesystemtocontinuetooperate
despite the failure of one site results in increased availability. Availability is
crucial for database systems used for real-time applications. Loss of access
to data by, for example, an airline may result in the loss of potential ticket
buyersto competitors.
17.4.1 AnExampleofaDistributed Database
Consider a banking system consisting of four branches in four different cities.
Eachbranchhasitsowncomputer,withadatabaseofalltheaccountsmaintained
at that branch. Each such installation is thus a site. There also exists one single
site that maintains information about all the branches of the bank.
To illustratethe difference between the two typesof transactions—local and
global—at the sites, consider a transaction to add $50 to account number A-177
locatedattheValleyviewbranch.IfthetransactionwasinitiatedattheValleyview
branch,thenitisconsideredlocal;otherwise,itisconsideredglobal.Atransaction
to transfer $50 from account A-177 to account A-305, which is located at the
Hillside branch, is a global transaction, since accounts in two different sites are
accessed as a resultof itsexecution.
In an ideal distributed database system, the sites would share a common
global schema (although some relations may be stored only at some sites), all
sites would run the same distributed database-management software, and the
sites would be aware of each other’s existence. If a distributed database is built
from scratch, it would indeed be possible to achieve the above goals. However,
in reality a distributed database has to be constructed by linking together mul-
tiple already-existing database systems, each with its own schema and possibly
running different database-management software. Such systems are sometimes
called multidatabase systems or heterogeneous distributed database systems.
WediscussthesesystemsinSection19.8,whereweshowhowtoachieveadegree
of global control despitethe heterogeneityofthe component systems.
17.4.2 Implementation Issues
Atomicityoftransactionsisanimportantissueinbuildingadistributeddatabase
system. If a transaction runs across two sites, unless the system designers are
careful,itmaycommitatonesiteandabortatanother,leadingtoaninconsistent
state. Transaction commit protocols ensure such a situation cannot arise. The
two-phase commit protocol (2PC) isthe mostwidelyusedof these protocols.
17.4 DistributedSystems 787
The basic idea behind 2PC is for each site to execute the transaction until it
entersthepartiallycommittedstate,andthenleavethecommitdecisiontoasin-
gle coordinator site; the transaction is said to be in the ready state at a site at this
point. The coordinator decides to commit the transaction only if the transaction
reachesthereadystateateverysitewhereitexecuted;otherwise(for example,if
thetransactionabortsatanysite),thecoordinatordecidestoabortthetransaction.
Every site where the transaction executed must follow the decision of the coor-
dinator. If a site fails when a transaction is in ready state, when the site recovers
from failure it should be in a position to either commit or abort the transaction,
depending on the decision of the coordinator. The 2PC protocol is described in
detailin Section19.4.1.
Concurrency control isanother issuein adistributeddatabase. Since a trans-
actionmayaccessdataitemsatseveralsites,transactionmanagersatseveralsites
mayneedtocoordinatetoimplementconcurrencycontrol.Iflockingisused,lock-
ingcanbeperformedlocallyatthesitescontainingaccesseddataitems,butthere
is also a possibility of deadlock involving transactions originating at multiple
sites. Therefore deadlock detection needs to be carried out across multiple sites.
Failuresare more common in distributedsystemssince not only may computers
fail, but communication links may also fail. Replication of data items, which is
thekeytothecontinuedfunctioningofdistributeddatabaseswhenfailuresoccur,
furthercomplicatesconcurrencycontrol.Section19.5providesdetailedcoverage
of concurrency control in distributeddatabases.
The standard transaction models, based on multiple actions carried out by a
single program unit, are often inappropriate for carrying out tasks that cross the
boundariesofdatabasesthatcannotorwillnotcooperatetoimplementprotocols
such as 2PC. Alternative approaches, based on persistent messaging for commu-
nication, are generally used for such tasks; persistent messaging is discussed in
Section19.4.3.
When the tasks to be carried out are complex, involving multiple databases
and/or multiple interactions with humans, coordination of the tasks and en-
suring transaction properties for the tasks become more complicated. Work?ow
management systems are systems designed to help with carrying out such tasks,
and are describedin Section26.2.
In case an organization has to choose between a distributed architecture and
a centralized architecture for implementing an application, the system architect
must balance the advantages against the disadvantages of distribution of data.
Wehavealreadyseentheadvantagesofusingdistributeddatabases.Theprimary
disadvantage of distributed database systems is the added complexity required
to ensure proper coordination among the sites. This increased complexity takes
variousforms:
• Software-development cost. It is more dif?cult to implement a distributed
database system;thus, itismore costly.
• Greater potential for bugs. Since the sites that constitute the distributed
systemoperateinparallel,itishardertoensurethecorrectnessofalgorithms,
788 Chapter17 Database-SystemArchitectures
especiallyoperationduringfailuresofpartofthesystem,andrecoveryfrom
failures.Thepotentialexistsforextremelysubtlebugs.
  Increased processing overhead. The exchange of messages and the addi-
tional computation required to achieve intersite coordination are a form of
overheadthatdoesnotariseincentralizedsystems.
There are several approaches to distributed database design, ranging from
fullydistributeddesignstoonesthatincludealargedegreeofcentralization.We
studytheminChapter19.
17.5 Network Types
Distributed databases and client–server systems are built around communica-
tion networks. There are basically two types of networks: local-area networks
and wide-area networks. The main difference between the two is the way in
whichtheyaredistributedgeographically.Inlocal-areanetworks,processorsare
distributedoversmallgeographicalareas,suchasasinglebuildingoranumber
of adjacent buildings. In wide-area networks, on the other hand, a number of
autonomous processors are distributed over a large geographical area (such as
the United States or the entire world). These differences imply major variations
in the speed and reliability of the communication network, and are re?ected in
thedistributedoperating-systemdesign.
printer laptop file server
workstation workstation workstation
gateway
application server
Figure 17.10 Local-area network.
17.5 NetworkTypes 789
17.5.1 Local-AreaNetworks
Local-areanetworks(LANs)(Figure17.10)emergedintheearly1970sasawayfor
computerstocommunicateandtosharedatawithoneanother.Peoplerecognized
that, for many enterprises, numerous small computers, each with its own self-
contained applications, are more economical than a single large system. Because
each small computer is likely to need access to a full complement of peripheral
devices (such as disks and printers), and because some form of data sharing is
likely to occur in a single enterprise, it was a natural step to connect these small
systemsintoa network.
LANsaregenerallyusedinanof?ceenvironment.Allthesitesinsuchsystems
areclose toone another, sothecommunication linkstendtohaveahigherspeed
and lowererrorratethan dotheircounterpartsinwide-areanetworks.The most
common linksinalocal-areanetworkaretwistedpair,coaxialcable, ?beroptics,
and wireless connections. Communication speeds range from tens of megabits
per second (for wireless local-area networks), to 1 gigabit per second for Gigabit
Ethernet.The mostrecentEthernetstandard is10-gigabit Ethernet.
Astorage-areanetwork (SAN) is a special type of high-speed local-area net-
work designed to connect large banks of storage devices (disks) to computers
that use the data (seeFigure 17.11).
Thus storage-area networks help build large-scale shared-disk systems.The
motivation for using storage-area networks to connect multiple computers to
large banks of storage devices is essentially the same as that for shared-disk
databases, namely:
• Scalabilityby addingmore computers.
• High availability,since dataare stillaccessible evenifa computerfails.
LAN/WAN
storage
array
storage
array
data-processing
center
Web content
provider
server
client
client
client
server
tape
library
SAN
Figure17.11 Storage-area network.
790 Chapter17 Database-SystemArchitectures
RAIDorganizationsareusedinthestoragedevicestoensurehighavailability
ofthedata,permittingprocessingtocontinueevenifindividualdisksfail.Storage-
areanetworksareusuallybuiltwithredundancy,suchasmultiplepathsbetween
nodes, so if a component such as a link or a connection to the network fails, the
network continues to function.
17.5.2 Wide-AreaNetworks
Wide-areanetworks(WANs)emergedinthelate1960s,mainlyasanacademicre-
searchprojecttoprovideef?cientcommunicationamongsites,allowinghardware
and software to be shared conveniently and economically by a wide community
ofusers.Systemsthatallowedremoteterminalstobeconnectedtoacentralcom-
puter via telephone lines were developed in the early 1960s, but they were not
true WANs.The?rst WAN tobedesignedanddevelopedwasthe Arpanet.Workon
theArpanetbeganin1968.TheArpanethasgrownfromafour-siteexperimental
networktoaworldwidenetworkofnetworks,theInternet,comprisinghundreds
ofmillionsofcomputersystems.TypicallinksontheInternetare?ber-opticlines
and, sometimes,satellitechannels. Data rates for wide-arealinks typically range
fromafewmegabitspersecondtohundredsofgigabitspersecond.Thelastlink,
toendusersites,hastraditionallybeenthe slowestlink,usingsuch technologies
as digital subscriber line(DSL)technology(supportingafewmegabitspersecond)
or dial-up modem connections over land-based telephone lines (supporting up
to 56 kilobits per second). Today, the last link is typically a cable modem or ?ber
optic connection (each supporting tens of megabits per second), or a wireless
connection supporting severalmegabits persecond.
Inadditiontolimitsondatarates,communicationinaWAN mustalsocontend
withsigni?cantlatency:amessagemaytakeuptoafewhundredmillisecondsto
bedeliveredacrosstheworld,bothduetospeedoflightdelays,andduetoqueu-
ingdelaysatanumberofroutersinthepathofthemessage.Applicationswhose
dataandcomputingresourcesaredistributedgeographicallyhavetobecarefully
designedto ensurelatency doesnot affectsystemperformance excessively.
WANs can be classi?ed into two types:
• Indiscontinuousconnection WANs, such as those based on mobile wireless
connections, hosts are connected tothe networkonly part ofthe time.
• In continuous connection WANs, such as the wired Internet, hosts are con-
nectedtothe network atalltimes.
Networksthatarenotcontinuouslyconnectedtypicallydonotallowtransac-
tionsacrosssites,butmaykeeplocalcopiesofremotedata,andrefreshthecopies
periodically(everynight,forinstance).Forapplicationswhereconsistencyisnot
critical, such as sharing of documents, groupware systems such as Lotus Notes
allow updates of remote data to be made locally, and the updates are then prop-
agated back to the remote site periodically. There is a potential for con?icting
updatesatdifferentsites,con?ictsthathavetobedetectedandresolved.Amech-
17.6 Summary 791
anism for detecting con?icting updates is described later, in Section 25.5.4; the
resolutionmechanismforcon?ictingupdatesis,however,applicationdependent.
17.6 Summary
• Centralized database systems run entirely on a single computer. With the
growthofpersonalcomputersandlocal-areanetworking,thedatabasefront-
end functionality has moved increasingly to clients, with server systems
providingthe back-end functionality. Client–serverinterfaceprotocolshave
helpedthe growth of client–serverdatabase systems.
• Servers can be either transaction servers or data servers, although the use
of transaction servers greatly exceeds the use of data servers for providing
database services.
?
Transactionservershavemultipleprocesses,possiblyrunningonmultiple
processors. So that these processes have access to common data, such as
thedatabasebuffer,systemsstoresuchdatainsharedmemory.Inaddition
toprocessesthathandlequeries,therearesystemprocessesthatcarryout
tasks such as lock and log managementand checkpointing.
?
Data-server systems supply raw data to clients. Such systems strive to
minimizecommunicationbetweenclientsandserversbycachingdataand
locks at the clients. Paralleldatabase systemsuse similaroptimizations.
• Parallel database systems consist of multiple processors and multiple disks
connected by a fast interconnection network. Speedup measures how much
wecanincreaseprocessingspeedbyincreasingparallelismforasingletrans-
action. Scaleup measures how well we can handle an increased number of
transactions by increasing parallelism. Interference, skew, and start-up costs
act asbarriersto gettingidealspeedupand scaleup.
• Paralleldatabasearchitecturesincludetheshared-memory,shared-disk,share-
d-nothing, and hierarchical architectures. These architectures have different
trade-offsof scalability versuscommunication speed.
• Adistributeddatabasesystemisacollectionofpartiallyindependentdatabase
systemsthat(ideally)shareacommonschema,andcoordinateprocessingof
transactions that access nonlocal data. The systems communicate with one
another through a communication network.
• Local-areanetworksconnectnodesthataredistributedoversmallgeograph-
ical areas, such as a single building or a few adjacent buildings. Wide-area
networks connect nodes spread over a large geographical area. The Internet
isthemost extensivelyusedwide-areanetwork today.
• Storage-area networks are a special type of local-area network designed to
provide fast interconnection between large banks of storage devices and
multiplecomputers.
792 Chapter17 Database-SystemArchitectures
ReviewTerms
• Centralizedsystems
• Serversystems
• Coarse-granularityparallelism
• Fine-granularityparallelism
• Database processstructure
• Mutualexclusion
• Thread
• Serverprocesses
?
Lock manager process
?
Database writerprocess
?
Log writerprocess
?
Checkpoint process
?
Process monitor process
• Client–serversystems
• Transaction server
• Queryserver
• Data server
?
Prefetching
?
De-escalation
?
Data caching
?
Cache coherency
?
Lock caching
?
Call back
• Parallelsystems
• Throughput
• Response time
• Speedup
?
Linearspeedup
?
Sublinear speedup
• Scaleup
?
Linearscaleup
?
Sublinearscaleup
?
Batch scaleup
?
Transaction scaleup
• Start-upcosts
• Interference
• Skew
• Interconnection networks
?
Bus
?
Mesh
?
Hypercube
• Paralleldatabase architectures
?
Sharedmemory
?
Shareddisk(clusters)
?
Sharednothing
?
Hierarchical
• Fault tolerance
• Distributedvirtualmemory
• Nonuniform memory architecture
(NUMA)
• Distributedsystems
• Distributeddatabase
?
Sites(nodes)
?
Local transaction
?
Global transaction
?
Local autonomy
• Multidatabase systems
• Network types
?
Local-areanetworks (LAN)
?
Wide-area networks (WAN)
?
Storage-areanetwork (SAN)
Exercises 793
PracticeExercises
17.1 Instead of storing shared structures in shared memory, an alternative
architecture would be to store them in the local memory of a special
process, and access the shared data by interprocess communication with
the process. What would be the drawback of such an architecture?
17.2 In typical client–server systems the server machine is much more pow-
erful than the clients; that is, its processor is faster, it may have multiple
processors,andithasmorememoryanddiskcapacity.Considerinsteada
scenario where client and server machines have exactly the same power.
Would it make sense to build a client–server system in such a scenario?
Why?Whichscenariowouldbebettersuitedtoadata-serverarchitecture?
17.3 Consider a database system based on a client–server architecture, with
theserveracting as a dataserver.
a. What is the effect of the speed of the interconnection between the
clientandtheserveronthechoicebetweentupleandpageshipping?
b. If page shipping is used, the cache of data at the client can be orga-
nized either as a tuple cache or a page cache. The page cache stores
data in units of a page, while the tuple cache stores data in units of
tuples. Assume tuples are smaller than pages. Describe one bene?t
of atuple cache overa page cache.
17.4 Suppose a transaction is written in C with embedded SQL,andabout80
percentofthetimeisspentinthe SQLcode,withtheremaining20percent
spentinCcode.Howmuchspeedupcanonehopetoattainifparallelism
isused only forthe SQL code?Explain.
17.5 Some database operations such as joins can see a signi?cant differencein
speedwhendata(forexample,oneoftherelationsinvolvedinajoin)?ts
in memory as compared to the situation where the data does not ?t in
memory.Showhowthisfactcanexplainthephenomenonofsuperlinear
speedup, where an application sees a speedup greater than the amount
of resourcesallocated to it.
17.6 Parallelsystemsoftenhaveanetworkstructurewheresetsof nprocessors
connecttoasingleEthernetswitch,andtheEthernetswitchesthemselves
connect to another Ethernet switch. Does this architecture correspond to
a bus, mesh or hypercube architecture? If not, how would you describe
thisinterconnection architecture?
Exercises
17.7 Whyisitrelativelyeasytoportadatabasefromasingleprocessormachine
toamultiprocessormachineifindividualqueriesneednotbeparallelized?
794 Chapter17 Database-SystemArchitectures
17.8 Transaction-server architectures are popular for client–server relational
databases, where transactions are short. On the other hand, data-server
architectures are popular for client–server object-oriented database sys-
tems, where transactions are expected to be relatively long. Give two
reasons why data servers may be popular for object-oriented databases
but not for relationaldatabases.
17.9 Whatislockde-escalation,andunderwhatconditionsisitrequired?Why
isitnot requiredifthe unit of datashipping isan item?
17.10 Suppose you were in charge of the database operations of a company
whosemainjobistoprocesstransactions.Supposethecompanyisgrow-
ing rapidly each year, and has outgrown its current computer system.
When you are choosing a new parallel computer, what measure is most
relevant—speedup,batch scaleup,ortransaction scaleup? Why?
17.11 Database systems are typically implemented as a set of processes (or
threads)sharing a shared memoryarea.
a. How isaccess to the shared memoryareacontrolled?
b. Is two-phase locking appropriate for serializing access to the data
structuresin shared memory? Explainyour answer.
17.12 Is it wise to allow a user process to access the shared memory area of a
database system?Explainyour answer.
17.13 What are the factors that can work against linear scaleup in a transaction
processingsystem?Whichofthefactorsarelikelytobethemostimportant
in each of the following architectures: shared memory, shared disk, and
shared nothing?
17.14 Memory systems can be divided into multiple modules, each of which
can be serving a separate request at a given time. What impact would
suchamemoryarchitecturehaveonthenumberofprocessorsthatcanbe
supportedin a shared-memorysystem?
17.15 Considerabankthathasacollectionofsites,eachrunningadatabasesys-
tem.Supposetheonlywaythedatabasesinteractisbyelectronictransfer
ofmoneybetweenthemselves,usingpersistentmessaging.Wouldsucha
systemqualifyasa distributeddatabase? Why?
Bibliographical Notes
Hennessyetal.[2006]providesanexcellentintroductiontotheareaofcomputer
architecture.Abadi[2009]providesanexcellentintroductiontocloudcomputing
and the challenges of running database transactions in such an environment.
Gray and Reuter [1993] provides a textbook description of transaction pro-
cessing, including the architecture of client–server and distributed systems. The
BibliographicalNotes 795
bibliographical notes of Chapter 5 provide references to more information on
ODBC, JDBC,and other database access APIs.
DeWitt and Gray [1992] surveys parallel database systems, including their
architecture and performance measures. A survey of parallel computer architec-
tures is presented by Duncan [1990]. Dubois and Thakkar [1992] is a collection
of papers on scalable shared-memory architectures. DEC clusters running Rdb
were among the earlycommercial usersof the shared-diskdatabase architecture.
Rdb is now owned by Oracle, and is called Oracle Rdb. The Teradata database
machine was among the earliest commercial systems to use the shared-nothing
database architecture. The Grace and the Gamma research prototypes also used
shared-nothing architectures.
OzsuandValduriez[1999]providestextbookcoverageofdistributeddatabase
systems. Further references pertaining to parallel and distributed database sys-
temsappearinthe bibliographical notesof Chapters18 and 19, respectively.
Comer[2009],Halsall[2006],andThomas[1996]describecomputernetwork-
ing and the Internet. Tanenbaum [2002], Kurose and Ross [2005], and Peterson
and Davie[2007] providegeneraloverviewsof computernetworks.
This page intentionally left blank 
CHAPTER
18
Parallel Databases
Inthischapter,wediscussfundamentalalgorithmsforparalleldatabasesystems
thatarebasedontherelationaldatamodel.Inparticular,we focusontheplace-
mentofdataonmultipledisksandtheparallelevaluationofrelationaloperations,
bothofwhichhavebeeninstrumentalinthesuccessofparalleldatabases.
18.1 Introduction
At one point over two decades ago, parallel database systems had been nearly
written off, even by some of their staunchest advocates. Today, they are suc-
cessfully marketed by practically every database-system vendor. Several trends
fueledthistransition:
  The transaction requirements of organizations have grown with increasing
useofcomputers.Moreover,thegrowthoftheWorldWideWebhascreated
many sites with millions of viewers, and the increasing amounts of data
collectedfromtheseviewershasproducedextremelylargedatabasesatmany
companies.
  Organizations are using these increasingly large volumes of data—such as
dataaboutwhatitemspeoplebuy,whatWeblinksusersclickon,andwhen
people make telephone calls—to plan their activities and pricing. Queries
used for such purposes are called decision-support queries,andthedata
requirementsfor such queries may run into terabytes. Single-processorsys-
temsarenotcapable ofhandlingsuch largevolumesofdataattherequired
rates.
  The set-oriented nature of database queries naturally lends itself to paral-
lelization.Anumberofcommercialandresearchsystemshavedemonstrated
thepowerandscalabilityofparallelqueryprocessing.
  Asmicroprocessorshavebecomecheap,parallelmachineshavebecomecom-
monandrelativelyinexpensive.
  Individualprocessorshavethemselvesbecomeparallelmachinesusingmul-
ticorearchitectures.
797
798 Chapter18 ParallelDatabases
AswediscussedinChapter17,parallelismisusedtoprovidespeedup,where
queriesareexecutedfasterbecausemoreresources,suchasprocessorsanddisks,
areprovided.Parallelismisalsousedtoprovidescaleup,whereincreasingwork-
loadsarehandledwithoutincreasedresponsetime,viaanincreaseinthedegree
ofparallelism.
We outlined in Chapter 17 the different architectures for parallel database
systems:shared-memory,shared-disk,shared-nothing,andhierarchicalarchitec-
tures. Brie?y, in shared-memory architectures, all processors share a common
memory and disks; in shared-disk architectures, processors have independent
memories,butsharedisks;inshared-nothingarchitectures,processorssharenei-
ther memory nor disks; and hierarchical architectures have nodes that share
neithermemorynordiskswitheachother,butinternallyeachnodehasashared-
memoryorashared-diskarchitecture.
18.2 I/OParallelism
Initssimplestform,I/Oparallelismreferstoreducingthetimerequiredtoretrieve
relations from disk by partitioning the relations over multiple disks. The most
commonformofdatapartitioninginaparalleldatabaseenvironmentishorizontal
partitioning.Inhorizontal partitioning, the tuples of a relation are divided (or
declustered) among many disks, so that each tuple resides on one disk. Several
partitioningstrategieshavebeenproposed.
18.2.1 PartitioningTechniques
Wepresentthreebasicdata-partitioningstrategies.Assumethattherearendisks,
D
0
,D
1
,...,D
n?1
,acrosswhichthedataaretobepartitioned.
  Round-robin. This strategy scans the relation in any order and sends the
ith tuple to disk number D
imodn
. The round-robin scheme ensures an even
distribution of tuples across disks; that is, each disk has approximately the
samenumberoftuplesastheothers.
  Hashpartitioning.Thisdeclusteringstrategydesignatesoneormoreattrib-
utes from the given relation’s schema as the partitioning attributes. A hash
functionischosenwhoserangeis {0,1,...,n ?1}.Eachtupleoftheoriginal
relationishashedonthepartitioningattributes.Ifthehashfunction returns
i,thenthetupleisplacedondisk D
i
.
1
  Rangepartitioning.Thisstrategydistributestuplesbyassigningcontiguous
attribute-valuerangestoeachdisk.Itchoosesapartitioningattribute,A,and
a partitioning vector [v
0
,v
1
,...,v
n?2
], such that, if i < j,thenv
i
<v
j
.The
relation is partitioned as follows: Consider a tuple t such that t[A] = x.If
1
Hash-functiondesignisdiscussedinSection11.6.1.
18.2 I/OParallelism 799
x <v
0
,thent goes on disk D
0
.Ifx ? v
n?2
,thent goes on disk D
n?1
.If
v
i
? x<v
i+1
,thent goesondisk D
i+1
.
For example, range partitioning with three disks numbered 0, 1, and 2
mayassigntupleswithvalueslessthan5todisk0,valuesbetween5and40
todisk1,andvaluesgreaterthan40todisk2.
18.2.2 ComparisonofPartitioningTechniques
Once a relation has been partitioned among several disks, we can retrieve it in
parallel,usingallthedisks.Similarly,whenarelationisbeingpartitioned,itcan
be written to multiple disks in parallel. Thus, the transfer rates for reading or
writing an entire relation are much faster with I/O parallelism than without it.
However, reading an entire relation, or scanning a relation, is only one kind of
accesstodata.Accesstodatacanbeclassi?edasfollows:
1. Scanningtheentirerelation.
2. Locating a tuple associatively (for example, employee name = “Campbell”);
these queries,called point queries, seek tuples that have a speci?ed value
foraspeci?cattribute.
3. Locating all tuples for which the value of a given attribute lies within a
speci?ed range (for example, 10000 < salary < 20000); these queries are
calledrangequeries.
The different partitioning techniques support these types of access at different
levelsofef?ciency:
  Round-robin.Theschemeisideallysuitedforapplicationsthatwishtoread
the entire relation sequentially for each query. With this scheme, both point
queriesandrangequeriesarecomplicatedtoprocess,sinceeachofthendisks
mustbeusedforthesearch.
  Hash partitioning. This scheme is best suited for point queries based on
the partitioning attribute. For example, if a relation is partitioned on the
telephone numberattribute,thenwecananswerthequery “Findtherecordof
theemployeewithtelephone number=555-3333”byapplyingthepartitioning
hashfunctionto555-3333andthensearchingthatdisk.Directingaqueryto
a single disk saves the start-up cost of initiating a query on multiple disks,
andleavestheotherdisksfreetoprocessotherqueries.
Hash partitioning is also useful for sequential scans of the entire relation.
If the hash function is a good randomizing function, and the partitioning
attributes form a key of the relation, then the number of tuples in each of
thedisksisapproximatelythesame,withoutmuchvariance.Hence,thetime
taken to scan the relation is approximately 1/nofthetimerequiredtoscan
therelationinasingledisksystem.
Thescheme,however,isnotwellsuitedforpointqueriesonnonpartitioning
attributes.Hash-basedpartitioningisalsonotwellsuitedforansweringrange
800 Chapter18 ParallelDatabases
queries, since, typically, hash functions do not preserve proximity within a
range. Therefore, all the disks need to be scanned for range queries to be
answered.
  Rangepartitioning.Thisschemeiswellsuitedforpointandrangequerieson
the partitioningattribute. Forpoint queries,we can consult the partitioning
vectortolocatethediskwherethetupleresides.Forrangequeries,weconsult
the partitioning vector to ?nd the range of disks on which the tuples may
reside. In both cases, the search narrows to exactly those disks that might
haveanytuplesofinterest.
An advantage of this feature is that, if there are only a few tuples in the
queried range, then the query is typically sent to one disk, as opposed to
all the disks. Since other disks can be used to answer other queries, range
partitioning results in higher throughput while maintaining good response
time. On the other hand, if there are many tuples in the queried range (as
there are when the queried range is a larger fraction of the domain of the
relation), many tuples have to be retrieved from a few disks, resulting in
an I/O bottleneck (hot spot) at those disks. In this example of execution
skew, all processing occurs in one—or only a few—partitions. In contrast,
hash partitioning and round-robin partitioning would engage all the disks
for such queries, giving a faster response time for approximately the same
throughput.
Thetypeofpartitioningalsoaffectsotherrelationaloperations,suchasjoins,
as we shall see in Section 18.5. Thus, the choice of partitioning technique also
dependsontheoperationsthatneedtobeexecuted.Ingeneral,hashpartitioning
orrangepartitioningarepreferredtoround-robinpartitioning.
In a system with many disks, the number of disks across which to partition
a relationcan be chosen in thisway: Ifa relationcontains only a fewtuplesthat
will ?t into a single disk block, then it is better to assign the relation to a single
disk.Largerelationsarepreferablypartitionedacrossalltheavailabledisks.Ifa
relation consists of m disk blocks and there are n disks available in the system,
thentherelationshouldbeallocatedmin(m,n)disks.
18.2.3 HandlingofSkew
Whenarelationispartitioned(byatechniqueotherthanround-robin),theremay
be a skew in the distribution of tuples, with a high percentage of tuples placed
insomepartitionsandfewertuplesinotherpartitions.Thewaysthatskewmay
appearareclassi?edas:
  Attribute-valueskew.
  Partitionskew.
Attribute-value skew refers to the fact that some values appear in the par-
titioning attributes of many tuples. All the tuples with the same value for the
18.2 I/OParallelism 801
partitioning attribute end up in the same partition, resulting in skew. Partition
skewreferstothefactthattheremaybeloadimbalanceinthepartitioning,even
whenthereisnoattributeskew.
Attribute-valueskewcanresultinskewedpartitioningregardlessofwhether
rangepartitioningorhashpartitioningisused.Ifthepartitionvectorisnotchosen
carefully, range partitioning may result in partition skew. Partition skew is less
likelywithhashpartitioning,ifagoodhashfunctionischosen.
AsSection17.3.1noted,evenasmallskewcanresultinasigni?cantdecrease
in performance. Skew becomes an increasing problem with a higher degree of
parallelism.Forexample,ifarelationof1000tuplesisdividedinto10parts,and
the division is skewed, then there may be some partitions of size less than 100
andsomepartitionsofsizemorethan100;ifevenonepartitionhappenstobeof
size200,thespeedupthatwewouldobtainbyaccessingthepartitionsinparallel
isonly5, insteadofthe10forwhich we wouldhave hoped.Ifthesamerelation
hastobepartitionedinto100parts,apartitionwillhave10tuplesonanaverage.
Ifevenone partitionhas 40 tuples(which ispossible,giventhelargenumber of
partitions)thespeedupthatwewouldobtainbyaccessingtheminparallelwould
be25,ratherthan100.Thus,weseethatthelossofspeedupduetoskewincreases
withparallelism.
A balanced range-partitioning vector can be constructed by sorting: The
relationis?rstsortedonthepartitioningattributes.Therelationisthenscanned
in sorted order. After every 1/n of the relation has been read, the value of the
partitioning attribute of the next tuple is added to the partition vector. Here, n
denotesthenumberofpartitionstobeconstructed.Incasetherearemanytuples
with the same value for the partitioning attribute, the technique can still result
in some skew. The main disadvantage of this method is the extra I/O overhead
incurredindoingtheinitialsort.
The I/O overhead for constructing balanced range-partition vectors can be
reduced by constructing and storing a frequency table, or histogram,oftheat-
tributevaluesforeachattributeofeachrelation.Figure18.1showsanexampleof
ahistogramforaninteger-valuedattributethattakesvaluesintherange1to25.A
histogramtakesuponlyalittlespace,sohistogramsonseveraldifferentattributes
can bestoredinthesystemcatalog. Itisstraightforward toconstruct abalanced
range-partitioning function given a histogram on the partitioning attributes. If
the histogram is not stored, it can be computed approximately by sampling the
relation, using only tuples from a randomly chosen subset of the disk blocks of
therelation.
Another approach to minimizing the effect of skew, particularly with range
partitioning, is to use virtual processors.Inthevirtual processor approach, we
pretend there are several times as many virtual processors as the number of real
processors.Anyof the partitioningtechniquesand query-evaluationtechniques
that we study later in this chapter can be used, but they map tuples and work
tovirtualprocessorsinsteadoftorealprocessors.Virtualprocessors,inturn,are
mappedtorealprocessors,usuallybyround-robinpartitioning.
The idea is that even if one range had many more tuples than the others
because of skew, these tuples would get split across multiple virtual processor
802 Chapter18 ParallelDatabases
value
frequency
50
40
30
20
10
1–5 6–10 11–15 16–20 21–25 
Figure18.1 Example of histogram.
ranges. Round-robin allocation of virtual processors to real processors would
distribute the extra work among multiple real processors, so that one processor
doesnothavetobearalltheburden.
18.3 InterqueryParallelism
In interquery parallelism, different queries or transactions execute in parallel
withoneanother.Transactionthroughputcanbeincreasedbythisformofparal-
lelism.However,theresponsetimesofindividualtransactionsarenofasterthan
theywouldbeifthetransactionswereruninisolation.Thus,theprimaryuseof
interquery parallelism is to scale up a transaction-processing system to support
alargernumberoftransactionspersecond.
Interquery parallelism is the easiest form of parallelism to support in a
database system—particularly in a shared-memory parallel system. Database
systemsdesignedforsingle-processorsystemscanbeusedwithfewornochanges
onashared-memoryparallelarchitecture,sinceevensequentialdatabasesystems
supportconcurrentprocessing.Transactionsthatwouldhaveoperatedinatime-
shared concurrent manner on a sequential machine operate in parallel in the
shared-memoryparallelarchitecture.
Supporting interquery parallelism is more complicated in a shared-disk or
shared-nothing architecture. Processors have to perform some tasks, such as
locking and logging, in a coordinated fashion, and that requires that they pass
messages to each other. A parallel database system must also ensure that two
processorsdonotupdatethesamedataindependentlyatthesametime.Further,
whenaprocessoraccessesorupdatesdata,thedatabasesystemmustensurethat
theprocessorhasthelatestversionofthedatainitsbufferpool.Theproblemof
ensuringthattheversionisthelatestisknownasthecache-coherencyproblem.
18.4 IntraqueryParallelism 803
Various protocols are available to guarantee cache coherency; often, cache-
coherency protocols are integrated with concurrency-control protocols so that
theiroverheadisreduced.Onesuchprotocolforashared-disksystemisthis:
1. Before any read or write access to a page, a transaction locks the page in
sharedorexclusivemode,asappropriate.Immediatelyafterthetransaction
obtains either a shared or exclusive lock on a page, it also reads the most
recentcopyofthepagefromtheshareddisk.
2. Beforeatransactionreleasesanexclusivelockonapage,it?ushesthepage
totheshareddisk;then,itreleasesthelock.
Thisprotocolensuresthat,whenatransactionsetsasharedorexclusivelockon
apage,itgetsthecorrectcopyofthepage.
More complex protocols avoid the repeated reading and writing to disk re-
quiredbytheprecedingprotocol.Suchprotocolsdonotwritepagestodiskwhen
exclusive locks are released. When a shared or exclusive lock is obtained, if the
most recent version of a page is in the buffer pool of some processor, the page
is obtained from there. The protocols have to be designed to handle concurrent
requests. The shared-disk protocols can be extended to shared-nothing architec-
tures by this scheme: Each page has a home processor P
i
, and is stored on disk
D
i
.Whenotherprocessorswanttoreadorwritethepage,theysendrequeststo
thehomeprocessor P
i
ofthepage,sincetheycannot directlycommunicatewith
thedisk.Theotheractionsarethesameasintheshared-diskprotocols.
The Oracle and Oracle Rdb systems are examples of shared-disk parallel
databasesystemsthatsupportinterqueryparallelism.
18.4 IntraqueryParallelism
Intraquery parallelism refers to the execution of a single query in parallel on
multiple processors and disks. Using intraquery parallelism is important for
speeding up long-running queries. Interquery parallelism does not help in this
task,sinceeachqueryisrunsequentially.
Toillustratetheparallelevaluationofaquery,consideraquerythatrequires
a relation to be sorted. Suppose that the relation has been partitioned across
multipledisksbyrangepartitioningonsomeattribute,andthesortisrequested
on the partitioning attribute. The sort operation can be implemented by sorting
eachpartitioninparallel,thenconcatenatingthesortedpartitionstogetthe?nal
sortedrelation.
Thus,wecanparallelizeaquerybyparallelizingindividualoperations.There
isanothersourceofparallelisminevaluatingaquery:Theoperatortreeforaquery
cancontainmultipleoperations.Wecanparallelizetheevaluationoftheoperator
tree by evaluating in parallel some of the operations that do not dependon one
another.Further,asChapter12mentions,wemaybeabletopipelinetheoutput
of one operation to another operation. The two operations can be executed in
804 Chapter18 ParallelDatabases
parallel on separate processors, one generating output that is consumed by the
other,evenasitisgenerated.
Insummary,theexecutionofasinglequerycanbeparallelizedintwodifferent
ways:
  Intraoperation parallelism. We can speed up processing of a query by par-
allelizing the execution of each individual operation, such as sort, select,
project,andjoin.WeconsiderintraoperationparallelisminSection18.5.
  Interoperation parallelism. We can speed up processing of a query by exe-
cutinginparallelthedifferentoperationsinaqueryexpression.Weconsider
thisformofparallelisminSection18.6.
The two forms of parallelism are complementary, and can be used simulta-
neously on a query. Since the number of operations in a typical query is small,
comparedtothenumberoftuplesprocessedbyeachoperation,the?rstformof
parallelismcan scale betterwith increasing parallelism.However, with the rela-
tivelysmallnumberofprocessorsintypicalparallelsystemstoday,bothformsof
parallelismareimportant.
In the following discussion of parallelizationof queries, we assume that the
queries are read only. The choice of algorithms for parallelizing query evalu-
ation depends on the machine architecture. Rather than present algorithms for
each architecture separately, we use a shared-nothing architecture model in our
description. Thus, we explicitlydescribe when data have to be transferred from
one processor to another. We can simulate this model easily by using the other
architectures,sincetransferofdatacanbedoneviasharedmemoryinashared-
memory architecture, and via shared disks in a shared-disk architecture. Hence,
algorithms for shared-nothing architectures can be used on the other architec-
tures,too.Wementionoccasionallyhowthealgorithmscanbefurtheroptimized
forshared-memoryorshared-disksystems.
Tosimplifythepresentationofthealgorithms,assumethattherearenproces-
sors, P
0
, P
1
,...,P
n?1
,andn disks D
0
,D
1
,...,D
n?1
,wherediskD
i
is associated
withprocessor P
i
.Arealsystemmayhavemultipledisksperprocessor.Itisnot
hard to extend the algorithms to allow multiple disks per processor: We simply
allow D
i
tobeasetofdisks.However,forsimplicity,weassumeherethat D
i
isa
singledisk.
18.5 IntraoperationParallelism
Since relational operations work on relations containing large sets of tuples, we
can parallelize the operations by executing them in parallel on differentsubsets
oftherelations.Sincethenumberoftuplesinarelationcanbelarge,thedegreeof
parallelism is potentially enormous. Thus, intraoperation parallelism is natural
inadatabasesystem.Weshallstudyparallelversionsofsomecommonrelational
operationsinSections18.5.1through18.5.3.
18.5 IntraoperationParallelism 805
18.5.1 ParallelSort
Suppose that we wish to sort arelation that resideson ndisks D
0
,D
1
,...,D
n?1
.
If the relation has been range-partitioned on the attributes on which it is to be
sorted,then,asnotedinSection18.2.2,wecansorteachpartitionseparately,and
can concatenate the results to get the full sorted relation. Since the tuples are
partitionedonndisks,thetimerequiredforreadingtheentirerelationisreduced
bytheparallelaccess.
Iftherelationhasbeenpartitionedinanyotherway,wecansortitinone of
twoways:
1. Wecanrange-partitionitonthesortattributes,andthensorteachpartition
separately.
2. Wecanuseaparallelversionoftheexternalsort–mergealgorithm.
18.5.1.1 Range-PartitioningSort
Range-partitioningsortworksintwosteps:?rstrangepartitioningtherelation,
then sorting each partition separately. When we sort by range partitioning the
relation, it is not necessary to range-partition the relation on the same set of
processors or disks as those on which that relation is stored. Suppose that we
chooseprocessors P
0
, P
1
,...,P
m
,wherem < n,tosorttherelation.Therearetwo
stepsinvolvedinthisoperation:
1. Redistribute the tuples in the relation, using a range-partition strategy, so
that all tuples that lie within the ith range are sent to processor P
i
,which
storestherelationtemporarilyondisk D
i
.
To implement range partitioning, in parallel every processor reads the
tuples from its disk and sends the tuples to their destination processors.
Eachprocessor P
0
, P
1
,...,P
m
alsoreceivestuplesbelongingtoitspartition,
and stores them locally. This step requires disk I/O and communication
overhead.
2. Each of the processors sorts its partition of the relation locally, without
interaction with the other processors. Each processor executes the same
operation—namely,sorting—onadifferentdataset.(Executionofthesame
operationinparallelondifferentsetsofdataiscalleddataparallelism.)
The?nalmergeoperationistrivial,becausetherangepartitioninginthe
?rst phase ensures that, for 1 ? i < j ? m, the key values in processor P
i
arealllessthanthekeyvaluesin P
j
.
Wemustdorangepartitioningwithagoodrange-partitionvector,sothateach
partition will have approximately the same number of tuples. Virtual processor
partitioningcanalsobeusedtoreduceskew.
806 Chapter18 ParallelDatabases
18.5.1.2 ParallelExternalSort–Merge
Parallelexternalsort–mergeisanalternativetorangepartitioning.Supposethat
arelationhasalreadybeenpartitionedamongdisks D
0
,D
1
,...,D
n?1
(itdoesnot
matterhow the relation has been partitioned).Parallelexternalsort–merge then
worksthisway:
1. Eachprocessor P
i
locallysortsthedataondisk D
i
.
2. The system then merges the sorted runs on each processor to get the ?nal
sortedoutput.
Themergingofthesortedrunsinstep2canbeparallelizedbythissequence
ofactions:
1. The system range-partitions the sorted partitions at each processor P
i
(all
bythesamepartitionvector)acrosstheprocessors P
0
, P
1
,...,P
m?1
.Itsends
thetuplesinsortedorder,sothateachprocessorreceivesthetuplesinsorted
streams.
2. Eachprocessor P
i
performsamergeonthestreamsastheyarereceived,to
getasinglesortedrun.
3. The system concatenates the sorted runs on processors P
0
, P
1
,...,P
m?1
to
getthe?nalresult.
Asdescribed,thissequenceofactionsresultsinaninterestingformofexecution
skew, since at ?rst every processor sends all blocks of partition 0 to P
0
,then
every processor sends all blocks of partition 1 to P
1
, and so on. Thus, while
sending happens in parallel, receiving tuples becomes sequential: First only P
0
receives tuples, then only P
1
receives tuples, and so on. To avoid this problem,
eachprocessorrepeatedlysendsablockofdatatoeachpartition.Inotherwords,
eachprocessorsendsthe?rstblockofeverypartition,thensendsthesecondblock
ofeverypartition,andsoon.Asaresult,allprocessorsreceivedatainparallel.
Some machines, such as the Teradata Purpose-Built Platform Family ma-
chines,usespecializedhardwaretoperformmerging.TheBYNETinterconnection
networkintheTeradatamachinescanmergeoutputfrommultipleprocessorsto
giveasinglesortedoutput.
18.5.2 ParallelJoin
The join operation requires that the system test pairs of tuples to see whether
they satisfy the join condition; if they do, the system adds the pair to the join
output.Paralleljoinalgorithmsattempttosplitthepairstobetestedoverseveral
processors.Eachprocessorthencomputespartofthejoinlocally.Then,thesystem
collects theresultsfrom eachprocessortoproducethe?nal result.
18.5 IntraoperationParallelism 807
18.5.2.1 PartitionedJoin
For certain kinds of joins, such as equi-joins and natural joins, it is possible to
partition the two input relations across the processors and to compute the join
locally at each processor. Suppose that we are using n processors and that the
relationstobejoinedarer ands.Partitionedjointhenworksthisway:Thesystem
partitionstherelationsr and s eachinto npartitions,denotedr
0
,r
1
,...,r
n?1
and
s
0
,s
1
,...,s
n?1
. The systemsendspartitions r
i
and s
i
toprocessor P
i
,wheretheir
joiniscomputedlocally.
Thepartitionedjointechniqueworkscorrectlyonlyifthejoinisanequi-join
(for example, r   r.A=s.B
s) and if we partition r and s by the same partitioning
function on their join attributes. The idea of partitioning is exactly the same as
thatbehindthepartitioningstepofhashjoin.Inapartitionedjoin,however,there
aretwodifferentwaysofpartitioningr and s:
  Rangepartitioningonthejoinattributes.
  Hashpartitioningonthejoinattributes.
In either case, the same partitioning function must be used for both relations.
Forrangepartitioning,thesamepartitionvectormustbeusedforbothrelations.
For hash partitioning, the same hash function must be used on both relations.
Figure18.2depictsthepartitioninginapartitionedparalleljoin.
Once the relations are partitioned, we can use any join technique locally at
eachprocessor P
i
tocomputethejoinof r
i
and s
i
.Forexample,hashjoin,merge
join,ornested-loopjoincouldbeused.Thus,wecanusepartitioningtoparallelize
anyjointechnique.
P
0
r
0
P
1
r
1
s
r
P
2
r
2
P
3
r
3
s
0
s
1
s
2
s
3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure18.2 Partitioned parallel join.
808 Chapter18 ParallelDatabases
If one or both of the relations r and s are already partitioned on the join
attributes (by either hash partitioning or range partitioning), the work needed
for partitioning is reduced greatly. If the relations are not partitioned, or are
partitioned on attributes other than the join attributes, then the tuples need to
be repartitioned.Each processor P
i
reads in the tuples on disk D
i
, computes for
eachtuplet thepartition j towhich t belongs,andsendstuple t toprocessor P
j
.
Processor P
j
storesthetuplesondisk D
j
.
We can optimize the join algorithm used locally at each processor to reduce
I/O by buffering some of the tuplesto memory, instead of writing them to disk.
WedescribesuchoptimizationsinSection18.5.2.3.
Skew presents a special problem when range partitioning is used, since a
partition vector that splits one relation of the join into equal-sized partitions
maysplittheotherrelationsintopartitionsofwidelyvaryingsize.Thepartition
vector should be such that |r
i
|+| s
i
| (that is, the sum of the sizes of r
i
and s
i
)is
roughlyequal overall the i = 0,1,..., n ?1.With a good hash function, hash
partitioningislikelyto haveasmallerskew, exceptwhen therearemany tuples
withthesamevaluesforthejoinattributes.
18.5.2.2 Fragment-and-ReplicateJoin
Partitioningisnotapplicabletoalltypesofjoins.Forinstance,ifthejoincondition
isaninequality,suchasr   r.a<s.b
s,itispossiblethatalltuplesinr joinwithsome
tuplein s (andviceversa).Thus,theremaybenoeasywayofpartitioningr and
s sothattuplesinpartitionr
i
joinwithonlytuplesinpartitions
i
.
Wecanparallelizesuchjoinsbyusingatechniquecalledfragmentandreplicate.
We?rstconsideraspecialcaseoffragmentandreplicate—asymmetricfragment-
and-replicatejoin—whichworksasfollows:
1. The system partitions one of the relations—say, r. Any partitioning tech-
niquecanbeusedonr,includinground-robinpartitioning.
2. Thesystemreplicatestheotherrelation,s,acrossalltheprocessors.
3. Processor P
i
thenlocallycomputesthejoinofr
i
withallofs,usinganyjoin
technique.
The asymmetric fragment-and-replicate scheme appears in Figure 18.3a. If r is
alreadystoredbypartitioning,thereisnoneedtopartitionitfurtherinstep1.All
thatisrequiredistoreplicates acrossallprocessors.
The general case of fragment-and-replicate join appears in Figure 18.3b; it
worksthisway:Thesystempartitionsrelationr into npartitions,r
0
,r
1
,...,r
n?1
,
and partitions s into m partitions, s
0
,s
1
,...,s
m?1
. As before, any partitioning
technique may be used on r and on s. The values of m and n do not need to
be equal, but they must be chosen so that there are at least m ? n processors.
Asymmetricfragmentand replicate issimplya specialcase of generalfragment
and replicate, where m = 1. Fragment and replicate reduces the sizes of the
relationsateachprocessor,comparedtoasymmetricfragmentandreplicate.
18.5 IntraoperationParallelism 809
r
0
P
0,0
s
0
s
1
s
2
s
s
3
s
m–1
r
1
r
r
2
r
3
r
n–1
P
n–1,m–1 
.
.
.
P
0
r
0
P
1
r
1
rs
P
2
r
2
P
3
r
3
.
.
.
.
.
.
P
1,0
P
2,0
P
0,1
P
1,1
P
2,1
P
0,2
P
1,2
P
0,3
. . .
.....
.
.
.
.
.
(a) Asymmetric
fragment and replicate
(b) Fragment and replicate
Figure18.3 Fragment-and-replicate schemes.
Let the processors be P
0,0
, P
0,1
,...,P
0,m?1
, P
1,0
,...,P
n?1,m?1
. Processor P
i,j
computes the join of r
i
with s
j
. Each processor must get those tuples in the
partitionson which itworks. Toaccomplish this, the systemreplicatesr
i
topro-
cessors P
i,0
, P
i,1
,...,P
i,m?1
(which form a row in Figure 18.3b), and replicates s
i
toprocessors P
0,i
, P
1,i
,...,P
n?1,i
(whichformacolumninFigure18.3b).Anyjoin
techniquecanbeusedateachprocessor P
i,j
.
Fragment and replicate works with any join condition, since every tuple in
r can be tested with every tuple in s. Thus, it can be used where partitioning
cannotbe.
Fragmentandreplicateusuallyhasahighercostthanpartitioningwhenboth
relationsareofroughlythesamesize,sinceatleastoneoftherelationshastobe
replicated.However,ifoneoftherelations—say, s—issmall,itmaybecheaper
to replicate s across all processors, rather than to repartition r and s on the join
attributes.In such a case, asymmetricfragment and replicateis preferable,even
thoughpartitioningcouldbeused.
18.5.2.3 PartitionedParallelHashJoin
The partitioned hash join of Section 12.5.5 can be parallelized. Suppose that we
have n processors, P
0
, P
1
,...,P
n?1
, and two relations r and s, such that the
relations r and s arepartitionedacrossmultipledisks.RecallfromSection12.5.5
810 Chapter18 ParallelDatabases
thatthesmallerrelationischosenasthebuildrelation.Ifthesizeof s islessthan
thatofr,theparallelhash-joinalgorithmproceedsthisway:
1. Choose a hash function—say, h
1
—that takes the join attribute value of
each tuple in r and s and maps the tuple to one of the n processors. Let r
i
denote the tuples of relation r that are mapped to processor P
i
; similarly,
let s
i
denote the tuples of relation s that are mapped to processor P
i
.Each
processor P
i
readsthetuplesofsthatareonitsdisk D
i
andsendseachtuple
totheappropriateprocessoronthebasisofhashfunction h
1
.
2. Asthedestinationprocessor P
i
receivesthetuplesofs
i
,itfurtherpartitions
thembyanotherhashfunction,h
2
,whichtheprocessorusestocomputethe
hash joinlocally.The partitioningatthisstage isexactlythe same asinthe
partitioningphaseofthesequentialhash-joinalgorithm.Eachprocessor P
i
executesthisstepindependentlyfromtheotherprocessors.
3. Oncethetuplesofshavebeendistributed,thesystemredistributesthelarger
relation r across the nprocessors by the hash function h
1
,inthesameway
asbefore.Asitreceiveseachtuple,thedestinationprocessorrepartitionsit
bythefunction h
2
,justastheproberelationispartitionedinthesequential
hash-joinalgorithm.
4. Each processor P
i
executes the build and probe phases of the hash-join
algorithm on the local partitions r
i
and s
i
of r and s to produce a partition
ofthe?nalresultofthehashjoin.
Thehashjoinateachprocessorisindependentofthatatotherprocessors,and
receiving the tuples of r
i
and s
i
is similar to reading them from disk. Therefore,
anyoftheoptimizationsofthehashjoindescribedinChapter12canbeappliedas
welltotheparallelcase.Inparticular,wecanusethehybridhash-joinalgorithm
to cache some of the incoming tuples in memory, and thus avoid the costs of
writingthemandofreadingthembackin.
18.5.2.4 ParallelNested-LoopJoin
Toillustratetheuseoffragment-and-replicate–basedparallelization,considerthe
casewheretherelation s ismuchsmallerthanrelationr.Supposethatrelationr
isstoredbypartitioning;theattributeonwhichitispartitioneddoesnotmatter.
Suppose too that there is an index on a join attribute of relation r at each of the
partitionsofrelationr.
We use asymmetric fragment and replicate, with relation s being replicated
andwiththeexistingpartitioningofrelationr.EachprocessorP
j
whereapartition
ofrelations isstoredreadsthetuplesofrelations storedin D
j
,andreplicatesthe
tuplestoeveryotherprocessor P
i
.Attheendofthisphase,relationsisreplicated
atallsitesthatstoretuplesofrelationr.
Now, each processor P
i
performs an indexed nested-loop join of relation s
with the ith partition of relation r. We can overlap the indexed nested-loop join
18.5 IntraoperationParallelism 811
with the distribution of tuples of relation s, to reduce the costs of writing the
tuplesofrelations todisk,andofreadingthemback.However,thereplicationof
relation s mustbesynchronizedwiththejoinsothatthereisenoughspaceinthe
in-memorybuffersateachprocessor P
i
toholdthetuplesofrelation s thathave
beenreceivedbutthathavenotyetbeenusedinthejoin.
18.5.3 OtherRelational Operations
Theevaluationofotherrelationaloperationsalsocanbeparallelized:
  Selection.Lettheselectionbe    (r).Consider?rstthecasewhere  isofthe
form a
i
= v,wherea
i
is an attribute and v is a value. If the relation r is
partitioned on a
i
, the selection proceeds at a single processor. If   is of the
form l ? a
i
? u—thatis,  is a range selection—and the relation has been
range-partitionedona
i
,thentheselectionproceedsateachprocessorwhose
partition overlaps with the speci?ed range of values. In all other cases, the
selectionproceedsinparallelatalltheprocessors.
  Duplicate elimination. Duplicates can be eliminated by sorting; either of
the parallel sort techniques can be used, optimized to eliminate duplicates
as soon as they appear during sorting. We can also parallelize duplicate
elimination by partitioning the tuples(by either range or hash partitioning)
andeliminatingduplicateslocallyateachprocessor.
  Projection. Projection without duplicate elimination can be performed as
tuples are read in from disk in parallel. If duplicates are to be eliminated,
eitherofthetechniquesjustdescribedcanbeused.
  Aggregation.Consideranaggregationoperation.Wecanparallelizetheop-
erationbypartitioningtherelationonthegroupingattributes,andthencom-
putingtheaggregatevalueslocallyateachprocessor.Eitherhashpartitioning
orrangepartitioningcanbeused.Iftherelationisalreadypartitionedonthe
groupingattributes,the?rststepcanbeskipped.
Wecanreducethecostoftransferringtuplesduringpartitioningbypartly
computing aggregate values before partitioning, at least for the commonly
usedaggregatefunctions.Consideranaggregationoperationonarelationr,
usingthesumaggregatefunctiononattribute B,withgroupingonattribute
A.Thesystemcanperformtheoperationateachprocessor P
i
onthosertuples
storedondiskD
i
.Thiscomputationresultsintupleswithpartialsumsateach
processor; there is one tuple at P
i
for each value for attribute Apresent in r
tuplesstoredon D
i
.Thesystempartitionstheresultofthelocalaggregation
on the grouping attribute A, and performs the aggregation again (on tuples
withthepartialsums)ateachprocessor P
i
togetthe?nalresult.
As a result of this optimization, fewer tuples need to be sent to other
processors during partitioning. This idea can be extended easily to the min
and max aggregate functions. Extensions to the count and avg aggregate
functionsareleftforyoutodoinExercise18.12.
812 Chapter18 ParallelDatabases
Theparallelizationofotheroperationsiscoveredinseveraloftheexercises.
18.5.4 CostofParallelEvaluationofOperations
We achieve parallelism by partitioning the I/O among multiple disks, and par-
titioning the CPU work among multiple processors. If such a split is achieved
withoutanyoverhead,andifthereisnoskewinthesplittingofwork,aparallel
operation using n processors will take 1/ntimesaslongasthesameoperation
onasingleprocessor.Wealreadyknowhowtoestimatethecostofanoperation
such as a join or a selection. The time cost of parallel processing would then be
1/nofthetimecostofsequentialprocessingoftheoperation.
Wemustalsoaccountforthefollowingcosts:
  Start-upcostsforinitiatingtheoperationatmultipleprocessors.
  Skewinthedistributionofworkamongtheprocessors,withsomeprocessors
gettingalargernumberoftuplesthanothers.
  Contention for resources—such as memory, disk, and the communication
network—resultingindelays.
  Costofassemblingthe?nal resultbytransmittingpartialresultsfromeach
processor.
Thetimetakenbyaparalleloperationcanbeestimatedas:
T
part
+ T
asm
+max(T
0
,T
1
,...,T
n?1
)
whereT
part
isthetimeforpartitioningtherelations,T
asm
isthetimeforassembling
the results, and T
i
is the time taken for the operation at processor P
i
. Assuming
that the tuples are distributed without any skew, the number of tuples sent to
each processor can be estimated as 1/n of the total number of tuples. Ignoring
contention,thecostT
i
oftheoperationsateachprocessor P
i
canthenbeestimated
bythetechniquesinChapter12.
Theprecedingestimatewillbeanoptimisticestimate,sinceskewiscommon.
Eventhoughbreakingdownasinglequeryintoanumberofparallelstepsreduces
the size of the average step, it is the time for processing the single slowest step
thatdeterminesthetimetakenforprocessingthequeryasawhole.Apartitioned
parallel evaluation, for instance, is only as fast as the slowest of the parallel
executions. Thus, any skew in the distribution of the work across processors
greatlyaffectsperformance.
The problem of skew in partitioning is closely related to the problem of
partition over?ow in sequential hash joins (Chapter 12). We can use over?ow
resolution and avoidance techniques developed for hash joins to handle skew
when hash partitioning is used. We can use balanced range partitioning and
virtual processor partitioning to minimize skew due to range partitioning, as in
Section18.2.3.
18.6 InteroperationParallelism 813
18.6 InteroperationParallelism
There are two forms of interoperation parallelism: pipelined parallelism and
independentparallelism.
18.6.1 PipelinedParallelism
AsdiscussedinChapter12,pipeliningformsanimportantsourceofeconomyof
computationfordatabasequeryprocessing.Recallthat,inpipelining,theoutput
tuples of one operation, A, are consumed by a second operation, B,evenbefore
the ?rst operation has produced the entire set of tuples in its output. The major
advantageofpipelinedexecutioninasequentialevaluationisthatwecancarry
outasequenceofsuchoperationswithoutwritinganyoftheintermediateresults
todisk.
Parallelsystemsusepipeliningprimarilyforthesamereasonthatsequential
systems do. However, pipelines are a source of parallelism as well, in the same
waythatinstructionpipelinesareasourceofparallelisminhardwaredesign.Itis
possibletorunoperations Aand Bsimultaneouslyondifferentprocessors,sothat
B consumes tuples in parallel with Aproducing them. This form of parallelism
iscalledpipelinedparallelism.
Considerajoinoffourrelations:
r
1
  r
2
  r
3
  r
4
We can set up a pipeline that allows the three joins to be computed in parallel.
Supposeprocessor P
1
isassignedthecomputationof temp
1
? r
1
  r
2
,andP
2
is assigned the computation of r
3
  temp
1
.AsP
1
computes tuples in r
1
  r
2
,
itmakesthesetuplesavailabletoprocessor P
2
.Thus,P
2
hasavailabletoitsome
ofthe tuplesin r
1
  r
2
before P
1
has ?nished itscomputation. P
2
can use those
tuplesthatareavailabletobegincomputationoftemp
1
  r
3
,evenbeforer
1
  r
2
is fully computed by P
1
. Likewise, as P
2
computes tuples in (r
1
  r
2
)   r
3
,it
makesthesetuplesavailableto P
3
,whichcomputesthejoinofthesetupleswith
r
4
.
Pipelined parallelism is useful with a small number of processors, but does
not scale up well. First, pipeline chains generally do not attain suf?cient length
toprovideahighdegreeofparallelism.Second,itisnotpossibletopipelinerela-
tional operators that do not produce output until all inputs have been accessed,
suchastheset-differenceoperation.Third,onlymarginalspeedupisobtainedfor
thefrequentcasesinwhichoneoperator’sexecutioncostismuchhigherthanare
thoseoftheothers.
Allthingsconsidered,whenthedegreeofparallelismishigh,pipeliningisa
less important source of parallelism than partitioning. The real reason for using
pipeliningisthat pipelinedexecutionscan avoidwritingintermediateresultsto
disk.
814 Chapter18 ParallelDatabases
18.6.2 Independent Parallelism
Operations in a query expression that do not depend on one another can be
executedinparallel.Thisformofparallelismiscalledindependentparallelism.
Consider the join r
1
  r
2
  r
3
  r
4
. Clearly, we can compute temp
1
?
r
1
  r
2
in parallel with temp
2
? r
3
  r
4
. When these two computations
complete,wecompute:
temp
1
  temp
2
Toobtainfurtherparallelism,wecanpipelinethetuplesintemp
1
andtemp
2
into
thecomputationof temp
1
  temp
2
,whichisitselfcarriedoutbyapipelinedjoin
(Section12.7.2.2).
Likepipelinedparallelism,independentparallelismdoesnotprovideahigh
degreeofparallelismandislessusefulinahighlyparallelsystem,althoughitis
usefulwithalowerdegreeofparallelism.
18.7 QueryOptimization
Queryoptimizersaccount in large measurefor the success of relationaltechnol-
ogy.Recallthataqueryoptimizertakesaqueryand?ndsthecheapestexecution
planamongthemanypossibleexecutionplansthatgivethesameanswer.
Query optimizers for parallel query evaluation are more complicated than
queryoptimizersforsequentialqueryevaluation.First,thecostmodelsaremore
complicated,sincepartitioningcostshavetobeaccountedfor,andissuessuchas
skewandresourcecontentionmustbetakenintoaccount.Moreimportantisthe
issue of how to parallelize a query. Suppose that we have somehow chosen an
expression(fromamongthoseequivalenttothequery)tobeusedforevaluating
thequery.Theexpressioncanberepresentedbyanoperatortree,asinSection12.1.
Toevaluateanoperatortreeinaparallelsystem,wemustmakethefollowing
decisions:
  Howtoparallelizeeachoperation,andhowmanyprocessorstouseforit.
  What operations to pipeline across different processors, what operations to
execute independently in parallel, and what operations to execute sequen-
tially,oneaftertheother.
Thesedecisionsconstitutethetaskofschedulingtheexecutiontree.
Determiningtheresourcesofeachkind—suchasprocessors,disks,andmem-
ory—that should be allocated to each operation in the tree is another aspect of
theoptimizationproblem.Forinstance,itmayappearwisetousethemaximum
amountofparallelismavailable,butitisagoodideanottoexecutecertainopera-
tionsinparallel.Operationswhosecomputationalrequirementsaresigni?cantly
smallerthan thecommunication overheadshould beclusteredwith one oftheir
18.8 DesignofParallelSystems 815
neighbors. Otherwise, the advantage of parallelism is negated by the overhead
ofcommunication.
One concern is that long pipelines do not lend themselves to good resource
utilization. Unless the operations are coarse grained, the ?nal operation of the
pipelinemaywaitforalongtimetogetinputs,whileholdingpreciousresources,
suchasmemory.Hence,longpipelinesshouldbeavoided.
Thenumberofparallelevaluationplansfromwhichtochooseismuchlarger
than the number of sequential evaluation plans. Optimizing parallel queries by
considering all alternatives is therefore much more expensive than optimizing
sequential queries. Hence, we usually adopt heuristic approaches to reduce the
number of parallel execution plans that we have to consider. We describe two
popularheuristicshere.
The ?rst heuristic is to consider only evaluation plans that parallelize every
operationacrossallprocessors,andthatdonotuseanypipelining.Thisapproach
isusedintheTeradatasystems.Findingthebestsuchexecutionplanislikedoing
query optimization in a sequential system. The main differences lie in how the
partitioningisperformedandwhatcost-estimationformulaisused.
Thesecondheuristicistochoosethemostef?cientsequentialevaluationplan,
andthentoparallelizetheoperationsinthatevaluationplan.TheVolcanoparal-
leldatabasepopularizedamodelofparallelizationcalledtheexchange-operator
model. This model uses existing implementations of operations, operating on
localcopiesofdata,coupledwithanexchangeoperationthatmovesdataaround
betweendifferentprocessors.Exchangeoperatorscanbeintroducedintoaneval-
uationplantotransformitintoaparallelevaluationplan.
Yet another dimension of optimization is the design of physical-storage or-
ganization to speed up queries. The optimal physical organization differs for
different queries. The database administrator must choose a physical organiza-
tion thatappearsto be good for the expectedmixof database queries.Thus, the
area of parallel query optimization is complex, and it is still an area of active
research.
18.8 DesignofParallelSystems
So far this chapter has concentrated on parallelization of data storage and of
queryprocessing.Sincelarge-scaleparalleldatabasesystemsareusedprimarily
for storing large volumes of data, and for processing decision-support queries
onthosedata,thesetopicsarethemostimportantinaparalleldatabasesystem.
Parallelloadingofdatafromexternalsourcesisanimportantrequirement,ifwe
aretohandlelargevolumesofincomingdata.
Alargeparalleldatabasesystemmustalsoaddresstheseavailabilityissues:
  Resiliencetofailureofsomeprocessorsordisks.
  Onlinereorganizationofdataandschemachanges.
Weconsidertheseissueshere.
816 Chapter18 ParallelDatabases
With a large number of processors and disks, the probability that at least
one processor or disk will malfunction is signi?cantly greater than in a single-
processor system with one disk. A poorly designed parallel system will stop
functioningifanycomponent(processorordisk)fails.Assumingthattheproba-
bilityoffailureofasingleprocessorordiskissmall,theprobabilityoffailureof
the systemgoesuplinearlywith thenumber ofprocessorsand disks.Ifa single
processor or disk would fail once every 5 years, a system with 100 processors
wouldhaveafailureevery18days.
Therefore, large-scale parallel database systems, such as Teradata, and IBM
Informix XPS, are designed to operate even if a processor or disk fails. Data are
replicatedacrossatleasttwoprocessors.Ifaprocessorfails,thedatathatitstored
can still be accessed from the other processors. The system keeps track of failed
processorsanddistributestheworkamongfunctioningprocessors.Requestsfor
data stored at the failed site are automatically routed to the backup sites that
storeareplicaofthedata.IfallthedataofaprocessorAarereplicatedatasingle
processor B, B willhavetohandlealltherequeststo Aaswellasthosetoitself,
andthatwillresultin B becomingabottleneck.Therefore,thereplicasofthedata
ofaprocessorarepartitionedacrossmultipleotherprocessors.
When we are dealing with large volumes of data (ranging in the terabytes),
simple operations, such as creating indices, and changes to schema, such as
addingacolumntoarelation,cantakealongtime—perhapshoursorevendays.
Therefore,itisunacceptableforthedatabasesystemtobeunavailablewhilesuch
operations are in progress. Most database systems allow such operations to be
performedonline,thatis,whilethesystemisexecutingothertransactions.
Consider,forinstance,onlineindexconstruction.Asystemthatsupportsthis
feature allows insertions, deletions, and updates on a relation even as an index
isbeingbuiltontherelation.Theindex-buildingoperationthereforecannotlock
the entire relation in shared mode, as it would have done otherwise. Instead,
the process keeps track of updates that occur while it is active and incorporates
the changes into the index being constructed. (Most database systems today
support online index construction, since this feature is very important even for
non-paralleldatabasesystems.)
Inrecentyears,anumberofcompanieshavedevelopednewparalleldatabase
products, including Netezza, DATAllegro (which was acquired by Microsoft),
Greenplum, and AsterData. Each of these products runs on systems containing
tenstothousandsofnodes,witheachnoderunninganinstanceofanunderlying
database; Each product manages the partitioning of data, as well as parallel
processingofqueries,acrossthedatabaseinstances.
Netezza,GreenplumandAsterDatause PostgreSQLastheunderlyingdata-
base; DATAllegro originally used Ingres as the underlying database system, but
moved to SQL Server subsequent to its acquisition by Microsoft. By building on
topofanexistingdatabasesystem,thesesystemsareabletoleveragethedatastor-
age, query processing, and transaction management features of the underlying
database, leaving them free to focus on data partitioning (including replication
for fault tolerance), fast interprocessor communication, parallel query process-
ing, and parallel-query optimization. Another bene?t of using a public domain
18.9 ParallelismonMulticoreProcessors 817
database such as PostgreSQL is that the software cost per node is very low; in
contrastcommercialdatabaseshaveasigni?cantper-processorcost.
It is also worth mentioning that Netezza and DATAllegro actually sell data
warehouse “appliances”, which include hardware and software, allowing cus-
tomerstobuildparalleldatabaseswithminimaleffort.
18.9 ParallelismonMulticoreProcessors
Parallelism has become commonplace on most computers today, even some of
thesmallest,duetocurrenttrendsincomputerarchitecture.Asaresult,virtually
all database systems today run on a parallel platform. In this section, we shall
explorebrie?ythereasonsforthisarchitecturaltrendand the effectsthishason
databasesystemdesignandimplementation.
18.9.1 ParallelismversusRawSpeed
Since the dawn of computers, processor speed has increased at an exponential
rate, doubling every 18 to 24 months. This increase results from an exponential
growthinthenumberoftransistorsthatcouldbe?twithinaunitareaofasilicon
chip, and is known popularly as Moore’s law, named after Intel co-founder
Gordon Moore. Technically, Moore’s law is not a law, but rather an observation
and a prediction regarding technology trends. Until recently, the increase in the
numberoftransistorsandthedecreaseintheirsizeledtoever-fasterprocessors.
AlthoughtechnologicalprogresscontinuestobehaveaspredictedbyMoore’slaw,
anotherfactorhasemergedtoslowthegrowthinprocessorspeed.Fastprocessors
are power inef?cient. This is problematic in terms of energy consumption and
cost, battery life for mobile computers, and heat dissipation(all the power used
eventuallyturns intoheat). Asa result,modernprocessorstypicallyare not one
singleprocessorbutratherconsistofseveralprocessorsononechip.Tomaintaina
distinctionbetweenon-chipmultiprocessorsandtraditionalprocessors,theterm
coreisusedforanon-chipprocessor.Thuswesaythatamachinehasamulticore
processor.
2
18.9.2 CacheMemoryandMultithreading
Eachcoreiscapableofprocessinganindependentstreamofmachineinstructions.
However,becauseprocessorsareabletoprocessdatafasterthanitcanbeaccessed
from main memory, main memory can become a bottleneck that limits overall
performance. For this reason, computer designers include one or more levels of
cache memory in a computer system. Cache memory is more costly than main
memory on a per-byte basis, but offers a faster access time. In multilevel cache
designs, the levels are called L1, L2, and so on, with L1 being the fastest cache
(andthusthemostcostlyperbyteandthereforethesmallest),L2thenextfastest,
2
The use of the term core here is different from the use of that term in the early days of computing to refer to a
main-memorytechnologybasedonmagneticcores.
818 Chapter18 ParallelDatabases
and so on. The result is an extension of the storage hierarchy that we discussed
inChapter10toincludethevariouslevelsofcachebelowmainmemory.
Although the database system can control the transfer of data between disk
and main memory, the computer hardware maintains control over the transfer
ofdataamongthevariouslevelsofcacheandbetweencacheandmainmemory.
Despite this lack of direct control, the database system’s performance can be
affectedbyhowcacheisutilized.Ifacoreneedstoaccessadataitemthatisnotin
cache, itmust be fetchedfrommain memory.Because mainmemoryissomuch
slower than processors, a signi?cant amount of potential processing speed may
belostwhileacorewaitsfordatafrommainmemory.Thesewaitsarereferredto
ascachemisses.
One way in which computer designers attempt to limit the impact of cache
missesisviamultithreading.Athreadisanexecutionstreamthatsharesmemory
3
withotherthreadsrunningonthesamecore.Ifthethreadcurrentlyexecutingon
a core suffers a cache miss (or other type of wait), the core proceeds to execute
anotherthread,therebynotwastingcomputingspeedwhilewaiting.
Threadsintroduceyetanothersourceofparallelismbeyondthemultiplicityof
cores.Eachnewgenerationofprocessorssupportsmorecoresandmorethreads.
The SunUltraSPARC T2processorhas8cores,eachofwhich supports8threads,
foratotalof64threadsononeprocessorchip.
The architecture trend of slower increase in raw speed accompanied by the
growth in the number of cores has signi?cant implications for database system
design,asweshallseeshortly.
18.9.3 AdaptingDatabase SystemDesignforModernArchitectures
Itwouldappearthatdatabasesystemsareanidealapplicationtotakeadvantage
of large numbers of cores and threads, since database systems support large
numbers of concurrent transactions. However, there are a variety of factors that
makeoptimaluseofmodernprocessorschallenging.
As we allow a higher degree of concurrency to take advantage of the par-
allelism of modern processors, we increase the amount of data that needs to be
in cache. This can result in more cache misses, perhaps so many that even a
multithreadedcorehastowaitfordatafrommemory.
Concurrenttransactionsneedsomesortofconcurrencycontroltoensurethe
ACID propertiesthat we discussed in Chapter 14. When concurrent transactions
accessdataincommon,somesortofrestrictionsmustbeimposedonthatconcur-
rentaccess.Thoserestrictions,whetherbasedonlocks,timestamps,orvalidation,
resultinwaitingorthelossofworkduetotransactionaborts.Toavoidexcessive
amounts of waiting or lost work, it is ideal that concurrent transactions con?ict
rarely, but attempting to ensure that can increase the amount of data needed in
cache,resultinginmorecachemisses.
Finally,therearecomponentsofadatabasesystemsharedbyalltransactions.
Inasystemusinglocking,thelocktableissharedbyalltransactionsandaccessto
3
Technically,inoperating-systemterminology,itsaddressspace.
18.10 Summary 819
itcanbecomeabottleneck.Similarproblemsexistforotherformsofconcurrency
control.Similarly,thebuffermanager,thelogmanager,andtherecoverymanager
servealltransactionsandarepotentialbottlenecks.
Becausehavingalargenumberofconcurrenttransactionsmaynottakeopti-
maladvantageofmodernprocessors,itisdesirableto?ndwaystoallowmultiple
corestoworkonasingletransaction.Thisrequiresthedatabasequeryprocessor
to?nd effectivewaystoparallelizequerieswithout creatingexcessivedemands
on cache. This can be done by creating pipelines of database operations from
queriesandby?ndingwaystoparallelizeindividualdatabaseoperations.
Theadaptationofdatabasesystemdesignanddatabasequeryprocessingto
multicoreandmultithreadedsystemsremainsanareaofactiveresearch.Seethe
bibliographicalnotesforfurtherdetails.
18.10 Summary
  Paralleldatabaseshavegainedsigni?cantcommercialacceptanceinthepast
20years.
  In I/O parallelism, relations are partitioned among available disks so that
they can be retrieved faster. Three commonly used partitioning techniques
areround-robinpartitioning,hashpartitioning,andrangepartitioning.
  Skew is a major problem, especially with increasing degrees of parallelism.
Balancedpartitioningvectors,usinghistograms,andvirtualprocessorparti-
tioningareamongthetechniquesusedtoreduceskew.
  In interquery parallelism, we run different queries concurrently to increase
throughput.
  Intraqueryparallelismattemptstoreducethecostofrunningaquery.There
aretwotypesofintraqueryparallelism:intraoperationparallelismandinter-
operationparallelism.
  We use intraoperation parallelism to execute relational operations, such as
sortsandjoins,inparallel.Intraoperationparallelismisnaturalforrelational
operations,sincetheyaresetoriented.
  Therearetwobasicapproachestoparallelizingabinaryoperationsuchasa
join.
?
In partitioned parallelism, the relations are split into several parts, and
tuplesinr
i
arejoinedonlywithtuplesfroms
i
.Partitionedparallelismcan
beusedonlyfornaturalandequi-joins.
?
Infragmentandreplicate,bothrelationsarepartitionedandeachpartition
isreplicated.Inasymmetricfragmentandreplicate,oneoftherelationsis
replicated while the other is partitioned. Unlike partitioned parallelism,
fragment and replicate and asymmetric fragment and replicate can be
usedwithanyjoincondition.
820 Chapter18 ParallelDatabases
Both parallelization techniques can work in conjunction with any join tech-
nique.
  In independent parallelism,different operations that do not depend on one
anotherareexecutedinparallel.
  In pipelinedparallelism,processors send the resultsof one operation to an-
otheroperationasthoseresultsarecomputed,withoutwaitingfortheentire
operationto?nish.
  Queryoptimizationinparalleldatabasesissigni?cantlymorecomplexthan
queryoptimizationinsequentialdatabases.
  Modernmulticoreprocessorsareintroducingnewresearchproblemsinpar-
alleldatabases.
ReviewTerms
  Decision-supportqueries
  I/Oparallelism
  Horizontalpartitioning
  Partitioningtechniques
?
Round-robin
?
Hashpartitioning
?
Rangepartitioning
  Partitioningattribute
  Partitioningvector
  Pointquery
  Rangequery
  Skew
?
Executionskew
?
Attribute-valueskew
?
Partitionskew
  Handlingofskew
?
Balancedrange-partitioning
vector
?
Histogram
?
Virtualprocessors
  Interqueryparallelism
  Cachecoherency
  Intraqueryparallelism
?
Intraoperationparallelism
?
Interoperationparallelism
  Parallelsort
?
Range-partitioningsort
?
Parallelexternalsort–merge
  Dataparallelism
  Paralleljoin
?
Partitionedjoin
?
Fragment-and-replicatejoin
?
Asymmetricfragment-and-
replicatejoin
?
Partitionedparallelhashjoin
?
Parallelnested-loopjoin
  Parallelselection
  Parallelduplicateelimination
  Parallelprojection
  Parallelaggregation
  Costofparallelevaluation
PracticeExercises 821
  Interoperationparallelism
?
Pipelinedparallelism
?
Independentparallelism
  Queryoptimization
  Scheduling
  Exchange-operatormodel
  Designofparallelsystems
  Onlineindexconstruction
  Multicoreprocessors
PracticeExercises
18.1 Inarangeselectiononarange-partitionedattribute,itispossiblethatonly
one disk may need to be accessed. Describe the bene?ts and drawbacks
ofthisproperty.
18.2 Whatformofparallelism(interquery,interoperation,orintraoperation)is
likelytobethemostimportantforeachofthefollowingtasks?
a. Increasingthethroughputofasystemwithmanysmallqueries
b. Increasingthethroughputofasystemwithafewlargequeries,when
thenumberofdisksandprocessorsislarge
18.3 With pipelined parallelism, it is often a good idea to perform several
operationsinapipelineonasingleprocessor,evenwhenmanyprocessors
areavailable.
a. Explainwhy.
b. Would the arguments you advanced in part a hold if the machine
hasashared-memoryarchitecture?Explainwhyorwhynot.
c. Wouldtheargumentsinpart a holdwithindependentparallelism?
(Thatis,aretherecaseswhere,eveniftheoperationsarenotpipelined
andtherearemanyprocessorsavailable,itisstillagoodideatoper-
formseveraloperationsonthesameprocessor?)
18.4 Consider join processing using symmetric fragment and replicate with
range partitioning. How can youoptimize the evaluationifthe join con-
ditionisoftheform | r.A?s.B|? k,wherek isasmallconstant?Here,
| x | denotes the absolute value of x. A join with such a join condition is
calledabandjoin.
18.5 Recallthathistogramsareusedforconstructingload-balancedrangepar-
titions.
a. Supposeyouhaveahistogramwherevaluesarebetween1and100,
and are partitioned into 10 ranges, 1–10, 11–20, ...,91–100, with
frequencies 15, 5, 20, 10, 10, 5, 5, 20, 5, and 5, respectively. Give a
load-balanced range partitioningfunction todividethe valuesinto
5partitions.
822 Chapter18 ParallelDatabases
b. Writeanalgorithmforcomputingabalancedrangepartitionwith p
partitions, given a histogram of frequency distributions containing
nranges.
18.6 Large-scale parallel database systems store an extra copy of each data
itemondisksattachedtoadifferentprocessor,toavoidlossofdataifone
oftheprocessorsfails.
a. Insteadof keepingthe extracopy ofdataitemsfrom aprocessorat
a single backup processor, it is a good idea to partition the copies
ofthedataitemsofaprocessoracrossmultipleprocessors.Explain
why.
b. Explainhowvirtual-processorpartitioningcanbeusedtoef?ciently
implementthepartitioningofthecopiesasdescribedabove.
c. Whatarethebene?tsanddrawbacksofusing RAIDstorageinstead
ofstoringanextracopyofeachdataitem?
18.7 Suppose we wish to index a large relation that is partitioned. Can the
ideaof partitioning(including virtual processorpartitioning) be applied
to indices? Explain your answer, considering the following two cases
(assumingforsimplicitythatpartitioningaswellasindexingareonsingle
attributes):
a. Wheretheindexisonthepartitioningattributeoftherelation.
b. Where the index is on an attribute other than the partitioning at-
tributeoftherelation.
18.8 Supposeawell-balancedrange-partitioningvectorhadbeenchosenfora
relation,buttherelationissubsequentlyupdated,makingthepartitioning
unbalanced. Even if virtual-processor partitioning is used, a particular
virtualprocessormayendupwithaverylargenumberoftuplesafterthe
update,andrepartitioningwouldthenberequired.
a. Suppose a virtual processor has a signi?cant excess of tuples (say,
twicetheaverage).Explainhowrepartitioningcanbedonebysplit-
ting the partition, thereby increasing the number of virtual proces-
sors.
b. If, instead of round-robin allocation of virtual processors, virtual
partitions can be allocated to processors in an arbitrary fashion,
with a mapping table tracking the allocation. If a particular node
has excess load (compared to the others), explain how load can be
balanced.
c. Assuming there are no updates, does query processing have to be
stopped while repartitioning, or reallocation of virtual processors,
iscarriedout?Explainyouranswer.
Exercises 823
Exercises
18.9 For each of the three partitioning techniques, namely round-robin, hash
partitioning,andrangepartitioning,giveanexampleofaqueryforwhich
thatpartitioningtechniquewouldprovidethefastestresponse.
18.10 Whatfactorscouldresultinskewwhenarelationispartitionedononeof
itsattributesby:
a. Hashpartitioning?
b. Rangepartitioning?
Ineachcase,whatcanbedonetoreducetheskew?
18.11 Give an example of a join that is not a simple equi-join for which parti-
tionedparallelismcanbeused.Whatattributesshouldbeusedforparti-
tioning?
18.12 Describeagoodwaytoparallelizeeachofthefollowing:
a. Thedifferenceoperation
b. Aggregationbythecountoperation
c. Aggregationbythecountdistinctoperation
d. Aggregationbytheavgoperation
e. Leftouterjoin,ifthejoinconditioninvolvesonlyequality
f. Leftouterjoin,ifthejoinconditioninvolvescomparisonsotherthan
equality
g. Fullouterjoin,ifthejoinconditioninvolvescomparisonsotherthan
equality
18.13 Describethebene?tsanddrawbacksofpipelinedparallelism.
18.14 Suppose you wish to handle a workload consisting of a large number of
smalltransactionsbyusingshared-nothingparallelism.
a. Is intraquery parallelism required in such a situation? If not, why,
andwhatformofparallelismisappropriate?
b. Whatformofskewwouldbeofsigni?cancewithsuchaworkload?
c. Suppose most transactions accessed one account record, which in-
cludesanaccount typeattribute,andanassociatedaccount type master
record, which provides information about the account type. How
wouldyoupartitionand/orreplicatedatatospeeduptransactions?
You may assume that the account type master relation is rarely up-
dated.
824 Chapter18 ParallelDatabases
18.15 The attribute on which a relation is partitioned can have a signi?cant
impactonthecostofaquery.
a. GivenaworkloadofSQLqueriesonasinglerelation,whatattributes
wouldbecandidatesforpartitioning?
b. How would you choose between the alternative partitioning tech-
niques,basedontheworkload?
c. Is it possible to partition a relation on more than one attribute?
Explainyouranswer.
BibliographicalNotes
Inthelate1970sandearly1980s,astherelationalmodelgainedreasonablysound
footing,peoplerecognizedthatrelationaloperatorsarehighlyparallelizableand
havegooddata?owproperties.Severalresearchprojectsincluding GAMMA(De-
Witt [1990]), XPRS (Stonebraker et al. [1989]) and Volcano (Graefe [1990]) were
launched to investigate the practicality of parallel execution of relational opera-
tors.
Teradatawasoneofthe?rstcommercialparalleldatabasesystems,andcon-
tinuestohavealargemarketshare.TheRedBrickWarehousewasanotherearly
parallel database system; Red Brick was was acquired by Informix, which was
itself acquired by IBM. More recent parallel database systems include Netezza,
DATAllegro(nowpartofMicrosoft),Greenplum,andAsterData.
Locking in parallel databases is discussed in Joshi [1991] and Mohan and
Narang[1992]. Cache-coherencyprotocolsforparalleldatabase systemsaredis-
cussed byDiaset al.[1989], Mohan and Narang[1992], and Rahm [1993]. Carey
etal.[1991]discussescachingissuesinaclient–serversystem.
Graefe and McKenna [1993b] presentsan excellentsurveyof queryprocess-
ing, including parallel processing of queries. The exchange-operator modelwas
advocatedbyGraefe[1990]andGraefeandMcKenna[1993b].
ParallelsortingisdiscussedinDeWittetal.[1992].Parallelsortingonmulti-
core and multithreaded processors is discussed in Garcia and Korth [2005] and
Chen et al. [2007]. Parallel join algorithms are described by Nakayama et al.
[1984], Richardson et al. [1987], Kitsuregawa and Ogawa [1990], and Wilschut
etal.[1995],amongotherworks.
Skew handling in parallel joins is described by Walton et al. [1991], Wolf
[1991],andDeWittetal.[1992].
Parallelquery-optimizationtechniques are described by Lu etal. [1991] and
Gangulyetal.[1992].
The adaptation of database-systemdesign and query-processingalgorithms
tomulticoreandmultithreadedarchitecturesisdiscussedintheproceedingsofthe
International Workshop on Data Management on Modern Hardware (DaMoN),
heldannuallysince2005.
CHAPTER
19
Distributed Databases
Unlike parallel systems, in which the processors are tightly coupled and consti-
tute a single database system, a distributed database system consists of loosely
coupledsitesthatshare nophysicalcomponents. Furthermore,the databasesys-
temsthatrunoneachsitemayhaveasubstantialdegreeofmutualindependence.
We discussed the basic structure of distributedsystemsin Chapter17.
Each site may participate in the execution of transactions that access data at
onesite,orseveralsites.Themaindifferencebetweencentralizedanddistributed
database systems is that, in the former, the data reside in one single location,
whereasinthelatter,thedataresideinseverallocations.Thisdistributionofdata
is the cause of many dif?culties in transaction processing and query processing.
In thischapter, we addressthese dif?culties.
We start by classifying distributed databases as homogeneous or heteroge-
neous, in Section 19.1. We then address the question of how to store data in a
distributeddatabaseinSection19.2.InSection19.3,weoutlineamodelfortrans-
action processing in a distributed database. In Section 19.4, we describe how to
implement atomic transactions in a distributed database by using special com-
mit protocols. In Section 19.5, we describe concurrency control in distributed
databases. In Section 19.6, we outline how to provide high availability in a dis-
tributeddatabasebyexploitingreplication,sothesystemcancontinueprocessing
transactions even when there is a failure. We address query processing in dis-
tributed databases in Section 19.7. In Section 19.8, we outline issues in handling
heterogeneousdatabases. In Section 19.10, we describe directorysystems,which
can be viewedas aspecializedform of distributeddatabases.
In this chapter, we illustrate all our examples using the bank database of
Figure 19.1.
19.1 Homogeneous and Heterogeneous Databases
Inahomogeneousdistributeddatabasesystem,allsiteshaveidenticaldatabase-
managementsystemsoftware,areawareofoneanother,andagreetocooperatein
processingusers’requests.Insuchasystem,localsitessurrenderaportionoftheir
autonomy in terms of their right to change schemas or database-management
825
826 Chapter19 DistributedDatabases
branch(branch name, branch city, assets)
account (account number, branch name, balance)
depositor (customer name, account number)
Figure 19.1 Banking database.
systemsoftware.Thatsoftwaremustalsocooperatewithothersitesinexchanging
information about transactions, to make transaction processing possible across
multiplesites.
Incontrast, inaheterogeneousdistributeddatabase,differentsitesmayuse
differentschemas,anddifferentdatabase-managementsystemsoftware.Thesites
may not be aware of one another, and they may provide only limited facilities
for cooperation in transaction processing. The differences in schemas are often a
majorproblemforqueryprocessing,whilethedivergenceinsoftwarebecomesa
hindrance for processingtransactions that access multiplesites.
Inthischapter,weconcentrateonhomogeneousdistributeddatabases.How-
ever,inSection19.8webrie?ydiscussissuesinheterogeneousdistributeddatabase
systems.
19.2 Distributed Data Storage
Considerarelationrthatistobestoredinthedatabase.Therearetwoapproaches
to storingthis relationin the distributeddatabase:
• Replication. The system maintains several identical replicas (copies) of the
relation, and stores each replica at a different site. The alternative to replica-
tion isto storeonly one copy of relation r.
• Fragmentation. The system partitions the relation into several fragments,
and storeseach fragmentat adifferentsite.
Fragmentation and replication can be combined: A relation can be partitioned
intoseveralfragmentsand theremaybeseveralreplicasofeachfragment.Inthe
following subsections, we elaborate on each of thesetechniques.
19.2.1 Data Replication
If relation r isreplicated,a copy of relation ris storedin two or more sites.In the
most extreme case, we have full replication, in which a copy is stored in every
site in the system.
There are anumber of advantagesand disadvantagestoreplication.
• Availability. If one of the sites containing relation r fails, then the relation r
canbefoundinanothersite.Thus,thesystemcancontinuetoprocessqueries
involving r,despitethe failureof one site.
19.2 DistributedDataStorage 827
• Increasedparallelism. In the case where the majorityof accessesto the rela-
tion r resultin only the readingof the relation,then severalsitescan process
queriesinvolvingrinparallel.Themorereplicasofrthereare,thegreaterthe
chance that the needed data will be found in the site where the transaction
is executing. Hence, data replication minimizes movement of data between
sites.
• Increasedoverheadonupdate.Thesystemmustensurethatallreplicasofa
relationrareconsistent;otherwise,erroneouscomputationsmayresult.Thus,
wheneverrisupdated,theupdatemustbepropagatedtoallsitescontaining
replicas.Theresultisincreasedoverhead.Forexample,inabankingsystem,
where account information is replicated in various sites, it is necessary to
ensure that the balance in a particularaccount agreesin allsites.
In general, replication enhances the performance of read operations and in-
creasestheavailabilityofdatatoread-onlytransactions.However,updatetrans-
actions incur greateroverhead. Controllingconcurrent updatesby severaltrans-
actionstoreplicateddataismorecomplexthanincentralizedsystems,whichwe
studied in Chapter 15. We can simplify the management of replicas of relation
r by choosing one of them as the primary copy of r. For example, in a banking
system,an account can be associatedwith the site inwhich the account has been
opened.Similarly,inanairline-reservationsystem,a?ightcanbeassociatedwith
thesiteatwhichthe?ightoriginates.Weshallexaminetheprimarycopyscheme
and otheroptions for distributedconcurrency control inSection 19.5.
19.2.2 Data Fragmentation
If relation r is fragmented, r is divided into a number of fragments r
1
, r
2
,...,r
n
.
These fragments contain suf?cient information to allow reconstruction of the
original relation r. There are two different schemes for fragmenting a relation:
horizontal fragmentation and vertical fragmentation. Horizontal fragmentation
splits the relation by assigning each tuple of r to one or more fragments. Vertical
fragmentation splitsthe relationby decomposingthe scheme Rof relation r.
In horizontal fragmentation,arelationr is partitioned into a number of
subsets, r
1
, r
2
,...,r
n
. Each tuple of relation r must belong to at least one of the
fragments, so that the originalrelationcan be reconstructed, ifneeded.
As an illustration, the account relation can be divided into several different
fragments, each of which consists of tuples of accounts belonging to a particular
branch. If the banking system has only two branches—Hillside and Valleyview
—then therearetwo differentfragments:
account
1
=   branch name = “Hillside”
(account)
account
2
=   branch name = “Valleyview”
(account)
Horizontal fragmentation is usually used to keep tuples at the sites where they
are used the most, to minimize datatransfer.
828 Chapter19 DistributedDatabases
In general, a horizontal fragment can be de?ned as a selection on the global
relation r.Thatis,weuseapredicateP
i
to construct fragmentr
i
:
r
i
=   P
i
(r)
We reconstruct the relation r by takingthe union of all fragments;that is:
r = r
1
? r
2
?···? r
n
In our example, the fragments are disjoint. By changing the selection predi-
cates used to construct the fragments, we can have a particular tuple of r appear
in more than one of ther
i
.
In its simplest form, vertical fragmentation is the same as decomposition
(see Chapter 8).Verticalfragmentation of r(R) involves the de?nition of several
subsetsof attributes R
1
, R
2
,...,R
n
of the schema R so that:
R = R
1
? R
2
?···? R
n
Each fragment r
i
of r isde?nedby:
r
i
= null
R
i
(r)
Thefragmentationshouldbedoneinsuchawaythatwecanreconstructrelation
r fromthe fragments by takingthe natural join:
r = r
1
  r
2
  r
3
  ···   r
n
Onewayofensuringthattherelationr canbereconstructedistoincludethe
primary-keyattributesof Rineach R
i
.Moregenerally,anysuperkeycanbeused.
It is often convenient to add a special attribute, called a tuple-id, to the schema
R. The tuple-id value of a tuple is a unique value that distinguishes the tuple
fromallothertuples.Thetuple-idattributethusservesasacandidatekeyforthe
augmented schema, and is included in each R
i
. The physical or logical address
for a tuplecan be used as atuple-id,since each tuplehas a unique address.
Toillustrateverticalfragmentation, considera universitydatabase witha re-
lation employee info that stores, for each employee, employee id, name, designation,
andsalary.Forprivacyreasons,thisrelationmaybefragmentedintoarelationem-
ployee private info containing employee id and salary, and another relation employee
public info containing attributes employee id, name,anddesignation.Thesemaybe
storedat differentsites,again, possiblyfor securityreasons.
Thetwotypesoffragmentationcanbeappliedtoasingleschema;forinstance,
the fragments obtained by horizontally fragmenting a relation can be further
partitioned vertically. Fragments can also be replicated. In general, a fragment
can be replicated,replicasof fragmentscan be fragmented further, and soon.
19.2 DistributedDataStorage 829
19.2.3 Transparency
Theuserofadistributeddatabasesystemshouldnotberequiredtoknowwhere
the data are physically located nor how the data can be accessed at the speci?c
local site.This characteristic, calleddatatransparency, can take severalforms:
• Fragmentationtransparency.Usersarenotrequiredtoknowhow arelation
has beenfragmented.
• Replication transparency. Users view each data object as logically unique.
The distributed system may replicate an object to increase either system
performance or data availability. Users do not have to be concerned with
what data objectshavebeenreplicated,or wherereplicashavebeenplaced.
• Locationtransparency.Usersarenotrequiredtoknowthephysicallocation
of the data. The distributed database system should be able to ?nd any data
aslong as the data identi?erissuppliedby the usertransaction.
Data items—such as relations, fragments, and replicas—must have unique
names. This propertyis easy to ensurein a centralizeddatabase.In a distributed
database,however,wemusttakecaretoensurethattwositesdonotusethesame
name fordistinctdata items.
One solution to this problem is to require all names to be registered in a
central name server. The name server helps to ensure that the same name does
not get used for different data items. We can also use the name server to locate a
data item,giventhe name of the item.This approach, however,suffers fromtwo
major disadvantages. First, the name server may become a performance bottle-
neckwhendataitemsarelocatedbytheirnames,resultinginpoorperformance.
Second, if the name server crashes, it may not be possible for any site in the
distributedsystemto continue to run.
A more widely used alternative approach requires that each site pre?x its
own site identi?er to any name that it generates. This approach ensures that
no two sites generate the same name (since each site has a unique identi?er).
Furthermore,nocentralcontrolisrequired.Thissolution,however,failstoachieve
locationtransparency,sincesiteidenti?ersareattachedtonames.Thus,theaccount
relation might be referred to as site17. account,oraccount@site17, rather than as
simply account.ManydatabasesystemsusetheInternetaddress(IPaddress)ofa
sitetoidentifyit.
Toovercomethisproblem,thedatabasesystemcancreateasetofalternative
names, or aliases, for data items. A user may thus refer to data items by simple
names that are translated by the system to complete names. The mapping of
aliases to the real names can be stored at each site. With aliases, the user can be
unaware of the physical location of a data item. Furthermore, the user will be
unaffected if the database administrator decides to move a data item from one
site toanother.
Users should not have to refer to a speci?c replica of a data item. Instead,
the system should determine which replica to reference on a read request, and
830 Chapter19 DistributedDatabases
should update all replicas on a write request. We can ensure that it does so by
maintaining a catalog table, which the system uses to determine all replicas for
the dataitem.
19.3 Distributed Transactions
Access to the various data items in a distributedsystem is usually accomplished
through transactions, which must preserve the ACID properties (Section 14.1).
Therearetwotypesoftransactionthatweneedtoconsider.Thelocaltransactions
are those that access and update data in only one local database; the global
transactions are those that access and update data in several local databases.
EnsuringtheACIDpropertiesofthelocaltransactionscanbedoneasdescribedin
Chapters14,15,and16.However,forglobaltransactions, thistaskismuchmore
complicated, since several sites may be participating in execution. The failure of
one of these sites, or the failure of a communication link connecting these sites,
may resultin erroneouscomputations.
In this section, we study the system structure of a distributed database and
itspossiblefailuremodes.InSection19.4,westudyprotocolsforensuringatomic
commitofglobaltransactions,andinSection19.5westudyprotocolsforconcur-
rencycontrolindistributeddatabases.InSection19.6,westudyhowadistributed
databasecancontinuefunctioningeveninthepresenceofvarioustypesoffailure.
19.3.1 System Structure
Each site has its own local transaction manager, whose function is to ensure the
ACID properties of those transactions that execute at that site. The various trans-
action managers cooperate to execute global transactions. To understand how
suchamanagercanbeimplemented,consideranabstractmodelofatransaction
system,in which each site contains two subsystems:
• The transaction manager manages the execution of those transactions (or
subtransactions) that access data stored in a local site. Note that each such
transaction may be either a local transaction (that is, a transaction that exe-
cutes at only that site) or part of a global transaction (that is, a transaction
that executesat severalsites).
• The transaction coordinator coordinates the execution of the various trans-
actions (both local and global) initiatedat that site.
The overallsystemarchitecture appearsin Figure 19.2.
The structure of a transaction manager is similar in many respects to the
structure of acentralized system.Each transaction manager isresponsiblefor:
• Maintaining a logfor recoverypurposes.
19.3 DistributedTransactions 831
TM
1
TM
n
computer 1 computer n
TC
1
TC
n
transaction
coordinator
transaction
manager
Figure 19.2 System architecture.
• Participatinginanappropriateconcurrency-controlschemetocoordinatethe
concurrent execution of the transactions executingatthat site.
As we shall see, we need to modify both the recovery and concurrency schemes
to accommodate the distributionof transactions.
The transaction coordinator subsystem is not needed in the centralized en-
vironment, since a transaction accesses data at only a single site. A transaction
coordinator, as its name implies, is responsible for coordinating the execution of
allthetransactionsinitiatedatthatsite.Foreachsuchtransaction,thecoordinator
isresponsible for:
• Startingthe execution ofthe transaction.
• Breaking the transaction into a number of subtransactions and distributing
these subtransactions tothe appropriatesitesfor execution.
• Coordinating the termination of the transaction, which may result in the
transaction being committed atallsitesor aborted atall sites.
19.3.2 System Failure Modes
Adistributedsystemmaysufferfromthesametypesoffailurethatacentralized
system does (for example, software errors, hardware errors, or disk crashes).
There are, however, additional types of failure with which we need to deal in a
distributedenvironment.The basic failure typesare:
• Failureof a site.
• Lossof messages.
832 Chapter19 DistributedDatabases
• Failureof a communication link.
• Networkpartition.
The loss or corruption of messages is always a possibility in a distributed
system.Thesystemusestransmission-controlprotocols,suchasTCP/IP,tohandle
sucherrors.Informationaboutsuchprotocolsmaybefoundinstandardtextbooks
on networking (see the bibliographical notes).
However, if two sites A and B are not directly connected, messages from
one to the other must be routed through a sequence of communication links. If a
communication linkfails,messagesthatwould havebeentransmittedacrossthe
link must be rerouted. In some cases, it is possible to ?nd another route through
the network, so that the messages are able to reach their destination. In other
cases, a failure may result in there being no connection between some pairs of
sites. A system is partitioned if it has been split into two (or more) subsystems,
called partitions, that lack any connection between them. Note that, under this
de?nition, a partitionmay consist of a singlenode.
19.4 Commit Protocols
If we are to ensure atomicity,all the sitesinwhich atransaction Texecutedmust
agree on the ?nal outcome of the execution. T must either commit at all sites, or
it must abort at allsites. To ensure this property,the transaction coordinator of T
must executea commit protocol.
Amongthesimplestandmostwidelyusedcommitprotocolsisthetwo-phase
commitprotocol (2PC),whichisdescribedinSection19.4.1. An alternative is the
three-phase commit protocol (3PC), which avoids certain disadvantages of the
2PCprotocolbutaddstocomplexityandoverhead.Section19.4.2brie?youtlines
the 3PCprotocol.
19.4.1 Two-Phase Commit
We ?rst describe how the two-phase commit protocol (2PC) operates during nor-
maloperation,thendescribehowithandlesfailuresand?nallyhowitcarriesout
recoveryand concurrency control.
ConsideratransactionTinitiatedatsite S
i
,wherethetransactioncoordinator
is C
i
.
19.4.1.1 TheCommitProtocol
When T completes its execution—that is, when all the sites at which T has exe-
cuted inform C
i
that Thas completed—C
i
startsthe 2PCprotocol.
• Phase1. C
i
adds the record <prepare T> to the log, and forces the log onto
stable storage. It then sends a prepare T message to all sites at which T
executed. On receiving such a message, the transaction manager at that site
19.4 CommitProtocols 833
determines whether it is willing to commit its portion of T.Iftheansweris
no,itaddsarecord <noT>tothelog,andthenrespondsbysendinganabort
T message to C
i
. If the answer is yes, it adds a record <ready T> to the log,
andforcesthelog(withallthelogrecordscorrespondingtoT)ontostable
storage.The transaction managerthen replieswith a ready T messageto C
i
.
• Phase 2.WhenC
i
receives responses to the prepare T message from all the
sites, or when a prespeci?ed interval of time has elapsed since the prepare
T message was sent out, C
i
can determine whether the transaction T can be
committedoraborted.Transaction TcanbecommittedifC
i
receivedaready
T message from all the participating sites. Otherwise, transaction T must be
aborted. Depending on the verdict, either a record <commit T> or a record
<abortT>isaddedtothelogandthelogisforcedontostablestorage.Atthis
point, the fate of the transaction has been sealed. Following this point, the
coordinatorsendseitheracommitToranabortTmessagetoallparticipating
sites.Whena sitereceivesthat message,itrecordsthe message in the log.
AsiteatwhichT executed can unconditionally abort T at any time before
it sends the message ready T to the coordinator. Once the message is sent, the
transaction is said to be in the ready state at the site. The ready T message is,
in effect, a promise by a site to follow the coordinator’s order to commit T or to
abort T. To make such a promise, the needed information must ?rst be stored
in stable storage. Otherwise, if the site crashes after sending ready T,itmaybe
unable to make good on its promise. Further, locks acquired by the transaction
mustcontinue tobe held until the transaction completes.
Sinceunanimity isrequiredtocommitatransaction, thefate of Tissealedas
soon as at least one site responds abort T. Since the coordinator site S
i
is one of
the sites at which T executed, the coordinator can decide unilaterally to abort T.
The?nalverdictregardingTisdeterminedatthetimethatthecoordinatorwrites
that verdict(commit or abort) to the log and forces that verdictto stable storage.
In some implementations of the 2PC protocol, a site sends an acknowledge T
message to the coordinator at the end of the second phase of the protocol. When
thecoordinatorreceivestheacknowledgeTmessagefromallthesites,itaddsthe
record <complete T>to the log.
19.4.1.2 HandlingofFailures
The 2PC protocol respondsin differentways to varioustypesof failures:
• Failure of a participating site. If the coordinator C
i
detects that a site has
failed, it takes these actions: If the site fails before responding with a ready
T message to C
i
, the coordinator assumes that it responded with an abort T
message.IfthesitefailsafterthecoordinatorhasreceivedthereadyTmessage
from the site, the coordinator executes the rest of the commit protocol in the
normal fashion, ignoring the failure of the site.
Whenaparticipatingsite S
k
recoversfromafailure,itmustexamineitslog
todeterminethefateofthosetransactionsthatwereinthemidstofexecution
834 Chapter19 DistributedDatabases
when the failure occurred. Let T be one such transaction. We consider each
ofthepossiblecases:
?
The log contains a <commit T> record. In this case, the site executes
redo(T).
?
The log contains an <abort T> record. In this case, the site executes
undo(T).
?
The log contains a <ready T> record. In this case, the site must consult
C
i
to determine the fate of T.IfC
i
is up, it noti?es S
k
regarding whether
T committed or aborted. In the former case, it executes redo(T); in the
latter case, it executes undo(T). If C
i
is down, S
k
must try to ?nd the
fate of T from other sites. It does so by sending a querystatus T message
to all the sites in the system. On receiving such a message, a site must
consult its log to determine whether T has executed there, and if T has,
whether Tcommittedoraborted.Itthennoti?es S
k
aboutthisoutcome.If
no site has the appropriate information (that is, whether T committed or
aborted),then S
k
canneitherabortnorcommitT.Thedecisionconcerning
T ispostponed until S
k
can obtain theneededinformation. Thus, S
k
must
periodicallyresendthequerystatusmessagetotheothersites.Itcontinues
to do so until a site that contains the needed information recovers. Note
that the siteat which C
i
residesalways has the neededinformation.
?
The log contains no control records (abort, commit, ready)concerningT.
Thus,weknowthat S
k
failedbeforerespondingtothe prepare Tmessage
from C
i
. Since the failure of S
k
precludes the sending of such a response,
by our algorithm C
i
mustabort T.Hence,S
k
mustexecute undo(T).
• Failureofthecoordinator.Ifthecoordinatorfailsinthemidstoftheexecution
of the commit protocol for transaction T, then the participating sites must
decidethe fate of T. We shall see that, in certain cases, the participating sites
cannot decide whether to commit or abort T, and therefore these sites must
wait forthe recoveryof the failed coordinator.
?
If an active site contains a <commit T> record in its log, then T must be
committed.
?
If an active site contains an <abort T> record in its log, then T must be
aborted.
?
If some active site does not contain a <ready T> record in its log, then
thefailedcoordinator C
i
cannothavedecidedtocommit T,becauseasite
thatdoesnothavea <ready T>record in its log cannot have sent a ready
T message to C
i
. However, the coordinator may have decided to abort T,
but not to commit T. Rather than wait for C
i
to recover, it is preferable to
abort T.
?
If none of the preceding cases holds, then all active sites must have a
<ready T> record in their logs, but no additional control records (such
19.4 CommitProtocols 835
as <abort T> or <commit T>). Since the coordinator has failed, it is
impossible to determine whether a decision has been made, and if one
has, what that decisionis, until the coordinator recovers.Thus, the active
sitesmustwaitforC
i
torecover.SincethefateofTremainsindoubt,Tmay
continue toholdsystemresources.Forexample,iflockingisused, Tmay
hold locks on dataat active sites.Such a situationis undesirable,because
it may be hours or days before C
i
is again active. During this time, other
transactions may be forced to wait for T. As a result, data items may be
unavailablenotonlyonthefailedsite(C
i
),butonactivesitesaswell.This
situation is called the blocking problem, because T is blocked pending
the recoveryof site C
i
.
• Networkpartition. When a network partitions,two possibilitiesexist:
1. The coordinator and all its participants remain in one partition. In this
case, the failure has no effecton the commitprotocol.
2. The coordinator and its participants belong to several partitions. From
the viewpoint of the sites in one of the partitions, it appears that the
sites in other partitions have failed. Sites that are not in the partition
containing the coordinator simply execute the protocol to deal with
failure of the coordinator. The coordinator and the sites that are in the
same partition as the coordinator follow the usual commit protocol,
assumingthat the sitesin the other partitionshave failed.
Thus, the major disadvantage of the 2PC protocol is that coordinator failure may
resultinblocking,whereadecisioneithertocommitortoabortTmayhavetobe
postponed until C
i
recovers.
19.4.1.3 RecoveryandConcurrencyControl
When a failed site restarts, we can perform recovery by using, for example, the
recovery algorithm described in Section 16.4. To deal with distributed commit
protocols, the recoveryprocedure musttreatin-doubttransactionsspecially; in-
doubt transactions are transactions for which a <ready T> log record is found,
but neither a <commit T> log record nor an <abort T> log record is found. The
recovering site must determine the commit–abort status of such transactions by
contacting other sites,asdescribedin Section19.4.1.2.
Ifrecoveryisdoneasjustdescribed,however,normaltransactionprocessing
at the site cannot begin until all in-doubt transactions have been committed or
rolledback.Findingthestatusofin-doubttransactionscanbeslow,sincemultiple
sitesmayhavetobecontacted.Further,ifthecoordinatorhasfailed,andnoother
sitehasinformationaboutthecommit–abortstatusofanincompletetransaction,
recovery potentially could become blocked if 2PC is used. As a result, the site
performingrestartrecoverymayremainunusable fora long period.
To circumvent this problem, recovery algorithms typically provide support
fornotinglockinformationinthelog.(Weareassumingherethatlockingisused
forconcurrencycontrol.)Insteadofwritinga <readyT>logrecord,thealgorithm
836 Chapter19 DistributedDatabases
writes a <ready T, L> log record, where L is a list of all write locks held by the
transaction T when the log record is written. At recovery time, after performing
local recoveryactions, for everyin-doubt transaction T, all the write locks noted
in the <ready T, L> logrecord (read fromthe log)are reacquired.
After lock reacquisition is complete for all in-doubt transactions, transaction
processing can start at the site, even before the commit–abort status of the in-
doubttransactionsisdetermined.Thecommitorrollbackofin-doubttransactions
proceedsconcurrentlywiththeexecutionofnewtransactions.Thus,siterecovery
is faster, and never gets blocked. Note that new transactions that have a lock
con?ictwithanywritelocksheldbyin-doubttransactionswillbeunabletomake
progressuntilthecon?ictingin-doubttransactionshavebeencommittedorrolled
back.
19.4.2 Three-Phase Commit
The three-phase commit (3PC) protocol is an extension of the two-phase commit
protocol that avoids the blocking problem under certain assumptions. In partic-
ular, it is assumed that no network partition occurs, and not more than k sites
fail, where k is some predetermined number. Under these assumptions, the pro-
tocol avoids blocking by introducing an extra third phase where multiple sites
are involved in the decision to commit. Instead of directly noting the commit
decisioninitspersistentstorage,thecoordinator?rstensuresthatatleastk other
sitesknow thatitintendedtocommitthetransaction. Ifthecoordinatorfails,the
remaining sites ?rst select a new coordinator. This new coordinator checks the
status of the protocol from the remaining sites; if the coordinator had decided
to commit, at least one of the other k sites that it informed will be up and will
ensure that the commit decision is respected. The new coordinator restarts the
thirdphaseoftheprotocolifsomesiteknewthattheoldcoordinatorintendedto
commit the transaction. Otherwise the new coordinator aborts the transaction.
While the 3PC protocol has the desirable property of not blocking unless k
sitesfail,ithasthedrawbackthatapartitioningofthenetworkmayappeartobe
thesameasmorethank sitesfailing,whichwouldleadtoblocking.Theprotocol
also has to be implemented carefully to ensure that network partitioning (or
more than k sites failing) does not result in inconsistencies, where a transaction
is committed in one partition and aborted in another. Because of its overhead,
the 3PC protocol is not widely used. See the bibliographical notes for references
givingmore detailsof the 3PC protocol.
19.4.3 Alternative Models of Transaction Processing
Formanyapplications,theblockingproblemoftwo-phasecommitisnotaccept-
able. The problem here is the notion of a single transaction that works across
multiplesites.Inthissection,wedescribehowtousepersistentmessagingtoavoid
theproblemofdistributedcommit,andthenbrie?youtlinethelargerissueof
work?ows; work?ows are considered inmore detailin Section 26.2.
To understand persistentmessaging, consider how one might transfer funds
betweentwodifferentbanks,eachwithitsowncomputer.Oneapproachistohave
19.4 CommitProtocols 837
a transaction span the two sites and use two-phase commit to ensure atomicity.
However,thetransactionmayhavetoupdatethetotalbankbalance,andblocking
could have a serious impact on all other transactions at each bank, since almost
all transactions atthe bank would update the total bank balance.
In contrast, consider how funds transfer by a bank check occurs. The bank
?rst deducts the amount of the check from the available balance and prints out
a check. The check is then physically transferred to the other bank where it is
deposited. After verifying the check, the bank increases the local balance by the
amountofthecheck.Thecheckconstitutesamessagesentbetweenthetwobanks.
Sothatfundsarenotlostorincorrectlyincreased,thecheckmustnotbelost,and
mustnotbeduplicatedanddepositedmorethanonce.Whenthebankcomputers
areconnectedby anetwork,persistentmessagesprovidethesameserviceasthe
check (but much faster, ofcourse).
Persistent messages are messages that are guaranteed to be delivered to
the recipient exactly once (neither less nor more), regardless of failures, if the
transactionsendingthemessagecommits,andareguaranteedtonotbedelivered
if the transaction aborts. Database recovery techniques are used to implement
persistent messaging on top of the normal network channels, as we shall see
shortly. In contrast, regular messages may be lost or may even be delivered
multipletimesinsome situations.
Errorhandlingismorecomplicatedwithpersistentmessagingthanwithtwo-
phasecommit.Forinstance,iftheaccountwherethecheckistobedepositedhas
been closed,the check must be sent back to the originating account and credited
backthere.Bothsitesmustthereforebeprovidedwitherror-handlingcode,along
withcodetohandlethepersistentmessages.Incontrast,withtwo-phasecommit,
the error would be detected by the transaction, which would then never deduct
the amount in the ?rst place.
The types of exception conditions that may arise depend on the application,
so it is not possible for the database system to handle exceptions automatically.
Theapplicationprogramsthatsendandreceivepersistentmessagesmustinclude
code to handle exception conditions and bring the system back to a consistent
state. For instance, it is not acceptable to just lose the money being transferred if
the receiving account has been closed; the money must be credited back to the
originating account, and if that is not possible for some reason, humans must be
alertedtoresolvethe situation manually.
Therearemanyapplicationswherethebene?tofeliminatingblockingiswell
worththeextraefforttoimplementsystemsthatusepersistentmessages.Infact,
few organizations would agree to support two-phase commit for transactions
originating outside the organization, since failures could result in blocking of
access to local data. Persistent messaging therefore plays an important role in
carrying out transactions that cross organizational boundaries.
Work?ows provide a general model of transaction processing involving mul-
tiple sites and possibly human processing of certain steps. For instance, when
a bank receives a loan application, there are many steps it must take, including
contactingexternalcredit-checkingagencies,beforeapprovingorrejectingaloan
application. The steps, together, form a work?ow. We study work?ows in more
838 Chapter19 DistributedDatabases
detailinSection26.2.Wealsonotethatpersistentmessagingformstheunderlying
basis for work?ows ina distributedenvironment.
We now consider the implementation of persistent messaging. Persistent
messagingcanbeimplementedontopofanunreliablemessaginginfrastructure,
which may lose messagesor deliverthemmultipletimes,by these protocols:
• Sendingsite protocol. When a transaction wishes to send a persistent mes-
sage,itwritesarecordcontainingthemessageinaspecialrelationmessages to
send, insteadofdirectlysendingoutthe message.The messageisalsogiven
a unique messageidenti?er.
A message delivery process monitors the relation, and when a new mes-
sage is found, it sends the message to its destination. The usual database
concurrency-control mechanisms ensure that the system process reads the
message only after the transaction that wrote the message commits; if the
transaction aborts, the usual recoverymechanism would deletethe message
fromthe relation.
The message delivery process deletes a message from the relation only
after it receives an acknowledgment from the destination site. If it receives
no acknowledgement from the destination site, after some time it sends the
message again. It repeats this until an acknowledgment is received. In case
of permanent failures, the system will decide, after some period of time,
that the message is undeliverable.Exception handling code providedby the
application isthen invokedto dealwith the failure.
Writing the message to a relation and processing it only after the trans-
action commits ensures that the message will be delivered if and only if the
transaction commits. Repeatedly sending it guarantees it will be delivered
evenifthere are (temporary)systemor networkfailures.
• Receiving site protocol. When a site receives a persistent message, it runs
a transaction that adds the message to a special received messages relation,
provided it is not already present in the relation (the unique message iden-
ti?er allows duplicates to be detected). After the transaction commits, or if
the message was already present in the relation, the receiving site sends an
acknowledgment back to the sendingsite.
Note that sending the acknowledgment before the transaction commits
is not safe, since a system failure may then result in loss of the message.
Checkingwhetherthemessagehasbeenreceivedearlierisessentialtoavoid
multipledeliveriesof themessage.
In many messaging systems, it is possible for messages to get delayed
arbitrarily, although such delays are very unlikely. Therefore, to be safe, the
message must never be deleted from the received messages relation. Deleting
it could result in a duplicate delivery not being detected. But as a result,
the received messages relation may grow inde?nitely. To deal with this prob-
lem, each message is given a timestamp, and if the timestamp of a received
message is older than some cutoff, the message is discarded. All messages
recorded in the received messagesrelationthatareolderthan thecutoffcanbe
deleted.
19.5 ConcurrencyControlinDistributedDatabases 839
19.5 Concurrency Control in Distributed Databases
Weshowherehow someoftheconcurrency-control schemesdiscussedinChap-
ter15 canbemodi?edsothat theycanbeusedina distributedenvironment.We
assumethateachsiteparticipatesintheexecutionofacommitprotocoltoensure
global transaction atomicity.
The protocols we describe in this section require updates to be done on
all replicas of a data item. If any site containing a replica of a data item has
failed,updatestothe dataitemcannot be processed.In Section19.6,we describe
protocolsthatcancontinuetransactionprocessingevenifsomesitesorlinkshave
failed,therebyprovidinghigh availability.
19.5.1 Locking Protocols
ThevariouslockingprotocolsdescribedinChapter15canbeusedinadistributed
environment.Theonlychangethatneedstobeincorporatedisinthewaythelock
managerdealswithreplicateddata.Wepresentseveralpossibleschemesthatare
applicable to an environment where data can be replicatedin severalsites.As in
Chapter15, we shall assumethe existenceofthe sharedand exclusive lockmodes.
19.5.1.1 SingleLock-ManagerApproach
Inthesinglelock-managerapproach,thesystemmaintainsasinglelockmanager
thatresidesinasinglechosensite—say S
i
.Alllockandunlockrequestsaremade
at site S
i
. When a transaction needs to lock a data item,it sends a lock requestto
S
i
.Thelockmanagerdetermineswhetherthelockcanbegrantedimmediately.If
thelockcanbegranted,thelockmanagersendsamessagetothateffecttothesite
at which the lock request was initiated. Otherwise, the request is delayed until
it can be granted, at which time a message is sent to the site at which the lock
requestwas initiated. The transaction can read the data item from any one of the
sitesatwhich areplicaofthedataitemresides.Inthecase ofawrite,allthe sites
wherea replicaofthe dataitemresidesmust beinvolvedinthewriting.
The scheme has these advantages:
• Simple implementation. This scheme requires two messages for handling
lock requestsand one messagefor handling unlock requests.
• Simple deadlock handling. Since all lock and unlock requests are made at
one site, the deadlock-handling algorithms discussed in Chapter 15 can be
applieddirectly.
The disadvantagesof the scheme are:
• Bottleneck.ThesiteS
i
becomes a bottleneck, since all requests must be pro-
cessedthere.
• Vulnerability.IfthesiteS
i
fails, the concurrency controller is lost. Either
processing must stop, or a recovery scheme must be used so that a backup
sitecan take overlock managementfrom S
i
, asdescribedin Section19.6.5.
840 Chapter19 DistributedDatabases
19.5.1.2 DistributedLockManager
A compromise between the advantages and disadvantages can be achieved
through the distributed-lock-manager approach, in which the lock-manager
function isdistributedoverseveralsites.
Each site maintains a local lock manager whose function is to administerthe
lock and unlock requestsfor those data itemsthat are stored in that site. When a
transaction wishes to lock a data item Q that is not replicated and resides at site
S
i
,amessageissenttothelockmanageratsite S
i
requestingalock(inaparticular
lockmode).Ifdataitem Qislockedinanincompatiblemode,thenthe requestis
delayed until it can be granted. Once it has determined that the lock request can
begranted,thelockmanagersendsamessagebacktotheinitiatorindicatingthat
ithas granted the lock request.
We discussseveralalternativeways of dealingwith replicationof dataitems
in Sections19.5.1.3to 19.5.1.6.
The distributed-lock-manager schemehastheadvantageofsimpleimple-
mentation, and reduces the degree to which the coordinator is a bottleneck. It
has a reasonably low overhead, requiring two message transfers for handling
lock requests, and one message transfer for handling unlock requests. However,
deadlock handling is more complex, since the lock and unlock requests are no
longer made at a single site: There may be intersite deadlocks even when there
is no deadlock within a single site. The deadlock-handling algorithms discussed
in Chapter 15 must be modi?ed, as we shall discuss in Section 19.5.4, to detect
global deadlocks.
19.5.1.3 PrimaryCopy
When a system uses data replication, we can choose one of the replicas as the
primarycopy.ForeachdataitemQ,theprimarycopyofQmustresideinprecisely
one site,which we call theprimarysite of Q.
When a transaction needs to lock a data item Q,itrequestsalockatthe
primary site of Q. As before, the response to the request is delayed until it can
be granted. The primary copy enables concurrency control for replicated data
to be handled like that for unreplicated data. This similarity allows for a simple
implementation. However, if the primary site of Q fails, Q is inaccessible, even
though othersitescontaining a replicamay be accessible.
19.5.1.4 MajorityProtocol
Themajorityprotocol works this way: If data item Q is replicated in n different
sites, then a lock-request message must be sent to more than one-half of the n
sitesinwhichQisstored.Eachlockmanagerdetermineswhetherthelockcanbe
grantedimmediately(asfarasitisconcerned).Asbefore,theresponseisdelayed
until the request can be granted. The transaction does not operate on Q until it
has successfully obtained a lock on a majorityof the replicasof Q.
Weassumefornowthatwritesareperformedonallreplicas,requiringallsites
containing replicas to be available. However, the major bene?t of the majority
19.5 ConcurrencyControlinDistributedDatabases 841
protocol is that it can be extended to deal with site failures, as we shall see in
Section 19.6.1. The protocol also deals with replicated data in a decentralized
manner,thusavoidingthedrawbacksofcentralcontrol.However,itsuffersfrom
thesedisadvantages:
• Implementation. The majority protocol is more complicated to implement
than are the previous schemes. It requires at least 2(n/2 + 1) messages for
handling lock requests and at least (n/2 + 1) messages for handling unlock
requests.
• Deadlock handling. In addition to the problem of global deadlocks due to
the use of a distributed-lock-manager approach, it is possible for a deadlock
tooccurevenifonlyonedataitemisbeinglocked.Asanillustration,consider
asystemwithfoursitesandfullreplication.Supposethattransactions T
1
and
T
2
wish to lock data item Q in exclusive mode. Transaction T
1
may succeed
in locking Q at sites S
1
and S
3
, while transaction T
2
may succeed in locking
Q at sites S
2
and S
4
. Each then must wait to acquire the third lock; hence, a
deadlock has occurred. Luckily, we can avoid such deadlocks with relative
ease, by requiring all sites to request locks on the replicas of a data item in
thesamepredeterminedorder.
19.5.1.5 BiasedProtocol
The biasedprotocol is another approach to handling replication. The difference
fromthemajorityprotocolisthatrequestsforsharedlocksaregivenmorefavor-
able treatmentthan requestsfor exclusivelocks.
• Sharedlocks.WhenatransactionneedstolockdataitemQ,itsimplyrequests
alockonQ fromthe lock managerat one sitethat contains a replicaof Q.
• Exclusivelocks.WhenatransactionneedstolockdataitemQ,itrequestsa
lock on Q from the lock manager atallsitesthat contain areplicaof Q.
Asbefore,the response tothe requestisdelayeduntil itcan be granted.
The biased scheme has the advantage of imposing less overhead on read
operations than does the majority protocol. This savings is especially signi?cant
in common cases in which the frequency of read is much greater than the fre-
quency of write. However, the additional overhead on writes is a disadvantage.
Furthermore, the biased protocol shares the majority protocol’s disadvantage of
complexityinhandling deadlock.
19.5.1.6 QuorumConsensusProtocol
Thequorumconsensusprotocolisageneralizationofthemajorityprotocol.The
quorum consensus protocol assigns each site a nonnegative weight. It assigns
readandwriteoperationsonanitem x twointegers,calledreadquorum Q
r
and
writequorum Q
w
, that must satisfythe following condition, where Sis the total
weightof all sitesat which x resides:
842 Chapter19 DistributedDatabases
Q
r
+ Q
w
> Sand2 ? Q
w
> S
To execute a read operation, enough replicas must be locked that their total
weightisatleastr.Toexecuteawriteoperation,enoughreplicasmustbe locked
so that theirtotalweight isatleast w.
A bene?t of the quorum consensus approach is that it can permit the cost of
either read or write locking to be selectively reduced by appropriately de?ning
thereadandwritequorums.Forinstance,withasmallreadquorum,readsneed
to obtain fewer locks, but the write quorum will be higher, hence writes need to
obtain more locks. Also, if higher weights are given to some sites (for example,
thoselesslikelytofail),fewersitesneedtobeaccessedforacquiringlocks.Infact,
by setting weights and quorums appropriately, the quorum consensus protocol
can simulatethe majorityprotocol and the biased protocols.
Likethemajorityprotocol,quorumconsensuscanbeextendedtoworkeven
in the presence of sitefailures,as we shallsee in Section19.6.1.
19.5.2 Timestamping
The principal idea behind the timestamping scheme in Section 15.4 is that each
transaction is given a unique timestamp that the system uses in deciding the
serialization order. Our ?rst task, then, in generalizingthe centralized scheme to
a distributed scheme is to develop a scheme for generating unique timestamps.
Then,thevariousprotocolscanoperatedirectlytothenonreplicatedenvironment.
There are two primary methods for generating unique timestamps, one cen-
tralized and one distributed. In the centralized scheme, a single site distributes
the timestamps. The site can use a logical counter or its own local clock for this
purpose.
In the distributed scheme, each site generates a unique local timestamp by
using either a logical counter or the local clock. We obtain the unique global
timestamp by concatenating the unique local timestamp with the site identi?er,
whichalsomustbeunique(Figure19.3).Theorderofconcatenationisimportant!
Weusethe siteidenti?erintheleastsigni?cantpositiontoensurethattheglobal
timestamps generated in one site are not always greater than those generated in
another site.Comparethis technique forgeneratingunique timestampswith the
one that we presentedin Section19.2.3for generatingunique names.
site
identi?er
global unique
identi?er
local unique
timestamp
Figure 19.3 Generation of unique timestamps.
19.5 ConcurrencyControlinDistributedDatabases 843
We may still have a problem if one site generates local timestamps at a rate
faster than that of the other sites. In such a case, the fast site’s logical counter
will be larger than that of other sites. Therefore, all timestamps generated by
the fast site will be larger than those generated by other sites. What we need
is a mechanism to ensure that local timestamps are generated fairly across the
system. We de?ne within each site S
i
a logical clock (LC
i
), which generates the
uniquelocaltimestamp.Thelogicalclockcanbeimplementedasacounterthatis
incrementedafteranewlocaltimestampisgenerated.Toensurethatthevarious
logicalclocks are synchronized, we requirethata site S
i
advance itslogicalclock
whenever a transaction T
i
with timestamp <x,y> visits that site and x is greater
than the currentvalue of LC
i
.Inthiscase,siteS
i
advancesitslogicalclock tothe
value x+1.
If the system clock is used to generate timestamps, then timestamps will be
assigned fairly, provided that no site has a system clock that runs fast or slow.
Since clocks may not be perfectly accurate, a technique similar to that for logical
clocks must be used to ensure that no clock gets far ahead of or behind another
clock.
19.5.3 Replication with Weak Degrees of Consistency
Many commercial databases today support replication, which can take one of
several forms. With master–slave replication, the database allows updates at
a primary site, and automatically propagates updates to replicas at other sites.
Transactions mayreadthereplicasatothersites,butarenotpermittedtoupdate
them.
An important feature of such replication is that transactions do not obtain
locks at remote sites. To ensure that transactions running at the replica sites see
a consistent (but perhaps outdated) view of the database, the replica should
re?ect a transaction-consistent snapshot of the data at the primary; that is, the
replica should re?ect all updates of transactions up to some transaction in the
serializationorder,andshouldnotre?ectanyupdatesoflatertransactionsinthe
serializationorder.
Thedatabasemaybecon?guredtopropagateupdatesimmediatelyafterthey
occur atthe primary,orto propagate updatesonly periodically.
Master–slave replication is particularly useful for distributing information,
forinstancefromacentralof?cetobranchof?cesofanorganization.Anotheruse
forthisformofreplicationisincreatingacopyofthedatabasetorunlargequeries,
sothatqueriesdonotinterferewithtransactions.Updatesshouldbepropagated
periodically—every night, for example—so that update propagation does not
interferewith queryprocessing.
TheOracledatabasesystemsupportsacreatesnapshotstatement,whichcan
create a transaction-consistent snapshot copy of a relation, or set of relations,
at a remote site. It also supports snapshot refresh, which can be done either
by recomputing the snapshot or by incrementally updating it. Oracle supports
automatic refresh,eithercontinuously or at periodicintervals.
844 Chapter19 DistributedDatabases
Withmultimasterreplication(alsocalledupdate-anywherereplication)up-
dates are permitted at any replica of a data item, and are automatically propa-
gated to all replicas. This model is the basic model used to manage replicas in
distributeddatabases.Transactionsupdatethelocalcopyandthesystemupdates
other replicastransparently.
One way of updating replicas is to apply immediate update with two-phase
commit, using one of the distributed concurrency-control techniques we have
seen. Many database systems use the biased protocol, where writes have to lock
andupdateallreplicasandreadslockandreadanyonereplica,astheircurrency-
control technique.
Manydatabasesystemsprovideanalternativeformofupdating:Theyupdate
at one site, with lazy propagation of updates to other sites, instead of immedi-
ately applying updates to all replicas as part of the transaction performing the
update. Schemes based on lazy propagation allow transaction processing (in-
cludingupdates)toproceedevenifasiteisdisconnectedfromthenetwork,thus
improvingavailability,but,unfortunately,dosoatthecostofconsistency.Oneof
two approaches isusually followedwhen lazypropagation isused:
• Updates at replicas are translated into updates at a primary site, which are
then propagated lazily to all replicas. This approach ensures that updates
to an item are ordered serially, although serializability problems can occur,
since transactions may read an old value of some other data item and use it
to performan update.
• Updates are performed at any replica and propagated to all other replicas.
Thisapproach can cause evenmore problems,since the same dataitemmay
be updated concurrently atmultiplesites.
Somecon?ictsduetothe lackofdistributedconcurrency control can bedetected
when updates are propagated to other sites (we shall see how in Section 25.5.4),
butresolvingthecon?ictinvolvesrollingbackcommittedtransactions,anddura-
bility of committed transactions is therefore not guaranteed. Further, human in-
tervention may be required to deal with con?icts. The above schemes should
thereforebe avoidedorusedwithcare.
19.5.4 Deadlock Handling
Thedeadlock-preventionanddeadlock-detectionalgorithmsinChapter15canbe
usedinadistributedsystem,providedthatmodi?cationsaremade.Forexample,
we can use the tree protocol by de?ning a global tree among the system data
items.Similarly,the timestamp-orderingapproach could be directlyappliedto a
distributedenvironment,as we saw inSection 19.5.2.
Deadlock prevention may result in unnecessary waiting and rollback. Fur-
thermore, certain deadlock-prevention techniques may require more sites to be
involvedin the executionof a transaction than would otherwise be the case.
If we allow deadlocks to occur and rely on deadlock detection, the main
problem in a distributed system is deciding how to maintain the wait-for graph.
19.5 ConcurrencyControlinDistributedDatabases 845
T
2
T
4
T
1
T
2
T
5
T
3
T
3
site S
1
site S
2
Figure 19.4 Local wait-for graphs.
Commontechniquesfordealingwiththisissuerequirethateachsitekeepalocal
wait-forgraph.Thenodesofthegraphcorrespondtoallthetransactions(localas
well as nonlocal) that are currently either holding or requesting any of the items
localtothatsite.Forexample,Figure19.4depictsasystemconsistingoftwosites,
eachmaintainingitslocalwait-forgraph.Notethattransactions T
2
and T
3
appear
inbothgraphs,indicatingthatthetransactionshaverequesteditemsatbothsites.
These local wait-for graphs are constructed in the usual manner for local
transactions and data items. When a transaction T
i
on site S
1
needs a resource in
site S
2
,it sendsa requestmessageto site S
2
. If the resource isheld by transaction
T
j
, the systeminsertsan edge T
i
? T
j
inthe local wait-for graphof site S
2
.
Clearly,ifanylocalwait-forgraphhasacycle,deadlockhasoccurred.Onthe
otherhand,thefactthattherearenocyclesinanyofthelocalwait-forgraphsdoes
not mean that there are no deadlocks.To illustratethis problem, we consider the
local wait-for graphs of Figure 19.4. Each wait-for graph is acyclic; nevertheless,
a deadlock exists in the system because the union of the local wait-for graphs
contains a cycle. Thisgraph appearsin Figure19.5.
In the centralized deadlock detection approach, the system constructs and
maintains a global wait-for graph (the union of all the local graphs) in a single
site: the deadlock-detection coordinator. Since there is communication delay in
the system, we must distinguish between two types of wait-for graphs. The real
graph describes the real but unknown state of the system at any instance in
time, as would be seen by an omniscient observer. The constructed graph is an
T
1
T
4
T
5
T
2
T
3
Figure 19.5 Global wait-for graph for Figure 19.4.
846 Chapter19 DistributedDatabases
T
1
T
2
T
2
T
1
T
3
S
2
T
1
T
3
coordinator
S
1
Figure 19.6 False cycles in the global wait-for graph.
approximationgeneratedbythecontrollerduringtheexecutionofthecontroller’s
algorithm.Obviously,thecontrollermustgeneratetheconstructedgraphinsuch
awaythat,wheneverthedetectionalgorithmisinvoked,thereportedresultsare
correct.Correctmeansinthiscasethat,ifadeadlockexists,itisreportedpromptly,
and ifthe systemreportsa deadlock,itisindeedin a deadlockstate.
The global wait-for graph can be reconstructed or updated under these con-
ditions:
• Wheneveranewedgeisinsertedinorremovedfromoneofthelocalwait-for
graphs.
• Periodically, when a number of changes have occurred in a local wait-for
graph.
• Wheneverthecoordinator needstoinvokethe cycle-detectionalgorithm.
When the coordinator invokes the deadlock-detection algorithm, it searches
its global graph. If it ?nds a cycle, it selects a victim to be rolled back. The
coordinatormustnotifyallthesitesthataparticulartransactionhasbeenselected
as victim.The sites,in turn, rollback the victimtransaction.
This scheme may produce unnecessaryrollbacks if:
• False cycles exist in the global wait-for graph. As an illustration, consider a
snapshotofthesystemrepresentedbythelocalwait-forgraphsofFigure19.6.
Suppose that T
2
releases the resource that it is holding in site S
1
,resulting
in the deletion of the edge T
1
? T
2
in S
1
.T ransactionT
2
then requests a
19.6 Availability 847
resourceheldby T
3
atsite S
2
,resultingintheadditionoftheedgeT
2
? T
3
in
S
2
.Iftheinsert T
2
? T
3
messagefrom S
2
arrivesbeforetheremove T
1
? T
2
messagefromS
1
,thecoordinatormaydiscoverthefalsecycleT
1
? T
2
? T
3
after the insert (but before the remove). Deadlock recovery may be initiated,
although no deadlockhas occurred.
Notethatthefalse-cyclesituationcouldnotoccurundertwo-phaselocking.
Thelikelihoodoffalsecyclesisusuallysuf?cientlylowthattheydonotcause
a seriousperformance problem.
• Adeadlockhasindeedoccurredandavictimhasbeenpicked,whileoneofthe
transactionswasabortedforreasonsunrelatedtothedeadlock.Forexample,
suppose that site S
1
in Figure 19.4 decides to abort T
2
. At the same time, the
coordinatorhasdiscoveredacycle,andhaspickedT
3
asavictim.BothT
2
and
T
3
are now rolledback, although only T
2
neededtobe rolledback.
Deadlock detection can be done in a distributed manner, with several sites
takingonpartsofthetask,insteadofitbeingdoneatasinglesite.However,such
algorithms are more complicated and more expensive. See the bibliographical
notes forreferencesto such algorithms.
19.6 Availability
One of the goals in using distributed databases is high availability;thatis,the
database must function almost all the time. In particular, since failures are more
likely in large distributed systems, a distributed database must continue func-
tioning even when there are various types of failures. The ability to continue
functioning evenduringfailuresis referredtoasrobustness.
For a distributed system to be robust, it must detect failures, recon?gure the
systemsothatcomputationmaycontinue,and recoverwhenaprocessororalink
isrepaired.
The different types of failures are handled in different ways. For example,
messagelossishandledbyretransmission.Repeatedretransmissionofamessage
across a link, without receipt of an acknowledgment, is usually a symptom of a
link failure. The network usually attempts to ?nd an alternative route for the
message.Failure to?nd such a route isusuallya symptomof network partition.
It is generally not possible, however, to differentiate clearly between site
failure and network partition. The system can usually detect that a failure has
occurred, but it may not be able to identify the type of failure. For example,
suppose that site S
1
is not able to communicate with S
2
.ItcouldbethatS
2
has
failed.However,another possibilityisthat the linkbetween S
1
and S
2
has failed,
resultinginnetworkpartition.Theproblemispartlyaddressedbyusingmultiple
links between sites, so that even if one link fails the sites will remain connected.
However, multiple link failure can still occur, so there are situations where we
cannot be sure whethera site failureor network partitionhas occurred.
848 Chapter19 DistributedDatabases
Suppose that site S
1
has discovered that a failure has occurred. It must then
initiate a procedure that will allow the system to recon?gure, and to continue
with the normal modeof operation.
• If transactions were active at a failed/inaccessible site at the time of the
failure, these transactions should be aborted. It is desirable to abort such
transactions promptly, since they may hold locks on data at sites that are
stillactive;waitingforthefailed/inaccessiblesitetobecomeaccessibleagain
may impede other transactions at sites that are operational. However, in
some cases, when data objects are replicated it may be possible to proceed
with reads and updates even though some replicas are inaccessible. In this
case, when a failed site recovers, if it had replicas of any data object, it must
obtainthecurrentvaluesofthesedataobjects,andmustensurethatitreceives
allfuture updates.We addressthisissue in Section19.6.1.
• If replicated data are stored at a failed/inaccessible site, the catalog should
be updatedso that queriesdo not referencethe copy at the failedsite.When
asiterejoins,caremustbetakentoensurethatdataatthesiteareconsistent,
as we shall seein Section19.6.3.
• If a failed site is a central server for some subsystem, an election must be
held to determine the new server (see Section 19.6.5). Examples of central
serversincludeanameserver,aconcurrencycoordinator,oraglobaldeadlock
detector.
Since it is, in general, not possible to distinguish between network link failures
andsitefailures,anyrecon?gurationschememustbedesignedtoworkcorrectly
in case of a partitioning of the network. In particular, these situations must be
avoidedto ensureconsistency:
• Two or more central serversare electedin distinctpartitions.
• More than one partitionupdatesa replicateddataitem.
Althoughtraditionaldatabasesystemsplaceapremiumonconsistency,there
are many applications today that value availability more than consistency. The
design of replication protocols is different for such systems, and is discussed in
Section19.6.6.
19.6.1 Majority-Based Approach
Themajority-basedapproachtodistributedconcurrencycontrolinSection19.5.1.4
can be modi?ed to work in spite of failures. In this approach, each data object
stores with it a version number to detect when it was last written. Whenever a
transaction writesanobject italsoupdatestheversionnumber inthisway:
• If data object a is replicated in n different sites, then a lock-request message
must be sent to more than one-half of the n sites at which a is stored. The
19.6 Availability 849
transaction does not operate on a until it has successfully obtained a lock on
a majorityofthe replicasof a.
• Read operations look at all replicas on which a lock has been obtained, and
read the value from the replica that has the highest version number. (Op-
tionally, they may also write this value back to replicas with lower version
numbers.) Writes read all the replicas just like reads to ?nd the highest ver-
sion number (this step would normally have been performed earlier in the
transactionbyaread,andtheresultcanbereused).Thenewversionnumber
is one more than the highest version number. The write operation writes all
the replicas on which it has obtained locks, and sets the version number at
allthereplicasto thenewversionnumber.
Failuresduring a transaction (whether network partitions or site failures) can be
toleratedaslongas(1)thesitesavailableatcommitcontainamajorityofreplicas
of all the objects written to and (2) during reads, a majority of replicas are read
to ?nd the version numbers. If these requirements are violated, the transaction
mustbeaborted.Aslongastherequirementsaresatis?ed,thetwo-phasecommit
protocol can be used,as usual, on the sitesthat are available.
In this scheme, reintegration is trivial; nothing needs to be done. This is
because writes would have updated a majority of the replicas, while reads will
read a majority of the replicas and ?nd at least one replica that has the latest
version.
Theversionnumberingtechniqueusedwiththemajorityprotocolcanalsobe
usedtomakethequorumconsensusprotocolworkinthepresenceoffailures.We
leave the (straightforward) details to the reader.However, the danger of failures
preventing the system from processing transactions increases if some sites are
givenhigherweights.
19.6.2 Read One, Write All Available Approach
As a special case of quorum consensus, we can employ the biased protocol by
giving unit weights to all sites, setting the read quorum to 1, and setting the
write quorum to n (all sites). In this special case, there is no need to use version
numbers; however, if even a single site containing a data item fails, no write to
the itemcan proceed,since the write quorumwillnot be available.This protocol
iscalled thereadone,writeall protocol since allreplicasmust be written.
To allow work to proceed in the event of failures, we would like to be able
touseareadone,writeallavailableprotocol.Inthisapproach,areadoperation
proceeds as in theread one,writeall scheme; any available replica can be read,
and a read lock is obtained at that replica. A write operation is shipped to all
replicas; and write locks are acquired on all the replicas. If a site is down, the
transaction managerproceedswithout waiting forthe siteto recover.
Whilethis approach appearsveryattractive,thereareseveralcomplications.
In particular, temporary communication failure may cause a site to appear to
be unavailable, resulting in a write not being performed, but when the link is
restored,thesiteisnotawarethatithastoperformsomereintegrationactionsto
850 Chapter19 DistributedDatabases
catchuponwritesithaslost.Further,ifthenetworkpartitions,eachpartitionmay
proceed to update the same data item, believing that sites in the other partitions
arealldead.
The read one, write all available scheme can be used if there is never any
network partitioning, but it can result in inconsistencies in the event of network
partitions.
19.6.3 Site Reintegration
Reintegration of a repaired site or link into the system requires care. When a
failed site recovers, it must initiate a procedure to update its system tables to
re?ectchangesmadewhileitwasdown.Ifthesitehadreplicasofanydataitems,
itmustobtainthecurrentvaluesofthesedataitemsandensurethatitreceivesall
future updates. Reintegration of a site is more complicated than it may seem to
be at ?rst glance, since there may be updates to the data items processed during
the timethat the site isrecovering.
An easy solution is to halt the entire system temporarilywhile the failed site
rejoins it. In most applications, however, such a temporary halt is unacceptably
disruptive. Techniques have been developed to allow failed sites to reintegrate
while concurrent updates to data items proceed concurrently. Before a read or
write lock is granted on any data item,the sitemust ensure that ithas caught up
onallupdatestothedataitem.Ifafailedlinkrecovers,twoormorepartitionscan
be rejoined. Since a partitioning of the network limits the allowable operations
by some or all sites, all sites should be informed promptly of the recovery of the
link.Seethebibliographicalnotesformoreinformationonrecoveryindistributed
systems.
19.6.4 Comparison with Remote Backup
Remotebackupsystems,whichwestudiedinSection16.9,andreplicationindis-
tributed databases are two alternative approaches to providinghigh availability.
The main difference between the two schemes is that with remote backup sys-
tems,actionssuchasconcurrencycontrolandrecoveryareperformedatasingle
site,andonlydataandlogrecordsarereplicatedattheothersite.Inparticular,re-
motebackupsystemshelpavoidtwo-phasecommit,anditsresultantoverheads.
Also,transactionsneedtocontactonlyonesite(theprimarysite),andthusavoid
the overhead of running transaction code at multiple sites. Thus remote backup
systemsoffera lower-costapproach tohigh availabilitythan replication.
On the other hand, replication can provide greater availability by having
multiplereplicasavailableand using the majorityprotocol.
19.6.5 Coordinator Selection
Severalofthealgorithmsthatwehavepresentedrequiretheuseofacoordinator.
Ifthecoordinatorfailsbecauseofafailureofthesiteatwhichitresides,thesystem
cancontinueexecutiononlybyrestartinganewcoordinatoronanothersite.One
19.6 Availability 851
way to continue execution is by maintaining a backup to the coordinator, which
isreadytoassume responsibilityifthe coordinatorfails.
A backup coordinator is a site that, in addition to other tasks, maintains
enough information locally to allow it to assume the role of coordinator with
minimal disruption to the distributed system. All messages directed to the co-
ordinator are received by both the coordinator and its backup. The backup co-
ordinator executes the same algorithms and maintains the same internal state
information (such as, for a concurrency coordinator, the lock table) as does the
actual coordinator. The only difference in function between the coordinator and
itsbackupisthatthebackupdoesnottakeanyactionthataffectsothersites.Such
actions are leftto the actual coordinator.
Intheeventthatthebackupcoordinatordetectsthefailureoftheactualcoor-
dinator,itassumestheroleofcoordinator.Sincethebackuphasalltheinformation
available to it that the failed coordinator had, processing can continue without
interruption.
The prime advantage to the backup approach is the ability to continue pro-
cessing immediately. If a backup were not ready to assume the coordinator’s re-
sponsibility,anewlyappointedcoordinatorwouldhavetoseekinformationfrom
all sites in the system so that it could execute the coordination tasks. Frequently,
the only source of some of the requisite information is the failed coordinator. In
this case, it may be necessary to abort several (or all) active transactions, and to
restartthem underthe control of the new coordinator.
Thus,thebackup-coordinatorapproachavoidsasubstantialamountofdelay
while the distributed system recovers from a coordinator failure. The disadvan-
tage is the overhead of duplicate execution of the coordinator’s tasks. Further-
more,acoordinatoranditsbackupneedtocommunicateregularlytoensurethat
theiractivitiesare synchronized.
In short, the backup-coordinator approach incurs overhead during normal
processingto allow fastrecoveryfrom a coordinator failure.
In the absence of a designated backup coordinator, or in order to handle
multiplefailures,anewcoordinatormaybechosendynamicallybysitesthatare
live.Electionalgorithmsenablethesitestochoosethesiteforthenewcoordinator
inadecentralizedmanner.Electionalgorithmsrequirethatauniqueidenti?cation
number be associated with each activesite in the system.
The bully algorithm for election works as follows: To keep the notation
and the discussion simple, assume that the identi?cation number of site S
i
is i
and that the chosen coordinator will always be the active site with the largest
identi?cation number. Hence, when a coordinator fails, the algorithm must elect
theactivesitethathasthelargestidenti?cationnumber.Thealgorithmmustsend
this number to each active site in the system. In addition, the algorithm must
provide a mechanism by which a site recovering from a crash can identify the
current coordinator. Suppose that site S
i
sends a request that is not answered
by the coordinator within a prespeci?ed time interval T.Inthissituation,itis
assumed that the coordinator has failed, and S
i
tries to elect itself as the site for
the new coordinator.
852 Chapter19 DistributedDatabases
Site S
i
sendsanelectionmessagetoeverysitethathasahigheridenti?cation
number. Site S
i
then waits, for a time interval T, for an answer from any one
of these sites. If it receives no response within time T, it assumes that all sites
with numbers greater than i have failed, and it elects itself as the site for the
newcoordinatorandsendsamessagetoinformallactivesiteswithidenti?cation
numbers lowerthan i that itisthe siteat which the new coordinator resides.
If S
i
doesreceiveananswer,itbeginsatimeinterval T
null
,toreceiveamessage
informing it that a site with a higher identi?cation number has been elected.
(Someothersiteiselectingitselfcoordinator,andshouldreporttheresultswithin
timeT
null
.)If S
i
receivesnomessagewithinT
null
,thenitassumesthesitewithahigher
number has failed,and site S
i
restartsthe algorithm.
Afterafailedsiterecovers,itimmediatelybeginsexecutionofthesamealgo-
rithm. If there are no active sites with higher numbers, the recovered site forces
all sites with lower numbers to let it become the coordinator site, evenif there is
a currently active coordinator with a lower number. It is for this reason that the
algorithmistermedthe bullyalgorithm.Ifthenetworkpartitions,thebullyalgo-
rithm elects a separate coordinator in each partition; to ensure that at most one
coordinatoriselected,winningsitesshouldadditionallyverifythatamajorityof
the sitesare in theirpartition.
19.6.6 Trading Off Consistency for Availability
The protocols we have seen so far require a (weighted) majority of sites be in
a partition for updates to proceed. Sites that are in a minority partition cannot
process updates; if a network failure results in more than two partitions, no
partition may have a majorityof sites. Undersuch a situation, the systemwould
becompletelyunavailableforupdates,anddependingontheread-quorum,may
even become unavailable for reads. The write-all-available protocol which we
saw earlierprovidesavailability,but not consistency.
Ideally, we would like to have consistency and availability, even in the face
of partitions. Unfortunately, this is not possible, a fact that is crystallized in the
so-called CAP theorem, which states that any distributed database can have at
most two of the following three properties:
• Consistency.
• Availability.
• Partition-tolerance.
The proof of the CAP theorem uses the following de?nition of consistency, with
replicateddata:anexecutionofasetofoperations(readsandwrites)onreplicated
data is said to be consistent if its result is the same as if the operations were
executed on a single site, in some sequential order, and the sequential order is
consistent with the ordering of operations issued by each process (transaction).
19.6 Availability 853
The notion of consistency is similar to atomicity of transactions, but with each
operation treated as a transaction, and is weaker than the atomicity property of
transactions.
In any large-scale distributed system, partitions cannot be prevented,and as
a result either of availability or consistency has to be sacri?ced. The schemes we
have seenearliersacri?ce availabilityfor consistency in the face of partitions.
Consider a Web-based social-networking system that replicates its data on
three servers, and a network partition occurs that prevents the servers from
communicating with each other. Since none of the partitions has a majority, it
would notbe possible toexecuteupdateson anyof thepartitions.Ifone of these
servers is in the same partition as a user, the user actually has access to data,
but would be unable to update the data, since another user may be concurrently
updating the same object in another partition, which could potentially lead to
inconsistency. Inconsistency is not as great a risk in a social-networking system
as in a banking database. A designer of such a system may decide that a user
who can access the system should be allowed to perform updates on whatever
replicasareaccessible,evenat theriskof inconsistency.
Incontrast to systemssuch as banking databasesthatrequirethe ACID prop-
erties,systemssuchasthesocial-networkingsystemmentionedabovearesaidto
require the BASE properties:
• Basicallyavailable.
• Softstate.
• Eventuallyconsistent.
The primary requirement is availability, even at the cost of consistency. Updates
should be allowed, even in the event of partitioning, following for example the
write-all-availableprotocol(whichissimilartomultimasterreplicationdescribed
inSection19.5.3).Softstatereferstothepropertythatthestateofthedatabasemay
not be preciselyde?ned,witheach replicapossiblyhavinga somewhat different
stateduetopartitioningofthenetwork.Eventuallyconsistentistherequirement
thatonceapartitioningisresolved,eventuallyallreplicaswillbecomeconsistent
with each other.
This last step requires that inconsistent copies of data items be identi?ed; if
oneisanearlierversionoftheother,theearlierversioncanbereplacedbythelater
version.Itispossible,however,thatthetwocopiesweretheresultofindependent
updatestoacommonbasecopy.Aschemefordetectingsuchinconsistentupdates,
calledtheversion-vectorscheme,isdescribedinSection25.5.4.
Restoring consistency in the face of inconsistent updates requires that the
updatesbemergedinsomewaythatisme aningful to the application. This step
cannot be handled by the database; instead the database detects and informs
the application about the inconsistency, and the application then decides how to
resolvethe inconsistency.
854 Chapter19 DistributedDatabases
19.7 Distributed Query Processing
In Chapter 13, we saw that there are a variety of methods for computing the
answer to a query. We examined several techniques for choosing a strategy for
processingaquerythatminimizetheamountoftimethatittakestocomputethe
answer. For centralized systems, the primary criterion for measuring the cost of
a particular strategy is the number of disk accesses. In a distributed system, we
must take intoaccount severalother matters,including:
• The cost of datatransmission overthe network.
• The potential gain in performance from having severalsites process parts of
the queryin parallel.
The relativecost of data transfer over the network and data transfer to and from
disk varies widely depending on the type of network and on the speed of the
disks.Thus,ingeneral,wecannot focussolelyondiskcostsoronnetworkcosts.
Rather, wemust ?nd a goodtrade-offbetweenthetwo.
19.7.1 Query Transformation
Consideranextremelysimplequery: “Find allthetuplesinthe account relation.”
Although the queryissimple—indeed,trivial—processingitisnot trivial,since
the account relation may be fragmented, replicated, or both, as we saw in Sec-
tion19.2.Ifthe accountrelationisreplicated,wehaveachoiceofreplicatomake.
If no replicas are fragmented, we choose the replica for which the transmission
cost is lowest. However, if a replica is fragmented, the choice is not so easy to
make,sinceweneedtocomputeseveraljoinsorunionstoreconstructtheaccount
relation. In this case, the number of strategies for our simple example may be
large.Queryoptimizationbyexhaustiveenumerationofallalternativestrategies
maynotbepracticalinsuchsituations.
Fragmentation transparency impliesthata usermay write a querysuch as:
  branch name =“Hillside”
(account)
Since account isde?ned as:
account
1
? account
2
the expressionthat resultsfrom the name translation scheme is:
  branch name =“Hillside”
(account
1
? account
2
)
Using the query-optimization techniques of Chapter 13, we can simplify the
precedingexpressionautomatically. The resultis the expression:
19.7 DistributedQueryProcessing 855
  branch name =“Hillside”
(account
1
) ?   branch name =“Hillside”
(account
2
)
which includes two subexpressions. The ?rst involves only account
1
,andthus
canbeevaluatedattheHillsidesite.Thesecondinvolvesonly account
2
,andthus
can beevaluatedatthe Valleyviewsite.
There isa furtheroptimizationthat can be made in evaluating:
  branch name =“Hillside”
(account
1
)
Sinceaccount
1
hasonlytuplespertainingtotheHillsidebranch,wecaneliminate
theselectionoperation.In evaluating:
  branch name =“Hillside”
(account
2
)
we can applythe de?nitionof the account
2
fragmentto obtain:
  branch name =“Hillside”
(  branch name =“Valleyview”
(account))
Thisexpressionistheemptyset,regardlessofthecontentsoftheaccountrelation.
Thus, our ?nal strategyis for the Hillside site to return account
1
as the result
of the query.
19.7.2 Simple Join Processing
As we saw in Chapter13, a major decisionin the selection of a query-processing
strategy is choosing a join strategy. Consider the following relational-algebra
expression:
account   depositor   branch
Assume that the three relations are neither replicated nor fragmented, and that
account is stored at site S
1
, depositor at S
2
,andbranch at S
3
.LetS
I
denote the site
atwhich the querywas issued.The systemneedstoproduce the resultat site S
I
.
Among thepossiblestrategiesfor processingthisqueryarethese:
• Ship copies of all three relations to site S
I
. Using the techniques of Chapter
13, choose astrategyfor processingthe entire querylocally atsite S
I
.
• Ship a copy of the account relation to site S
2
,andcomputetemp
1
= account  depositorat S
2
.Shiptemp
1
from S
2
to S
3
,andcomputetemp
2
=temp
1
  branch
at S
3
. Shipthe resulttemp
2
to S
I
.
• Devise strategies similar to the previous one, with the roles of S
1
, S
2
, S
3
exchanged.
No one strategy is always the best one. Among the factors that must be
consideredarethevolumeofdatabeingshipped,thecostoftransmittingablock
856 Chapter19 DistributedDatabases
of data between a pair of sites, and the relative speed of processing at each site.
Consider the ?rst two strategies listed. Suppose indices present at S
2
and S
3
are
usefulforcomputingthejoin.Ifweshipallthreerelationsto S
I
,wewouldneedto
eitherre-createtheseindicesatS
I
,oruseadifferent,possiblymoreexpensive,join
strategy. Re-creation of indices entails extra processing overhead and extra disk
accesses.Withthesecondstrategyapotentiallylargerelation(account  depositor)
mustbeshippedfrom S
2
to S
3
.Thisrelationrepeatsthenameofacustomeronce
for each account that the customer has. Thus, the second strategy may result in
extranetwork transmission compared tothe ?rststrategy.
19.7.3 Semijoin Strategy
Suppose that we wish to evaluate the expression r
1
  r
2
,wherer
1
and r
2
are
stored at sites S
1
and S
2
, respectively. Let the schemas of r
1
and r
2
be R
1
and R
2
.
Supposethatwewishtoobtaintheresultat S
1
.Iftherearemanytuplesofr
2
that
donotjoinwithanytupleofr
1
,thenshippingr
2
to S
1
entailsshippingtuplesthat
fail to contribute to the result. We want to remove such tuples before shipping
datato S
1
, particularlyifnetwork costs are high.
Apossible strategytoaccomplish allthisis:
1. Compute temp
1
? null
R
1
? R
2
(r
1
)atS
1
.
2. Ship temp
1
from S
1
to S
2
.
3. Compute temp
2
?r
2
  temp
1
at S
2
.
4. Ship temp
2
from S
2
to S
1
.
5. Computer
1
  temp
2
at S
1
. The resultingrelationisthe same asr
1
  r
2
.
Before considering the ef?ciency of this strategy, let us verify that the strategy
computes the correct answer. In step 3, temp
2
has the result of r
2
  null
R
1
? R
2
(r
1
).
In step5, we compute:
r
1
  r
2
  null
R
1
? R
2
(r
1
)
Sincejoinisassociativeand commutative,we canrewritethisexpressionas:
(r
1
  null
R
1
? R
2
(r
1
))   r
2
Since r
1
  null
(R
1
? R
2
)
(r
1
) = r
1
, the expression is, indeed, equal to r
1
  r
2
,the
expressionwe aretryingto evaluate.
This strategy is particularly advantageous when relatively few tuples of r
2
contribute to the join. This situation is likely to occur if r
1
is the result of a
relational-algebraexpressioninvolvingselection.In such a case, temp
2
mayhave
signi?cantly fewer tuples than r
2
. The cost savings of the strategy result from
having to ship only temp
2
, rather than all of r
2
,toS
1
. Additional cost is incurred
in shipping temp
1
to S
2
. If a suf?ciently small fraction of tuples in r
2
contribute
19.8 HeterogeneousDistributedDatabases 857
to the join, the overhead of shipping temp
1
will be dominated by the savings of
shippingonly a fraction of the tuplesinr
2
.
This strategy is called asemijoin strategy, after the semijoin operator of the
relationalalgebra, denoted  .Thesemijoinofr
1
withr
2
,denotedr
1
  r
2
,is:
null
R
1
(r
1
  r
2
)
Thus, r
1
  r
2
selects those tuples of relation r
1
that contributed to r
1
  r
2
.Instep
3, temp
2
=r
2
  r
1
.
For joins of several relations, this strategy can be extended to a series of
semijoin steps. A substantial body of theory has been developed regarding the
use of semijoins for query optimization. Some of this theory is referenced in the
bibliographical notes.
19.7.4 Join Strategies that Exploit Parallelism
Considera join of four relations:
r
1
  r
2
  r
3
  r
4
where relation r
i
is stored at site S
i
. Assume that the result must be presented
at site S
1
. There are many possible strategiesfor parallelevaluation. (We studied
the issue of parallel processing of queries in detail in Chapter 18.) In one such
strategy, r
1
is shipped to S
2
,andr
1
  r
2
computed at S
2
. At the same time, r
3
is
shipped to S
4
,andr
3
  r
4
computed at S
4
.SiteS
2
can ship tuples of (r
1
  r
2
)
to S
1
as they are produced, rather than wait for the entire join to be computed.
Similarly, S
4
canshiptuplesof(r
3
  r
4
)toS
1
.Oncetuplesof( r
1
  r
2
)and( r
3
  r
4
)
arriveat S
1
,thecomputationof( r
1
  r
2
)  (r
3
  r
4
)can begin, with the pipelined
join technique of Section 12.7.2.2. Thus, computation of the ?nal join result at
S
1
canbedoneinparallelwiththecomputationof( r
1
  r
2
)atS
2
, and with the
computation of (r
3
  r
4
)atS
4
.
19.8 Heterogeneous Distributed Databases
Many new database applications require data from a variety of preexisting
databases located in a heterogeneous collection of hardware and software en-
vironments.Manipulationofinformation located inaheterogeneousdistributed
database requires an additional software layer on top of existing database sys-
tems. This software layer is called a multidatabase system. The local database
systems may employ different logical models and data-de?nition and data-
manipulation languages, and may differ in their concurrency-control and trans-
action-management mechanisms. A multidatabase system creates the illusion of
logical database integrationwithout requiringphysicaldatabase integration.
Full integration of heterogeneous systems into a homogeneous distributed
database is oftendif?cult orimpossible:
858 Chapter19 DistributedDatabases
• Technicaldif?culties. The investmentin application programs based on ex-
istingdatabasesystemsmaybehuge,andthecostofconvertingtheseappli-
cations may be prohibitive.
• Organizationaldif?culties. Evenif integrationis technically possible, it may
not be politically possible, because the existing database systems belong to
different corporations or organizations. In such cases, it is important for
a multidatabase system to allow the local database systems to retain a high
degreeofautonomyoverthelocaldatabaseandtransactionsrunningagainst
that data.
For these reasons, multidatabase systems offer signi?cant advantages that
outweightheiroverhead.Inthissection,weprovideanoverviewofthechallenges
faced in constructing a multidatabase environment from the standpoint of data
de?nition and queryprocessing.
19.8.1 Uni?ed View of Data
Each local database management system may use a different data model. For
instance, some may employ the relational model, whereas others may employ
olderdatamodels,suchasthenetworkmodel(seeAppendixD)orthehierarchical
model(seeAppendixE).
Sincethemultidatabasesystemissupposedtoprovidetheillusionofasingle,
integrated database system, a common data model must be used. A commonly
used choice is the relational model, with SQL as the common query language.
Indeed, there are several systems available today that allow SQL queries to a
nonrelational database-management system.
Another dif?culty is the provision of a common conceptual schema. Each
localsystemprovidesitsownconceptualschema.Themultidatabasesystemmust
integrate these separate schemas into one common schema. Schema integration
isa complicated task, mainly because of the semanticheterogeneity.
Schema integration is not simply straightforward translation between data-
de?nition languages. The same attribute names may appear in different local
databasesbutwithdifferentmeanings.Thedatatypesusedinonesystemmaynot
besupportedbyothersystems,andtranslationbetweentypesmaynotbesimple.
Evenforidenticaldatatypes,problemsmayarisefromthephysicalrepresentation
ofdata:Onesystemmayuse8-bit ASCII,another16-bitUnicode,andyetanother
EBCDIC; ?oating-point representations may differ; integers may be represented
in big-endian or little-endian form. At the semantic level, an integer value for
length may be inches in one system and millimeters in another, thus creating an
awkward situation in which equality of integers is only an approximate notion
(as is always the case for ?oating-point numbers). The same name may appear
in different languages in different systems. For example, a system based in the
United States may refer to the city “Cologne,” whereas one in Germany refers to
itas “K¨ oln.”
Alltheseseeminglyminordistinctionsmustbeproperlyrecordedinthecom-
mon global conceptual schema. Translation functions must be provided. Indices
19.8 HeterogeneousDistributedDatabases 859
mustbeannotatedforsystem-dependentbehavior(forexample,thesortorderof
nonalphanumeric characters is not the same in ASCII as in EBCDIC). As we noted
earlier, the alternative of converting each database to a common format may not
be feasiblewithout obsoleting existingapplicationprograms.
19.8.2 Query Processing
Query processing in a heterogeneous database can be complicated. Some of the
issuesare:
• Given a query on a global schema, the query may have to be translated
into queries on local schemas at each of the sites where the query has to be
executed.Thequeryresultshavetobetranslatedbackintotheglobalschema.
The task is simpli?ed by writing wrappers for each data source, which
provideaviewofthelocaldataintheglobalschema.Wrappersalsotranslate
queries on the global schema into queries on the local schema, and translate
resultsbackintotheglobalschema.Wrappersmaybeprovidedbyindividual
sites,or maybe writtenseparatelyas partof the multidatabase system.
Wrappers can even be used to provide a relational view of nonrelational
data sources, such as Web pages (possibly with forms interfaces), ?at ?les,
hierarchical and networkdatabases, and directorysystems.
• Somedatasourcesmayprovideonlylimitedquerycapabilities;forinstance,
theymaysupportselections,butnotjoins.Theymayevenrestricttheformof
selections, allowing selections only on certain ?elds; Web data sources with
form interfaces are an example of such data sources. Queries may therefore
havetobebrokenup,tobepartlyperformedatthedatasourceandpartlyat
thesiteissuingthequery.
• In general, more than one site may need to be accessed to answer a given
query.Answersretrievedfromthesitesmayhavetobeprocessedtoremove
duplicates. Suppose one site contains account tuples satisfying the selection
balance < 100,whileanothercontainsaccounttuplessatisfyingbalance > 50.
Aqueryontheentireaccountrelationwouldrequireaccesstobothsitesand
removalofduplicateanswersresultingfromtupleswithbalancebetween50
and 100, which are replicatedatboth sites.
• Global query optimization in a heterogeneous database is dif?cult, since
the query execution system may not know what the costs are of alternative
queryplansatdifferentsites.Theusualsolutionistorelyononlylocal-level
optimization,and justuse heuristicsat the global level.
Mediator systems are systems that integrate multiple heterogeneous data
sources, providing an integrated global view of the data and providing query
facilitiesontheglobalview.Unlikefull-?edgedmultidatabasesystems,mediator
systems do not bother about transaction processing. (The terms mediator and
multidatabaseareoftenusedinaninterchangeablefashion,andsystemsthatare
called mediators may support limited forms of transactions.) The term virtual
860 Chapter19 DistributedDatabases
databaseisusedtorefertomultidatabase/mediatorsystems,since theyprovide
theappearanceofasingledatabasewithaglobalschema,althoughdataexiston
multiplesitesinlocal schemas.
19.8.3 Transaction Management in Multidatabases
Amultidatabase systemsupportstwo typesof transactions:
1. Local transactions. These transactions are executed by each local database
systemoutsideof the multidatabasesystem’scontrol.
2. Globaltransactions. These transactions are executed under the multidata-
base system’scontrol.
The multidatabase system is aware of the fact that local transactions may run at
thelocalsites,butitisnotawareofwhatspeci?ctransactionsarebeingexecuted,
or of what datathey mayaccess.
Ensuringthelocalautonomyofeachdatabasesystemrequiresthatnochanges
bemadetoitssoftware.Adatabasesystematonesitethusisnotabletocommu-
nicate directlywith one at any othersitetosynchronize the executionofa global
transaction active atseveralsites.
Since the multidatabase system has no control over the execution of local
transactions,eachlocalsystemmustuseaconcurrency-controlscheme(forexam-
ple,two-phaselockingortimestamping)toensurethatitsscheduleisserializable.
Inaddition,incase oflocking, thelocalsystemmustbeable toguardagainstthe
possibilityof localdeadlocks.
The guarantee of local serializability is not suf?cient to ensure global serial-
izability. As an illustration, consider two global transactions T
1
and T
2
,eachof
which accesses and updates two data items, A and B, located at sites S
1
and S
2
,
respectively. Suppose that the local schedules are serializable. It is still possible
to have a situation where, at site S
1
, T
2
follows T
1
,whereas,atS
2
, T
1
follows T
2
,
resultingina nonserializableglobal schedule.Indeed,evenif thereis no concur-
rency among global transactions (that is, a global transaction is submitted only
after the previous one commits or aborts), local serializability is not suf?cient to
ensure global serializability(seePractice Exercise19.14).
Depending on the implementation of the local database systems, a global
transaction may not be able to control the precise locking behavior of its local
subtransactions. Thus, even if all local database systems follow two-phase lock-
ing,itmaybepossibleonlytoensurethateachlocaltransactionfollowstherules
oftheprotocol.Forexample,onelocaldatabasesystemmaycommititssubtrans-
action and release locks, while the subtransaction at another local system is still
executing.Ifthelocalsystemspermitcontroloflockingbehaviorandallsystems
follow two-phase locking, then the multidatabase system can ensure that global
transactionslockinatwo-phasemannerandthelockpointsofcon?ictingtransac-
tions would then de?ne their global serializationorder. If differentlocal systems
follow differentconcurrency-control mechanisms,however,thisstraightforward
sort ofglobal control doesnot work.
19.9 Cloud-BasedDatabases 861
Therearemanyprotocolsforensuringconsistencydespiteconcurrentexecu-
tionofglobalandlocaltransactionsinmultidatabasesystems.Somearebasedon
imposingsuf?cientconditionstoensureglobalserializability.Othersensureonly
a form of consistency weaker than serializability,but achieve this consistency by
less restrictive means. Section 26.6 describes approaches to consistency without
serializability;otherapproaches are citedin the bibliographical notes.
Early multidatabase systems restricted global transactions to be read only.
Theythusavoidedthepossibilityofglobaltransactionsintroducinginconsistency
to the data, but were not suf?ciently restrictive to ensure global serializability. It
isindeedpossibletogetsuchglobalschedulesandtodevelopaschemetoensure
global serializability,and we ask you todo both in Practice Exercise19.15.
There are a number of general schemes to ensure global serializability in an
environmentwhereupdateaswellasread-onlytransactionscanexecute.Several
of these schemes are based on the idea of a ticket. A special data item called
a ticket is created in each local database system. Every global transaction that
accesses data at a site must write the ticket at that site. This requirementensures
that global transactions con?ict directly at every site they visit. Furthermore,the
globaltransactionmanagercancontroltheorderinwhichglobaltransactionsare
serialized, by controlling the order in which the tickets are accessed. References
to such schemesappearin the bibliographical notes.
Ifwewanttoensureglobalserializabilityinanenvironmentwherenodirect
local con?icts are generated in each site, some assumptions must be made about
the schedules allowed by the local database system. For example, if the local
schedules are such that the commit order and serialization order are always
identical, we can ensure serializability by controlling only the order in which
transactions commit.
A relatedproblemin multidatabase systemsis that of global atomic commit.
If all local systems follow the two-phase commit protocol, that protocol can be
usedtoachieveglobalatomicity.However,localsystemsnotdesignedtobe part
ofadistributedsystemmaynotbeabletoparticipateinsuchaprotocol.Evenifa
localsystemiscapableofsupportingtwo-phasecommit,theorganizationowning
thesystemmaybeunwillingtopermitwaitingincaseswhereblockingoccurs.In
such cases, compromisesmay be made that allow for lack of atomicity in certain
failure modes. Further discussion of these matters appears in the literature (see
the bibliographical notes).
19.9 Cloud-Based Databases
Cloud computing is a relatively new concept in computing that emerged in
the late 1990s and the 2000s, ?rst under the name software as a service.Initial
vendors of software services provided speci?c customizable applications that
they hosted on their own machines. The concept of cloud computing developed
as vendors began to offer generic computers as a service on which clients could
run software applications of their choosing. A client can make arrangements
with a cloud-computing vendor to obtain a certain number of machines of a
862 Chapter19 DistributedDatabases
certain capacity as well as a certain amount of data storage. Both the number of
machinesand theamountofstoragecangrowand shrinkasneeded.Inaddition
toprovidingcomputingservices,manyvendorsalsoprovideotherservicessuch
as data storage services, map services, and other services that can be accessed
using a Web-serviceapplication programminginterface.
Many enterprises are ?nding the model of cloud computing and services
bene?cial.Itsavescliententerprisestheneedtomaintainalargesystem-support
staff and allows new enterprises to begin operation without having to make a
large, up-front capital investment in computing systems. Further, as the needs
of the enterprise grow, more resources (computing and storage) can be added
as required; the cloud-computing vendor generally has very large clusters of
computers,making iteasyfor the vendortoallocate resourceson demand.
Avarietyofvendorsoffercloudservices.Theyincludetraditionalcomputing
vendors as well as companies, such as Amazon and Google, that are seeking to
leveragethe large infrastructurethey have in place for theircore businesses.
Web applications that needto storeand retrievedata for verylarge numbers
ofusers(rangingfrommillionstohundredsofmillions)havebeenamajordriver
of cloud-based databases. The needs of these applications differ from those of
traditionaldatabaseapplications,sincetheyvalueavailabilityandscalabilityover
consistency. Several cloud-based data-storage systems have been developed in
recentyearstoservetheneedsofsuchapplications.Wediscussissuesinbuilding
such data-storagesystemson the cloud in Section19.9.1.
In Section 19.9.2, we consider issues in running traditional database systems
on a cloud. Cloud-based databases have features of both homogeneous and het-
erogeneoussystems.Althoughthedataareownedbyoneorganization(theclient)
and are part of one uni?ed distributed database, the underlying computers are
owned and operated by another organization (the service vendor). The comput-
ersareremotefromtheclient’slocation(s)andareaccessedovertheInternet.Asa
result,some of the challenges of heterogeneousdistributedsystemsremain, par-
ticularlyasregardstransactionprocessing.However,manyoftheorganizational
and politicalchallengesof heterogeneoussystemsare avoided.
Finally,inSection19.9.3,wediscussseveraltechnicalaswellasnontechnical
challenges that cloud databasesface today.
19.9.1 Data Storage Systems on the Cloud
Applications on the Web have extremely high scalability requirements. Popular
applicationshavehundredsofmillionsofusers,andmanyapplicationshaveseen
theirloadincreasemanyfoldwithinasingleyear,orevenwithinafewmonths.To
handlethedatamanagementneedsofsuchapplications,datamustbepartitioned
across thousands of processors.
A number of systems for data storage on the cloud have been developed
anddeployedoverthepastfewyearstoaddressdatamanagementrequirements
of such applications; these include Bigtable from Google, Simple Storage Service
(S3) from Amazon, which provides a Web interface to Dynamo,whichisakey-
valuestoragesystem,Cassandra,fromFaceBook,whichissimilartoBigtable,and
19.9 Cloud-BasedDatabases 863
Sherpa/PNUTSfromYahoo!,thedatastoragecomponentoftheAzureenvironment
from Microsoft, and severalother systems.
In this section, we provide an overview of the architecture of such data-
storage systems. Although some people refer to these systems as distributed
databasesystems,theydonotprovidemanyofthefeatureswhich areviewedas
standard on database systems today, such as support for SQL,orfortransactions
with the ACID properties.
19.9.1.1 DataRepresentation
AsanexampleofdatamanagementneedsofWebapplications,considerthepro-
?leofauser,whichneedstobeaccessibletoanumberofdifferentapplicationsthat
are run by an organization. The pro?le contains a varietyof attributes, and there
arefrequentadditionstotheattributesstoredinthepro?le.Someattributesmay
contain complex data. A simple relational representation is often not suf?cient
for such complexdata.
Some cloud-based data-storage systems support XML (described in Chap-
ter23)forrepresentingsuchcomplexdata.OtherssupporttheJavaScriptObject
Notation(JSON)representation,whichhasfoundincreasingacceptanceforrepre-
sentingcomplexdata.TheXMLandJSONrepresentationsprovide?exibilityinthe
setofattributesthatarecordcontains,aswellasthetypesoftheseattributes.Yet
others,suchasBigtable,de?netheirowndatamodelforcomplexdataincluding
supportforrecordswithaverylargenumberofoptionalcolumns.Werevisitthe
Bigtabledata modellaterin thissection.
Further, many such Web applications either do not need extensive query
language support, or at least, can manage without such support. The primary
mode of data access is to store data with an associated key, and to retrieve data
with that key. In the above user pro?le example, the key for user-pro?le data
would be the user’s identi?er. There are applications that conceptually require
joins, but implement the joins by a form of view materialization. For example,
in a social-networking application, each user should be shown new posts from
all her friends. Unfortunately, ?nding the set of friends and then querying each
one to ?nd their posts may lead to a signi?cant amount of delay when the data
are distributed across a large number of machines. An alternative is as follows:
whenever a user makes a post, a message is sent to all friends of that user, and
the data associated with each of the friends is updated with a summary of the
new post. When that user checks for updates, all required data are available in
one place and can be retrievedquickly.
Thus, cloud data-storage systems are, at their core, based on two primitive
functions,put(key,value),usedtostorevalueswithanassociatedkey,andget(key),
which retrievesthestoredvalueassociatedwiththespeci?edkey.Somesystems
such asBigtable additionallyproviderange querieson keyvalues.
In Bigtable, a record is not stored as a single value, but is instead split into
component attributes that are stored separately. Thus, the key for an attribute
value conceptually consists of (record-identi?er, attribute-name). Each attribute
value is just a string as far as Bigtable is concerned. To fetch all attributes of a
864 Chapter19 DistributedDatabases
JSON
JavaScriptObjectNotation,or JSON, isatextualrepresentationofcomplexdata
types which is widely used for transmitting data between applications, as well
astostorecomplexdata.JSONsupportstheprimitivedatatypesinteger,realand
string,aswellasarrays,and“objects”,whichareacollectionof(attribute-name,
value) pairs. An exampleofa JSON objectis:
{
"ID":"22222",
"name":{
"?rstname:"Albert",
"lastname: "Einstein"
},
"deptname":"Physics",
"children":[
{ "?rstname":"Hans", "lastname":"Einstein"},
{ "?rstname":"Eduard", "lastname":"Einstein"}
]
}
Theaboveexampleillustrates objects,which contain (attribute-name,value)
pairs, as well as arrays, delimited by square brackets. JSON can be viewed as a
simpli?ed form of XML; XML is coveredinChapter23.
LibrarieshavebeendevelopedtotransformdatabetweentheJSONrepresen-
tation and the object representation used in the JavaScript and PHP scripting
languages, as well as otherprogramminglanguages.
record,arangequery,ormorepreciselyapre?x-matchqueryconsistingofjustthe
recordidenti?er,isused.Theget()functionreturnstheattributenamesalongwith
the values. For ef?cient retrieval of all attributes of a record, the storage system
stores entries sorted by the key, so all attribute values of a particular record are
clusteredtogether.
In fact, the record identi?er can itself be structured hierarchically, although
to Bigtable itself the record identi?er is just a string. For example, an application
that storespagesretrievedfrom aweb crawl could mapa URL of the form:
www.cs.yale.edu/people/silberschatz.html
to the record identi?er:
edu.yale.cs.www/people/silberschatz.html
sothatpagesareclusteredinausefulorder.Asanotherexample,therecordshown
19.9 Cloud-BasedDatabases 865
in the JSONexample(seeexampleboxonJSON) can be represented by a record
with identi?er “22222”, with multipleattribute namessuch as “name.?rstname”,
“deptname”, “children[1].?rstname” or “children[2].lastname”.
Further,asingleinstance ofBigtablecanstoredataformultipleapplications,
with multiple tables per application, by simply pre?xing the application name
and table name tothe recordidenti?er.
Data-storage systems typically allow multiple versions of data items to be
stored.Versionsareoftenidenti?edbytimestamp,butmaybealternativelyiden-
ti?ed by an integer value that is incremented whenever a new version of a data
item is created. Lookups can specify the required version of a data item, or can
picktheversionwiththehighestversionnumber.InBigtable,forexample,akey
actually consists of threeparts: (record-identi?er,attribute-name, timestamp).
19.9.1.2 PartitioningandRetrievingData
Partitioning of data is, of course, the key to handling extremely large scale in
data-storagesystems.Unlikeregularparalleldatabases,itisusuallynotpossible
todecideonapartitioningfunctionaheadoftime.Further,ifloadincreases,more
servers need to be added and each server should be able to take on parts of the
load incrementally.
To solve both these problems, data-storage systems typically partition data
into relatively small units (small on such systems may mean of the order of
hundreds of megabytes). These partitions are often called tablets, re?ecting the
fact that each tablet is a fragment of a table. The partitioning of data should be
done on the search key, so that a request for a speci?c key value is directed to a
single tablet; otherwise each request would require processing at multiple sites,
increasing the load on the systemgreatly.Two approachesare used:eitherrange
partitioning is used directly on the key, or a hash function is applied on the key,
and range partitioning isappliedon the resultof the hash function.
Thesitetowhichatabletisassignedactsasthemastersiteforthattablet.All
updatesareroutedthrough thissite,and updatesarethenpropagatedtoreplicas
of the tablet. Lookups are also sent to the same site, so that reads are consistent
with writes.
The partitioning of data into tablets is not ?xed up front, but happens dy-
namically. Asdata are inserted,if a tabletgrows too big, it is broken into smaller
parts.Further,evenifatabletisnotlargeenoughtomeritbeingbrokenup,ifthe
load (get/put operations) on that tablet are excessive, the tablet may be broken
into smaller tablets, which can be distributed across two or more sites to share
the load. Usually the number of tablets is much larger than the number of sites,
for the same reason that virtualpartitioning isused inparalleldatabases.
It is important to know which site in the overall system is responsible for a
particular tablet. This can be done by having a tablet controller site which tracks
the partitioning function, to map a get() request to one or more tablets, and a
mapping function from tablets to sites, to ?nd which site were responsible for
which tablet. Each request coming into the system must be routed to the correct
site; if a single tablet controller site is responsible for this task, it would soon
866 Chapter19 DistributedDatabases
Routers
Requests Requests Requests
Tablets
Tablets servers
Tablets
controlle
Master copy of
partition table/
tablet mapping
Figure 19.7 Architecture of a cloud data storage system.
get overloaded. Instead, the mapping information can be replicated on a set of
routersites,whichrouterequeststothesitewiththeappropriatetablet.Protocols
to update mapping information when a tablet is split or moved are designed in
such a way that no locking is used; a request may as a result end up at a wrong
site.Theproblemishandledbydetectingthatthesiteisnolongerresponsiblefor
the key speci?ed by the request, and rerouting the request based on up-to-date
mapping information.
Figure 19.7 depicts the architecture of a cloud data-storage system, based
loosely on the PNUTS architecture. Other systems provide similar functionality,
although their architecture may vary. For example, Bigtable does not have sepa-
rate routers; the partitioning and tablet-server mapping information is stored in
theGoogle?lesystem,andclientsreadtheinformationfromthe?lesystem,and
decidewheretosendtheirrequests.
19.9.1.3 TransactionsandReplication
Data-storage systems on the cloud typically do not fully support ACID trans-
actions. The cost of two-phase commit is too high, and two-phase commit can
lead to blocking in the event of failures, which is not acceptable to typical Web
applications.Thismeansthatsuchsystemstypicallydonotevensupportatrans-
actionallyconsistentsecondaryindex:thesecondaryindexwouldbepartitioned
on a different attribute from the key used for storing the data, and an insert or
update would then need to update two sites, which would require two-phase
commit.Atbest,suchsystemssupporttransactionsondatawithinasingletablet,
which is controlled by a a single master site. Sherpa/PNUTS also providesa test-
19.9 Cloud-BasedDatabases 867
and-set function, which allows an update to a data item to be conditional on the
current version of the data item being the same as a speci?ed version number. If
thecurrentversionnumberofthedataitemismorerecentthanthespeci?edver-
sion number, the update is not performed.The test-and-setfunction can be used
by applications to implement a limited form of validation-based concurrency
control, with validationrestrictedto dataitemsin a single tablet.
In a system with thousands of sites, at any time it is almost guaranteed that
several of the sites will be down. A data-storage system on the cloud must be
able to continue normal processing even with many sites down. Such systems
replicatedata(suchastablets)tomultiplemachinesinacluster,sothatacopyof
the data is likely to be available even if some machines of a cluster are down. (A
cluster is a collection of machines in a data center.) For example, the Google File
System (GFS), which is a distributed fault-tolerant ?le system, replicates all ?le
systemblocksatthreeormorenodesinacluster.Normaloperationcancontinue
as long as at least one copy of the data is available (key system data, such as
the mapping of ?les to nodes, is replicated at more nodes, a majority of which
need to be available). In addition, replication is also used across geographically
distributedclusters,for reasonsthat we shallsee shortly.
Sinceeachtabletiscontrolledbyasinglemastersite,ifthesitefailsthetablet
shouldbereassignedtoadifferentsitethathasacopyofthetablet,whichbecomes
the new master site for the tablet. Updates to a tablet are logged, and the log is
itselfreplicated.Whenasitefails,thetabletsatthesiteareassignedtoothersites;
the new master site of each tablet is responsible for performing recovery actions
usingthelogtobringitscopyofthetablettoanup-to-dateconsistentstate,after
which updatesand lookups can be performedon the tablet.
In Bigtable, as an example, mapping information is stored in an index struc-
ture, and the index as well as the actual tablet data are stored in the ?le system.
Tabletdataupdatesarenot?ushedimmediately,butlogdataare.The?lesystem
ensures that the ?le system data are replicated and will be available even in the
face of failure of a few nodes in the cluster. Thus, when a tablet is reassigned,
the new master site for the tablet has access to up-to-date log data. Yahoo!’s
Sherpa/PNUTSsystem,ontheotherhand,explicitlyreplicatestabletstomultiple
nodes in a cluster, instead of using a distributed ?le system, and uses a reliable
distributed-messagingsystemto implementahighly available log.
Unfortunately,itisnotuncommonforanentiredatacentertobecomeunavail-
able-for example, due to natural disasters or ?res. Replication at a remote site is
therefore essential for high availability. For many Web applications, round-trip
delays across a long-distance network can affect performance signi?cantly, a
problemthatisincreasingwiththeuseofAjaxapplicationsthatrequiremultiple
roundsofcommunicationbetweenthebrowserandtheapplication.Todealwith
thisproblem,usersareconnectedwithapplicationserversthatareclosesttothem
geographically,anddataarereplicatedatmultipledatacenterssothatoneofthe
replicasislikelyto be close to the applicationserver.
However, the danger of partitioning of the network is increased as a result.
GiventhatmostWebapplicationsplaceagreaterpremiumonavailabilitythanon
consistency, data-storage systemson the cloud usuallyallow updatestoproceed
868 Chapter19 DistributedDatabases
even in the event of a partitioning, and provide support for restoring consis-
tency later, as discussed earlier in Section 19.6.6. Multimaster replication with
lazy propagation of updates, which we saw in Section 19.5.3, is typically used
for processing updates. Lazy propagation implies that updates are not propa-
gated to replicas as part of the updating transaction, although they are typically
propagatedas soon as possible, typicallyusing a messaginginfrastructure.
In addition to propagating updates to replicas of a data item, updates to
secondaryindices,orto certainkindsof materializedviews(suchastheupdates
fromfriends,inasocial-networkingapplicationwesawearlierinSection19.9.1.1),
can be sent using the messaging infrastructure. Secondary indices are basically
tables, partitioned just like regular tables, based on the index search key; an
updateofarecordinatablecanbemappedtoupdatesofoneormoretabletsina
secondaryindexonthe table.Thereisnotransactional guaranteeon theupdates
ofsuchsecondaryindicesormaterializedviews,andonlyabest-effortguarantee
in termsofwhen the updatesreach theirdestination.
19.9.2 Traditional Databases on the Cloud
We now consider the issue of implementing a traditional distributed database
system,supporting ACID propertiesand queries,ona cloud.
Theconceptofcomputingutilitiesisanoldone,envisionedbackinthe1960s.
The?rstmanifestationoftheconceptwasintimesharingsystemsinwhichseveral
users shared access to a single mainframe computer. Later, in the late 1960s, the
conceptofvirtualmachineswasdeveloped,inwhichauserwasgiventheillusion
ofhavingaprivatecomputer,whileinrealityasinglecomputersimulatedseveral
virtual machines.
Cloudcomputingmakesextensiveuseofthevirtual-machineconcepttopro-
vide computing services. Virtual machines provide great ?exibility since clients
may choose their own software environment including not only the application
software but also the operating system. Virtual machines of several clients can
run on a single physical computer, if the computing needs of the clients are low.
On the other hand, an entire computer can be allocated to each virtual machine
ofaclientwhosevirtualmachineshaveahighload.Aclientmayrequestseveral
virtual machines over which to run an application. This makes it easy to add or
subtract computing power as workloads grow and shrink simply by adding or
releasingvirtualmachines.
Having a set of virtual machines works well for applications that are easily
parallelized. Database systems, as we have seen, fall into this category. Each
virtual machine can run database system code locally and behave in a manner
similartoa site in ahomogeneous distributeddatabase system.
19.9.3 Challenges with Cloud-Based Databases
Cloud-based databases certainly have several important advantages compared
to building a computing infrastructure from scratch, and are in fact essential for
certain applications.
19.9 Cloud-BasedDatabases 869
However,cloud-baseddatabasesystemsalsohaveseveraldisadvantagesthat
weshallnowexplore.Unlikepurelycomputationalapplicationsinwhichparallel
computations run largely independently, distributed database systems require
frequentcommunication and coordination among sitesfor:
• accesstodataonanotherphysicalmachine,eitherbecausethedataareowned
byanothervirtualmachineorbecausethedataarestoredonastorageserver
separatefrom the computerhosting the virtualmachine.
• obtaining locks on remote data.
• ensuringatomic transaction commit viatwo-phase commit.
In our earlier study of distributed databases, we assumed (implicitly) that
the database administrator had control over the physical location of data. In a
cloud system, the physical location of data is under the control of the vendor,
not the client. As a result, the physical placement of data may be suboptimal in
terms of communication cost, and this may result in a large number of remote
lock requestsand large transfers of data across virtualmachines. Effective query
optimization requires that the optimizer have accurate cost measures for opera-
tions. Lacking knowledge of the physical placement of data, the optimizer has
to rely on estimates that may be highly inaccurate, resulting in poor execution
strategies. Because remote accesses are relatively slow compared to local access,
these issuescan have a signi?cant impacton performance.
The above issues are a particular challenge for implementing traditional
database applications on the cloud, although less challenging for simple data-
storagesystems.Thenextfewchallengeswediscussapplyequallytobothappli-
cation scenarios.
Thematterofreplicationfurthercomplicatescloud-baseddatamanagement.
Cloud systems replicate client data for availability. Indeed many contracts have
clauses imposing penalties on the vendor if a certain level of availability is not
maintained. This replication is done by the vendor without speci?c knowledge
of the application. Since replication is under control of the cloud and not under
the control of the database system, care must be used when the database system
accesses data so as to ensure that the latest versions of the data are read. Failure
to take these issues properly into account can result in a loss of the atomicity or
isolationproperties.Inmanycurrentclouddatabaseapplications,theapplication
itselfmay need totake some responsibilityforconsistency.
Users of cloud computing must be willing to accept that their data are held
by another organization. This may present a variety of risks in terms of security
and legal liability. If the cloud vendor suffers a security breach, client data may
be divulged, causing the client to face legal challenges from its customers. Yet
the client has no direct control over cloud-vendor security. These issues become
more complex if the cloud vendor chooses to store data (or replicas of data) in
a foreign country. Various legal jurisdictions differ in their privacy laws. So, for
example, if a German company’s data are replicated on a server in New York,
then the privacy laws of the United States rather than those of Germany or the
870 Chapter19 DistributedDatabases
EuropeanUnionapply.Thecloudvendormightberequiredtoreleaseclientdata
to the U.S. government even though the client never knew that its data would
wind upunderU.S.jurisdiction.
Speci?ccloudvendorsoffertheirclientsvaryingdegreesofcontroloverhow
their data are distributed and replicated. Some vendors offer database services
directlyto theirclientsratherthan requireclientsto contract for raw storage and
virtualmachines overwhich torun theirown database systems.
The market for cloud services continues to evolve rapidly, but it is clear that
a database administrator who is contracting for cloud services has to consider
a wide variety of technical, economic, and legal issues in order to ensure the
privacyand securityofdata,guaranteesof the ACID properties(oran acceptable
approximationthereof),andadequateperformancedespitethelikelihoodofdata
beingdistributedoverawidegeographicarea.Thebibliographicalnotesprovide
some of the current thinking on these topics. Much new literature is likely to
appear in the next few years, and many of the current issues in cloud databases
are being addressedby the researchcommunity.
19.10 Directory Systems
Consider an organization that wishes to make data about its employees avail-
able to a variety of people in the organization; examples of the kinds of data
includename,designation,employee-id,address,emailaddress,phonenumber,
faxnumber,andsoon.Intheprecomputerizationdays,organizationswouldcre-
atephysicaldirectoriesofemployeesanddistributethemacrosstheorganization.
Eventoday,telephonecompanies createphysical directoriesofcustomers.
In general, a directory is a listing of information about some class of objects
suchaspersons.Directoriescanbeusedto?ndinformationaboutaspeci?cobject,
or in the reverse direction to ?nd objects that meet a certain requirement. In the
world of physical telephone directories, directories that satisfy lookups in the
forward direction are called white pages, while directories that satisfy lookups
inthereversedirectionarecalledyellowpages.
In today’s networked world, the need for directories is still present and, if
anything, evenmore important. However,directoriestoday need to be available
overa computernetwork, rather than in a physical(paper)form.
19.10.1 Directory Access Protocols
Directory information can be made available through Web interfaces, as many
organizations, and phone companies in particular, do. Such interfaces are good
forhumans.However,programstooneedtoaccessdirectoryinformation.Direc-
tories can be used for storing other types of information, much like ?le system
directories. For instance, Web browsers can store personal bookmarks and other
browser settings in a directory system. A user can thus access the same settings
from multiple locations, such as at home and at work, without having to share a
?le system.
19.10 DirectorySystems 871
Several directory access protocols have been developed to provide a stan-
dardizedwayofaccessingdatainadirectory.Themostwidelyusedamongthem
todayis theLightweightDirectoryAccessProtocol(LDAP).
Obviously all the types of data in our examples can be stored without much
trouble in a database system, and accessed through protocols such as JDBC or
ODBC.Thequestionthenis,whycomeupwithaspecializedprotocolforaccessing
directoryinformation? There are atleasttwo answers to the question.
• First,directoryaccessprotocolsaresimpli?edprotocolsthatcatertoalimited
type of access to data. They evolved in parallel with the database access
protocols.
• Second, and more important, directory systems provide a simple mecha-
nismtonameobjectsinahierarchicalfashion,similarto?lesystemdirectory
names, which can be used in a distributed directory system to specify what
information is stored in each of the directory servers. For example, a partic-
ular directory server may store information for Bell Laboratories employees
in Murray Hill, while another may store information for Bell Laboratories
employees in Bangalore, giving both sites autonomy in controlling their lo-
cal data. The directory access protocol can be used to obtain data from both
directories across a network. More important, the directory system can be
set up to automatically forward queries made at one site to the other site,
without userintervention.
For these reasons, several organizations have directory systems to make or-
ganizational information available online through a directory access protocol.
Information inan organizational directorycan be usedfora varietyofpurposes,
such as to ?nd addresses, phone numbers, or email addresses of people, to ?nd
whichdepartmentspeoplearein,andtotrackdepartmenthierarchies.Directories
are also used to authenticate users: applications can collect authentication infor-
mationsuch aspasswordsfromusersand authenticate themusingthe directory.
As may be expected, several directory implementations ?nd it bene?cial to
userelationaldatabasestostoredata,insteadofcreatingspecial-purposestorage
systems.
19.10.2 LDAP: Lightweight Directory Access Protocol
Ingeneraladirectorysystemisimplementedasoneormoreservers,whichservice
multipleclients.Clientsusetheapplicationprogrammerinterfacede?nedbythe
directory system to communicate with the directory servers. Directory access
protocols alsode?ne a datamodeland access control.
The X.500 directory access protocol, de?ned by the International Organiza-
tion for Standardization (ISO), is a standard for accessing directory information.
However,theprotocolisrathercomplex,andisnotwidelyused.TheLightweight
DirectoryAccessProtocol(LDAP)providesmanyoftheX.500features,butwith
lesscomplexity,andiswidelyused.Intherestofthissection,weshalloutlinethe
datamodeland accessprotocol detailsof LDAP.
872 Chapter19 DistributedDatabases
19.10.2.1 LDAPDataModel
In LDAP, directories store entries, which are similar to objects. Each entry must
have a distinguished name (DN), which uniquely identi?es the entry. A DN is
in turn made up of a sequence of relative distinguished names (RDNs).For
example,anentrymay havethe following distinguishedname:
cn=Silberschatz, ou=Computer Science, o=Yale University, c=USA
As you can see, the distinguished name in this example is a combination of a
name and (organizational) address, starting with a person’s name, then giving
the organizational unit (ou), the organization (o), and country (c). The order of
thecomponentsofadistinguishednamere?ectsthenormalpostaladdressorder,
rather than the reverse order used in specifying path names for ?les. The set of
RDNsforaDNisde?ned by the schema of the directorysystem.
Entriescanalsohaveattributes.LDAPprovidesbinary,string,andtimetypes,
and additionally the types tel for telephone numbers, and PostalAddress for
addresses(linesseparatedbya“$”character).Unlikethoseintherelationalmodel,
attributesaremultivaluedbydefault,soitispossibletostoremultipletelephone
numbers or addressesfor anentry.
LDAP allows the de?nition ofobjectclasses with attribute names and types.
Inheritance can be used in de?ning object classes. Moreover,entriescan be spec-
i?ed to be of one or more object classes. It is not necessary that there be a single
most-speci?c object classto which an entry belongs.
Entries are organized into a directory information tree (DIT), according to
their distinguished names. Entries at the leaf level of the tree usually represent
speci?c objects. Entries that are internal nodes represent objects such as orga-
nizational units, organizations, or countries. The children of a node have a DN
containing all the RDNs of the parent, and one or more additional RDNs. For in-
stance, an internal node may have a DN c=USA, and all entries below it have the
value USAfor the RDN c.
Theentiredistinguishednameneednotbestoredinanentry.Thesystemcan
generate the distinguished name of an entry by traversing up the DIT from the
entry,collectingthe RDN=valuecomponentstocreatethefulldistinguishedname.
Entriesmayhavemorethanonedistinguishedname—forexample,anentry
forapersoninmorethanoneorganization.Todealwithsuchcases,theleaflevel
of a DIT can be analias, which points to an entryin another branch of the tree.
19.10.2.2 DataManipulation
Unlike SQL, LDAP does not de?ne either a data-de?nition language or a data-
manipulation language. However, LDAP de?nes a network protocol for carrying
outdatade?nitionandmanipulation.UsersofLDAPcaneitheruseanapplication
programming interface or use tools provided by various vendors to perform
data de?nition and manipulation. LDAP also de?nes a ?le format called LDAP
Data Interchange Format (LDIF) that can be used for storing and exchanging
information.
19.10 DirectorySystems 873
Thequeryingmechanismin LDAPisverysimple,consistingofjustselections
and projections, without any join. A querymust specifythe following:
• Abase—that is,anodewithina DIT—bygivingitsdistinguishedname(the
path fromthe rootto the node).
• A search condition, which can be a Boolean combination of conditions on
individual attributes. Equality, matching by wild-card characters, and ap-
proximate equality (the exact de?nition of approximate equality is system
dependent)aresupported.
• A scope, which can be just the base, the base and its children, or the entire
subtreebeneaththe base.
• Attributesto return.
• Limitson number of resultsand resource consumption.
The query can also specify whether to automatically dereference aliases; if alias
dereferencesare turned off, alias entriescan be returnedas answers.
One way of querying an LDAP data source is by using LDAP URLs. Examples
of LDAP URLsare:
ldap:://codex.cs.yale.edu/o=Yale University,c=USA
ldap:://codex.cs.yale.edu/o=Yale University,c=USA??sub?cn=Silberschatz
The ?rst URL returns all attributes of all entries at the server with organization
being Yale University,and country being USA. The second URL executes a search
query (selection) cn=Silberschatz on the subtree of the node with distinguished
nameo=Yale University, c=USA.Thequestionmarksinthe URLseparatedifferent
?elds. The ?rst ?eld is the distinguished name, here o=Yale University,c=USA.
The second ?eld, the list of attributes to return, is left empty, meaning return
all attributes. The third attribute, sub, indicates that the entire subtree is to be
searched.The lastparameteris the search condition.
A second way of querying an LDAP directory is by using an application
programming interface. Figure 19.8 shows a piece of C code used to connect
to an LDAP server and run a query against the server. The code ?rst opens a
connection to an LDAP server by ldap open and ldap bind.Itthenexecutesa
querybyldap search s.Theargumentstoldap search saretheLDAPconnection
handle, the DNofthebasefromwhichthesearchshouldbedone,thescopeof
the search, the search condition, the list of attributes to be returned, and an
attributecalledattrsonly,which,ifsetto1,wouldresultinonlytheschemaofthe
result being returned, without any actual tuples. The last argument is an output
argumentthat returnsthe resultof the search as an LDAPMessage structure.
The?rstforloopiteratesoverandprintseachentryintheresult.Notethatan
entrymayhavemultipleattributes,andthesecondforloopprintseachattribute.
Sinceattributesin LDAPmaybemultivalued,thethirdforloopprintseachvalue
of an attribute. The calls ldap msgfree and ldap value free free memory that is
874 Chapter19 DistributedDatabases
#include <stdio.h>
#include <ldap.h>
main() {
LDAP *ld;
LDAPMessage *res, *entry;
char *dn, *attr, *attrList[] = {“telephoneNumber”, NULL};
BerElement *ptr;
int vals, i;
ld = ldap open(“codex.cs.yale.edu”, LDAP PORT);
ldap simple bind(ld, “avi”, “avi-passwd”);
ldap search s(ld, “o=Yale University, c=USA”, LDAP SCOPE SUBTREE,
“cn=Silberschatz”, attrList, /*attrsonly*/ 0, &res);
printf(“found %d entries”,ldapcount entries(ld, res));
for (entry=ldap ?rst entry(ld, res); entry != NULL;
entry = ldap next entry(ld, entry))
{
dn = ldap get dn(ld, entry);
printf(“dn: %s”,dn);
ldap memfree(dn);
for (attr = ldap ?rst attribute(ld, entry, &ptr);
attr ! NULL;
attr = ldap next attribute(ld, entry, ptr))
{
printf(“%s: ”, attr);
vals = ldap get values(ld, entry, attr);
for (i=0; vals[i] != NULL; i++)
printf(“%s, ”, vals[i]);
ldap value free(vals);
}
}
ldap msgfree(res);
ldap unbind(ld);
}
Figure 19.8 Example of LDAP code in C.
allocatedbytheLDAPlibraries.Figure19.8doesnotshowcodeforhandlingerror
conditions.
The LDAP API also contains functions tocreate, update,and deleteentries,as
well as other operations on the DIT. Each function call behaves like a separate
transaction; LDAP doesnot supportatomicity of multipleupdates.
19.10.2.3 DistributedDirectoryTrees
Informationaboutanorganizationmaybesplitintomultiple DITs,eachofwhich
stores information about some entries. The suf?x of a DIT is a sequence of
19.11 Summary 875
RDN=value pairs that identify what information the DIT stores; the pairs are con-
catenated tothe restofthe distinguishedname generatedby traversingfromthe
entrytothe root.Forinstance, the suf?xofa DITmaybe o=Lucent, c=USA,while
anothermayhavethesuf?xo=Lucent,c=India.TheDITsmaybeorganizationally
and geographicallyseparated.
AnodeinaDIT may contain a referral to another node in another DIT;for
instance, the organizational unit Bell Labs under o=Lucent, c=USA may have its
own DIT, in which case the DIT for o=Lucent, c=USA would have a node ou=Bell
Labs representingareferralto the DIT for Bell Labs.
Referrals are the key component that help organize a distributed collection
of directories into an integrated system. When a server gets a query on a DIT,it
may return a referral to the client, which then issues a query on the referenced
DIT. Access to the referenced DIT is transparent, proceeding without the user’s
knowledge.Alternatively,theserveritselfmayissuethequerytothereferredDIT
and returnthe resultsalong with locallycomputed results.
The hierarchical naming mechanism used by LDAP helps break up control
of information across parts of an organization. The referral facility then helps
integrateall the directoriesinan organization into a single virtualdirectory.
Although it is not an LDAP requirement, organizations often choose to break
upinformationeitherbygeography(forinstance, an organizationmaymaintain
a directory for each site where the organization has a large presence) or by orga-
nizational structure (for instance, each organizational unit, such as department,
maintains itsown directory).
Many LDAP implementations support master–slave and multimaster repli-
cation of DITs, although replication is not part of the current LDAP version 3
standard. Workon standardizingreplicationin LDAP is inprogress.
19.11 Summary
• A distributed database system consists of a collection of sites, each of which
maintains a local database system. Each site is able to process local transac-
tions:thosetransactionsthataccessdatainonlythatsinglesite.Inaddition,a
sitemayparticipateintheexecutionofglobaltransactions:thosetransactions
thataccessdatainseveralsites.Theexecutionofglobaltransactionsrequires
communication among the sites.
• Distributeddatabasesmaybe homogeneous,where allsiteshaveacommon
schemaanddatabasesystemcode,orheterogeneous,wheretheschemasand
systemcodes maydiffer.
• Thereareseveralissuesinvolvedinstoringarelationinthedistributeddata-
base, including replication and fragmentation. It is essential that the system
minimize the degree to which a user needs to be aware of how a relation is
stored.
• Adistributedsystemmaysufferfromthesametypesoffailurethatcanaf?ict
a centralized system. There are, however, additional failures with which we
876 Chapter19 DistributedDatabases
need to deal in a distributed environment, including the failure of a site,
the failure of a link, loss of a message, and network partition. Each of these
problems needs to be considered in the design of a distributed recovery
scheme.
• Toensureatomicity,allthesitesinwhichatransactionTexecutedmustagree
onthe?naloutcomeoftheexecution.Teithercommitsatallsitesorabortsat
allsites.Toensurethisproperty,thetransactioncoordinatorofTmustexecute
a commit protocol. The most widely used commit protocol is the two-phase
commit protocol.
• Thetwo-phasecommitprotocolmayleadtoblocking,thesituationinwhich
the fate of a transaction cannot be determined until a failed site (the coordi-
nator) recovers. We can use the three-phase commit protocol to reduce the
probabilityof blocking.
• Persistentmessagingprovidesanalternativemodelforhandlingdistributed
transactions. The model breaks a single transaction into parts that are exe-
cuted at different databases. Persistent messages (which are guaranteed to
be delivered exactly once, regardless of failures), are sent to remote sites
to request actions to be taken there. While persistent messaging avoids the
blocking problem,applicationdevelopershave to writecode to handle vari-
ous typesof failures.
• The various concurrency-control schemes used in a centralized system can
be modi?ed for use in adistributedenvironment.
?
In the case of locking protocols, the only change that needs to be incor-
porated is in the way that the lock manager is implemented. There are
a variety of different approaches here. One or more central coordinators
may be used. If, instead, a distributed-lock-manager approach is taken,
replicateddatamust be treatedspecially.
?
Protocolsforhandlingreplicateddataincludetheprimarycopy,majority,
biased, and quorum consensus protocols. These have different trade-offs
in termsof cost and ability towork in the presence of failures.
?
In the case of timestamping and validation schemes, the only needed
change is to develop a mechanism for generating unique global times-
tamps.
?
Manydatabasesystemssupportlazyreplication,whereupdatesareprop-
agated to replicasoutsidethe scope of the transaction thatperformedthe
update.Suchfacilitiesmustbeusedwithgreatcare,sincetheymayresult
in nonserializable executions.
• Deadlock detection in a distributed-lock-manager environment requires co-
operation between multiple sites, since there may be global deadlocks even
whenthereareno local deadlocks.
19.11 Summary 877
• To provide high availability, a distributed database must detect failures, re-
con?gure itself so that computation may continue, and recover when a pro-
cessor or a link is repaired. The task is greatly complicated by the fact that it
ishard todistinguishbetweennetwork partitionsand site failures.
Themajorityprotocolcanbeextendedbyusingversionnumberstopermit
transaction processing to proceed even in the presence of failures.While the
protocolhasasigni?cantoverhead,itworksregardlessofthetypeoffailure.
Less-expensive protocols are available to deal with site failures, but they
assume network partitioningdoesnot occur.
• Some of the distributed algorithms require the use of a coordinator. To pro-
videhighavailability,thesystemmustmaintainabackupcopythatisreadyto
assume responsibilityifthe coordinator fails. Another approach isto choose
thenewcoordinatorafterthecoordinatorhasfailed.Thealgorithmsthatde-
terminewhichsiteshouldactasacoordinatorarecalledelectionalgorithms.
• Querieson a distributeddatabase may needto access multiplesites.Several
optimizationtechniquesareavailabletoidentifythebestsetofsitestoaccess.
Queriescanberewrittenautomaticallyintermsoffragmentsofrelationsand
then choices can be made among the replicas of each fragment. Semijoin
techniques may be employed to reduce data transfer involved in joining
relations(orfragments orrelicasthereof)across distinctsites.
• Heterogeneous distributed databases allow sites to have their own schemas
anddatabasesystemcode.Amultidatabasesystemprovidesanenvironment
in which new database applications can access data from a variety of pre-
existing databases located in various heterogeneous hardware and software
environments.Thelocaldatabasesystemsmayemploydifferentlogicalmod-
els and data-de?nition and data-manipulation languages, and may differ in
theirconcurrency-controlandtransaction-managementmechanisms.Amul-
tidatabasesystemcreatestheillusionoflogicaldatabaseintegration,without
requiringphysical database integration.
• A large number of data-storage systems on the cloud have been built in
recent years, in response to data storage needs of extremely large-scale Web
applications. These data-storage systems allow scalability to thousands of
nodes,withgeographicdistribution,andhighavailability.However,theydo
not support the usual ACID properties, and they achieve availability during
partitionsatthecostofconsistencyofreplicas.Currentdata-storagesystems
also do not support SQL, and provide only a simple put()/get() interface.
While cloud computing is attractive evenfor traditionaldatabases,there are
several challenges due to lack of control on data placement and geographic
replication.
• Directory systems can be viewed as a specialized form of database, where
informationisorganizedina hierarchicalfashion similartotheway ?lesare
organizedina?lesystem.Directoriesareaccessedbystandardizeddirectory
accessprotocolssuchas LDAP.Directoriescanbedistributedacrossmultiple
878 Chapter19 DistributedDatabases
sitestoprovideautonomytoindividualsites.Directoriescancontainreferrals
to other directories, which help build an integrated view whereby a query
is sent to a single directory, and it is transparently executed at all relevant
directories.
Review Terms
• Homogeneous distributed
database
• Heterogeneousdistributed
database
• Data replication
• Primarycopy
• Data fragmentation
?
Horizontal fragmentation
?
Verticalfragmentation
• Data transparency
?
Fragmentation transparency
?
Replication transparency
?
Location transparency
• Nameserver
• Aliases
• Distributedtransactions
?
Local transactions
?
Global transactions
• Transaction manager
• Transaction coordinator
• Systemfailuremodes
• Networkpartition
• Commitprotocols
• Two-phase commit protocol (2PC)
?
Ready state
?
In-doubt transactions
?
Blocking problem
• Three-phase commit protocol
(3PC)
• Persistentmessaging
• Concurrency control
• Single lock manager
• Distributedlock manager
• Protocols for replicas
?
Primarycopy
?
Majorityprotocol
?
Biasedprotocol
?
Quorum consensus protocol
• Timestamping
• Master–slave replication
• Multimaster(update-anywhere)
replication
• Transaction-consistent snapshot
• Lazy propagation
• Deadlockhandling
?
Localwait-for graph
?
Global wait-for graph
?
False cycles
• Availability
• Robustness
?
Majority-basedapproach
?
Read one, write all
?
Read one, write all available
?
Sitereintegration
• Coordinatorselection
PracticeExercises 879
• Backupcoordinator
• Electionalgorithms
• Bullyalgorithm
• Distributedqueryprocessing
• Semijoinstrategy
• Multidatabasesystem
?
Autonomy
?
Mediators
?
Local transactions
?
Global transactions
?
Ensuring global serializability
?
Ticket
• Cloud computing
• Cloud datastorage
• Tablet
• Directorysystems
• LDAP: LightweightDirectory
Access Protocol
?
Distinguishedname (DN)
?
Relative distinguished names
(RDNs)
?
Directoryinformation
tree(DIT)
• Distributeddirectorytrees
?
DIT suf?x
?
Referral
Practice Exercises
19.1 Howmightadistributeddatabasedesignedforalocal-areanetworkdiffer
fromone designedfor awide-areanetwork?
19.2 Tobuildahighlyavailabledistributedsystem,youmustknowwhatkinds
of failurescan occur.
a. Listpossible typesof failurein a distributedsystem.
b. Which itemsinyourlistfrompartaarealso applicabletoa central-
izedsystem?
19.3 Consider a failure that occurs during 2PC for a transaction. For each pos-
sible failure that you listed in Practice Exercise 19.2a, explain how 2PC
ensurestransaction atomicity despitethe failure.
19.4 Consider a distributed system with two sites, A and B.CansiteA distin-
guish among the following?
• B goesdown.
• Thelink between Aand B goesdown.
• Bisextremelyoverloadedandresponsetimeis100timeslongerthan
normal.
What implications does your answer have for recovery in distributed
systems?
880 Chapter19 DistributedDatabases
19.5 The persistent messaging scheme described in this chapter depends on
timestampscombinedwithdiscardingofreceivedmessagesiftheyaretoo
old. Suggest an alternative scheme based on sequence numbers instead
of timestamps.
19.6 Give an example where the read one, write all available approach leads
to an erroneousstate.
19.7 Explain the difference between data replication in a distributed system
and the maintenance ofa remotebackup site.
19.8 Giveanexamplewherelazyreplicationcanleadtoaninconsistentdatabase
state even when updates get an exclusive lock on the primary (master)
copy.
19.9 Consider the following deadlock-detection algorithm. When transaction
T
i
,atsiteS
1
,requestsaresourcefrom T
j
,atsiteS
3
,arequestmessagewith
timestamp n is sent. The edge (T
i
,T
j
,n) is inserted in the local wait-for
graph of S
1
.Theedge( T
i
,T
j
,n) is inserted in the local wait-for graph of
S
3
only if T
j
has received the request message and cannot immediately
grant the requested resource. A request from T
i
to T
j
in the same site is
handledintheusualmanner;notimestampsareassociatedwiththeedge
(T
i
,T
j
).Acentralcoordinatorinvokesthedetectionalgorithmbysending
an initiating messageto each sitein the system.
On receiving this message, a site sends its local wait-for graph to the
coordinator.Notethatsuchagraphcontainsallthelocalinformationthat
the site has about the state of the real graph. The wait-for graph re?ects
an instantaneous state of the site, but it is not synchronized with respect
to any othersite.
When the controllerhas receivedareplyfromeach site,itconstructs a
graph as follows:
• The graph contains a vertexfor everytransaction in the system.
• Thegraphhasanedge(T
i
,T
j
) ifand only if:
?
Thereisanedge(T
i
,T
j
) inone of the wait-for graphs.
?
Anedge(T
i
,T
j
,n)(forsome n)appearsinmore than one wait-for
graph.
Show that, if there is a cycle in the constructed graph, then the system is
ina deadlockstate,and that, ifthereisno cycle inthe constructed graph,
then the system was not in a deadlock state when the execution of the
algorithm began.
19.10 Considera relationthat isfragmented horizontally by plant number:
employee (name, address, salary, plant number)
PracticeExercises 881
Assume that each fragment has two replicas:one stored at the New York
site and one stored locally at the plant site. Describe a good processing
strategyforthe following queriesenteredat the San Jose site.
a. Find allemployeesatthe Boca plant.
b. Findtheaveragesalaryof allemployees.
c. Findthehighest-paidemployeeateachofthefollowingsites:Toronto,
Edmonton, Vancouver, Montreal.
d. Findthelowest-paidemployeeinthe company.
19.11 Computer  s forthe relationsofFigure 19.9.
19.12 Giveanexampleofanapplicationideallysuitedforthecloudandanother
that would be hard to implement successfully in the cloud. Explain your
answer.
19.13 GiventhattheLDAPfunctionalitycanbeimplementedontopofadatabase
system,what isthe need for the LDAP standard?
19.14 Consider a multidatabase system in which it is guaranteed that at most
one global transaction is active at any time, and every local site ensures
local serializability.
a. Suggest ways in which the multidatabase system can ensure that
thereis atmost one activeglobal transaction atany time.
b. Show by example that it is possible for a nonserializable global
schedule toresultdespitethe assumptions.
19.15 Consider a multidatabase system in which every local site ensures local
serializability,and allglobal transactions are readonly.
a. Showbyexamplethatnonserializableexecutionsmayresultinsuch
a system.
b. Show how you could use a ticket scheme to ensure global serializ-
ability.
r
A B C
s
C D E
1 2 3 3 4 5
4 5 6 3 6 8
1 2 4 2 3 2
5 3 2 1 4 1
8 9 7 1 2 3
Figure 19.9 Relations for Practice Exercise 19.11.
882 Chapter19 DistributedDatabases
Exercises
19.16 Discuss the relativeadvantagesof centralized and distributeddatabases.
19.17 Explainhowthefollowingdiffer:fragmentationtransparency,replication
transparency, and location transparency.
19.18 When is it useful to have replication or fragmentation of data? Explain
your answer.
19.19 Explainthenotionsoftransparencyandautonomy.Whyarethesenotions
desirablefrom ahuman-factors standpoint?
19.20 If we apply a distributed version of the multiple-granularity protocol of
Chapter 15 to a distributed database, the site responsible for the root of
the DAG may become a bottleneck. Suppose we modify that protocol as
follows:
• Only intention-mode locksare allowed on the root.
• All transactions are given all possible intention-mode locks on the
rootautomatically.
Show that these modi?cations alleviate this problem without allowing
any nonserializable schedules.
19.21 Studyandsummarizethefacilitiesthatthedatabasesystemyouareusing
providesfordealingwithinconsistentstatesthatcanbereachedwithlazy
propagation ofupdates.
19.22 Discuss the advantages and disadvantages of the two methods that we
presentedinSection19.5.2for generatinggloballyunique timestamps.
19.23 Considerthe relations:
employee (name, address, salary, plant number)
machine (machine number, type, plant number)
Assume that the employee relation is fragmented horizontally by plant
number, and that each fragment is stored locally at its corresponding
plant site. Assume that the machine relation is stored in its entirety at the
Armonksite.Describeagoodstrategyforprocessingeachofthefollowing
queries.
a. Findallemployeesattheplantthatcontainsmachinenumber1130.
b. Find all employees at plants that contain machines whose type is
“millingmachine.”
c. Find all machines atthe Almadenplant.
d. Findemployee  machine.
BibliographicalNotes 883
19.24 For each of the strategies of Exercise 19.23, state how your choice of a
strategydependson:
a. Thesiteatwhichthequerywasentered.
b. Thesiteatwhich the resultis desired.
19.25 Is the expression r
i
  r
j
necessarily equal to r
j
  r
i
?Underwhat
conditions doesr
i
  r
j
= r
j
  r
i
hold?
19.26 If a cloud data-storage service is used to store two relations r and s and
we need to join r and s, why might it be useful to maintain the join
as a materialized view? In your answer, be sure to distinguish among
various meanings of “useful”: overall throughput, ef?cient use of space,
and responsetimeto userqueries.
19.27 Why do cloud-computing services support traditional database systems
bestbyusingavirtualmachineinsteadofrunningdirectlyontheservice
provider’sactual machine?
19.28 Describehow LDAPcanbeusedtoprovidemultiplehierarchicalviewsof
data,without replicatingthebase-leveldata.
Bibliographical Notes
TextbookdiscussionsofdistributeddatabasesareofferedbyOzsuandValduriez
[1999]. Breitbartetal. [1999b] presentsan overviewof distributeddatabases.
The implementation of the transaction concept in a distributed database is
presentedbyGray[1981]andTraigeretal.[1982].The2PCprotocolwasdeveloped
by Lampson and Sturgis [1976]. The three-phase commit protocol is from Skeen
[1981].MohanandLindsay[1983]discussestwomodi?edversionsof 2PC,called
presume commit and presume abort, that reduce the overhead of 2PC by de?ning
defaultassumptionsregardingthe fate of transactions.
ThebullyalgorithminSection19.6.5isfromGarcia-Molina[1982].Distributed
clock synchronization is discussed in Lamport [1978]. Distributed concurrency
control iscoveredby Bernsteinand Goodman [1981].
The transaction manager of R* is described in Mohan et al. [1986]. Valida-
tion techniques for distributed concurrency-control schemes are described by
Schlageter[1981] and Bassiouni [1988].
The problem of concurrent updates to replicated data was revisited in the
context of data warehouses by Gray et al. [1996]. Anderson et al. [1998] dis-
cusses issues concerning lazy replication and consistency. Breitbart et al. [1999a]
describeslazy update protocolsfor handling replication.
The user manuals of various database systems provide details of how they
handle replication and consistency. Huang and Garcia-Molina [2001] addresses
exactly-once semanticsin a replicatedmessaging system.
Knapp [1987] surveys the distributed deadlock-detection literature. Practice
Exercise19.9 isfromStuartetal. [1984].
884 Chapter19 DistributedDatabases
DistributedqueryprocessingisdiscussedinEpsteinetal.[1978]andHevner
and Yao [1979]. Daniels et al. [1982] discusses the approach to distributed query
processingtaken by R*.
Dynamic query optimization in multidatabases is addressed by Ozcan et al.
[1997]. Adali et al. [1996] and Papakonstantinou et al. [1996] describe query-
optimization issuesin mediatorsystems.
Transaction processing in multidatabase systems is discussed in Mehrotra
etal. [2001]. The ticketscheme ispresentedin Georgakopoulosetal. [1994]. 2LSR
isintroduced in Mehrotraetal. [1991].
A collection of papers on data management on cloud systems is in Ooi and
S. Parthasarathy [2009]. The implementation of Google’s Bigtable is described in
Chang et al. [2008], while Cooper et al. [2008] describe Yahoo!’s PNUTS system.
Experience in building a database using Amazon’s S3 cloud-based storage is
described in Brantner et al. [2008]. An approach to making transactions work
correctly in cloud systems is discussed in Lomet et al. [2009]. The CAP theorem
was conjectured by Brewer [2000], and was formalized and proved by Gilbert
and Lynch [2002].
Howes etal. [1999] providestextbookcoverage of LDAP.
PART
6
DATAWAREHOUSING,DATA
MINING,AND
INFORMATION RETRIEVAL
Database queries are often designed to extract speci?c information, such as the
balance of an account or the sum of a customer’s account balances. However,
queries designed to help formulate a corporate strategy usually requires access
to large amounts of data originating at multiple sources.
Adatawarehouseisarepositoryofdatagatheredfrommultiplesourcesand
stored under a common, uni?ed database schema. Data stored in warehouse are
analyzed by a variety of complex aggregations and statistical analyses, often ex-
ploitingSQLconstructsfordataanalysiswhichwesawinChapter5.Furthermore,
knowledge-discovery techniques may be used to attempt to discover rules and
patternsfromthedata.Forexample,aretailermaydiscoverthatcertainproducts
tend to be purchased together, and may use that information to develop mar-
keting strategies. This process of knowledge discovery from data is called data
mining. Chapter 20 addresses the issues of data warehousing and data mining.
Inourdiscussionssofar,wehavefocusedonrelativelysimple,well-structured
data. However,there isan enormous amount of unstructured textualdataon the
Internet, on intranets within organizations, and on the computers of individual
users. Users wish to ?nd relevant information from this vast body of mostly tex-
tual information, using simple query mechanisms such as keyword queries. The
?eld of information retrievaldealswith queryingof such unstructured data, and
pays particular attention to the ranking of query results. Although this area of
research is several decades old, it has undergone tremendous growth with the
developmentoftheWorldWideWeb.Chapter21providesanintroductiontothe
?eld of information retrieval.
885
This page intentionally left blank 
CHAPTER
20
Data Warehousing and Mining
Businesses have begun to exploit the burgeoning data online to make better
decisionsabouttheiractivities,suchaswhatitemstostockandhowbesttotarget
customers to increase sales. There are two aspects to exploiting such data. The
?rstaspectistogatherdatafrommultiplesourcesintoacentralrepository,called
adatawarehouse.Issuesinvolvedinwarehousingincludetechniquesfordealing
with dirty data, that is, data with some errors, and with techniques for ef?cient
storageand indexingoflargevolumesofdata.
The second aspect is to analyze the gathered data to ?nd information or
knowledge that can be the basis for business decisions.Some kinds ofdataanal-
ysiscanbedonebyusing SQLconstructsforonlineanalyticalprocessing(OLAP),
which we saw in Section 5.6 (Chapter 5), and by using tools and graphical in-
terfaces for OLAP. Another approach to getting knowledge from data is to use
data mining, which aims at detecting various types of patterns in large volumes
of data. Data mining supplements various types of statistical techniques with
similargoals.
20.1 Decision-Support Systems
Database applications can be broadly classi?ed into transaction-processing and
decision-supportsystems.Transaction-processingsystemsaresystemsthatrecord
information about transactions, such as product sales information for compa-
nies, or course registration and grade information for universities. Transaction-
processing systems are widely used today, and organizations have accumulated
a vast amount ofinformation generatedby these systems.Decision-support sys-
tems aim to get high-level information out of the detailed information stored in
transaction-processing systems, and to use the high-level information to make
a variety of decisions. Decision-support systems help managers to decide what
products to stock in a shop, what products to manufacture in a factory, or which
oftheapplicants shouldbe admittedtoauniversity.
For example, company databases often contain enormous quantities of in-
formation about customers and transactions. The size of the information storage
required may range up to hundreds of gigabytes, or even terabytes, for large
887
888 Chapter20 DataWarehousingandMining
retailchains.Transactioninformationforaretailermayincludethenameoriden-
ti?er(suchascredit-cardnumber)ofthecustomer,theitemspurchased,theprice
paid, and the dates on which the purchases were made. Information about the
itemspurchased may include the name of the item,the manufacturer, the model
number,thecolor,andthesize.Customerinformationmayincludecredithistory,
annual income,residence,age,andeveneducationalbackground.
Such large databases can be treasure troves of information for making busi-
ness decisions, such as what items to stock and what discounts to offer. For
instance, a retail company may notice a sudden spurt in purchases of ?annel
shirts in the Paci?c Northwest, may realize that there is a trend, and may start
stockingalargernumberofsuchshirtsinshopsinthatarea.Asanotherexample,
a car company may ?nd, on querying its database, that most of its small sports
cars are bought by young women whose annual incomes are above $50,000. The
company may then target its marketing to attract more such women to buy its
smallsportscars,andmayavoidwastingmoneytryingtoattractothercategories
ofpeopletobuythosecars.Inbothcases,thecompanyhasidenti?edpatternsin
customerbehaviorand hasusedthepatternstomakebusinessdecisions.
Thestorage andretrievalofdatafordecisionsupportraisesseveralissues:
• Althoughmanydecision-supportqueriescanbewrittenin SQL,otherseither
cannot be expressed in SQL or cannot be expressed easily in SQL. Several
SQLextensionshavethereforebeenproposedtomakedataanalysiseasier.In
Section 5.6, we covered SQL extensions for data analysis and techniques for
online analytical processing(OLAP).
• Database query languages are not suited to the performance of detailedsta-
tistical analyses of data. There are several packages, such as SAS and S++,
that help in statistical analysis. Such packages have been interfaced with
databases, to allow large volumes of data to be stored in the database and
retrievedef?cientlyforanalysis.The?eldofstatisticalanalysisisalargedis-
cipline on its own; see the references in the bibliographical notes for more
information.
• Largecompanieshavediversesourcesofdatathattheyneedtouseformaking
business decisions. The sources may store the data under differentschemas.
For performance reasons (as well as for reasons of organization control), the
data sources usually will not permit other parts of the company to retrieve
dataondemand.
To execute queries ef?ciently on such diverse data, companies have
built data warehouses. Data warehouses gather data from multiple sources
under a uni?ed schema, at a single site. Thus, they provide the user a single
uniforminterfacetodata.Westudyissuesinbuildingandmaintainingadata
warehouse inSection20.2.
• Knowledge-discovery techniques attempt to discover automatically statisti-
calrulesandpatternsfromdata.The?eldofdataminingcombinesknowledge-
discovery techniques invented by arti?cial intelligence researchers and sta-
20.2 DataWarehousing 889
tistical analysts, with ef?cient implementation techniques that enable them
tobe usedonextremelylargedatabases.Section20.3discussesdatamining.
Theareaofdecisionsupportcanbebroadlyviewedascoveringalltheabove
areas, although some people use the term in a narrower sense that excludes
statisticalanalysis anddatamining.
20.2 Data Warehousing
Large companies have presences in many places, each of which may generate
a large volume of data. For instance, large retail chains have hundreds or thou-
sands of stores, whereas insurance companies may have data from thousands
of local branches. Further,largeorganizations havea complex internalorganiza-
tion structure, and therefore different data may be present in different locations,
or on different operational systems, or under different schemas. For instance,
manufacturing-problemdataandcustomer-complaintdatamaybestoredondif-
ferentdatabasesystems.Organizationsoftenpurchasedatafromexternalsources,
suchasmailingliststhatareusedforproductpromotions,orcreditscoresofcus-
tomers that are provided by credit bureaus, to decide on credit-worthiness of
customers.
1
Corporate decision makersrequireaccess to information from multiple such
sources.Settingupqueriesonindividualsourcesisbothcumbersomeandinef?-
cient.Moreover,thesourcesofdatamaystoreonlycurrentdata,whereasdecision
makersmayneedaccesstopastdataaswell;forinstance,informationabouthow
purchase patterns have changed in the past year could be of great importance.
Datawarehousesprovideasolutiontotheseproblems.
A data warehouse is a repository (or archive) of information gathered from
multiple sources, stored under a uni?ed schema, at a single site. Once gathered,
the data are stored for a long time, permitting access to historical data. Thus,
data warehouses provide the user a single consolidated interface to data, mak-
ingdecision-supportquerieseasiertowrite.Moreover,byaccessinginformation
for decision support from a data warehouse, the decision maker ensures that
online transaction-processing systems are not affected by the decision-support
workload.
20.2.1 Components of a Data Warehouse
Figure20.1showsthearchitectureofatypicaldatawarehouse,andillustratesthe
gatheringofdata,thestorageofdata,andthequeryinganddataanalysissupport.
Among theissuestobeaddressedinbuildingawarehousearethe following:
1
Credit bureaus are companies that gather information about consumers from multiple sources and compute a credit-
worthiness score foreach consumer.
890 Chapter20 DataWarehousingandMining
data
loaders
DBMS
data warehouse
query and
analysis tools
data source n
data source 2
data source 1
...
Figure 20.1 Data-warehouse architecture.
• When and how to gather data. In a source-driven architecture for gather-
ing data, the data sources transmit new information, either continually (as
transaction processing takes place), or periodically (nightly, for example).
In adestination-drivenarchitecture, the data warehouse periodicallysends
requestsfornewdatatothe sources.
Unless updates at the sources are replicated at the warehouse via two-
phasecommit,thewarehousewillneverbequiteup-to-datewiththesources.
Two-phase commit is usually far too expensive to be an option, so data
warehousestypicallyhaveslightlyout-of-datedata.That,however,isusually
not aproblemfordecision-supportsystems.
• Whatschematouse.Datasourcesthathavebeenconstructedindependently
arelikelytohavedifferentschemas.Infact,theymayevenusedifferentdata
models. Part of the task of a warehouse is to perform schema integration,
and to convert data to the integrated schema before they are stored. As a
result, the data stored in the warehouse are not just a copy of the data at the
sources.Instead,theycanbethoughtofasamaterializedviewofthedataat
thesources.
• Datatransformationandcleansing.Thetaskofcorrectingandpreprocessing
dataiscalleddatacleansing.Datasourcesoftendeliverdatawithnumerous
minorinconsistencies,whichcanbecorrected.Forexample,namesareoften
misspelled,andaddressesmayhavestreet,area,orcitynamesmisspelled,or
postalcodesenteredincorrectly.Thesecanbecorrectedtoareasonableextent
by consulting a database of street names and postal codes in each city. The
approximate matching of data required for this task is referred to as fuzzy
lookup.
Address lists collected from multiple sources may have duplicates that
need to be eliminated in a merge–purge operation (this operation is also
referred to as deduplication). Records for multiple individuals in a house
20.2 DataWarehousing 891
may be grouped together so only one mailing is sent to each house; this
operationiscalledhouseholding.
Datamaybetransformedinwaysotherthancleansing,suchaschanging
the units of measure, or converting the data to a differentschema by joining
datafrommultiplesourcerelations.Datawarehousestypicallyhavegraphi-
cal tools to support data transformation. Such tools allow transformation to
bespeci?edasboxes,andedgescanbecreatedbetweenboxestoindicatethe
?owofdata.Conditionalboxescanroutedatatoanappropriatenextstepin
transformation. SeeFigure30.7 for an exampleof a transformation speci?ed
using thegraphical toolprovidedby Microsoft SQL Server.
• How to propagate updates. Updates on relations at the data sources must
be propagated to the data warehouse. If the relations at the data warehouse
areexactlythesameasthoseatthedatasource,thepropagationisstraight-
forward. If they are not, the problem of propagating updates is basically the
view-maintenance problem,which was discussedinSection13.5.
• Whatdatatosummarize.Therawdatageneratedbyatransaction-processing
system may be too large to store online. However, we can answer many
queries by maintaining just summary data obtained by aggregation on a
relation, rather than maintaining the entire relation. For example, instead of
storing data about every sale of clothing, we can store total sales of clothing
by item nameand category.
Suppose that a relation r has been replaced by a summary relation s.
Users may still be permitted to pose queries as though the relation r were
availableonline.Ifthequeryrequiresonlysummarydata,itmaybepossible
totransform itintoan equivalentone using s instead;seeSection13.5.
The different steps involved in getting data into a data warehouse are called
extract, transform, and load or ETL tasks; extraction refers to getting data from
thesources,while loadreferstoloadingthe dataintothedatawarehouse.
20.2.2 Warehouse Schemas
Datawarehousestypicallyhaveschemasthataredesignedfordataanalysis,using
tools such as OLAP tools.Thus, the dataare usually multidimensionaldata,with
dimensionattributesandmeasureattributes.Tablescontainingmultidimensional
data are called fact tables and are usually very large. A table recording sales
informationforaretailstore,withonetupleforeachitemthatissold,isatypical
exampleofafacttable.Thedimensionsofthe salestablewouldincludewhatthe
item is (usually an item identi?er such as that used in bar codes), the date when
the item is sold, which location (store) the item was sold from, which customer
bought the item, and so on. The measure attributes may include the number of
itemssoldandthe priceofthe items.
To minimize storage requirements, dimension attributes are usually short
identi?ers that are foreign keys into other tables called dimension tables.For
instance, a fact table sales would have attributes item id, store id, customer id,and
892 Chapter 20 Data Warehousing and Mining
Figure20.2 Star schema for a data warehouse.
date, and measure attributes number and price. The attribute store id is a foreign
key into a dimension table store, which has other attributes such as store location
(city,state,country). The item id attributeof the sales tablewould be a foreignkey
into a dimension table item info, which would contain information such as the
name of the item, the category to which the item belongs, and other item details
such as color and size. The customer id attribute would be a foreign key into a
customer table containing attributes such as name and address of the customer.
Wecanalsoviewthedateattributeasaforeignkeyintoadate infotablegivingthe
month, quarter, and year of each date.
TheresultantschemaappearsinFigure20.2.Suchaschema,withafacttable,
multiple dimension tables, and foreign keys from the fact table to the dimension
tables, is called a star schema. More complex data-warehouse designs may have
multiple levels of dimension tables; for instance, the item info table may have an
attribute manufacturer id that is a foreign key into another table giving details of
the manufacturer. Such schemas are called snow?ake schemas.Complexdata-
warehouse designs may also have more than one fact table.
20.2.3 Column-Oriented Storage
Databasestraditionallystoreallattributesofatupletogether,andtuplesarestored
sequentially in a ?le. Such a storage layout is referred to as row-oriented storage.
In contrast, in column-oriented storage, each attribute of a relation is stored in a
separate ?le, with values from successive tuples stored at successive positions in
the ?le. Assuming ?xed-size data types, the value of attribute Aof the ith tuple
of a relation can be found by accessing the ?le corresponding to attribute A,and
reading the value at offset (i?1) timesthe size (in bytes) of values in attribute A.
20.3 DataMining 893
Column-oriented storage has at least two major bene?ts over row-oriented
storage:
1. When a query needs to access only a few attributes of a relation with a
large number of attributes, the remaining attributes need not be fetched
from disk into memory. In contrast, in row-oriented storage, not only are
irrelevantattributesfetchedintomemory,buttheymayalsogetprefetched
into processor cache, wasting cache space and memory bandwidth, if they
arestoredadjacenttoattributesusedinthequery.
2. Storingvaluesofthesametypetogetherincreasestheeffectivenessofcom-
pression;compressioncangreatlyreduceboththediskstoragecostandthe
timetoretrievedatafromdisk.
On the other hand, column-oriented storage has the drawback that storing or
fetchingasingletuplerequiresmultiple I/Ooperations.
As a result of the above trade-offs, column-oriented storage is not widely
usedfortransaction-processingapplications.However,column-orientedstorage
is gaining increasing acceptance for data-warehousing applications, where ac-
cessesarerarelytoindividualtuples,butratherrequirescanningandaggregating
multipletuples.
Sybase IQ was one of the early products to use column-oriented storage,
but there are now several research projects and companies that have developed
databases based on column-oriented storage systems. These systems have been
able to demonstrate signi?cant performance gains for many data-warehousing
applications.Seethebibliographicalnotesforreferencesonhowcolumn-oriented
storesareimplemented,and queriesoptimizedand processedonsuch stores.
20.3 Data Mining
Thetermdataminingreferslooselytotheprocessofsemiautomaticallyanalyzing
large databases to ?nd useful patterns. Like knowledge discovery in arti?cial
intelligence (also called machine learning) or statistical analysis, data mining
attemptstodiscoverrulesand patterns fromdata. However, datamining differs
from machine learning and statistics in that it deals with large volumes of data,
stored primarily on disk. That is, data mining deals with “knowledge discovery
indatabases.”
Some typesof knowledge discoveredfrom a database can be representedby
asetofrules. The following is an example of a rule, stated informally: “Young
women with annual incomes greater than $50,000 are the most likely people to
buy small sports cars.” Of course such rules are not universally true, and have
degreesof“support”and“con?dence,”asweshallsee.Othertypesofknowledge
are represented by equations relating different variables to each other, or by
othermechanismsforpredictingoutcomeswhenthevaluesofsomevariablesare
known.
894 Chapter20 DataWarehousingandMining
There are a variety of possible types of patterns that may be useful, and
differenttechniquesare usedto?nd differenttypesofpatterns.We shallstudya
fewexamplesofpatternsandseehowtheymaybeautomaticallyderivedfroma
database.
Usuallythereisamanualcomponenttodatamining,consistingofpreprocess-
ingdatatoaformacceptabletothealgorithmsandpostprocessingofdiscovered
patterns to ?nd novel ones that could be useful. There may also be more than
one type of pattern that can be discovered from a given database, and manual
interaction may be needed to pick useful types of patterns. For this reason, data
miningisreallyasemiautomaticprocessinreallife.However,inourdescription
we concentrate onthe automaticaspectof mining.
Thediscoveredknowledgehasnumerousapplications.Themostwidelyused
applications are those that require some sort ofprediction.Forinstance,whena
person applies for a credit card, the credit-card company wants to predict if the
personisagood creditrisk.Thepredictionistobe basedonknown attributesof
theperson,suchasage,income,debts,andpastdebt-repaymenthistory.Rulesfor
making the prediction are derived from the same attributes of past and current
credit-card holders, along with their observed behavior, such as whether they
defaultedon their credit-card dues. Other typesof prediction include predicting
whichcustomersmayswitchovertoacompetitor(thesecustomersmaybeoffered
specialdiscountstotemptthemnottoswitch),predictingwhichpeoplearelikely
to respond to promotional mail (“junk mail”), or predicting what types of phone
callingcardusage arelikelytobe fraudulent.
Another class of applications looks forassociations, for instance, books that
tend to be bought together. If a customer buys a book, an online bookstore may
suggestotherassociatedbooks.Ifapersonbuysacamera,thesystemmaysuggest
accessories that tend to be bought along with cameras. A good salesperson is
awareofsuchpatternsandexploitsthemtomakeadditionalsales.Thechallenge
is to automate the process. Other types of associations may lead to discovery of
causation. For instance, discovery of unexpected associations between a newly
introduced medicine and cardiac problems led to the ?nding that the medicine
may cause cardiac problemsin some people.The medicine was then withdrawn
from themarket.
Associations are an example of descriptive patterns. Clusters are another
example of such patterns. For example, over a century ago a cluster of typhoid
cases was found around a well, which led to the discovery that the water in
the well was contaminated and was spreading typhoid. Detection of clusters of
diseaseremainsimportanteventoday.
20.4 Classi?cation
As mentioned in Section 20.3, prediction is one of the most important types
of data mining. We describe classi?cation, study techniques for building one
typeofclassi?ers,calleddecision-treeclassi?ers,andthenstudyotherprediction
techniques.
20.4 Classi?cation 895
Abstractly, the classi?cation problem is this: Given that items belong to one
of several classes, and given past instances (called training instances)ofitems
alongwiththeclassestowhichtheybelong,theproblemistopredicttheclassto
which a new item belongs. The class of the new instance is not known, so other
attributesof theinstance must beusedtopredicttheclass.
Classi?cation can be done by ?nding rules that partition the given data into
disjointgroups.Forinstance,supposethatacredit-cardcompanywantstodecide
whetherornottogiveacreditcardtoanapplicant.Thecompanyhasavarietyof
information about the person, such as her age, educational background, annual
income,and current debts,that itcan usefor makinga decision.
Some of this information could be relevant to the credit-worthiness of the
applicant,whereassomemaynotbe.Tomakethedecision,thecompanyassigns
a credit-worthiness level of excellent, good, average, or bad to each of a sample
setof currentcustomersaccordingtoeachcustomer’spaymenthistory.Then,the
company attempts to ?nd rules that classify its current customers into excellent,
good, average, or bad, on the basis of the information about the person, other
thantheactualpaymenthistory(whichisunavailablefornewcustomers).Letus
considerjusttwoattributes:educationlevel(highestdegreeearned)andincome.
Therulesmaybe of thefollowing form:
?person P, P.degree = masters and P.income > 75,000
? P.credit = excellent
? person P, P.degree = bachelors or
(P.income ? 25,000and P.income ? 75,000) ? P.credit = good
Similarruleswouldalsobepresentfortheothercredit-worthinesslevels(average
and bad).
Theprocessofbuildingaclassi?erstartsfromasampleofdata,calledatrain-
ingset. For each tuple in the training set, the class to which the tuple belongs is
already known. For instance, the training set for a credit-card application may
be the existing customers, with their credit-worthiness determined from their
payment history. The actual data, or population, may consist of all people, in-
cludingthosewhoarenotexistingcustomers.Thereareseveralwaysofbuilding
aclassi?er,aswe shall see.
20.4.1 Decision-Tree Classi?ers
The decision-tree classi?er is a widely used technique for classi?cation. As the
name suggests,decision-treeclassi?ers use a tree; each leaf node has an associ-
ated class, and each internal node has a predicate(or more generally,afunction)
associatedwithit.Figure20.3shows anexampleofadecisiontree.
To classify a new instance, we start at the root and traverse the tree to reach
a leaf; at an internal node we evaluate the predicate (or function) on the data
instance,to?ndwhich childtogoto.Theprocesscontinuesuntilwereachaleaf
node. For example, if the degree level of a person is masters, and the person’s
896 Chapter 20 Data Warehousing and Mining
degree
income income income income
bachelors masters doctorate
none
bad
average good
bad average good excellent
 <50K
>100K
<25K >=25K
>=50K <50K
<25K
>75K
25 to 75K 50 to 100K
Figure 20.3 Classi?cation tree.
income is 40K, starting from the root we follow the edge labeled “masters,” and
from there the edge labeled “25K to 75K,” to reach a leaf. The class at the leaf is
“good,” so we predict that the credit risk of that person is good.
20.4.1.1 Building Decision-Tree Classi?ers
The question then is how to build a decision-tree classi?er, given a set of training
instances.Themostcommonwayofdoingsoistouseagreedyalgorithm,which
works recursively, starting at the root and building the tree downward. Initially
thereisonlyonenode,theroot,andalltraininginstancesareassociatedwiththat
node.
Ateachnode,ifall,or“almostall”traininginstancesassociatedwiththenode
belong to the same class, then the node becomes a leaf node associated with that
class. Otherwise, a partitioning attribute and partitioning conditions must be
selected to create child nodes. The data associated with each child node is the set
of training instances that satisfy the partitioning condition for that child node.
In our example, the attribute degree is chosen, and four children, one for each
value of degree,are created.The conditions for the four children nodes are degree
=none,degree=bachelors,degree=masters,anddegree = doctorate, respectively.
The data associated with each child consist of training instances satisfying the
condition associated with that child. At the node corresponding to masters, the
attribute income is chosen, with the range of values partitioned into intervals 0 to
25K, 25K to 50K, 50K to 75K, and over 75K. The data associated with each node
consistoftraininginstanceswiththedegreeattributebeingmastersandtheincome
attributebeingineachoftheseranges,respectively.Asanoptimization,since the
20.4 Classi?cation 897
class for the range 25K to 50K and the range 50K to 75K is the same under the
node degree = masters, the two ranges have been merged into a single range 25K
to75K.
20.4.1.2 BestSplits
Intuitively,bychoosingasequenceofpartitioningattributes,westartwiththeset
of all training instances, which is “impure”in the sensethat it contains instances
from many classes, and ends up with leaves which are “pure” in the sense that
at each leaf all training instances belong to only one class. We shall see shortly
howtomeasurepurityquantitatively.Tojudgethebene?tofpickingaparticular
attribute and condition for partitioning of the data at a node, we measure the
purityofthedataatthechildrenresultingfrompartitioningbythatattribute.The
attributeand conditionthat resultinthemaximum purityarechosen.
The purity of a set S of training instances can be measured quantitatively in
severalways.Supposethereare k classes,andoftheinstancesin Sthefractionof
instances inclass i is p
i
.One measureof purity,theGinimeasure, isde?nedas:
Gini(S) = 1 ?
k
  i?1
p
2
i
When all instances are in a single class, the Gini value is 0, while it reaches its
maximum (of 1 ?1/k) if each class has the same number of instances. Another
measureofpurityistheentropymeasure, which isde?nedas:
Entropy(S)=?
k
  i?1
p
i
log
2
p
i
Theentropyvalueis0ifallinstancesareinasingleclass,andreachesitsmaximum
wheneachclasshasthesamenumberofinstances.Theentropymeasurederives
from informationtheory.
When a set Sis splitinto multiple sets S
i
,i = 1,2,...,r, we can measure the
purityoftheresultantsetofsetsas:
Purity(S
1
,S
2
,...,S
r
) =
r
  i=1
|S
i
|
|S|
purity(S
i
)
That is, the purity is the weighted average of the purity of the sets S
i
.Theabove
formula can be used with both the Gini measure and the entropy measure of
purity.
The information gain due to a particular split of S into S
i
,i = 1,2,...,r is
then:
Information gain(S, {S
1
,S
2
,...,S
r
}) = purity(S) ?purity(S
1
,S
2
,...,S
r
)
898 Chapter20 DataWarehousingandMining
Splits into fewer sets are preferable to splits into many sets, since they lead
to simpler and more meaningful decision trees. The number of elements in each
of the sets S
i
may also be taken into account; otherwise, whether a set S
i
has 0
elementsor1elementwouldmakeabigdifferenceinthenumberofsets,although
the split is the same for almost all the elements. The information content of a
particularsplitcan bede?nedintermsofentropyas:
Information content(S, {S
1
,S
2
,...,S
r
})=?
r
  i?1
|S
i
|
|S|
log
2
|S
i
|
|S|
All of this leads to a de?nition: Thebestsplit for an attribute is the one that
givesthe maximuminformationgainratio,de?nedas:
Information gain(S, {S
1
,S
2
,...,S
r
})
Information content(S, {S
1
,S
2
,...,S
r
})
20.4.1.3 FindingBestSplits
Howdowe?ndthebestsplitforanattribute?Howtosplitanattributedepends
on the type of the attribute. Attributes can be either continuous valued,thatis,
the values can be ordered in a fashion meaningful to classi?cation, such as age
or income, or they can be categorical; that is, they have no meaningful order,
such as department names or country names. We do not expect the sort order of
departmentnames orcountry names tohaveany signi?cance to classi?cation.
Usuallyattributesthatarenumbers(integers/reals)aretreatedascontinuous
valuedwhilecharacterstringattributesaretreatedascategorical,butthismaybe
controlledbytheuserofthesystem.Inourexample,wehavetreatedtheattribute
degreeas categorical,and theattribute incomeas continuous valued.
We?rstconsiderhowto?ndbestsplitsforcontinuous-valuedattributes.For
simplicity, we shall consider only binarysplits of continuous-valued attributes,
that is, splits that result in two children. The case of multiway splits is more
complicated;seethe bibliographical notesforreferencesonthesubject.
To?ndthebestbinarysplitofacontinuous-valuedattribute,we?rstsortthe
attribute values in the training instances. We then compute the information gain
obtained by splitting at each value. For example, if the training instances have
values 1,10,15, and 25 for an attribute, the split points considered are 1, 10, and
15; in each case values less than or equal to the split point form one partition
and the rest of the values form the other partition. The best binary split for the
attributeisthe splitthat givesthe maximuminformation gain.
For a categorical attribute, we can have a multiway split, with a child for
each value of the attribute. This works ?ne for categorical attributes with only
a few distinct values, such as degree or gender. However, if the attribute has
many distinct values, such as department names in a large company, creating a
child for each value is not a good idea. In such cases, we would try to combine
multiple values into each child, to create a smaller number of children. See the
bibliographical notesforreferenceson howtodoso.
20.4 Classi?cation 899
procedureGrowTree(S)
Partition(S);
procedurePartition(S)
if (purity(S)>   p
or |S| <   s
)then
return;
foreachattribute A
evaluatesplitsonattribute A;
Usebestsplitfound (acrossallattributes)topartition
Sinto S
1
,S
2
,...,S
r
;
for i = 1,2,...,r
Partition(S
i
);
Figure 20.4 Recursive construction of a decision tree.
20.4.1.4 Decision-TreeConstructionAlgorithm
Themainideaofdecision-treeconstructionistoevaluatedifferentattributesand
different partitioning conditions, and pick the attribute and partitioning condi-
tion that results in the maximum information-gain ratio. The same procedure
works recursivelyon each of the sets resulting from the split,thereby recursively
constructing a decision tree. If the data can be perfectly classi?ed, the recursion
stopswhenthepurityofasetis0.However,oftendataarenoisy,orasetmaybe
so small that partitioning it further may not be justi?ed statistically. In this case,
therecursionstopswhenthepurityofasetis “suf?cientlyhigh,”andtheclassof
the resulting leaf is de?nedas the class of the majority of the elementsof the set.
Ingeneral,differentbranches ofthetreecould growtodifferentlevels.
Figure 20.4 shows pseudocode for a recursive tree-construction procedure,
which takes a set of training instances Sas parameter.The recursion stops when
the set is suf?ciently pure or the set S is too small for further partitioning to be
statisticallysigni?cant.Theparameters  p
and  s
de?necutoffsforpurityandsize;
thesystemmaygivethemdefaultvalues,which maybeoverriddenbyusers.
There are a wide variety of decision-tree construction algorithms, and we
outlinethedistinguishingfeaturesofafewofthem.Seethebibliographicalnotes
for details. With very large data sets, partitioning may be expensive, since it
involvesrepeatedcopying. Severalalgorithmshavethereforebeendevelopedto
minimize the I/O and computation cost when the training data are larger than
availablememory.
Several of the algorithms also prune subtrees of the generated decision tree
to reduceover?tting: A subtree is over?tted if it has been so highly tuned to the
speci?cs of the training data that it makes many classi?cation errors on other
data. A subtree is pruned by replacing it with a leaf node. There are different
pruning heuristics; one heuristic uses part of the training data to build the tree
andanotherpartofthetrainingdatatotestit.Theheuristicprunesasubtreeifit
?nds that misclassi?cation on the test instances would be reduced if the subtree
werereplacedbyaleafnode.
900 Chapter20 DataWarehousingandMining
We can generate classi?cation rules from a decision tree, if we so desire. For
eachleafwegeneratearuleasfollows:Theleft-handsideistheconjunctionofall
thesplitconditionsonthepathtotheleaf,andtheclassistheclassofthemajority
of thetraining instances at theleaf.Anexampleofsuchaclassi?cation ruleis:
degree = masters and income > 75000 ? excellent
20.4.2 Other Types of Classi?ers
There are several types of classi?ers other than decision-tree classi?ers. Two
types that have been quite useful are neural-net classi?ers, Bayesian classi?ers,and
Support Vector Machine classi?ers. Neural-net classi?ers use the training data to
train arti?cial neural nets. There is a large body of literature on neural nets, and
we donot considerthem furtherhere.
Bayesian classi?ers ?nd the distribution of attribute values for each class
in the training data; when given a new instance d, they use the distribution
information to estimate, for each class c
j
, the probability that instance d belongs
toclassc
j
,denotedby p(c
j
|d),inamanneroutlinedhere.Theclasswithmaximum
probabilitybecomes thepredictedclass for instance d.
To ?nd the probability p(c
j
|d)ofinstanced being inclass c
j
, Bayesianclassi-
?ers useBayes’theorem,which says:
p(c
j
|d) =
p(d|c
j
)p(c
j
)
p(d)
where p(d|c
j
) is the probability of generating instance d given class c
j
, p(c
j
)is
the probability of occurrence of class c
j
,andp(d) is the probability of instance d
occurring.Ofthese, p(d)canbeignoredsinceitisthesameforallclasses. p(c
j
)is
simplythefraction oftraining instances that belong toclass c
j
.
Forexample,letusconsideraspecialcasewhereonlyoneattribute,income,is
usedforclassi?cation,andsupposeweneedtoclassifyapersonwhoseincomeis
76000.Weassumethatincomevaluesarebrokenupintobuckets,andassumethat
thebucketcontaining76000containsvaluesintherange(75000,80000).Suppose
among instances of class excellent, the probability of income being in (75000,
80000)is0.1,whileamonginstancesofclassgood,theprobabilityofincomebeing
in (75000, 80000) is 0.05. Suppose also that overall 0.1fractionofpeopleare
classi?ed as excellent,and0 .3 are classi?ed as good. Then, p(d|c
j
)p(c
j
)forclass
excellent is .01, while for class good,itis0 .015. The person would therefore be
classi?edinclass good.
In general, multiple attributes need to be considered for classi?cation. Then,
?nding p(d|c
j
) exactly is dif?cult, since it requires the distribution of instances
of c
j
, across all combinations of values for the attributes used for classi?cation.
The number of such combinations (for example of income buckets, with degree
valuesandotherattributes)canbeverylarge.Withalimitedtrainingsetusedto
?nd the distribution, most combinations would not have even a single training
set matching them, leading to incorrect classi?cation decisions. To avoid this
20.4 Classi?cation 901
problem,aswellastosimplifythetaskofclassi?cation,naiveBayesianclassi?ers
assumeattributeshaveindependentdistributions,andtherebyestimate:
p(d|c
j
) = p(d
1
|c
j
) ? p(d
2
|c
j
)?···?p(d
n
|c
j
)
Thatis,theprobabilityoftheinstanced occurringistheproductoftheprobability
of occurrence of eachof theattributevalues d
i
of d,giventheclassis c
j
.
The probabilities p(d
i
|c
j
) derive from the distribution of values for each at-
tribute i, for each class c
j
. This distribution is computed from the training in-
stances that belong to each class c
j
; the distribution is usually approximated by
a histogram. For instance, we may divide the range of values of attribute i into
equal intervals, and store the fraction of instances of class c
j
that fall in each
interval.Givenavalued
i
forattributei,thevalueofp(d
i
|c
j
)issimplythefraction
of instances belonging toclass c
j
that fall inthe intervaltowhich d
i
belongs.
A signi?cant bene?t of Bayesian classi?ers is that they can classify instances
with unknown and null attribute values—unknown or null attributes are just
omitted from the probability computation. In contrast, decision-tree classi?ers
cannot meaningfully handle situations where an instance to be classi?ed has a
null valuefor a partitioningattribute used to traversefurther down the decision
tree.
TheSupportVectorMachine(SVM)isatypeofclassi?erthathasbeenfound
togiveveryaccurateclassi?cationacrossarangeofapplications.Weprovidesome
basic intuition about Support Vector Machine classi?ers here; see the references
inthebibliographical notes for furtherinformation.
Support Vector Machine classi?ers can best be understood geometrically. In
the simplest case, consider a set of points in a two-dimensional plane, some
belongingtoclass A,andsomebelongingtoclass B.Wearegivenatrainingsetof
pointswhoseclass(Aor B)isknown, andweneedtobuildaclassi?erofpoints,
using these training points. This situation is illustrated in Figure 20.5, where the
pointsinclass Aaredenotedby Xmarks,whilethoseinclass B aredenotedby O
marks.
Suppose we can draw a line on the plane, such that all points in class Alie
to one side and all points in line B lie to the other. Then, the line can be used to
classify new points, whose class we don’t already know. But there may be many
possible such lines that can separate points in class Afrom points in class B.A
few such lines are shown in Figure 20.5. The Support Vector Machine classi?er
chooses the line whose distance from the nearest point in either class (from the
pointsinthetrainingdataset)ismaximum.Thisline(calledthe maximum margin
line) is then used to classify other points into class Aor B, depending on which
side of the line they lie on. In Figure 20.5, the maximum margin line is shown in
bold,whilethe otherlinesareshown as dashedlines.
Theaboveintuitioncanbegeneralizedtomorethantwodimensions,allowing
multiple attributes to be used for classi?cation; in this case, the classi?er ?nds a
dividing plane, not a line. Further, by ?rst transforming the input points using
certain functions, called kernel functions, Support Vector Machine classi?ers can
?nd nonlinear curves separating the sets of points. This is important for cases
902 Chapter20 DataWarehousingandMining
Figure 20.5 Example of a support vector machine classi?er.
where the points are not separable by a line or plane. In the presence of noise,
some points of one class may lie in the midst of points of the other class. In such
cases, there may not be any line or meaningful curve that separates the points
in the two classes; then, the line or curve that most accurately divides the points
into thetwo classes ischosen.
Although the basic formulation of Support Vector Machines is for binary
classi?ers,i.e.,thosewithonlytwoclasses,theycanbeusedforclassi?cationinto
multiple classes as follows: If there are N classes, we build N classi?ers, with
classi?er i performingabinaryclassi?cation,classifyingapointeitherasinclass
i or not in class i.Givenapoint,eachclassi?eri also outputs a value indicating
how related a given point is to class i. We then apply all N classi?ers on a given
point,and choose the class forwhich the relatednessvalueisthehighest.
20.4.3 Regression
Regressiondealswiththepredictionofavalue,ratherthanaclass.Givenvalues
for a set of variables, X
1
, X
2
,...,X
n
,wewishtopredictthevalueofavariable
Y. Forinstance, we could treatthelevelof educationasanumber and income as
another number, and, on the basis of these two variables, we wish to predict the
likelihood of default, which could be a percentage chance of defaulting, or the
amountinvolvedinthedefault.
One wayistoinfercoef?cients a
0
,a
1
,a
2
,...,a
n
suchthat:
Y = a
0
+a
1
? X
1
+a
2
? X
2
+···+a
n
? X
n
Findingsuchalinearpolynomialiscalledlinearregression.Ingeneral,wewish
to ?nd a curve (de?ned by a polynomial or other formula) that ?ts the data; the
processisalsocalledcurve?tting.
20.4 Classi?cation 903
The ?t may be only approximate,because of noise in the dataor because the
relationship is not exactly a polynomial, so regression aims to ?nd coef?cients
that give the best possible ?t. There are standard techniques in statistics for
?nding regression coef?cients. We do not discuss these techniques here, but the
bibliographical notesprovidereferences.
20.4.4 Validating a Classi?er
It is important to validate a classi?er, that is, to measure its classi?cation error
rate,beforedecidingtouseitforanapplication.Consideranexampleofaclassi-
?cationproblemwhereaclassi?erhastopredict,basedonsomeinputs(theexact
inputsarenotrelevanthere),whetherapersonissufferingfromaparticulardis-
ease X or not. A positive prediction says that the person has the disease, and a
negative predictionsays the persondoes not have the disease.(The terminology
ofpositive/negativepredictioncanbeusedforanybinaryclassi?cationproblem,
not justdiseaseclassi?cation.)
Asetoftestcaseswheretheoutcomeisalreadyknown(inourexample,cases
where it is already known whether or not the person actually has the disease)
is used to measure the quality (that is, the error rate) of the classi?er. A true
positiveisacasewherethepredictionwaspositive,andthepersonactuallyhad
thedisease,whileafalsepositiveisacasewherethepredictionwaspositive,but
thepersondidnothavethedisease.Truenegativeandfalsenegativearede?ned
similarlyfor thecase wherethe predictionwasnegative.
Given a set of test cases, let t pos, f pos, t neg and f neg denote the number of
true positives, false positives, true negatives and false negatives generated. Let
pos and negdenotethe actual number ofpositivesand negatives(itiseasytosee
that pos = t pos + f neg,andneg= f pos + t neg).
Thequalityofclassi?cation canbe measuredinseveraldifferentways:
1. Accuracy,de?nedas(t pos+t neg)/(pos+neg),thatis,thefractionofthetime
when theclassi?ergivesthecorrect classi?cation.
2. Recall(alsoknownassensitivity)de?nedas t pos/pos,thatis,howmanyof
the actual positivecasesareclassi?edaspositive.
3. Precision, de?ned as t pos/(t pos+f pos), that is, how often the positive pre-
dictioniscorrect.
4. Speci?city, de?ned as t neg/neg.
Whichofthesemeasuresshouldbeusedforaspeci?capplicationdependsonthe
needs of that application. For example, a high recall is important for a screening
test, which is to be followed up by a more precise test, so that patients with
the disease are not missed out. In contrast a researcher who wants to ?nd a
few actual patients of the disease for further follow up, but is not interested in
?nding all patients, may value high precision over recall. Different classi?ers
maybeappropriateforeachoftheseapplications.Thisissueisexploredfurther
inExercise20.5.
904 Chapter20 DataWarehousingandMining
A set of testcases where the outcome is alreadyknown can be used eitherto
trainortomeasurethequalitytheclassi?er.Itisabadideatouseexactlythesame
set of test cases to train as well as to measure the quality of the classi?er, since
the classi?er has already seen the correct classi?cation of the test cases during
training; this can lead to arti?cially high measures of quality. The quality of a
classi?ermustthereforebemeasuredontestcasesthathavenotbeenseenduring
training.
Therefore,asubsetoftheavailabletestcasesisusedfortrainingandadisjoint
subset is used for validation. In cross validation, the available test cases are
dividedinto k partsnumbered 1 to k,fromwhichk differenttestsetsarecreated
as follows: test set i uses the ith part for validation, after training the classi?er
usingtheotherk?1parts.Theresults(t pos,f pos,etc.)fromallktestsetsareadded
upbeforecomputingthequalitymeasures.Crossvalidationprovidesmuchmore
accurate measures than merely partitioning the data into a single training and a
singletestset.
20.5 Association Rules
Retail shops are often interested in associations between different items that
peoplebuy. Examplesofsuchassociations are:
• Someonewho buys breadis quitelikelyalsotobuy milk.
• A person who bought the book Database System Concepts is quite likely also
to buy thebook Operating System Concepts.
Association information can be used in several ways. When a customer buys a
particular book, an online shop may suggest associated books. A grocery shop
may decide to place bread close to milk, since they are often bought together, to
help shoppers ?nish their task faster. Or, the shop may place them at opposite
endsofarow,andplaceotherassociateditemsinbetweentotemptpeopletobuy
thoseitemsaswell,astheshopperswalkfromoneendoftherowtotheother.A
shopthatoffersdiscountsononeassociateditemmaynotofferadiscountonthe
other,since thecustomer willprobably buy theother anyway.
Anexampleofanassociationruleis:
bread ? milk
In the context of grocery-store purchases, the rule says that customers who buy
bread also tend to buy milk with a high probability. An association rule must
have an associated population: The population consists of a set of instances.
In the grocery-store example, the population may consist of all grocery-store
purchases;eachpurchaseisaninstance.Inthecaseofabookstore,thepopulation
may consist of all people who made purchases, regardless of when they made a
purchase.Eachcustomerisaninstance.Inthebookstoreexample,theanalysthas
20.5 AssociationRules 905
decidedthatwhenapurchaseismadeisnotsigni?cant,whereasforthegrocery-
storeexample,theanalystmayhavedecidedtoconcentrateonsinglepurchases,
ignoringmultiplevisitsbythe samecustomer.
Rules have an associated support,aswellasanassociatedcon?dence.These
arede?nedinthe context ofthe population:
• Support is a measure of what fraction of the population satis?es both the
antecedentandthe consequentof the rule.
Forinstance,supposeonly0.001percentofallpurchasesincludemilkand
screwdrivers.Thesupportfor the rule:
milk ? screwdrivers
is low. The rule may not even be statistically signi?cant—perhaps there was
onlyasinglepurchasethatincludedbothmilkandscrewdrivers.Businesses
are usually not interested in rules that have low support, since they involve
fewcustomers,and arenot worthbothering about.
Ontheotherhand,if50percentofallpurchasesinvolvemilkandbread,
then support for rules involving bread and milk (and no other item) is rela-
tively high, and such rules may be worth attention. Exactly what minimum
degreeofsupportisconsidereddesirabledependsonthe application.
• Con?dence is a measure of how often the consequent is true when the an-
tecedentistrue.For instance, therule:
bread ? milk
has a con?dence of 80 percent if 80 percent of the purchases that include
bread also include milk. A rule with a low con?dence is not meaningful. In
business applications, rules usually have con?dences signi?cantly less than
100 percent, whereas in other domains, such as in physics, rules may have
highcon?dences.
Note that the con?dence of bread ? milk may be very different from the
con?dence of milk ? bread,althoughboth havethe samesupport.
Todiscoverassociationrulesofthe form:
i
1
,i
2
,...,i
n
? i
0
we ?rst ?nd sets of items with suf?cient support, called large itemsets.Inour
example,we ?nd setsofitemsthat areincludedinasuf?cientlylargenumber of
instances. Weshall seeshortlyhowtocomputelargeitemsets.
For each large itemset, we then output all rules with suf?cient con?dence
that involve all and only the elements of the set. For each large itemset S,we
outputarule S ?s ? s foreverysubset s ? S,providedS ?s ? s hassuf?cient
con?dence;thecon?denceoftheruleisgivenbysupportofs dividedbysupport
of S.
906 Chapter20 DataWarehousingandMining
Wenowconsiderhowtogeneratealllargeitemsets.Ifthenumberofpossible
sets of items is small, a single pass over the data suf?ces to detect the level of
support for all the sets. A count, initialized to 0, is maintained for each set of
items.Whenapurchaserecordisfetched,thecountisincrementedforeachsetof
items such that all items in the set are contained in the purchase. For instance, if
apurchaseincludeditemsa, b,andc, counts would be incremented for {a}, {b},
{c}, {a,b}, {b,c}, {a,c},and{a,b,c}. Those sets with a suf?ciently high count at
the endofthe passcorrespondtoitemsthat havea highdegreeof association.
Thenumberofsetsgrowsexponentially,makingtheprocedurejustdescribed
infeasible if the number of items is large. Luckily, almost all the sets would
normallyhaveverylowsupport;optimizationshavebeendevelopedtoeliminate
most such sets from consideration. These techniques use multiple passes on the
database,consideringonlysome setsineachpass.
In theaprioritechnique for generating large itemsets, only sets with single
itemsare consideredin the ?rst pass. In the second pass, sets with two itemsare
considered,and soon.
At the end of a pass, all sets with suf?cient support are output as large
itemsets.Setsfoundtohavetoolittlesupportattheendofapassareeliminated.
Once a set is eliminated, none of its supersets needs to be considered. In other
words, in pass i we need to count only supports for sets of size i such that all
subsets of the set have been found to have suf?ciently high support; it suf?ces
to test all subsets of size i ?1 to ensure this property. At the end of some pass i,
we would ?nd that no set of size i has suf?cient support, so we do not need to
considerany setofsize i +1.Computationthenterminates.
20.6 Other Types of Associations
Using plain association rules has several shortcomings. One of the major short-
comings is that many associations are not very interesting, since they can be
predicted. For instance, if many people buy cereal and many people buy bread,
wecanpredictthatafairlylargenumberofpeoplewouldbuyboth,evenifthere
is no connection between the two purchases. In fact, even if buying cereal has a
mildnegativein?uenceonbuyingbread(thatis,customerswhobuycerealtend
topurchasebreadlessoftenthantheaveragecustomer),theassociationbetween
cerealand breadmaystillhaveahighsupport.
What would be more interesting is if there is a deviation from the expected
co-occurrence of the two. In statistical terms, we look for correlations between
items;correlationscanbepositive,inthattheco-occurrenceishigherthanwould
have been expected, or negative, in that the items co-occur less frequently than
predicted. Thus, if purchase of bread is not correlated with cereal, it would not
be reported, even if there was a strong association between the two. There are
standard measures of correlation, widely used in the area of statistics. See a
standardtextbook onstatisticsformoreinformation about correlations.
Anotherimportantclassofdata-miningapplicationsissequenceassociations
(or sequence correlations). Time-series data, such as stock prices on a sequence
20.7 Clustering 907
of days, form an example of sequence data. Stock-market analysts want to ?nd
associationsamongstock-marketpricesequences.Anexampleofsuchanassoci-
ationisthefollowingrule:“Wheneverbondratesgoup,thestockpricesgodown
within 2 days.” Discovering such associations between sequences can help us to
makeintelligentinvestmentdecisions.Seethebibliographicalnotesforreferences
toresearchonthistopic.
Deviations from temporal patterns are often interesting. For instance, if a
companyhasbeengrowingatasteadyrateeachyear,adeviationfromtheusual
growth rate is surprising. If sales of winter clothes go down in summer, it is
not surprising,since we can predictitfrompast years;adeviationthat we could
nothavepredictedfrompastexperiencewouldbeconsideredinteresting.Mining
techniquescan?nddeviationsfromwhatonewouldhaveexpectedonthebasisof
pasttemporal or sequentialpatterns. Seethe bibliographical notes forreferences
toresearchonthistopic.
20.7 Clustering
Intuitively, clustering refers to the problem of ?nding clusters of points in the
given data. The problem of clustering can be formalized from distance metrics
in several ways. One way is to phrase it as the problem of grouping points into
k sets (for a given k) so that the average distance of points from the centroid of
their assigned cluster is minimized.
2
Another way is to group points so that the
averagedistancebetweeneverypairofpointsineachclusterisminimized.There
areotherde?nitionstoo;seethebibliographicalnotesfordetails.Buttheintuition
behind allthesede?nitionsisto groupsimilarpoints togetherina singleset.
Anothertypeofclusteringappearsinclassi?cationsystemsinbiology.(Such
classi?cation systems do not attempt to predict classes; rather they attempt to
cluster related items together.) For instance, leopards and humans are clustered
under the class mammalia, while crocodiles and snakes are clustered under rep-
tilia. Both mammalia and reptilia come under the common class chordata. The
clustering of mammaliahas further subclusters, such as carnivora and primates.
We thus have hierarchical clustering. Given characteristics of different species,
biologistshavecreatedacomplexhierarchicalclusteringschemegroupingrelated
speciestogetheratdifferentlevelsofthehierarchy.
Hierarchical clustering is also useful in other domains—for clustering doc-
uments, for example. Internet directory systems (such as the Yahoo! directory)
clusterrelateddocumentsinahierarchicalfashion(seeSection21.9).Hierarchical
clustering algorithms can be classi?ed as agglomerative clustering algorithms,
which start by building small clusters and then create higher levels, or divisive
2
Thecentroidofasetofpointsisde?nedasapointwhosecoordinateoneachdimensionistheaverageofthecoordinates
of all the points of that set on that dimension. For example in two dimensions, the centroid of a set of points { (x
1
, y
1
),
(x
2
, y
2
),...,( x
n
, y
n
)} isgivenby
    n
i=1
x
i
n
,
  n
i=1
y
i
n
  .
908 Chapter20 DataWarehousingandMining
clustering algorithms, which ?rst create higher levelsof the hierarchical cluster-
ing,thenre?ne eachresultingclusterintolower-levelclusters.
The statistics community has studied clustering extensively. Database re-
search has provided scalable clustering algorithms that can cluster very large
datasets(thatmaynot?tinmemory).TheBirchclusteringalgorithmisonesuch
algorithm.Intuitively,datapointsareinsertedintoamultidimensionaltreestruc-
ture (based on R-trees, described in Section 25.3.5.3), and guided to appropriate
leaf nodes on the basis of nearness to representativepoints in the internal nodes
of the tree. Nearby points are thus clustered together in leaf nodes, and summa-
rized if there are more points than ?t in memory. The result of this ?rst phase of
clustering is to create a partially clustered data set that ?ts in memory. Standard
clusteringtechniquescanthenbeexecutedonthein-memorydatatogetthe?nal
clustering.SeethebibliographicalnotesforreferencestotheBirchalgorithm,and
other techniquesfor clustering,includingalgorithmsfor hierarchical clustering.
An interesting application of clustering is to predict what new movies (or
booksor music)apersonislikelytobeinterestedin,onthe basisof:
1. The person’spastpreferencesinmovies.
2. Other peoplewithsimilarpastpreferences.
3. The preferencesofsuch peoplefornewmovies.
One approach to this problem is as follows: To ?nd people with similar past
preferences we create clusters of people based on their preferences for movies.
The accuracy of clustering can be improved by previously clustering movies by
theirsimilarity,soevenifpeoplehavenotseenthesamemovies,iftheyhaveseen
similar movies they would be clustered together. We can repeat the clustering,
alternatelyclusteringpeople,thenmovies,thenpeople,andsoonuntilwereach
an equilibrium. Given a new user, we ?nd a cluster of users most similar to
that user, on the basis of the user’s preferences for movies already seen. We
then predict movies in movie clusters that are popular with that user’s cluster
as likely to be interesting to the new user. In fact, this problem is an instance of
collaborative ?ltering, where users collaborate in the task of ?ltering information
to ?nd informationof interest.
20.8 Other Forms of Data Mining
Textmining applies data-mining techniques to textual documents. For instance,
therearetoolsthatformclustersonpagesthatauserhasvisited;thishelpsusers
when they browse the history of their browsing to ?nd pages they have visited
earlier.Thedistancebetweenpagescanbebased,forinstance,oncommonwords
in the pages (see Section 21.2.2). Another application is to classify pages into a
Web directory automatically, according to their similarity with other pages (see
Section21.9).
20.9 Summary 909
Data-visualizationsystemshelpuserstoexaminelargevolumesofdata,and
to detect patterns visually. Visual displays of data—such as maps, charts, and
other graphical representations—allow data to be presented compactly to users.
Asinglegraphicalscreencanencodeasmuchinformationasafarlargernumber
of text screens. For example, if the user wants to ?nd out whether production
problems at plants are correlated to the locations of the plants, the problem
locations can be encoded in a special color—say, red—on a map. The user can
thenquicklydiscoverlocationswhereproblemsareoccurring.Theusermaythen
form hypotheses about why problems are occurring in those locations, and may
verifythe hypothesesquantitativelyagainstthe database.
Asanotherexample,informationaboutvaluescanbeencodedasacolor,and
can be displayed with as little as one pixel of screen area. To detect associations
between pairs of items, we can use a two-dimensional pixel matrix, with each
row and each column representing an item. The percentage of transactions that
buybothitemscanbeencodedbythecolorintensityofthepixel.Itemswithhigh
associationwillshowupasbrightpixelsinthescreen—easytodetectagainstthe
darkerbackground.
Data-visualization systems do not automatically detect patterns, but they
providesystemsupportforuserstodetectpatterns.Sincehumansareverygood
atdetectingvisualpatterns,datavisualizationisanimportantcomponentofdata
mining.
20.9 Summary
• Decision-support systems analyze online data collected by transaction-pro-
cessingsystems, tohelp peoplemake business decisions. Since most organi-
zationsareextensivelycomputerizedtoday,averylargebodyofinformation
is available for decision support. Decision-support systems come in various
forms,including OLAP systemsand data-miningsystems.
• Datawarehouseshelpgatherandarchiveimportantoperationaldata.Ware-
houses are used for decision support and analysis on historical data, for
instance, to predict trends. Data cleansing from input data sources is often a
majortaskindatawarehousing.Warehouseschemastendtobemultidimen-
sional,involvingoneorafewverylargefacttablesandseveralmuchsmaller
dimensiontables.
• Column-oriented storage systems providegood performance for many data
warehousing applications.
• Data mining is the process of semiautomatically analyzing large databases
to ?nd useful patterns. There are a number of applications of data mining,
such as prediction of values based on past examples, ?nding of associations
betweenpurchases,and automaticclusteringofpeopleand movies.
• Classi?cation deals with predicting the class of test instances by using at-
tributes of the test instances, based on attributes of training instances, and
910 Chapter20 DataWarehousingandMining
the actual class of training instances. There are several types of classi?ers,
suchas:
?
Decision-tree classi?ers, which perform classi?cation by constructing a
tree based on training instances with leaves having class labels. The tree
is traversedfor eachtest instance to?nd a leaf,and the class of the leafis
the predictedclass. Severaltechniquesare availabletoconstruct decision
trees,mostofthembasedongreedyheuristics.
?
Bayesian classi?ers are simpler to construct than decision-tree classi?ers,
and theywork betterinthecase of missing/nullattributevalues.
?
The Support Vector Machine is another widely used classi?cation tech-
nique.
• Association rules identify items that co-occur frequently, for instance, items
thattendtobeboughtbythesamecustomer.Correlationslookfordeviations
fromexpectedlevelsofassociation.
• Other types of data mining include clustering, text mining, and data visual-
ization.
Review Terms
• Decision-supportsystems
• Statisticalanalysis
• Data warehousing
?
Gathering data
?
Source-drivenarchitecture
?
Destination-drivenarchitec-
ture
?
Datacleansing
null Merge–purge
null Householding
?
Extract,transform, load
(ETL)
• Warehouse schemas
?
Fact table
?
Dimensiontables
?
Starschema
• Column-orientedstorage
• Data mining
• Prediction
• Associations
• Classi?cation
?
Training data
?
Test data
• Decision-treeclassi?ers
?
Partitioning attribute
?
Partitioning condition
?
Purity
null Gini measure
null Entropymeasure
?
Informationgain
?
Informationcontent
?
Informationgainratio
?
Continuous-valuedattribute
PracticeExercises 911
?
Categoricalattribute
?
Binarysplit
?
Multiwaysplit
?
Over?tting
• Bayesianclassi?ers
?
Bayes’theorem
?
NaiveBayesianclassi?ers
• SupportVectorMachine (SVM)
• Regression
?
Linearregression
?
Curve?tting
• Validation
?
Accuracy
?
Recall
?
Precision
?
Speci?city
?
Crossvalidation
• Associationrules
?
Population
?
Support
?
Con?dence
?
Largeitemsets
• Other typesof associations
• Clustering
?
Hierarchicalclustering
?
Agglomerativeclustering
?
Divisiveclustering
• Textmining
• Datavisualization
Practice Exercises
20.1 Describebene?tsanddrawbacksofasource-drivenarchitectureforgath-
ering of data at a data warehouse, as compared to a destination-driven
architecture.
20.2 Why is column-oriented storage potentially advantageous in a database
systemthatsupportsadatawarehouse?
20.3 Suppose that there are two classi?cation rules, one that says that people
withsalariesbetween$10,000and$20,000haveacreditratingofgood,and
another that says that people with salaries between $20,000 and $30,000
have a credit rating of good. Under what conditions can the rules be re-
placed,without any loss of information, by a single rulethat says people
withsalariesbetween$10,000and $30,000haveacreditratingof good?
20.4 ConsidertheschemadepictedinFigure20.2.GiveanSQLquerytosumma-
rizesalesnumbersandpricebystoreanddate,alongwiththehierarchies
onstoreand date.
20.5 Consider a classi?cation problem where the classi?er predicts whether a
personhasaparticulardisease.Supposethat95%ofthepeopletesteddo
912 Chapter20 DataWarehousingandMining
notsufferfromthedisease.(Thatis,poscorrespondsto5%andnegto95%
ofthetestcases.)Considerthefollowingclassi?ers:
• Classi?erC
1
whichalwayspredictsnegative(aratheruselessclassi?er
of course).
• Classi?er C
2
which predicts positive in 80% of the cases where the
personactuallyhasthedisease,butalsopredictspositivein5%ofthe
caseswhere thepersondoesnothavethe disease.
• Classi?er C
3
which predicts positive in 95% of the cases where the
person actually has the disease, but also predicts positive in 20% of
thecaseswherethepersondoesnot havethedisease.
Giventheabove classi?ers,answerthe following questions.
a. For each of the above classi?ers, compute the accuracy, precision,
recalland speci?city.
b. If you intend to use the results of classi?cation to perform further
screening for the disease, how would you choose between the clas-
si?ers.
c. Ontheotherhand,ifyouintendtousetheresultofclassi?cationto
start medication, where the medication could have harmful effects
ifgiventosomeonewhodoesnothavethedisease,howwouldyou
choose betweenthe classi?ers?
Exercises
20.6 Draw a diagram that shows how the classroom relation of our university
example as shown in Appendix A would be stored under a column-
orientedstoragestructure.
20.7 Explain why the nested-loops join algorithm (see Section 12.5.1) would
work poorly on database stored in a column-oriented manner. Describe
an alternative algorithm that would work better and explain why your
solutionisbetter.
20.8 Construct a decision-tree classi?er with binary splits at each node, using
tuples in relation r(A, B,C) shown below as training data; attribute C
denotes the class. Show the ?nal tree, and with each node show the best
splitforeach attributealong withitsinformationgainvalue.
(1, 2,a), (2,1,a), (2,5, b),(3, 3,b), (3,6,b),
(4, 5,b), (5,5, c),(6, 3,b), (6,7,c)
20.9 Suppose half of all the transactions in a clothes shop purchase jeans, and
one third of all transactions in the shop purchase T-shirts. Suppose also
Tools 913
that half of the transactions that purchase jeans also purchase T-shirts.
Writedownallthe(nontrivial)associationrulesyoucandeducefromthe
aboveinformation, givingsupportand con?dence of eachrule.
20.10 Considerthe problemof?nding largeitemsets.
a. Describe how to ?nd the support for a given collection of itemsets
by using a single scan of the data. Assume that the itemsets and
associatedinformation, suchas counts, will?t inmemory.
b. Suppose an itemset has support less than j. Show that no superset
ofthisitemsetcan havesupportgreaterthan orequalto j.
20.11 Create a small example of a set of transactions showing that although
many transactions contain two items, that is, the itemset containing the
two items has a high support, purchase of one of the items may have a
negativecorrelationwithpurchaseof the other.
20.12 Theorganizationofparts,chapters,sections,andsubsectionsinabookis
relatedtoclustering.Explainwhy,and towhat form ofclustering.
20.13 Suggesthow predictivemining techniquescan be used by a sportsteam,
usingyourfavoritesportasanexample.
Tools
A variety of tools are available for each of the applications we have studied in
thischapter. Mostdatabasevendorsprovide OLAPtoolsaspartoftheirdatabase
systems, or as add-on applications. These include OLAP tools from Microsoft
Corp., SAP, IBMandOracle.TheMondrianOLAP serverisapublic-domain OLAP
server.Manycompaniesalsoprovideanalysistoolsforspeci?capplications,such
as customerrelationshipmanagement.
Major database vendors also offer data warehousing products coupled with
their database systems. These provide support functionality for data modeling,
cleansing, loading, and querying. The Web site www.dwinfocenter.org provides in-
formationondata-warehousing products.
There is also a wide variety of general-purpose data-mining tools, including
data-miningsuitesfromtheSASInstitute,IBMIntelligentMiner,andOracle.There
are also several open-source data-mining tools, such as the widely used Weka,
andRapidMiner.Theopen-sourcebusinessintelligencesuitePentahohasseveral
components including an ETL tool, the Mondrian OLAP server, and data-mining
toolsbased onWeka.
A good deal of expertise is required to apply general-purpose mining tools
for speci?c applications. As a result, a large number of mining tools have been
developed to address specialized applications. The Web site www.kdnuggets.com
provides an extensive directory of mining software, solutions, publications, and
so on.
914 Chapter20 DataWarehousingandMining
Bibliographical Notes
De?nitions of statistical functions can be found in standard statistics textbooks
such asBulmer[1979] and Ross [1999].
Poe [1995] and Mattison [1996] provide textbook coverage of data ware-
housing. Zhuge et al. [1995] describes view maintenance in a data-warehousing
environment.Chaudhurietal.[2003]describestechniquesforfuzzymatchingfor
data cleaning, while Sarawagi et al. [2002] describes a system for deduplication
using activelearningtechniques.
Abadi et al. [2008] presents a comparison of column-oriented and row-
orientedstorage,includingissuesrelatedtoqueryprocessingandoptimization.
Witten and Frank [1999] and Han and Kamber [2000] provide textbook cov-
erage of data mining. Mitchell [1997] is a classic textbook on machine learning,
and covers classi?cation techniques in detail. Fayyad et al. [1995] presents an
extensivecollectionofarticlesonknowledgediscoveryanddatamining.Kohavi
andProvost[2001]presentsacollectionofarticlesonapplicationsofdatamining
toelectroniccommerce.
Agrawaletal.[1993b]providesanearlyoverviewofdataminingindatabases.
Algorithms for computing classi?ers with large training sets are described by
Agrawal etal.[1992] and Shafer etal.[1996]; the decision-treeconstruction algo-
rithm described in this chapter is based on the SPRINT algorithm of Shafer et al.
[1996].CortesandVapnik[1995]introducedseveralkeyresultsonSupportVector
Machines,whileCristianiniandShawe-Taylor[2000]providestextbookcoverage
of SupportVectorMachines.
Agrawal et al. [1993a] introduced the notion of association rules, and an
ef?cient algorithm for association rule mining was presented by Agrawal and
Srikant [1994]. Algorithms for mining of different forms of association rules are
described by Srikant and Agrawal [1996a] and Srikant and Agrawal [1996b].
Chakrabarti et al. [1998] describes techniques for mining surprising temporal
patterns. Techniques for integrating data cubes with data mining are described
by Sarawagi [2000].
Clusteringhaslongbeenstudiedintheareaofstatistics,andJainandDubes
[1988]providestextbookcoverageofclustering.NgandHan[1994]describesspa-
tial clustering techniques. Clustering techniques for large datasets are described
by Zhang et al. [1996]. Breese et al. [1998] provides an empirical analysis of dif-
ferentalgorithmsforcollaborative?ltering.Techniquesforcollaborative?ltering
of news articlesaredescribedby Konstanetal.[1997].
Chakrabarti[2002]andManningetal.[2008]providetextbookdescriptionof
information retrieval, including extensive coverage of data-mining tasks related
to textual and hypertext data, such as classi?cation and clustering. Chakrabarti
[2000] provides a survey of hypertext mining techniques such as hypertext clas-
si?cation andclustering.
CHAPTER
21
Information Retrieval
Textual data is unstructured, unlike the rigidly structured data in relational
databases. The term information retrieval generally refers to the querying of
unstructured textual data. Information-retrieval systems have much in common
with database systems, in particular, the storage and retrieval of data on sec-
ondary storage. However, the emphasis in the ?eld of information systems is
differentfromthatindatabasesystems,concentratingonissuessuchasquerying
based on keywords; the relevance of documents to the query; and the analysis,
classi?cation, and indexing of documents. Web search engines today go beyond
the paradigm of retrievingdocuments, and address broader issues such as what
informationtodisplayinresponsetoakeywordquery,tosatisfytheinformation
needsofauser.
21.1 Overview
The ?eld of information retrieval has developed in parallel with the ?eld of
databases. In the traditional model used in the ?eld of information retrieval,
information is organized into documents, and it is assumed that there is a large
number of documents. Data contained in documents are unstructured, without
any associated schema. The process of information retrieval consists of locating
relevant documents, on the basis of user input, such as keywords or example
documents.
TheWebprovidesaconvenientwaytogetto,andtointeractwith,information
sources across the Internet. However, a persistent problem facing the Web is the
explosion of stored information, with little guidance to help the user to locate
what is interesting. Information retrievalhas played a critical role in making the
Weba productiveandusefultool,especiallyforresearchers.
Traditionalexamplesofinformation-retrievalsystemsareonlinelibrarycata-
logsandonlinedocument-managementsystemssuchasthosethatstorenewspa-
per articles. The data in such systems are organized as a collection of documents;
a newspaper article and a catalog entry (in a library catalog) are examples of
documents. In the context of the Web, usually each HTML page is considered to
be adocument.
915
916 Chapter21 InformationRetrieval
A user of such a system may want to retrieve a particular document or a
particularclassofdocuments.Theintendeddocumentsaretypicallydescribedby
asetofkeywords—for example, the keywords “database system” may be used
to locate books on database systems, and the keywords “stock” and “scandal”
maybeusedtolocatearticlesaboutst ock-market scandals. Documents have
associatedwiththemasetofkeywords,anddocumentswhosekeywordscontain
those suppliedbythe userareretrieved.
Keyword-based information retrieval can be used not only for retrieving
textual data, but also for retrieving other types of data, such as video and audio
data,thathavedescriptivekeywordsassociatedwiththem.Forinstance,avideo
movie may have associated with it keywords such as its title, director, actors,
and genre, while an image or video clip may have tags, which are keywords
describingthe imageorvideoclip,associatedwithit.
There are several differences between this model and the models used in
traditionaldatabasesystems.
• Databasesystemsdealwithseveraloperationsthatarenotaddressedininfor-
mation-retrieval systems. For instance, database systems deal with updates
and with the associated transactional requirements of concurrency control
and durability. These matters are viewed as less important in information
systems. Similarly, database systems deal with structured information or-
ganized with relatively complex data models (such as the relational model
or object-oriented data models), whereas information-retrieval systems tra-
ditionally have used a much simpler model, where the information in the
databaseis organizedsimplyas a collectionof unstructureddocuments.
• Information-retrieval systems deal with several issues that have not been
addressedadequatelyindatabasesystems.Forinstance,the?eldofinforma-
tionretrievalhasdealtwiththeissueofqueryingcollectionsofunstructured
documents, focusing on issues such as keyword queries, and of ranking of
documentson estimateddegreeofrelevanceofthe documentsto thequery.
Inadditiontosimplekeywordqueriesthatarejustsetsofwords,information-
retrieval systems typically allow query expressions formed using keywords and
the logicalconnectivesand,or,andnot.Forexample,ausercouldaskforalldoc-
uments that contain the keywords “motorcycle and maintenance,” or documents
that contain the keywords “computer or microprocessor,” or even documents
that contain the keyword “computer but not database.” A query containing key-
words without any of the above connectives is assumed to have ands implicitly
connecting the keywords.
In full text retrieval, all the words in each document are considered to be
keywords.For unstructureddocuments, full textretrievalis essential since there
may be no information about what words in the document are keywords. We
shall use the wordterm to refer to the words in a document, since all words are
keywords.
21.2 RelevanceRankingUsingTerms 917
In its simplest form, an information-retrieval system locates and returns all
documentsthatcontainallthekeywordsinthequery,ifthequeryhasnoconnec-
tives;connectives arehandled as you would expect.More-sophisticatedsystems
estimaterelevanceof documents toa queryso that the documents can beshown
inorderofestimatedrelevance.Theyuseinformationabouttermoccurrences,as
wellas hyperlinkinformation, toestimaterelevance.
Information-retrieval systems, as exempli?ed by Web search engines, have
today evolved beyond just retrieving documents based on a ranking scheme.
Today,searchenginesaimtosatisfyauser’sinformationneeds,byjudgingwhat
topicaqueryisabout,anddisplayingnotonlyWebpagesjudgedasrelevant,but
also displaying other kinds of information about the topic. For example, given a
queryterm “cricket”, a search engine may displayscores from ongoing or recent
cricketmatches,ratherthanjustdisplaytop-rankeddocumentsrelatedtocricket.
As another example, in response to a query “New York”, a search engine may
show a map of New York, and images of New York, in addition to Web pages
related to New York.
21.2 RelevanceRankingUsingTerms
The set of all documents that satisfy a query expression may be very large; in
particular,therearebillionsofdocumentsontheWeb,andmostkeywordqueries
on a Web search engine ?nd hundreds of thousands of documents containing
the keywords.Fulltextretrievalmakesthis problem worse:each document may
contain many terms, and even terms that are mentioned only in passing are
treatedequivalentlywithdocumentswherethetermisindeedrelevant.Irrelevant
documents maybe retrievedas aresult.
Information-retrievalsystemsthereforeestimaterelevanceofdocumentstoa
query, and return only highly ranked documents as answers. Relevance ranking
isnot an exactscience,but therearesomewell-acceptedapproaches.
21.2.1 RankingUsingTF-IDF
The?rstquestiontoaddressis,givenaparticulartermt,howrelevantisaparticu-
lardocumentd totheterm.Oneapproachistousethethenumberofoccurrences
of the term in the document as a measure of its relevance, on the assumption
that relevant terms are likely to be mentioned many times in a document. Just
countingthenumberofoccurrencesofatermisusuallynotagoodindicator:?rst,
the number of occurrences depends on the length of the document, and second,
a document containing 10 occurrences of a term may not be 10 times as relevant
as adocument containing one occurrence.
OnewayofmeasuringTF(d,t),therelevanceofadocumentd toatermt,is:
TF(d,t) = log
  1 +
n(d,t)
n(d)
 
918 Chapter21 InformationRetrieval
where n(d) denotes the number of terms in the document and n(d,t) denotes
the number of occurrences of term t in the document d. Observe that this metric
takes the length of the document into account. The relevance grows with more
occurrencesofaterminthedocument,althoughitisnotdirectlyproportionalto
the numberof occurrences.
Many systems re?ne the above metric by using other information. For in-
stance,ifthetermoccursinthetitle,ortheauthorlist,ortheabstract,thedocument
would be considered more relevant to the term. Similarly, if the ?rst occurrence
of a term is late in the document, the document may be considered less relevant
than if the ?rst occurrence is early in the document. The above notions can be
formalized by extensions of the formula we have shown for TF(d,t). In the in-
formationretrievalcommunity,therelevanceofadocumenttoatermisreferred
to astermfrequency(TF),regardlessof theexact formulaused.
AqueryQmaycontainmultiplekeywords.Therelevanceofadocumenttoa
querywithtwoormorekeywordsisestimatedbycombiningtherelevancemea-
suresofthedocumenttoeachkeyword.Asimplewayofcombiningthemeasures
is to add them up. However, not all terms used as keywords are equal. Suppose
a query uses two terms, one of which occurs frequently, such as “database”,
and another that is less frequent, such as “Silberschatz”. A document contain-
ing “Silberschatz” but not “database” should be ranked higher than a document
containing the term “database” but not “Silberschatz”.
To ?x the above problem, weights are assigned to terms using the inverse
documentfrequency(IDF), de?ned as:
IDF(t) =
1
n(t)
where n(t) denotes the number of documents (among those indexed by the sys-
tem)thatcontainthetermt.Therelevanceofadocumentd toasetofterms Qis
then de?ned as:
r(d,Q) =
  t?Q
TF(d,t) ?IDF(t)
This measure can be further re?ned if the user is permitted to specify weights
w(t)fortermsinthequery,inwhichcasetheuser-speci?edweightsarealsotaken
into account by multiplyingTF(t)byw(t) inthe aboveformula.
Theaboveapproachofusingtermfrequencyandinversedocumentfrequency
as ameasureof therelevanceofadocument iscalledthe TF–IDFapproach.
Almost all text documents (in English) contain words such as “and,”“ or,”
“a,” and so on, and hence these words are useless for querying purposes since
theirinversedocumentfrequencyisextremelylow.Information-retrievalsystems
de?neasetofwords,calledstopwords,containing100orsoofthemostcommon
words, and ignore these words when indexing a document. Such words are not
used as keywords, and are discarded if present in the keywords supplied by the
user.
21.2 RelevanceRankingUsingTerms 919
Another factor taken into account when a query contains multiple terms is
theproximityofthetermsinthedocument.Ifthetermsoccurclosetoeachother
in the document, the document would be ranked higher than if they occur far
apart.Theformulaforr(d,Q)canbemodi?edtotakeproximityofthetermsinto
account.
Given a query Q, the job of an information-retrieval system is to return doc-
uments in descending order of their relevance to Q. Since there may be a very
large number of documents that are relevant, information-retrieval systems typ-
ically return only the ?rst few documents with the highest degree of estimated
relevance,andpermituserstointeractivelyrequestfurtherdocuments.
21.2.2 Similarity-BasedRetrieval
Certain information-retrieval systems permit similarity-based retrieval. Here,
the user can give the system document A, and ask the system to retrieve docu-
ments that are “similar” to A. The similarity of a document to another may be
de?ned, for example, on the basis of common terms. One approach is to ?nd k
termsin AwithhighestvaluesofTF(A,t) ?IDF(t),andtousethesek termsasa
queryto?ndrelevanceofotherdocuments.Thetermsinthequeryarethemselves
weighted byTF(A,t) ?IDF(t).
Moregenerally,thesimilarityofdocumentsisde?nedbythecosinesimilarity
metric.Letthetermsoccurringineitherofthetwodocumentsbet
1
,t
2
,...,t
n
.Let
r(d,t) =TF(d,t)?IDF(t).Thenthecosinesimilaritymetricbetweendocuments
d and e isde?nedas:
  n
i=1
r(d,t
i
)r(e,t
i
)
    n
i=1
r(d,t
i
)
2
    n
i=1
r(e,t
i
)
2
Youcaneasilyverifythatthecosinesimilaritymetricofadocumentwithitselfis
1,while that betweentwo documents thatdo not shareany termsis0.
The name “cosine similarity” comes from the fact that the above formula
computes the cosine of the angle between two vectors, one representing each
document, de?ned as follows: Let there be n words overall across all the docu-
ments being considered. An n-dimensional space is de?ned, with each word as
oneofthedimensions.Adocumentd isrepresentedbyapointinthisspace,with
thevalueoftheithcoordinateofthepointbeingr(d,t
i
).Thevectorfordocument
d connectstheorigin(allcoordinates=0)tothepointrepresentingthedocument.
Themodelofdocumentsaspointsandvectorsinann-dimensionalspaceiscalled
thevectorspacemodel.
If the set of documents similar to a query document A is large, the system
may present the user a few of the similar documents, allow the user to choose
the most relevant few, and start a new search based on similarity to Aandto the
chosen documents. The resultant set of documents is likely to be what the user
intendedto ?nd.This ideaiscalledrelevancefeedback.
Relevance feedback can also be used to help users ?nd relevant documents
from a large set of documents matching the given query keywords. In such a
920 Chapter21 InformationRetrieval
situation,usersmaybeallowedtoidentifyoneorafewofthereturneddocuments
as relevant; the system then uses the identi?ed documents to ?nd other similar
ones.Theresultantsetofdocumentsislikelytobewhattheuserintendedto?nd.
An alternative to the relevance feedback approach is to require users to modify
the query by adding more keywords; relevance feedback can be easier to use, in
additionto givinga better?nal setof documentsas the answer.
Inordertoshowtheuserarepresentativesetofdocumentswhenthenumber
ofdocumentsisverylarge,asearchsystemmayclusterthedocuments,basedon
their cosine similarity. Then, a few documents from each cluster may be shown,
sothatmorethanoneclusterisrepresentedinthesetofanswers.Clusteringwas
described earlier in Section 20.7, and several techniques have been developedto
cluster sets of documents. See the bibliographical notes for references to more
informationon clustering.
As a special case of similarity, there are often multiple copies of a document
on the Web; this could happen, for example, if a Web site mirrors the contents
of another Web site. In this case, it makes no sense to return multiple copies of a
highlyrankeddocumentasseparateanswers;duplicatesshouldbedetected,and
only one copy shouldbe returnedas ananswer.
21.3 RelevanceUsingHyperlinks
Early Web-search engines ranked documents by using only TF–IDF based rele-
vance measures like those described in Section 21.2. However, these techniques
had some limitations when used on verylarge collections of documents, such as
the set of all Web pages. In particular, many Web pages have all the keywords
speci?ed in a typical search engine query; further, some of the pages that users
wantasanswersoftenhavejustafewoccurrencesofthequeryterms,andwould
notgetaveryhigh TF–IDFscore.
However, researchers soon realized that Web pages have very important
informationthatplaintextdocumentsdonothave,namelyhyperlinks.Thesecan
be exploited to get better relevance ranking; in particular, the relevance ranking
ofapageisin?uencedgreatlybyhyperlinksthatpointtothepage.Inthissection,
we studyhowhyperlinksareusedfor ranking of Webpages.
21.3.1 PopularityRanking
The basic idea of popularity ranking (also called prestige ranking) is to ?nd
pagesthatarepopular,andtorankthemhigherthanotherpagesthatcontainthe
speci?ed keywords. Since most searches are intended to ?nd information from
popular pages,ranking suchpageshigher is generallya good idea.For instance,
the term “google” may occur in vast numbers of pages, but the page google.com
is the most popular among the pages that contain the term “google.” The page
google.com should therefore be ranked as the most relevant answer to a query
consisting of the term “google”.
21.3 RelevanceUsingHyperlinks 921
TraditionalmeasuresofrelevanceofapagesuchastheTF–IDFbasedmeasures
that we saw in Section 21.2, can be combined with the popularity of the page to
get an overall measure of the relevance of the page to the query. Pages with the
highestoverallrelevancevaluearereturnedasthe topanswers toaquery.
This raises the questionof how to de?ne and how to ?nd the popularity of a
page. One way would be to ?nd how many timesa page is accessed and use the
numberasameasureofthesite’spopularity.However,gettingsuchinformation
is impossible without the cooperation of the site, and while a few sites may be
persuaded to reveal this information, it is dif?cult to get it for all sites. Further,
sitesmaylieabout theiraccess frequency,inorderto getrankedhigher.
A very effective alternative is to use hyperlinks to a page as a measure of
its popularity. Many people have bookmark ?les that contain links to sites that
they use frequently. Sites that appear in a large number of bookmark ?les can
be inferred to be very popular sites. Bookmark ?les are usually stored privately
and not accessible on the Web. However, many users do maintain Web pages
with links to their favorite Web pages. Many Web sites also have links to other
relatedsites, which can also be used to infer the popularity of the linked sites. A
Web search engine can fetch Web pages (by a process called crawling, which we
describeinSection21.7),and analyzethem to?nd links betweenthepages.
A?rstsolutiontoestimatingthepopularityofapageistousethenumberof
pages that link to the page as a measure of its popularity. However, this by itself
has the drawback that many sites have a number of useful pages, yet external
links often point only to the root page of the site. The root page in turn has links
to other pages in the site. These other pages would then be wrongly inferred to
be not verypopular,and wouldhave alowranking inanswering queries.
One alternative is to associate popularity with sites, rather than with pages.
Allpagesatasitethengetthepopularityofthesite,andpagesotherthantheroot
page of a popular site would also bene?t from the site’s popularity. However,
thequestionofwhatconstitutesasitethenarises.IngeneraltheInternetaddress
pre?xofapageURLwouldconstitutethesitecorrespondingtothepage.However,
there are many sites that host a large number of mostly unrelated pages, such as
home page servers in universities and Web portals such as groups.yahoo.com or
groups.google.com. For such sites, the popularity of one part of the site does not
implypopularityof another part of thesite.
A simpler alternative is to allow transfer of prestige from popular pages to
pages to which they link. Under this scheme, in contrast to the one-person one-
vote principles of democracy, a link from a popular page x to a page y is treated
as conferring moreprestigeto page ythan alink froma not-so-popular page z.
1
This notion of popularity is in fact circular, since the popularity of a page
is de?ned by the popularity of other pages, and there may be cycles of links
between pages. However, the popularity of pages can be de?ned by a system
of simultaneous linear equations, which can be solved by matrix manipulation
1
This is similar in some sense to giving extra weight to endorsements of products by celebrities (such as ?lm stars), so
itssigni?cance isopento question, althoughitis effectiveand widelyusedinpractice.
922 Chapter21 InformationRetrieval
techniques. The linear equations can be de?ned in such a way that they have a
uniqueand well-de?nedsolution.
It is interesting to note that the basic idea underlying popularity ranking is
actuallyquiteold,and?rstappearedinatheoryofsocialnetworkingdeveloped
by sociologists in the 1950s. In the social-networking context, the goal was to
de?netheprestigeofpeople.Forexample,thepresidentoftheUnitedStateshas
high prestigesince a largenumber of peopleknow him. If someone isknown by
multiple prestigious people, then she also has high prestige, even if she is not
known by as large a number of people. The use of a set of linear equations to
de?nethe popularitymeasurealso datesback tothis work.
21.3.2 PageRank
The Web search engine Google introduced PageRank,whichisameasureof
popularityofapagebasedonthepopularityofpagesthatlinktothepage.Using
the PageRank popularity measure to rank answers to a query gave results so
much better than previously used ranking techniques that Google became the
most widelyusedsearchengine,inarathershort periodoftime.
PageRank can be understood intuitively using a random walk model.Sup-
pose a person browsing the Web performs a random walk (traversal) on Web
pages as follows: the ?rst step starts at a random Web page, and in each step,
the random walker does one of the following. With a probability   the walker
jumpstoarandomlychosenWebpage,andwithaprobabilityof1?  thewalker
randomlychoosesoneoftheoutlinksfromthecurrentWebpageandfollowsthe
link. The PageRank of a page is then the probability that the random walker is
visitingthepageat anygivenpointintime.
Note that pages that are pointed to from many Web pages are more likely
to be visited, and thus will have a higher PageRank. Similarly, pages pointed to
by Web pages with a high PageRank will also have a higher probability of being
visited,and thus willhavea higher PageRank.
PageRank can be de?ned by a set of linear equations, as follows: First, Web
pagesaregivenintegeridenti?ers.ThejumpprobabilitymatrixT isde?nedwith
T[i, j] set to the probability that a random walker who is following a link out of
page i follows the link to page j. Assuming that each link from i has an equal
probability of beingfollowed T[i, j] = 1/N
i
,whereN
i
is thenumber of links out
of page i. Most entries of T are 0 and it is best represented as an adjacency list.
ThenthePageRank P[j]foreachpagej can be de?nedas:
P[j] =   /N +(1 ?  ) ?
N
  i=1
(T[i, j] ? P[i])
where   is a constant between 0 and 1, and N the number of pages;   represents
the probabilityof astepintherandom walk being ajump.
Thesetofequationsgeneratedasaboveareusuallysolvedbyananiterative
technique,startingwitheach P[i]setto1/N.Eachstepoftheiterationcomputes
newvaluesforeach P[i]usingtheP valuesfromthepreviousiteration.Iteration
21.3 RelevanceUsingHyperlinks 923
stops when the maximum change in any P[i] value in an iteration goes below
somecutoff value.
21.3.3 OtherMeasuresofPopularity
BasicmeasuresofpopularitysuchasPageRankplayanimportantroleinranking
ofqueryanswers,butarebynomeanstheonlyfactor.TheTF–IDFscoresofapage
are used to judge its relevance to the query keywords, and must be combined
with the popularity ranking. Other factors must also be taken into account, to
handlelimitationsof PageRank and relatedpopularitymeasures.
Information about how often a site is visited would be a useful measure of
popularity, but as mentioned earlier it is hard to obtain in general. However,
search engines do track what fraction of times users click on a page, when it
is returned as an answer. This fraction can be used as a measure of the site’s
popularity. To measure the click fraction, instead of providing a direct link to
thepage,thesearchengineprovidesanindirectlinkthroughthesearchengine’s
site, which records the page click, and transparently redirects the browser to the
originallink.
2
One drawback of the PageRank algorithm is that it assigns a measure of
popularitythatdoesnottakequerykeywordsintoaccount.Forexample,thepage
google.com is likely to have a very high PageRank because many sites contain a
linktoit.Supposeitcontainsawordmentionedinpassing,suchas“Stanford”(the
advancedsearchpageatGoogledidinfactcontainthiswordatonepointseveral
years ago). A search on the keyword Stanford would then return google.com as
thehighest-rankedanswer,aheadofamorerelevantanswersuchastheStanford
UniversityWebpage.
One widely used solution to this problem is to use keywords in the anchor
text of links to a page to judge what topics the page is highly relevant to. The
anchortextofalinkconsistsofthetextthatappearswithinthe HTMLa hreftag.
For example,the anchor textof thelink:
<a href="http://stanford.edu"> Stanford University</a>
is “Stanford University”. If many links to the page stanford.edu have the word
Stanford in their anchor text, the page can be judged to be very relevant to the
keyword Stanford. Text near the anchor text may also be taken into account; for
example,aWebsitemaycontainthetext“Stanford’shomepageishere”,butmay
haveusedonlytheword“here”asanchortextinthelinktotheStanfordWebsite.
Popularity based on anchor text is combined with other measures of popu-
larity,andwith TF–IDFmeasures,togetanoverallranking forqueryanswers,as
discussed in Section 21.3.5. As an implementation trick, the words in the anchor
2
Sometimes this indirection is hidden from the user. For example when you point the mouse at a link (such as db-
book.com) in a Google query result, the link appears to point directly to the site. However, at least as of mid 2009,
when you actually click on the link, Javascript code associated with the page actually rewrites the link to go indirectly
throughGoogle’ssite.Ifyouusethebackbuttonofthebrowsertogobacktothequeryresultpage,andpointtothelink
again, the change inthe linkedURL becomesvisible.
924 Chapter21 InformationRetrieval
text are often treated as part of the page, with a term frequency based on the
thepopularityofthepageswheretheanchor textappears.Then, TF–IDFranking
automatically takesanchor textintoaccount.
An alternative approach to taking keywords into account when de?ning
popularityistocomputeameasureofpopularityusingonlypagesthatcontainthe
querykeywords,insteadofcomputingpopularityusingallavailableWebpages.
Thisapproachismoreexpensive,sincethecomputationofpopularityrankinghas
tobedonedynamicallywhenaqueryisreceived,whereasPageRankiscomputed
statically once, and reused for all queries. Web search engines handling billions
of queries per day cannot afford to spend so much time answering a query. As a
result,althoughthisapproachcangivebetteranswers,itisnotverywidelyused.
The HITS algorithm was based on the above idea of ?rst ?nding pages that
containthequerykeywords,andthencomputingapopularitymeasureusingjust
thissetofrelatedpages.Inadditionitintroducedanotionofhubsandauthorities.
Ahubisapagethatstoreslinkstomanyrelatedpages;itmaynotinitselfcontain
actualinformationonatopic,butpointstopagesthatcontainactualinformation.
In contrast, an authority is a page that contains actual information on a topic,
although it may not store links to many related pages. Each page then gets a
prestige value as a hub (hub-prestige), and another prestige value as an authority
(authority-prestige).Thede?nitionsofprestige,asbefore,arecyclicandarede?ned
by a set of simultaneous linear equations. A page gets higher hub-prestige if it
points to many pages with high authority-prestige, while a page gets higher
authority-prestigeifitispointedtobymanypageswithhighhub-prestige.Given
aquery,pageswithhighestauthority-prestigearerankedhigherthanotherpages.
Seethe bibliographical notesfor referencesgivingfurtherdetails.
21.3.4 SearchEngineSpamming
Searchenginespamming refers to the practice of creating Web pages, or sets of
Webpages,designedtogetahigh relevancerank for some queries,eventhough
the sites are not actually popular sites. For example,a travel site may want to be
ranked high for queries with the keyword “travel”.ItcangethighTF–IDF scores
by repeating the word “travel” many times in its page.
3
Even a site unrelated
to travel, such as a pornographic site, could do the same thing, and would get
highly ranked for a query on the word travel. In fact, this sort of spamming of
TF–IDF was common in the early days of Web search, and there was a constant
battle between such sites and search engines that tried to detect spamming and
denythem ahigh ranking.
PopularityrankingschemessuchasPageRankmakethejobofsearchengine
spammingmoredif?cult,sincejustrepeatingwordstogetahighTF–IDFscorewas
nolongersuf?cient.However,eventhesetechniquescanbespammed,bycreating
a collection of Web pages that point to each other, increasing their popularity
3
Repeated words in a Web page may confuse users; spammers can tackle this problem by delivering different pages
to search engines than to other users, for the same URL, or by making the repeated words invisible, for example, by
formatting the wordsinsmall white fonton awhite background.
21.4 Synonyms,Homonyms,andOntologies 925
rank. Techniques such as using sites instead of pages as the unit of ranking
(withappropriatelynormalizedjumpprobabilities)havebeenproposedtoavoid
some spamming techniques, but are not fully effective against other spamming
techniques. The war between search engine spammers and the search engines
continueseventoday.
ThehubsandauthoritiesapproachoftheHITSalgorithmismoresusceptibleto
spamming.AspammercancreateaWebpagecontaininglinkstogoodauthorities
onatopic,andgainsahighhubscoreasaresult.Inadditionthespammer’sWeb
page includes links to pages that they wish to popularize, which may not have
any relevance to the topic. Because these linked pages are pointed to by a page
withhighhub score,theygeta highbut undeservedauthority score.
21.3.5 CombiningTF-IDFandPopularityRankingMeasures
We have seen two broad kinds of features used in ranking, namely TF–IDF and
popularityscoressuchasPageRank.TF–IDFitselfre?ectsacombinationofseveral
factors including raw term frequency and inverse document frequency, occur-
rence of a term in anchor text linking to the page, and a variety of other factors
such as occurrence of the term in the title, occurrence of the term early in the
document,and largerfont sizeforthe term,among other factors.
How to combine the scores of a page on each these factors, to generate an
overallpagescore,isamajorproblemthatmustbeaddressedbyanyinformation
retrieval system. In the early days of search engines, humans created functions
to combine scores into an overall score. But today, search engines use machine-
learningtechniquestodecidehowtocombinescores.Typically,ascorecombining
formulais?xed,buttheformulatakesasparametersweightsfordifferentscoring
factors. By using a training set of query results ranked by humans, a machine-
learning algorithm can come up with an assignment of weights for each scoring
factor that resultsinthe bestranking performanceacross multiplequeries.
Wenote that mostsearchenginesdonot revealhow theycomputerelevance
rankings; theybelievethatrevealingtheirranking techniqueswouldallowcom-
petitors to catch up, and would make the job of search engine spamming easier,
resultinginpoorerqualityof results.
21.4 Synonyms,Homonyms,andOntologies
Considerthe problem of locating documents about motorcycle maintenance, us-
ing the query “motorcycle maintenance”. Suppose that the keywords for each
documentare thewordsin thetitleand the namesof the authors. The document
titled Motorcycle Repair would not be retrieved, since the word “maintenance”
doesnot occur initstitle.
Wecansolvethatproblembymakinguseofsynonyms.Eachwordcanhave
asetofsynonymsde?ned,andtheoccurrenceofawordcanbereplacedbytheor
of all its synonyms (including the word itself). Thus, the query “motorcycle and
repair” can be replaced by “motorcycle and (repair or maintenance).” This query
would ?nd thedesireddocument.
926 Chapter21 InformationRetrieval
Keyword-basedqueriesalsosufferfromtheoppositeproblem,ofhomonyms,
that is single words with multiple meanings. For instance, the word “object” has
different meanings as a noun and as a verb. The word “table” may refer to a
dinnertable,or toa tableina relationaldatabase.
In fact, a danger even with using synonyms to extend queries is that the
synonyms may themselves have different meanings. For example, “allowance”
is a synonym for one meaning of the word “maintenance”, but has a different
meaning than what the user intended in the query “motorcycle maintenance”.
Documents that use the synonyms with an alternative intended meaning would
be retrieved.The user is then leftwondering why the system thought that a par-
ticularretrieveddocument(forexample,usingtheword“allowance”)isrelevant,
ifitcontains neitherthekeywordstheuserspeci?ed,nor wordswhoseintended
meaning in the document is synonymous with speci?ed keywords! It is there-
fore a bad idea to use synonyms to extend a query without ?rst verifying the
synonyms withtheuser.
Abetterapproachtotheaboveproblemisforthesystemtounderstandwhat
concept each word in a document represents, and similarly to understand what
conceptsauserislookingfor,andtoreturndocumentsthataddresstheconcepts
that the user is interested in. A system that supports concept-based querying
has to analyze each document to disambiguate each word in the document, and
replace it with the concept that it represents; disambiguation is usually done by
lookingatothersurroundingwordsinthedocument.Forexample,ifadocument
contains words such as database or query, the word table probably should be
replaced by the concept “table: data” whereas if the document contains words
such as furniture, chair, or wood near the word table, the word table should
be replaced by the concept “table: furniture.” Disambiguation based on nearby
words is usually harder for user queries, since queries contain very few words,
so concept-based query systems would offer several alternative concepts to the
user, who picks one or more before the search continues.
Concept-basedqueryinghasseveraladvantages;forexample,aqueryinone
language can retrieve documents in other languages, so long as they relate to
thesameconcept.Automatedtranslationmechanisms canbeusedsubsequently
if the user does not understand the language in which the document is writ-
ten. However, the overhead of processing documents to disambiguate words is
veryhighwhenbillionsofdocumentsarebeinghandled.Internetsearchengines
thereforegenerallydidnotsupportconcept-basedqueryinginitially,butinterest
inconcept-basedapproachesisgrowingrapidly.However,concept-basedquery-
ing systemshave beenbuilt andusedfor otherlargecollections ofdocuments.
Querying based on concepts can be extended further by exploiting concept
hierarchies. For example, suppose a person issues a query “?ying animals”;a
document containing information about “?ying mammals” is certainly relevant,
sinceamammalisananimal.However,thetwoconceptsarenotthesame,andjust
matching concepts would not allow the document to be returned as an answer.
Concept-based querying systems can support retrieval of documents based on
concept hierarchies.
21.5 IndexingofDocuments 927
Ontologiesarehierarchicalstructuresthatre?ectrelationshipsbetweencon-
cepts. The most common relationship is the is-a relationship; for example, a
leopard is-a mammal, and a mammal is-a animal. Other relationships, such as
part-of arealsopossible;forexample,anairplanewing is part-of anairplane.
TheWordNetsystemde?nesalargevarietyofconceptswithassociatedwords
(called a synset in WordNet terminology). The words associated with a synset
are synonyms for the concept; a word may of course be a synonym for several
different concepts. In addition to synonyms, WordNet de?nes homonyms and
other relationships. In particular, the is-a and part-of relationships that it de?nes
connect concepts, and in effect de?ne an ontology. The Cyc project is another
effortto createan ontology.
In addition to language-wide ontologies, ontologies have been de?ned for
speci?c areas to deal with terminology relevant to those areas. For example, on-
tologies have been created to standardize terms used in businesses; this is an
important step in building a standard infrastructure for handling order process-
ing and other interorganization ?ow of data. As another example, consider a
medical insurance company that needs to get reports from hospitals containing
diagnosis and treatment information. An ontology that standardizes the terms
helps hospital staff to understand the reports unambiguously. This can greatly
helpinanalysisofthereports,forexampletotrackhowmanycasesofaparticular
diseaseoccurredina particulartimeframe.
It is also possible to build ontologies that link multiple languages. For ex-
ample,WordNetshavebeenbuiltfordifferentlanguages,andcommonconcepts
between languages can be linked to each other. Such a system can be used for
translationoftext.Inthecontextofinformationretrieval,amultilingualontology
can be used to implement a concept-based search across documents in multiple
languages.
The largest effort in using ontologies for concept-based queries is the Se-
mantic Web. The Semantic Web is led by the World Wide Web Consortium and
consistsofacollectionoftools,standards,andlanguagesthatpermitdataonthe
Web to be connected based on their semantics, or meaning. Instead of being a
centralized repository, the Semantic Web is designed to permit the same kind of
decentralized,distributedgrowththathasmadetheWorldWideWebsosuccess-
ful.Keytothisisthecapabilitytointegratemultiple,distributedontologies.Asa
result,anyone withaccess to theInternetcan addtothe SemanticWeb.
21.5 IndexingofDocuments
An effective index structure is important for ef?cient processing of queries in
an information-retrieval system. Documents that contain a speci?ed keyword
can be located ef?ciently by using an inverted index that maps each keyword
K
i
to a list S
i
of (identi?ers of) the documents that contain K
i
. For example, if
documentsd
1
,d
9
and d
21
contain theterm “Silberschatz”, theinvertedlistfor the
keyword Silberschatz would be “d
1
;d
9
;d
21
”. To support relevance ranking based
on proximity of keywords, such an index may provide not just identi?ers of
928 Chapter21 InformationRetrieval
documents, but also a list of locations within the document where the keyword
appears. For example, if “Silberschatz” appeared at position 21 in d
1
, positions
1and19ind
2
,andpositions4 ,29 and 46 in d
3
, the inverted list with positions
would be “d
1
/21;d
9
/1,19;d
21
/4,29,46”. The inverted lists may also include with
eachdocument thetermfrequencyof theterm.
Such indices must be stored on disk, and each list S
i
can span multiple disk
pages. To minimize the number of I/O operations to retrieve each list S
i
,the
system would attempt to keep each list S
i
in a set of consecutive disk pages, so
theentirelistcanberetrievedwithjustonediskseek.AB
+
-treeindexcanbeused
to mapeachkeyword K
i
toitsassociatedinvertedlist S
i
.
The and operation ?nds documents that contain all of a speci?ed set of key-
words K
1
,K
2
,...,K
n
.Weimplementtheandoperationby?rstretrievingthesets
ofdocumentidenti?ers S
1
,S
2
,...,S
n
ofalldocumentsthatcontaintherespective
keywords. The intersection, S
1
? S
2
?···?S
n
, of the sets gives the document
identi?ers of the desired set of documents. The or operation gives the set of all
documents that contain at least one of the keywords K
1
,K
2
,...,K
n
.W eimple-
ment the or operation by computing the union, S
1
? S
2
?···?S
n
,ofthesets.
The not operation ?nds documents that do not contain a speci?ed keyword K
i
.
Given a set of document identi?ers S, we can eliminate documents that contain
the speci?ed keyword K
i
by taking the difference S ? S
i
,whereS
i
is the set of
identi?ersof documents that containthe keyword K
i
.
Given a set of keywords in a query, many information-retrieval systems do
not insist that the retrieved documents contain all the keywords (unless an and
operationisusedexplicitly).Inthiscase,alldocumentscontainingatleastoneof
thewordsareretrieved(asintheoroperation),butarerankedbytheirrelevance
measure.
To use term frequency for ranking, the index structure should additionally
maintainthenumberoftimestermsoccurineachdocument.Toreducethiseffort,
theymayuseacompressedrepresentationwithonlyafewbitsthatapproximates
the term frequency. The index should also store the document frequency of each
term(that is,thenumber of documentsinwhich the termappears).
If the popularity ranking is independent of the index term (as is the case for
PageRank),thelistS
i
canbesortedonthepopularityranking(andsecondarily,for
documents with the same popularity ranking, on document-id). Then, a simple
merge can be used to compute and and or operations. For the case of the and
operation,ifweignoretheTF–IDFcontributiontotherelevancescore,andmerely
requirethatthedocumentshouldcontainthegivenkeywords,mergingcanstop
once K answershavebeenobtained,iftheuserrequiresonlythetop K answers.
In general, the results with highest ?nal score (after including TF–IDF scores) are
likelytohavehighpopularityscores,andwouldappearnearthefrontofthelists.
Techniqueshavebeendevelopedtoestimatethebestpossiblescoresofremaining
results, and these can be used to recognize that answers not yet seen cannot be
partof the top K answers. Processing ofthe listscanthenterminateearly.
However, sorting on popularity score is not fully effective in avoiding long
inverted list scans, since it ignores the contribution of the TF–IDF scores. An
alternative in such cases is to break up the inverted list for each term into two
21.6 MeasuringRetrievalEffectiveness 929
parts. The ?rst part contains documents that have a high TF–IDF score for that
term(for example,documents wherethe termoccurs inthedocument title,or in
anchor text referencing the document). The second part contains all documents.
Each part of the list can be sorted in order of (popularity, document-id). Given
a query, merging the ?rst parts of the list for each term is likely to give several
answers with an overall high score. If suf?cient high-scoring answers are not
found using the ?rst parts of the lists, the second parts of the lists are used to
?ndallremaininganswers.Ifadocumentscoreshighon TF–IDF,itislikelytobe
found when merging the ?rst parts of the lists. See the bibliographical notes for
relatedreferences.
21.6 MeasuringRetrievalEffectiveness
Each keyword may be contained in a large number of documents; hence, a com-
pact representation is critical to keep space usage of the index low. Thus, the
sets of documents for a keyword are maintained in a compressed form. So that
storage space is saved, the index is sometimes stored such that the retrieval is
approximate;afewrelevantdocumentsmaynotberetrieved(calledafalsedrop
orfalsenegative),orafewirrelevantdocumentsmayberetrieved(calledafalse
positive). A good index structure will not have any false drops, but may permit
afewfalsepositives;thesystemcan?lterthemawaylaterby lookingatthekey-
wordsthattheyactuallycontain.InWebindexing,falsepositivesarenotdesirable
either,since theactual document may not bequicklyaccessible for?ltering.
Two metrics are used to measure how well an information-retrieval system
is able to answer queries. The ?rst, precision, measures what percentage of the
retrieved documents are actually relevant to the query. The second,recall,mea-
sures what percentage of the documents relevant to the query were retrieved.
Ideallybothshould be 100 percent.
Precisionandrecallarealsoimportantmeasuresforunderstandinghowwell
a particular document-ranking strategy performs. Ranking strategies can result
infalsenegativesand falsepositives,but ina moresubtlesense.
• False negatives may occur when documents are ranked, as a result of rele-
vantdocumentsreceivingalowranking.Ifthesystemfetchedalldocuments
down to those with very low ranking there would be very few false neg-
atives. However, humans would rarely look beyond the ?rst few tens of
returned documents, and may thus miss relevant documents because they
are not ranked high. Exactly what is a false negative depends on how many
documentsareexamined.Thereforeinsteadofhavingasinglenumberasthe
measure of recall, we can measure the recall as a function of the number of
documentsfetched.
• Falsepositivesmayoccurbecauseirrelevantdocumentsgethigherrankings
than relevant documents. This too depends on how many documents are
examined. One option is to measure precision as a function of number of
documentsfetched.
930 Chapter21 InformationRetrieval
Abetterandmoreintuitivealternativeformeasuringprecisionistomeasure
it as a function of recall. With this combined measure, both precision and recall
can becomputedas a function ofnumber of documents,if required.
For instance, we can say that with a recall of 50 percent the precision was 75
percent, whereas at a recall of 75 percent the precision dropped to 60 percent. In
general, we can draw a graph relating precisionto recall. These measurescan be
computedforindividualqueries,thenaveragedoutacross asuiteofqueriesina
querybenchmark.
Yet another problem with measuring precision and recall lies in how to de-
?ne which documents are really relevant and which are not. In fact, it requires
understandingofnaturallanguage,andunderstandingoftheintentofthequery,
to decide if a document is relevant or not. Researchers therefore have created
collections of documents and queries, and have manually tagged documents as
relevantorirrelevanttothequeries.Differentrankingsystemscanberunonthese
collections to measuretheiraverageprecisionand recallacross multiplequeries.
21.7 CrawlingandIndexingtheWeb
Web crawlers areprogramsthatlocateandgatherinformationontheWeb.They
recursively follow hyperlinks present in known documents to ?nd other docu-
ments.Crawlersstartfromaninitialsetof URLs,whichmaybecreatedmanually.
Each of the pages identi?ed by these URLs are fetched from the Web. The Web
crawlerthenlocatesall URLlinksinthesepages,andaddsthemtothesetof URLs
tobecrawled,iftheyhavenotalreadybeenfetched,oraddedtotheto-be-crawled
set. This process is again repeated by fetching all pages in the to-be-crawled set,
and processing the links in these pages in the same fashion. By repeating the
process, all pages that are reachable by any sequence of links from the initial set
of URLswouldbe eventuallyfetched.
Since the number of documents on the Web is very large, it is not possible
to crawl the whole Web in a short period of time; and in fact, all search engines
cover only some portions of the Web, not all of it, and their crawlers may take
weeks or months to perform a single crawl of all the pages they cover. There are
usually many processes,running on multiplemachines, involvedincrawling. A
databasestoresasetoflinks(orsites)tobecrawled;itassignslinksfromthissetto
eachcrawlerprocess.Newlinksfoundduringacrawlareaddedtothedatabase,
and may be crawled later if they are not crawled immediately. Pages have to be
refetched (that is, links recrawled) periodically to obtain updated information,
and to discard sites that no longer exist, so that the information in the search
indexiskeptreasonably up-to-date.
See the references in the bibliography for a number of practical details in
performing a Web crawl, such as in?nite sequences of links created by dynam-
ically generated pages (called a spider trap), prioritization of page fetches, and
ensuring that Websitesarenot ?oodedby aburst of requestsfroma crawler.
Pagesfetchedduringacrawlarehandedovertoaprestigecomputationand
indexing system, which may be running on a different machine. The prestige
21.8 InformationRetrieval:BeyondRankingofPages 931
computationandindexingsystemsthemselvesrunonmultiplemachinesinpar-
allel. Pages can be discarded after they are used for prestige computation and
added to the index; however, they are usually cached by the search engine, to
givesearchengineusersfastaccesstoacachedcopyofapage,eveniftheoriginal
Websitecontaining the pageisnot accessible.
It is not a good idea to add pages to the same index that is being used for
queries, since doing so would require concurrency control on the index, and
would affect query and update performance. Instead, one copy of the index is
usedtoanswerquerieswhileanothercopyisupdatedwithnewlycrawledpages.
Atperiodicintervalsthecopiesswitchover,withtheoldonebeingupdatedwhile
thenewcopy is beingusedfor queries.
To support very high query rates, the indices may be kept in main memory,
and there are multiple machines; the system selectively routes queries to the
machinestobalancetheloadamongthem.Popularsearchenginesoftenhavetens
of thousands of machines carrying out the various tasks of crawling, indexing,
and answering userqueries.
Web crawlers depend on all relevant pages being reachable through hyper-
links.However,many sitescontaining largecollectionsof datamaynot makeall
the data available as hyperlinked pages. Instead, they provide search interfaces,
whereuserscanenterterms,orselectmenuoptions,andgetresults.Asanexam-
ple,adatabaseof?ightinformationisusuallymadeavailableusingsuchasearch
interface,withoutanyhyperlinkstothepagescontaining?ightinformation.Asa
result,theinformationinsidesuchsitesisnotaccessibletoanormalWebcrawler.
Theinformation insuchsitesisoftenreferredto asdeepWeb information.
Deep Web crawlers extract some such information by guessing what terms
wouldmakesensetoenter,orwhatmenuoptionstochoose,insuchsearchinter-
faces. By entering each possible term/option and executing the search interface,
they are able to extract pages with data that they would not have been able to
?nd otherwise. The pages extracted by a deep Web crawl may be indexed just
like regular Web pages. The Google search engine, for example, includes results
from deepWebcrawls.
21.8 InformationRetrieval:BeyondRankingofPages
Information-retrievalsystemswereoriginallydesignedto?ndtextualdocuments
relatedtoaquery,andlaterextendedto?ndingpagesontheWebthatarerelated
toaquery.Peopleusesearchenginesformanydifferenttasks,fromsimpletasks
such as locating a Web site that they want to use, to a broader goal of ?nding
information on a topic of interest. Web search engines have become extremely
good at the task of locating Web sites that a user wants to visit. The task of
providing information on a topic of interest is much harder, and we study some
approaches inthissection.
Thereisalsoanincreasingneedforsystemsthattrytounderstanddocuments
(toalimitedextent),andanswerquestionsbasedonthe(limited)understanding.
One approach is to create structured information from unstructured documents
932 Chapter21 InformationRetrieval
andtoanswerquestionsbasedonthestructuredinformation.Anotherapproach
applies natural language techniques to ?nd documents related to a question
(phrasedinnatural language) andreturnrelevantsegmentsofthe documentsas
ananswer to thequestion.
21.8.1 DiversityofQueryResults
Today, search engines do not just return a ranked list of Web pages relevant to
a query. They also return image and video results relevant to a query. Further,
thereareavarietyofsitesprovidingdynamicallychangingcontentsuchassports
scores, or stock market tickers. To get current information from such sites, users
wouldhaveto?rstclickonthequeryresult.Instead,searchengineshavecreated
“gadgets,” which take data from a particular domain, such as sports updates,
stockprices,orweatherconditions,andformattheminanicegraphicalmanner,
to be displayed as results for a query. Search engines have to rank the set of
gadgetsavailable in termsof relevanceto a query,and displaythe most relevant
gadgets, along with Web pages, images, videos, and other types of results. Thus
aqueryresulthas adiversesetof resulttypes.
Search terms are often ambiguous. For example, a query “eclipse” may be
referringtoasolarorlunareclipse,ortotheintegrateddevelopmentenvironment
(IDE)calledEclipse.Ifallthehighlyrankedpagesfortheterm“eclipse”areabout
the IDE, a user looking for information about solar or lunar eclipsesmay be very
dissatis?ed. Search engines therefore attempt to provide a set of results that are
diverse in terms of their topics, to minimize the chance that a user would be
dissatis?ed. To do so, at indexing time the search engine must disambiguate the
sense in which a word is used in a page; for example, it must decide whether
the use of the word “eclipse” in a page refers to the IDE or the astronomical
phenomenon. Then, givenaquery,the searchengine attemptsto provideresults
that arerelevanttothemost commonsensesinwhich thequerywords areused.
TheresultsobtainedfromaWebpageneedtobesummarizedasasnippetin
a query result. Traditionally, search engines provided a few words surrounding
the query keywords as a snippet that helps indicate what the page contains.
However,therearemanydomainswherethesnippetcanbegeneratedinamuch
more meaningful manner. For example, if a user queries about a restaurant, a
search engine can generate a snippet containing the restaurant’s rating, a phone
number, and a link to a map, in addition to providing a link to the restaurant’s
home page. Such specialized snippets are often generated for results retrieved
from adatabase,for example,a databaseof restaurants.
21.8.2 Information Extraction
Information-extractionsystemsconvertinformationfromtextualformtoamore
structuredform.Forexample,areal-estateadvertisementmaydescribeattributes
of a home in textual form, such as “two-bedroom three-bath house in Queens,
$1 million”, from which an information extraction system may extract attributes
suchasnumberofbedrooms,numberofbathrooms,costandneighborhood.The
original advertisement could have used various terms such as 2BR, or two BR,
21.8 InformationRetrieval:BeyondRankingofPages 933
or two bed, to denote two bedrooms. The extracted information can be used
to structure the data in a standard way. Thus, a user could specify that he is
interested in two-bedroom houses, and a search system would be able to return
all relevant houses based on the structured data, regardless of the terms used in
theadvertisement.
An organization that maintains a database of company information may
use an information-extraction system to extract information automatically from
newspaperarticles;theinformationextractedwouldrelatetochangesinattributes
ofinterest,suchasresignations,dismissals,orappointmentsofcompanyof?cers.
As another example, search engines designed for ?nding scholarly research
articles,suchasCiteseerandGoogleScholar,crawltheWebtoretrievedocuments
thatarelikelytoberesearcharticles.Theyexaminesomefeaturesofeachretrieved
document, such as the presence of words such as “bibliography”, “references”,
and “abstract”, to judge if a document is in fact a scholarly research article.They
then extract the title, list of authors, and the citations at the end of the article, by
using information extraction techniques. The extracted citation information can
be used to link each article to articles that it cites, or to articles that cite it; such
citationlinksbetweenarticlescan be veryusefulfor aresearcher.
Severalsystemshavebeenbuiltforinformationextractionforspecializedap-
plications.Theyuselinguistictechniques,pagestructure,anduser-de?nedrules
forspeci?cdomainssuchasrealestateadvertisementsorscholarlypublications.
Forlimiteddomains,suchasaspeci?cWebsite,itispossibleforahumantospec-
ify patterns that can be used to extract information. For example,on a particular
Website,apatternsuchas“Price:<number>$”,where<number>indicatesany
number, may match locations where the price is speci?ed. Such patterns can be
createdmanually for a limitednumber of Websites.
However, on the Web scale with millions of Web sites, manual creation of
such patterns is not feasible.Machine-learning techniques, which can learn such
patternsgivenasetoftrainingexamples,arewidelyusedtoautomatetheprocess
of informationextraction.
Information extraction usually has errors in some fraction of the extracted
information; typically this is because some page had information in a format
that syntactically matched a pattern, but did not actually specify a value (such
as the price). Information extraction using simple patterns, which separately
match parts of a page, is relatively error prone. Machine-learning techniques
can perform much more sophisticated analysis, based on interactions between
patterns, to minimize errors in the information extracted, while maximizing the
amount of information extracted. See the references in the bibliographical notes
for moreinformation.
21.8.3 QuestionAnswering
Information retrieval systems focus on ?nding documents relevant to a given
query. However, the answer to a query may lie in just one part of a document,
or in small parts of several documents.Questionanswering systems attempt to
providedirectanswerstoquestionsposedbyusers.Forexample,aquestionofthe
934 Chapter21 InformationRetrieval
form “Who killed Lincoln?”maybestbeansweredbyalinethatsays“Abraham
Lincoln was shot by John Wilkes Booth in 1865.” Note that the answer does not
actually contain the words “killed” or “who”, but the system infers that “who”
can be answered by a name, and “killed”is relatedto “shot”.
Question answering systems targeted at information on the Web typically
generate one or more keyword queries from a submitted question, execute the
keyword queries against Web search engines, and parse returned documents to
?ndsegmentsofthedocumentsthatanswerthequestion.Anumberoflinguistic
techniques and heuristics are used to generate keyword queries, and to ?nd
relevantsegmentsfromthe document.
An issue in answering questions is that different documents may indicate
different answers to a question. For example, if the question is “How tall is a
giraffe?” different documents may give different numbers as an answer. These
answers form a distribution of values, and a question answering system may
choose the average, or median value of the distribution as the answer to be
returned; to re?ect the fact that the answer is not expected to be precise, the
system may return the average along with the standard deviation (for example,
average of 16 feet, with a standard deviation of 2 feet), or a range based on the
averageand the standarddeviation(for example,between14 and18 feet).
Current-generation question answering systems are limited in power, since
they do not really understand either the question or the documents used to
answer the question. However, they are useful for a number of simple question
answering tasks.
21.8.4 QueryingStructuredData
StructureddataareprimarilyrepresentedineitherrelationalorXMLform.Several
systems have been built to support keyword querying on relational and XML
data (see Chapter 23). A common theme between these systems lies in ?nding
nodes (tuples or XML elements) containing the speci?ed keywords, and ?nding
connecting paths (or common ancestors, in the case of XMLdata)betweenthem.
For example, a query “Zhang Katz” on a university database may ?nd the
name “Zhang” occurring in a student tuple, and the name “Katz” in an instructor
tuple, and a path through the advisor relation connecting the two tuples. Other
paths, such as student “Zhang” taking a course taught by “Katz” may also be
found in response to this query. Such queries may be used for ad hoc browsing
and querying of data, when the user does not know the exact schema and does
not wish to take the effort to write an SQL query de?ning what she is searching
for. Indeed it is unreasonable to expect lay users to write queries in a structured
querylanguage,whereaskeywordqueryingisquitenatural.
Since queries are not fully de?ned, they may have many different types of
answers,which mustberanked.Anumberoftechniqueshavebeenproposedto
rank answers in such a setting,based on the lengths of connecting paths, and on
techniques for assigning directions and weights to edges. Techniques have also
beenproposedforassigningpopularityrankstotuplesand XMLelements,based
21.9 DirectoriesandCategories 935
on links such as foreign key and IDREF links. See the bibliographical notes for
moreinformationonkeywordsearching of relationaland XML data.
21.9 DirectoriesandCategories
Atypicallibraryusermayuseacatalogtolocateabookforwhichsheislooking.
When she retrieves the book from the shelf, however, she is likely to browse
through other books that are located nearby. Libraries organize books in such a
way that related books are kept close together. Hence, a book that is physically
near the desiredbook may be of interest as well, making it worthwhile for users
to browsethrough suchbooks.
To keep related books close together, libraries use a classi?cation hierar-
chy. Books on science are classi?ed together. Within this set of books, there is a
?nerclassi?cation,withcomputer-sciencebooksorganizedtogether,mathematics
booksorganizedtogether,andsoon.Sincethereisarelationbetweenmathemat-
ics and computer science, relevant sets of books are stored close to each other
physically. At yet another level in the classi?cation hierarchy, computer-science
booksarebrokendownintosubareas,suchasoperatingsystems,languages,and
algorithms. Figure 21.1 illustrates a classi?cation hierarchy that may be used by
a library. Because books can be kept at only one place, each book in a library is
classi?edinto exactlyone spotintheclassi?cation hierarchy.
Inaninformation-retrievalsystem,thereisnoneedtostorerelateddocuments
closetogether.However,suchsystemsneedtoorganizedocumentslogicallysoasto
permitbrowsing.Thus,suchasystemcoulduseaclassi?cationhierarchysimilar
books
algorithms
graph algorithms
math
science ?ction engineering
computer science
Figure21.1 A classi?cation hierarchy for a library system.
936 Chapter21 InformationRetrieval
books
algorithms
graph algorithms
math
science ?ction engineering
computer science
Figure21.2 A classi?cation DAG for a library information-retrieval system.
to one that libraries use, and, when it displays a particular document, it can also
displayabriefdescriptionof documentsthat areclose inthe hierarchy.
In an information-retrieval system, there is no need to keep a document in a
single spot in the hierarchy. A document that talks of mathematics for computer
scientists could be classi?ed under mathematics as well as under computer sci-
ence. All that is stored at each spot is an identi?er of the document (that is, a
pointer to the document), and it is easy to fetch the contents of the document by
using theidenti?er.
Asaresultofthis?exibility,notonlycanadocumentbeclassi?edundertwo
locations, but also a subarea in the classi?cation hierarchy can itself occur under
two areas. The class of “graph algorithm” documents can appear both under
mathematics and under computer science. Thus, the classi?cation hierarchy is
now a directedacyclic graph (DAG), as shown in Figure 21.2. A graph-algorithm
document may appear in a single location in the DAG, but can be reached via
multiplepaths.
Adirectoryissimplyaclassi?cation DAGstructure.Eachleafofthedirectory
storeslinkstodocumentsonthetopicrepresentedbytheleaf.Internalnodesmay
alsocontainlinks,forexample,todocumentsthatcannotbeclassi?edunderany
of thechild nodes.
To ?nd information on a topic, a user would start at the root of the directory
and follow paths down the DAG until reaching a node representing the desired
topic. While browsing down the directory,the user can ?nd not only documents
onthetopicheisinterestedin,butalso?ndrelateddocumentsandrelatedclasses
in the classi?cation hierarchy. The user may learn new information by browsing
through documents(or subclasses)within therelatedclasses.
21.10 Summary 937
OrganizingtheenormousamountofinformationavailableontheWebintoa
directorystructureisa daunting task.
• The?rstproblemisdeterminingwhatexactlythedirectoryhierarchyshould
be.
• The second problem is, given a document, deciding which nodes of the
directoryarecategoriesrelevanttothe document.
To tackle the ?rst problem, portals such as Yahoo! have teams of “Internet
librarians” who come up with the classi?cation hierarchy and continually re?ne
it.
The second problem can also be tackled manually by librarians, or Web site
maintainers may be responsible for deciding where their sites should lie in the
hierarchy. There are also techniques for deciding automatically the location of
documents based on computing their similarity to documents that have already
beenclassi?ed.
Wikipedia, the online encyclopedia, addresses the classi?cation problem in
the reverse direction. Each page in Wikipedia has a list of categories to which
it belongs. For example, as of 2009, the Wikipedia page on giraffes had several
categories including “Mammals of Africa”.Inturn,the“Mammals of Africa”
category itself belongs to the category “Mammals by geography”,whichinturn
belongs to the category “Mammals”, which in turn has a category “Vertebrates”,
and so on. The category structure is useful to browse other instances of the
samecategory,forexample,to?ndothermammalsofAfrica,orothermammals.
Conversely,a querythat looks for mammals can use the category information to
infer that a giraffe is a mammal. The Wikipedia category structure is not a tree,
butisalmostaDAG;itisnotactuallyaDAGsinceithasafewinstancesofloops,
which probably re?ectcategorizationerrors.
21.10 Summary
• Information-retrieval systems are used to store and query textual data such
asdocuments.Theyuseasimplerdatamodelthandodatabasesystems,but
providemorepowerful queryingcapabilitieswithinthe restrictedmodel.
• Queries attempt to locate documents that are of interest by specifying, for
example,setsofkeywords.Thequerythatauserhasinmindusuallycannot
be stated precisely; hence, information-retrieval systems order answers on
thebasis ofpotentialrelevance.
• Relevanceranking makesuseof severaltypesof information, such as:
?
Termfrequency:how important eachtermistoeach document.
?
Inversedocument frequency.
?
Popularity ranking.
938 Chapter21 InformationRetrieval
• Similarityofdocumentsisusedtoretrievedocumentssimilartoanexample
document.The cosine metricis usedto de?nesimilarity,andis basedonthe
vectorspacemodel.
• PageRank and hub/authority rank are two ways to assign prestige to pages
onthebasisoflinkstothepage.ThePageRankmeasurecanbeunderstoodin-
tuitivelyusingarandom-walkmodel.Anchortextinformationisalsousedto
compute a per-keyword notion of popularity. Information-retrieval systems
needtocombinescoresonmultiplefactors suchas TF–IDFandPageRank,to
getanoverallscore fora page.
• Searchenginespammingattemptstoget(anundeserved)highrankingfora
page.
• Synonymsandhomonymscomplicatethetaskofinformationretrieval.Con-
cept-based querying aims at ?nding documents containing speci?ed con-
cepts, regardless of the exact words (or language) in which the concept is
speci?ed. Ontologies are used to relate concepts using relationships such as
is-aor part-of.
• Invertedindicesareusedtoanswer keywordqueries.
• Precision and recall are two measures of the effectiveness of an information
retrievalsystem.
• Web search engines crawl the Web to ?nd pages, analyze them to compute
prestigemeasures,andindexthem.
• Techniqueshavebeendevelopedtoextractstructuredinformationfromtex-
tualdata,toperformkeywordqueryingonstructureddata,andtogivedirect
answers tosimplequestionsposedinnatural language.
• Directorystructuresandcategoriesareusedtoclassifydocumentswithother
similardocuments.
ReviewTerms
• Information-retrievalsystems
• Keywordsearch
• Fulltextretrieval
• Term
• Relevance ranking
?
Term frequency
?
Inversedocumentfrequency
?
Relevance
?
Proximity
• Similarity-basedretrieval
?
Vectorspacemodel
?
Cosinesimilaritymetric
?
Relevancefeedback
• Stopwords
• Relevanceusing hyperlinks
?
Popularity/prestige
PracticeExercises 939
?
Transfer of prestige
• PageRank
?
Random walkmodel
• Anchor-text–based relevance
• Hub/authority ranking
• Searchenginespamming
• Synonyms
• Homonyms
• Concepts
• Concept-basedquerying
• Ontologies
• SemanticWeb
• Invertedindex
• Falsedrop
• Falsenegative
• Falsepositive
• Precision
• Recall
• Web crawlers
• DeepWeb
• Queryresultdiversity
• Information extraction
• Questionanswering
• Queryingstructureddata
• Directories
• Classi?cationhierarchy
• Categories
PracticeExercises
21.1 Compute the relevance (using appropriate de?nitions of term frequency
and inversedocument frequency) of each of the Practice Exercisesin this
chapter tothe query “SQL relation”.
21.2 Supposeyouwantto?nddocumentsthatcontainatleastk ofagivenset
of n keywords. Suppose also you have a keyword index that gives you a
(sorted)list ofidenti?ersofdocuments that contain aspeci?edkeyword.
Givean ef?cientalgorithmto?nd the desiredsetofdocuments.
21.3 Suggest how to implement the iterative technique for computing Page-
Rank given that the T matrix(evenin adjacency list representation)does
not?tinmemory.
21.4 Suggest how a document containing a word (such as “leopard”)canbe
indexed such that it is ef?ciently retrieved by queries using a more gen-
eral concept (such as “carnivore” or “mammal”). You can assume that
the concept hierarchy is not very deep, so each concept has only a few
generalizations (a concept can, however, have a large number of special-
izations).Youcanalsoassumethatyouareprovidedwithafunctionthat
returns the concept for each word in a document. Also suggest how a
query using a specialized concept can retrieve documents using a more
generalconcept.
21.5 Supposeinvertedlistsaremaintainedinblocks,witheachblocknotingthe
largestpopularityrankand TF–IDFscoresofdocumentsintheremaining
940 Chapter21 InformationRetrieval
blocks in the list. Suggest how merging of inverted lists can stop early if
theuserwants only thetop K answers.
Exercises
21.6 Usingasimplede?nitionoftermfrequencyasthenumberofoccurrences
of the term in a document, give the TF–IDF scores of each term in the set
of documentsconsisting ofthis and thenext exercise.
21.7 Create a small example of four small documents, each with a PageRank,
and create inverted lists for the documents sorted by the PageRank. You
donotneedtocomputePageRank,justassumesomevaluesforeachpage.
21.8 Suppose you wish to perform keyword querying on a set of tuples in
a database, where each tuple has only a few attributes, each containing
only afewwords.Doestheconcept of termfrequencymakesenseinthis
context? And that of inverse document frequency? Explain your answer.
Also suggest how you can de?ne the similarity of two tuples using TF–
IDF concepts.
21.9 WebsitesthatwanttogetsomepublicitycanjoinaWebring,wherethey
createlinkstoothersitesinthering,inexchangeforothersitesinthering
creating links to their site. What is the effect of such rings on popularity
ranking techniquessuch as PageRank?
21.10 The Google search engine provides a feature whereby Web sites can dis-
play advertisements supplied by Google. The advertisements supplied
are basedon the contents of the page.Suggesthow Googlemight choose
which advertisementstosupplyfor apage,giventhe pagecontents.
21.11 One way to create a keyword-speci?c version of PageRank is to modify
the random jump such that a jump is only possible to pages containing
the keyword. Thus pages that do not contain the keyword but are close
(in terms of links) to pages that contain the keyword also get a nonzero
rank for that keyword.
a. Give equations de?ning such a keyword-speci?c version of Page-
Rank.
b. Give a formula for computing the relevance of a page to a query
containing multiplekeywords.
21.12 The idea of popularity ranking using hyperlinks can be extended to re-
lational and XML data, using foreign key and IDREF edges in place of
hyperlinks. Suggest how such a ranking scheme may be of value in the
following applications:
a. Abibliographicdatabasethathaslinksfromarticlestoauthorsofthe
articlesandlinksfromeacharticletoeveryarticlethatitreferences.
BibliographicalNotes 941
b. A sales database that has links from each sales record to the items
that weresold.
Also suggest why prestige ranking can give less than meaningful results
inamoviedatabasethat recordswhich actor has actedinwhich movies.
21.13 What is the difference between a false positive and a false drop? If it
is essential that no relevant information be missed by an information
retrievalquery,isitacceptabletohaveeitherfalsepositivesorfalsedrops?
Why?
Tools
Google (www.google.com) is currently the most popular search engine, but there
are a number of other search engines, such as Microsoft Bing (www.bing.com)
and Yahoo! search (search.yahoo.com). The site searchenginewatch.com provides a
varietyof information about search engines.Yahoo! (dir.yahoo.com)andtheOpen
Directory Project (dmoz.org)provideclassi?cationhierarchies forWebsites.
BibliographicalNotes
Manning et al. [2008], Chakrabarti [2002], Grossman and Frieder [2004], Witten
et al. [1999], and Baeza-Yates and Ribeiro-Neto [1999] provide textbook descrip-
tions of information retrieval. In particular, Chakrabarti [2002] and Manning
etal.[2008] providedetailedcoverageof Webcrawling, ranking techniques,and
mining techniques relatedto information retrievalsuch as text classi?cation and
clustering.
Brin and Page [1998] describes the anatomy of the Google search engine,
including the PageRank technique, while a hubs- and authorities-based rank-
ing technique called HITS is described by Kleinberg [1999]. Bharat and Hen-
zinger [1998] presents a re?nement of the HITS ranking technique. These tech-
niques, as well as other popularity-based ranking techniques (and techniques
to avoid search engine spamming) are described in detail in Chakrabarti [2002].
Chakrabarti et al. [1999] addresses focused crawling of the Web to ?nd pages
related to a speci?c topic. Chakrabarti [1999] provides a survey of Web resource
discovery.
Indexing of documents is covered in detail by Witten et al. [1999]. Jones and
Willet[1997]isacollectionofarticlesoninformationretrieval.Salton[1989]isan
early textbook on information-retrieval systems. A number of practical issues in
ranking and indexing of Web pages, as done in an early version of the Google
search engine, are discussedin Brinand Page [1998]. Unfortunately,there are no
publicly available details of how exactly ranking is done currently by any of the
leadingsearchengines.
TheCiteseersystem(citeseer.ist.psu.edu)maintainsaverylargedatabaseofre-
searcharticles,withcitationlinksbetweenthepublications,andusescitationsto
942 Chapter21 InformationRetrieval
rankpublications.Itincludesatechniqueforadjustingthecitationrankingbased
on the age of a publication, to compensate for the fact that citations to a publi-
cation increase as time passes; without the adjustment, older documents tend to
get a higher ranking than they truly deserve. Google Scholar (scholar.google.com)
providesasimilarsearchabledatabaseofresearcharticlesincorporatingcitations
betweenarticles.Itisworthnotingthatthesesystemsuseinformationextraction
techniquestoinferthetitleandlistofauthorsofanarticle,aswellasthecitations
attheendof thearticle.Theythencreatecitationlinksbetweenarticlesbasedon
(approximate)matching of thearticletitleandauthor listwiththe citationtext.
Informationextractionandquestionansweringhavehadafairlylonghistory
in the arti?cial intelligence community. Jackson and Moulinier [2002] provides
textbook coverage of natural language processing techniques with an empha-
sis on information extraction. Soderland [1999] describes information extraction
using the WHISK system, while Appelt and Israel [1999] provides a tutorial on
informationextraction.
The annual Text RetrievalConference (TREC) has a number of tracks, each of
which de?nes a problem and infrastructure to test the quality of solutions to the
problem. Details on TREC may be found at trec.nist.gov. More information about
WordNet can be found at wordnet.princeton.edu and globalwordnet.org.Thegoalof
the Cyc system is to provide a formal representationof large amounts of human
knowledge.Itsknowledgebasecontainsalargenumberofterms,andassertions
abouteachterm.Cycalsoincludesasupportfornaturallanguageunderstanding
and disambiguation.Informationabout theCycsystemmay befound at cyc.com
and opencyc.org.
The evolution of Web search toward concepts and semantics rather than
keywords is discussed in Dalvi et al. [2009]. The annual International Semantic
Web Conference (ISWC)isoneofthemajorconferenceswherenewdevelopments
intheSemanticWebarepresented.Detailsmaybe found at semanticweb.org.
Agrawal et al. [2002], Bhalotia et al. [2002] and Hristidis and Papakonstanti-
nou[2002]coverkeywordqueryingofrelationaldata.Keywordqueryingof XML
datais addressedby Florescuetal.[2000] and Guo etal.[2003], among others.
PART
7
SPECIALTY DATABASES
Several application areas for database systems are limited by the restrictions of
the relational data model. As a result, researchers have developed several data
models based on an object-oriented approach, to deal with these application
domains.
Theobject-relationalmodel,describedinChapter22,combinesfeaturesofthe
relational and object-oriented models. This model provides the rich type system
of object-oriented languages, combined with relations as the basis for storage of
data. It applies inheritance to relations, not just to types. The object-relational
data model provides a smooth migration path from relational databases, which
isattractivetorelationaldatabasevendors.Asaresult,startingwith SQL:1999,the
SQL standard includes a number of object-oriented features in its type system,
while continuing to use the relational model as the underlying model.
The term object-oriented database is used to describe a database system that
supportsdirectaccesstodatafromobject-orientedprogramminglanguages,with-
out requiring a relational query language as the database interface. Chapter 22
also provides a brief overview of object-oriented databases.
The XML language was initially designed as a way of adding markup infor-
mationtotextdocuments,buthasbecomeimportantbecauseofitsapplicationsin
data exchange. XML provides a way to represent data that have nested structure,
and furthermore allows a great deal of ?exibility in structuring of data, which
is important for certain kinds of nontraditional data. Chapter 23 describes the
XML language, and then presents different ways of expressing queries on data
representedin XML,includingthe XQuery XMLquerylanguage,and SQL/XML,an
extension of SQL which allows the creation of nested XML output.
943
This page intentionally left blank 
CHAPTER
22
Object-Based Databases
Traditional database applications consist of data-processing tasks, such as bank-
ing and payroll management, with relatively simple data types that are well
suitedtotherelationaldatamodel.Asdatabasesystemswereappliedtoawider
rangeofapplications,suchascomputer-aideddesignandgeographicalinforma-
tionsystems,limitationsimposedbytherelationalmodelemergedasanobstacle.
The solution was the introduction of object-based databases, which allow one to
dealwithcomplexdatatypes.
22.1 Overview
The ?rst obstacle faced by programmers using the relational data model was
the limited type system supported by the relational model. Complex applica-
tiondomainsrequirecorrespondinglycomplexdatatypes,suchasnestedrecord
structures,multivaluedattributes,andinheritance,whicharesupportedbytradi-
tionalprogramminglanguages.Suchfeaturesareinfactsupportedinthe E-Rand
extended E-R notations, but had to be translated to simpler SQL data types. The
object-relational data model extends the relational data model by providing a
richertypesystemincludingcomplexdatatypesandobjectorientation.Relational
query languages, inparticular SQL, needto be correspondinglyextendedto deal
with the richer type system. Such extensions attempt to preserve the relational
foundations—in particular, the declarative access to data—while extending the
modeling power. Object-relational database systems, that is, database systems
basedontheobject-relationmodel,provideaconvenientmigrationpathforusers
of relationaldatabaseswho wishtouse object-orientedfeatures.
The second obstacle was the dif?culty in accessing database data from pro-
gramswritteninprogramminglanguagessuchasC++orJava.Merelyextending
thetypesystemsupportedbythedatabasewasnotenoughtosolvethisproblem
completely. Differences between the type system of the database and the type
systemoftheprogramminglanguagemakedatastorageandretrievalmorecom-
plicated, and need to be minimized. Having to express database access using a
language (SQL) that is different from the programming language again makes
the job of the programmer harder. It is desirable, for many applications, to have
945
946 Chapter 22 Object-Based Databases
programminglanguageconstructsorextensionsthatpermitdirectaccesstodata
in the database, without having to go through an intermediatelanguage such as
SQL.
Inthischapter,we?rstexplainthemotivationforthedevelopmentofcomplex
data types. We then study object-relational database systems, speci?cally using
features that were introduced in SQL:1999 and SQL:2003. Note that most database
products support only a subset of the SQL features described here and for sup-
ported features, the syntax often differs slightly from the standard. This is the
resultofcommercialsystemsintroducingobject-relationalfeaturestothemarket
before the standards were ?nalized. Refer to the user manual of the database
systemyou useto?nd outwhat featuresitsupports.
We then address the issue of supporting persistence for data that is in the
nativetypesystemofanobject-orientedprogramminglanguage.Twoapproaches
areusedinpractice:
1. Build an object-oriented database system, that is, a database system that
nativelysupportsanobject-orientedtypesystem,andallowsdirectaccessto
data from an object-oriented programming language using the native type
systemof thelanguage.
2. Automaticallyconvertdatafromthenativetypesystemoftheprogramming
language to a relational representation, and vice versa. Data conversion is
speci?edusinganobject-relational mapping.
We providea briefintroductionto boththeseapproaches.
Finally, we outline situations in which the object-relational approach is bet-
ter than the object-oriented approach, and vice versa, and mention criteria for
choosing betweenthem.
22.2 ComplexDataTypes
Traditionaldatabaseapplicationshaveconceptuallysimpledatatypes.Thebasic
data items are records that are fairly small and whose ?elds are atomic—that
is, they are not further structured, and ?rst normal form holds (see Chapter 8).
Further, there are only a few record types.
In recent years, demand has grown for ways to deal with more complex
data types. Consider, for example, addresses. While an entire address could be
viewed as an atomic data item of type string, this view would hide details such
as the street address, city, state, and postal code, which could be of interest
to queries. On the other hand, if an address were represented by breaking it
into the components (streetaddress,city, state, and postal code), writing queries
wouldbemorecomplicatedsincetheywouldhavetomentioneach?eld.Abetter
alternativeistoallowstructureddatatypesthatallowatypeaddresswithsubparts
street address, city, state,andpostal code.
Asanotherexample,considermultivaluedattributesfromtheE-Rmodel.Such
attributesarenatural,forexample,forrepresentingphonenumbers,sincepeople
22.2 ComplexDataTypes 947
title author
_
array publisher keyword
_
set
(name, branch)
Compilers [Smith, Jones] (McGraw-Hill, NewYork) {parsing, analysis}
Networks [Jones, Frick] (Oxford, London) {Internet, Web}
Figure22.1 Non-1NF books relation, books.
may have more than one phone. The alternative of normalization by creating a
newrelationisexpensiveandarti?cial for thisexample.
With complex type systems we can represent E-R model concepts, such as
composite attributes, multivalued attributes, generalization, and specialization
directly,without a complextranslationto therelationalmodel.
In Chapter 8, we de?ned ?rst normal form (1NF), which requires that all at-
tributes have atomic domains. Recall that a domain is atomic if elements of the
domainareconsideredtobe indivisibleunits.
The assumptionof 1NF is a natural one inthe database applicationexamples
we have considered. However, not all applications are best modeled by 1NF
relations. For example, rather than view a database as a set of records, users of
certain applications view it as a set of objects (or entities). These objects may
require several records for their representation. A simple, easy-to-use interface
requires a one-to-one correspondence between the user’s intuitive notion of an
object and thedatabasesystem’snotion of adata item.
Consider, for example, a library application, and suppose we wish to store
thefollowing information foreach book:
• Booktitle.
• Listof authors.
• Publisher.
• Setof keywords.
We can see that, if we de?ne a relation for the preceding information, several
domains willbenonatomic.
• Authors. A book may have a list of authors, which we can represent as an
array. Nevertheless, we may want to ?nd all books of which Jones was one
of the authors. Thus, we are interested in a subpart of the domain element
“authors.”
• Keywords. If we store a set of keywords for a book, we expect to be able to
retrieveall books whose keywordsinclude one or morespeci?edkeywords.
Thus,we viewthedomainofthe setof keywordsas nonatomic.
• Publisher. Unlike keywords and authors, publisher does not have a set-valued
domain.However,wemayview publisherasconsistingofthesub?elds name
and branch.Thisviewmakesthe domainof publisher nonatomic.
Figure22.1shows anexamplerelation, books.
948 Chapter 22 Object-Based Databases
title author position
Compilers Smith
Compilers Jones
Networks Jones
Networks Frick
1
2
1
2
authors
title keyword
Compilers parsing
Compilers analysis
Networks Internet
Networks We b
keywords
title pub
_
name pub
_
branch
Compilers McGraw-Hill New York
Networks Oxford London
books4
Figure22.2 4NF version of the relation books.
Forsimplicity,weassumethatthetitleofabookuniquelyidenti?esthebook.
1
We can then represent the same information using the following schema, where
the primarykeyattributesareunderlined:
• authors(title, author, position)
• keywords(title, keyword)
• books4(title,pubname, pub branch)
Theaboveschemasatis?es4NF.Figure22.2showsthenormalizedrepresentation
of thedata fromFigure22.1.
Although our example book database can be adequately expressed without
usingnestedrelations,theuseofnestedrelationsleadstoaneasier-to-understand
model.Thetypicaluserorprogrammerofaninformation-retrievalsystemthinks
of the database in terms of books having sets of authors, as the non-1NF design
models. The 4NF design requires queries to join multiple relations, whereas the
non-1NF designmakesmany typesof querieseasier.
On the other hand, it may be better to use a ?rst normal form representation
in other situations. For instance, consider the takes relationship in our university
example.Therelationshipismany-to-manybetweenstudentandsection.Wecould
1
Thisassumptiondoesnotholdintherealworld.Booksareusuallyidenti?edbya10-digitISBNnumberthatuniquely
identi?eseach publishedbook.
22.3 Structured Types and Inheritance in SQL 949
conceivably store a set of sections with each student, or a set of students with
each section, or both. If we store both, we would have data redundancy (the
relationshipofaparticularstudenttoaparticularsectionwouldbestoredtwice).
Theabilitytousecomplexdatatypessuchassetsandarrayscanbeusefulin
many applications but shouldbe usedwithcare.
22.3 StructuredTypesandInheritanceinSQL
Before SQL:1999,theSQLtypesystemconsistedofafairlysimplesetofprede?ned
types. SQL:1999addedanextensivetypesystemto SQL,allowingstructuredtypes
and typeinheritance.
22.3.1 StructuredTypes
Structured types allow composite attributes of E-R designs to be represented
directly. For instance, we can de?ne the following structured type to represent a
compositeattribute namewithcomponent attribute ?rstnameand lastname:
create type Nameas
(?rstnamevarchar(20),
lastname varchar(20))
?nal;
Similarly, the following structured type can be used to represent a composite
attribute address:
create type Addressas
(streetvarchar(20),
city varchar(20),
zipcode varchar(9))
not?nal;
Such types are called user-de?ned types in SQL
2
. The above de?nition corre-
spondstothe E-RdiagraminFigure7.4.The?nalandnot?nalspeci?cationsare
relatedtosubtyping, which wedescribelater,inSection22.3.2.
3
We can now use these types to create composite attributes in a relation, by
simply declaring an attribute to be of one of these types. For example, we could
createa table personas follows:
2
To illustrate our earlier note about commercial implementations de?ning their syntax before the standards were
developed,wepointoutthat Oracle requiresthe keywordobject followingas.
3
The ?nal speci?cation for Name indicates that we cannot create subtypes for name, whereas the not ?nal speci?cation
for Address indicatesthat we cancreate subtypes of address.
950 Chapter 22 Object-Based Databases
create table person (
name Name,
address Address,
dateOfBirth date);
The components of a composite attribute can be accessed using a “dot” no-
tation; for instance name.?rstname returns the ?rstname component of the name
attribute.Anaccesstoattribute namewouldreturnavalueofthestructuredtype
Name.
Wecanalsocreateatablewhoserowsareofauser-de?nedtype.Forexample,
we could de?ne a type PersonTypeand createthetable person as follows:
4
create type PersonType as (
name Name,
address Address,
dateOfBirth date)
not?nal
create table personof PersonType;
Analternativewayofde?ningcompositeattributesinSQListouseunnamed
rowtypes.Forinstance,therelationrepresentingpersoninformationcouldhave
beencreatedusing rowtypesas follows:
create table person r (
name row(?rstname varchar(20),
lastname varchar(20)),
address row(streetvarchar(20),
city varchar(20),
zipcode varchar(9)),
dateOfBirth date);
This de?nition is equivalent to the preceding table de?nition, except that the
attributes name and address have unnamed types, and the rows of the table also
havean unnamedtype.
Thefollowingqueryillustrateshowtoaccesscomponentattributesofacom-
positeattribute.The query?nds the lastname andcity of eachperson.
select name.lastname, address.city
from person;
A structured type can have methods de?ned on it. We declare methods as
partof the typede?nitionof astructuredtype:
4
Most actual systems, being case insensitive, would not permit name to be used both as an attribute name and a data
type.
22.3 Structured Types and Inheritance in SQL 951
create type PersonType as (
name Name,
address Address,
dateOfBirth date)
not?nal
method ageOnDate(onDate date)
returnsinterval year;
Wecreatethemethod bodyseparately:
create instance method ageOnDate(onDate date)
returnsinterval year
for PersonType
begin
return onDate ?self.dateOfBirth;
end
Note that the for clause indicates which type this method is for, while the
keywordinstanceindicatesthatthismethodexecutesonaninstanceofthePerson
type. The variable self refers to the Person instance on which the method is
invoked. The body of the method can contain procedural statements, which we
saw earlier in Section 5.2. Methods can update the attributes of the instance on
which theyareexecuted.
Methods can be invoked on instances of a type. If we had created a table
personoftype PersonType,wecouldinvokethemethod ageOnDate()asillustrated
below,to ?nd theage of eachperson.
select name.lastname, ageOnDate(current date)
from person;
In SQL:1999, constructor functions are used to create values of structured
types.Afunctionwiththesamenameasastructuredtypeisaconstructorfunction
for the structured type. For instance, we could declare a constructor for the type
Namelikethis:
create function Name(?rstnamevarchar(20), lastname varchar(20))
returns Name
begin
set self.?rstname= ?rstname;
set self.lastname = lastname;
end
We can then use newName(’John’,’Smith’)tocreateavalueofthetypeName.
We can construct a row value by listing its attributes within parentheses. For
instance,ifwedeclareanattribute nameasarowtypewithcomponents ?rstname
952 Chapter 22 Object-Based Databases
and lastname we can construct this value for it: (’Ted’, ’Codd’) without using a
constructor.
Bydefaulteverystructuredtypehasaconstructorwithnoarguments,which
setstheattributestotheirdefaultvalues.Anyotherconstructorshavetobecreated
explicitly. There can be more than one constructor for the same structured type;
although theyhavethe samename,they mustbedistinguishableby the number
of argumentsand typesoftheirarguments.
The following statement illustrates how we can create a new tuple in the
Person relation. We assume that a constructor has been de?ned for Address,just
like the constructor we de?ned for Name.
insertinto Person
values
(new Name(’John’, ’Smith’),
new Address(’20MainSt’,’New York’, ’11001’),
date ’1960-8-22’);
22.3.2 TypeInheritance
Supposethat wehavethe following typede?nitionfor people:
create type Person
(name varchar(20),
addressvarchar(20));
Wemaywanttostoreextrainformationinthedatabaseaboutpeoplewhoare
students,andaboutpeoplewhoareteachers.Sincestudentsandteachersarealso
people,wecan useinheritance to de?nethestudentand teachertypesin SQL:
create type Student
under Person
(degreevarchar(20),
departmentvarchar(20));
create type Teacher
under Person
(salary integer,
departmentvarchar(20));
Both Student and Teacher inherit the attributes of Person—namely,name and
address. Student and Teacher are said to be subtypes of Person,andPerson is a
supertypeof Student,aswellasofTeacher.
Methods of a structured type are inherited by its subtypes, just as attributes
are. However, a subtype can rede?ne the effect of a method by declaring the
method again, using overriding method in place of method in the method dec-
laration.
22.3 Structured Types and Inheritance in SQL 953
The SQL standard requires an extra ?eld at the end of the type de?nition,
whosevalueiseither?nalornot?nal.Thekeyword?nalsaysthatsubtypesmay
not be created from the given type, while not ?nal says that subtypes may be
created.
Now suppose that we want to store information about teaching assistants,
whoaresimultaneouslystudentsandteachers,perhapsevenindifferentdepart-
ments.Wecandothisifthetypesystemsupportsmultipleinheritance,wherea
type is declared as a subtype of multiple types. Note that the SQL standard does
not support multiple inheritance, although future versions of the SQL standard
may supportit,so we discusstheconcept here.
Forinstance,ifourtypesystemsupportsmultipleinheritance,wecande?ne
a typefor teaching assistant as follows:
create type TeachingAssistant
under Student, Teacher;
TeachingAssistant inherits all the attributes of Student and Teacher.Thereisa
problem, however, since the attributes name, address,anddepartment are present
in Student,aswellasinTeacher.
Theattributesnameand addressareactuallyinheritedfromacommonsource,
Person. So there is no con?ict caused by inheriting them from Student as well as
Teacher. However, the attribute department is de?ned separately in Student and
Teacher. In fact, a teaching assistant may be a student of one department and a
teacher in another department. To avoid a con?ict between the two occurrences
of department,wecanrenamethembyusinganas clause, as in this de?nition of
thetype TeachingAssistant:
create type TeachingAssistant
under Student with(department as student dept),
Teacher with(department as teacher dept);
In SQL,asinmostotherlanguages,avalueofastructuredtypemusthaveex-
actlyonemost-speci?ctype.Thatis,eachvaluemustbeassociatedwithonespeci?c
type,calleditsmost-speci?ctype,whenitiscreated.Bymeansofinheritance,itis
alsoassociatedwitheachofthesupertypesofitsmost-speci?ctype.Forexample,
suppose that an entity has the type Person,aswellasthetypeStudent. Then, the
most-speci?c type of the entity is Student,sinceStudent is a subtype of Person.
However,anentitycannothavethetypeStudentaswellasthetype Teacher unless
it has a type, such as TeachingAssistant, that is a subtype of Teacher,aswellasof
Student (which is not possible in SQL since multiple inheritance is not supported
by SQL).
954 Chapter 22 Object-Based Databases
22.4 TableInheritance
Subtables in SQL correspond to the E-R notion of specialization/generalization.
For instance,supposewe de?nethe people tableas follows:
create table people of Person;
We can then de?ne tables students and teachers as subtables of people,as
follows:
create table studentsof Student
under people;
create table teachersof Teacher
under people;
Thetypesofthesubtables(Studentand Teacher intheaboveexample)aresubtypes
of the type of the parent table (Person in the above example). As a result, every
attribute present in the table people is also present in the subtables students and
teachers.
Further, when we declare students and teachers as subtables of people, every
tuple present in students or teachers becomes implicitly present in people.Thus,
if a query uses the table people, it will ?nd not only tuples directly inserted into
thattable,butalsotuplesinsertedintoitssubtables,namelystudentsand teachers.
However, only those attributes that are present in people can be accessed by that
query.
SQLpermitsusto?ndtuplesthatareinpeoplebutnotinitssubtablesbyusing
“only people” in place of people in a query. The only keyword can also be used in
delete and update statements. Without the only keyword, a delete statement on
a supertable, such as people, also deletes tuples that were originally inserted in
subtables (suchas students);for example,a statement:
delete from people where P;
would delete all tuples from the table people,aswellasitssubtablesstudents
and teachers,thatsatisfyP.Iftheonly keyword is added to the above statement,
tuples that were inserted in subtables are not affected, even if they satisfy the
where clause conditions. Subsequent queries on the supertable would continue
to ?nd thesetuples.
Conceptually,multipleinheritanceispossiblewithtables,justasitispossible
withtypes.For example,wecan createatable oftype TeachingAssistant:
create table teaching assistants
of TeachingAssistant
under students, teachers;
22.4 Table Inheritance 955
As a result of the declaration, every tuple present in the teaching assistants
tableisalsoimplicitlypresentinthe teachersandinthe studentstable,andinturn
in the people table. We note, however, that multiple inheritance of tables is not
supportedby SQL.
There are some consistency requirements for subtables. Before we state the
constraints, we need a de?nition: we say that tuples in a subtable and parent
table correspond if they have the same values for all inherited attributes. Thus,
correspondingtuplesrepresentthe sameentity.
Theconsistency requirementsfor subtablesare:
1. Each tuple of the supertable can correspond to at most one tuple in each of
itsimmediatesubtables.
2. SQL has an additional constraint that all the tuples corresponding to each
other mustbe derivedfrom one tuple(insertedintoone table).
Forexample,withoutthe?rstcondition,wecouldhavetwotuplesinstudents(or
teachers) thatcorrespond tothe sameperson.
Thesecondconditionrulesoutatupleinpeoplecorrespondingtobothatuple
in students and a tuple in teachers, unless all these tuples are implicitly present
because a tuple was inserted in a table teaching assistants,whichisasubtableof
both teachersand students.
Since SQL does not support multiple inheritance, the second condition actu-
ally preventsa person from being both a teacher and a student. Even if multiple
inheritanceweresupported,thesameproblemwouldariseifthesubtableteaching
assistants were absent. Obviously it would be useful to model a situation where
a person can be a teacher and a student, even if a common subtable teaching
assistants is not present. Thus, it can be useful to remove the second consis-
tencyconstraint.Doingsowouldallowanobjecttohavemultipletypes,without
requiringit tohavea most-speci?ctype.
For example, suppose we again have the type Person,withsubtypesStudent
and Teacher, and the corresponding table people,withsubtablesteachers and stu-
dents.Wecanthenhaveatupleinteachersandatupleinstudentscorrespondingto
thesametupleinpeople.ThereisnoneedtohaveatypeTeachingAssistantthatisa
subtype of both Student and Teacher.WeneednotcreateatypeTeachingAssistant
unlesswewishto storeextraattributesor rede?nemethodsina manner speci?c
to peoplewho areboth studentsandteachers.
Wenote,however,that SQLunfortunatelyprohibitssuchasituation,because
of consistency requirement 2. Since SQL also does not support multiple inheri-
tance,wecannotuseinheritancetomodelasituationwhereapersoncanbeboth
a student and a teacher. As a result, SQL subtables cannot be used to represent
overlappingspecializationsfrom the E-R model.
We can of course create separate tables to represent the overlapping special-
izations/generalizations without using inheritance. The process was described
earlier,inSection7.8.6.1.Intheaboveexample,wewouldcreatetablespeople,stu-
dents,andteachers,withthestudentsandteacherstablescontainingtheprimary-key
956 Chapter 22 Object-Based Databases
attributeofPersonandotherattributesspeci?ctoStudentand Teacher,respectively.
The peopletablewouldcontaininformationaboutallpersons,includingstudents
and teachers. We would then have to add appropriate referential-integrity con-
straints to ensure that students and teachers are also represented in the people
table.
In other words, we can create our own improved implementation of the
subtable mechanism using existing features of SQL, with some extra effort in
de?ning the table, as well as some extra effort at query time to specify joins to
access requiredattributes.
We note that SQL de?nes aprivilegecalled under,which is requiredinorder
to create a subtype or subtable under another type or table. The motivation for
this privilegeissimilartothat for the referencesprivilege.
22.5 ArrayandMultisetTypesinSQL
SQL supports two collectiontypes:arrays and multisets;array typeswere added
in SQL:1999, while multiset types were added in SQL:2003.Recallthatamultiset is
an unordered collection, where an element may occur multiple times. Multisets
arelikesets,exceptthat asetallows eachelementtooccur at mostonce.
Suppose we wish to record information about books, including a set of key-
words for each book. Supposealso that we wished to storethe names of authors
of a book as an array; unlike elements in a multiset, the elements of an array are
ordered,sowecandistinguishthe?rstauthorfromthesecondauthor,andsoon.
Thefollowingexampleillustrateshowthesearrayandmultiset-valuedattributes
canbede?nedinSQL:
create type Publisher as
(name varchar(20),
branch varchar(20));
create type Book as
(title varchar(20),
author array varchar(20) array[10],
pub date date,
publisher Publisher,
keyword setvarchar(20) multiset);
create table books of Book;
The ?rst statement de?nes a type called Publisher with two components: a name
and a branch. The second statement de?nes a structured type Book that contains
a title,anauthor array, which is an array of up to 10 author names, a publication
date, a publisher (of type Publisher), and a multiset of keywords. Finally, a table
books containing tuplesof type Book iscreated.
22.5 Arrayand Multiset Types in SQL 957
Notethatweusedanarray,insteadofamultiset,tostorethenamesofauthors,
sincetheorderingofauthorsgenerallyhassomesigni?cance,whereaswebelieve
that theorderingof keywordsassociatedwitha book isnot signi?cant.
In general, multivalued attributes from an E-R schema can be mapped to
multiset-valuedattributesinSQL;iforderingisimportant,SQLarrayscanbeused
insteadof multisets.
22.5.1 CreatingandAccessingCollectionValues
Anarrayof valuescanbe createdin SQL:1999 inthisway:
array[’Silberschatz’, ’Korth’, ’Sudarshan’]
Similarly,a multisetof keywordscanbe constructedas follows:
multiset[’computer’, ’database’, ’SQL’]
Thus,wecancreateatupleofthetypede?nedbythebooks relation as:
(’Compilers’,array[’Smith’, ’Jones’], new Publisher(’McGraw-Hill’, ’NewYork’),
multiset[’parsing’, ’analysis’])
Herewehavecreatedavaluefortheattribute Publisherbyinvokinga constructor
function for Publisher with appropriate arguments. Note that this constructor
for Publisher must be created explicitly, and is not present by default; it can be
declaredjustlikethe constructor for Name,which wesawearlierinSection22.3.
If we want to insert the preceding tuple into the relation books,wecould
executethe statement:
insert into books
values (’Compilers’, array[’Smith’, ’Jones’],
new Publisher(’McGraw-Hill’, ’NewYork’),
multiset[’parsing’, ’analysis’]);
We can access or update elements of an array by specifying the array index,
for example author array[1].
22.5.2 QueryingCollection-Valued Attributes
We now consider how to handle collection-valued attributes in queries. An ex-
pressionevaluatingtoacollectioncanappearanywherethatarelationnamemay
appear, such as in a from clause, as the following paragraphs illustrate. We use
thetable books that wede?nedearlier.
If we want to ?nd all books that have the word “database” as one of their
keywords,we canuse thisquery:
958 Chapter 22 Object-Based Databases
select title
from books
where ’database’in(unnest(keyword set));
Note that we have used unnest(keyword set)inapositionwhereSQL without
nestedrelationswould haverequiredaselect-from-where subexpression.
Ifwe know thata particularbook has threeauthors,we couldwrite:
select author array[1], author array[2], author array[3]
from books
where title =’Database SystemConcepts’;
Now, suppose that we want a relation containing pairs of the form “title,
author name”foreachbookandeachauthorofthebook.Wecanusethisquery:
select B.title, A.author
from books as B, unnest(B.author array) as A(author);
Sincetheauthor arrayattributeofbooksisacollection-valued?eld,unnest(B.author
array)canbeusedinafrom clause, where a relation is expected. Note that the
tuple variable B is visible to this expression since it is de?ned earlier in the from
clause.
When unnesting an array, the previous query loses information about the
orderingofelementsinthearray.The unnestwithordinalityclausecanbeused
to get this information, as illustrated by the following query. This query can be
usedtogeneratetheauthorsrelation,whichwesawearlier,fromthebooksrelation.
select title, A.author, A.position
from books as B,
unnest(B.author array) with ordinalityas A(author, position);
Thewithordinalityclausegeneratesanextraattributewhichrecordsthepo-
sitionoftheelementinthearray.Asimilarquery,butwithoutthewithordinality
clause,can beusedto generatethe keyword relation.
22.5.3 NestingandUnnesting
The transformation of a nested relation into a form with fewer (or no) relation-
valuedattributesiscalledunnesting.Thebooksrelationhastwoattributes,author
array and keyword set, that are collections, and two attributes, title and publisher,
thatarenot.Supposethatwewanttoconverttherelationintoasingle?atrelation,
withnonestedrelationsorstructuredtypesasattributes.Wecanusethefollowing
querytocarry out thetask:
22.5 Arrayand Multiset Types in SQL 959
title author pub
_
name pub
_
branch keyword
Compilers Smith McGraw-Hill New York parsing
Compilers Jones McGraw-Hill New York parsing
Compilers Smith McGraw-Hill New York analysis
Compilers Jones McGraw-Hill New York analysis
Networks Jones Oxford London Internet
Networks Frick Oxford London Internet
Networks Jones Oxford London We b
Networks Frick Oxford London We b
Figure22.3 ?at books: result of unnesting attributes author array and keyword set of relation
books.
select title, A.author, publisher.name as pub name, publisher.branch
as pub branch, K.keyword
from books as B,unnest(B.author array) as A(author),
unnest(B.keyword set)as K(keyword);
ThevariableBinthefromclauseisdeclaredtorangeoverbooks.Thevariable
A is declared to range over the authors in author array for the book B,andK is
declaredtorange overthekeywordsinthe keyword setof thebook B.Figure22.1
shows an instance books relation, and Figure 22.3 shows the relation, which we
call ?at books, that is the result of the preceding query. Note that the relation ?at
books isin 1NF,since allitsattributesareatomicvalued.
The reverse process of transforming a 1NF relation into a nested relation
is called nesting. Nesting can be carried out by an extension of grouping in
SQL. In the normal use of grouping in SQL, a temporary multiset relation is
(logically) created for each group, and an aggregate function is applied on the
temporaryrelationtogetasingle(atomic)value.Thecollectfunctionreturnsthe
multiset of values, so instead of creating a single value, we can create a nested
relation. Suppose that we are given the 1NF relation ?at books, as in Figure 22.3.
Thefollowing queryneststhe relationontheattribute keyword:
select title, author, Publisher(pub name, pub branch)as publisher,
collect(keyword) as keyword set
from ?at books
groupby title, author, publisher;
The result of the query on the ?at books relation from Figure 22.3 appears in
Figure22.4.
If we want to nest the author attribute also into a multiset, we can use the
query:
960 Chapter 22 Object-Based Databases
title author publisher keyword
_
set
(pub
_
name,pub
_
branch)
Compilers Smith (McGraw-Hill, New York) {parsing, analysis}
{parsing, analysis} Compilers Jones (McGraw-Hill, New York)
Networks Jones (Oxford, London) {Internet, Web}
{Internet, Web} Networks Frick (Oxford, London)
Figure22.4 A partially nested version of the ?at books relation.
select title, collect(author) as author set,
Publisher(pub name, pub branch)as publisher,
collect(keyword) as keyword set
from ?at books
groupby title, publisher;
Anotherapproachtocreatingnestedrelationsistousesubqueriesintheselect
clause.Anadvantageofthesubqueryapproachisthatan orderbyclausecanbe
used in the subquery to generate results in the order desired for the creation of
an array. The following query illustrates this approach; the keywords array and
multiset specify that an array and multiset (respectively) are to be created from
the resultsof thesubqueries.
select title,
array( select author
from authors as A
where A.title = B.title
orderby A.position) as author array,
Publisher(pub name, pub branch)as publisher,
multiset( select keyword
from keywords as K
where K.title = B.title)as keyword set,
from books4 as B;
Thesystemexecutesthenestedsubqueriesintheselect clauseforeachtuple
generated by the from and where clauses of the outer query. Observe that the
attribute B.title from the outer query is used in the nested queries,to ensure that
only thecorrect setsof authors and keywordsaregeneratedfor eachtitle.
SQL:2003 provides a variety of operators on multisets, including a function
set(M) that returns a duplicate-free version of a multiset M,anintersection
aggregateoperation,whichreturnstheintersectionofallthemultisetsinagroup,
afusionaggregateoperation,whichreturnstheunionofallmultisetsinagroup,
and a submultiset predicate, which checks if a multiset is contained in another
multiset.
22.6 Object-Identity and ReferenceTypes in SQL 961
The SQL standard does not provide any way to update multiset attributes
exceptbyassigninganewvalue.Forexample,todeleteavalue vfromamultiset
attribute A,wewouldhavetosetitto( Aexcept all multiset[v]).
22.6 Object-Identity andReference TypesinSQL
Object-oriented languages provide the ability to refer to objects. An attribute of
a type can be a reference to an object of a speci?ed type. For example, in SQL we
can de?ne a type Department with a ?eld name and a ?eld head that isa reference
to thetype Person,andatabledepartmentsof type Department,as follows:
create type Department(
namevarchar(20),
head ref(Person)scope people);
create table departmentsof Department;
Here, the reference is restricted to tuples of the table people.Therestrictionof
the scope of a reference to tuples of a table is mandatory in SQL,anditmakes
referencesbehavelikeforeignkeys.
Wecanomitthedeclarationscopepeoplefromthetypedeclarationandinstead
makeanadditionto the create tablestatement:
create table departmentsof Department
(head with optionsscope people);
The referenced table must have an attribute that stores the identi?er of the
tuple. We declare this attribute, called the self-referential attribute, by adding a
ref isclause to the create table statement:
create table people of Person
refis person id system generated;
Here, person id is an attribute name, not a keyword, and the create table
statementspeci?esthattheidenti?erisgeneratedautomaticallybythedatabase.
In order to initialize a reference attribute, we need to get the identi?er of the
tuplethat istobe referenced.We can get the identi?ervalue ofa tupleby means
ofaquery.Thus,tocreateatuplewiththereferencevalue,wemay?rstcreatethe
tuplewithanull referenceand thensetthereferenceseparately:
962 Chapter 22 Object-Based Databases
insertinto departments
values (’CS’, null);
update departments
set head=( select p.person id
from people as p
where name= ’John’)
where name=’ CS’;
An alternative to system-generated identi?ers is to allow users to generate
identi?ers. The type of the self-referential attribute must be speci?ed as part of
the type de?nition of the referenced table, and the table de?nition must specify
that thereferenceis user generated:
create type Person
(name varchar(20),
addressvarchar(20))
refusing varchar(20);
create table people of Person
refis person id user generated;
When inserting a tuple in people, we must then provide a value for the iden-
ti?er:
insert into people (person id, name, address) values
(’01284567’, ’John’, ’23 Coyote Run’);
No other tuple for people or its supertables or subtables can have the same
identi?er.Wecanthenusetheidenti?ervaluewheninsertingatupleinto depart-
ments,without theneedfor aseparatequerytoretrievetheidenti?er:
insertinto departments
values (’CS’, ’01284567’);
It is even possible to use an existing primary-key value as the identi?er, by
including the ref fromclauseinthe typede?nition:
create type Person
(name varchar(20) primarykey,
addressvarchar(20))
ref from(name);
create table people of Person
ref is person id derived;
22.7 Implementing O-R Features 963
Note that the table de?nition must specify that the reference is derived, and
must still specify a self-referential attribute name. When inserting a tuple for
departments,wecan thenuse:
insert into departments
values (’CS’, ’John’);
References are dereferenced in SQL:1999 by the ?> symbol. Consider the
departments table de?ned earlier. We can use this query to ?nd the names and
addressesof the headsof alldepartments:
select head?>name, head?>address
from departments;
Anexpressionsuch as “head?>name” iscalleda path expression.
Since head is a reference to a tuple in the people table, the attribute name
in the preceding query is the name attribute of the tuple from the people table.
Referencescanbeusedtohidejoinoperations;intheprecedingexample,without
thereferences,the head?eldof departmentwould be declared a foreign key of the
table people.To?ndthenameandaddressoftheheadofadepartment,wewould
requireanexplicitjoinoftherelationsdepartmentsandpeople.Theuseofreferences
simpli?esthe queryconsiderably.
We can use the operation deref to return the tuple pointed to by a reference,
and then access itsattributes,as shown below:
select deref(head).name
from departments;
22.7 ImplementingO-RFeatures
Object-relational database systems are basically extensions of existing relational
database systems. Changes are clearly required at many levels of the database
system.However,tominimizechangestothestorage-systemcode(relationstor-
age,indices,etc.),thecomplexdatatypessupportedbyobject-relationalsystems
can betranslatedtothe simplertypesystemof relationaldatabases.
To understand how to do this translation, we need look only at how some
features of the E-R model are translated into relations. For instance, multivalued
attributesinthe E-Rmodelcorrespondtomultiset-valuedattributesintheobject-
relational model. Composite attributes roughly correspond to structured types.
ISA hierarchies in the E-R model correspond to table inheritance in the object-
relationalmodel.
The techniques for converting E-R model features to tables, which we saw in
Section7.6, can be used,with some extensions, to translate object-relational data
to relationaldataatthe storagelevel.
964 Chapter 22 Object-Based Databases
Subtables can be stored in an ef?cient manner, without replication of all
inherited?elds,inoneof two ways:
• Each table stores the primary key (which may be inherited from a parent
table) and the attributes that are de?ned locally. Inherited attributes (other
thantheprimarykey)donotneedtobestored,andcanbederivedbymeans
of a joinwiththe supertable,basedonthe primarykey.
• Eachtablestoresallinheritedand locallyde?nedattributes.When atupleis
inserted,itisstoredonlyinthetableinwhichitisinserted,anditspresenceis
inferredineachofthesupertables.Accesstoallattributesofatupleisfaster,
since ajoinisnot required.
However, in case the type system allows an entity to be represented
in two subtables without being present in a common subtable of both, this
representation can result in replication of information. Further, it is hard
to translate foreign keys referring to a supertable into constraints on the
subtables;toimplementsuchforeignkeysef?ciently,thesupertablehastobe
de?ned as a view, and the database system would have to support foreign
keysonviews.
Implementations may choose to represent array and multiset types directly,
or may choose to use a normalized representation internally. Normalized repre-
sentationstendtotakeupmorespaceandrequireanextrajoin/groupingcostto
collectdatainanarrayormultiset.However,normalizedrepresentationsmaybe
easiertoimplement.
The ODBC and JDBC application program interfaces have been extended to
retrieve and store structured types. JDBC provides a method getObject() that
is similar to getString() but returns a Java Struct object, from which the
componentsofthestructuredtypecanbeextracted.Itisalsopossibletoassociate
a Javaclass withan SQL structuredtype,and JDBCwillthenconvertbetweenthe
types.Seethe ODBC or JDBC referencemanuals for details.
22.8 PersistentProgrammingLanguages
Database languages differ from traditional programming languages in that they
directly manipulate data that are persistent—that is, data that continue to exist
evenaftertheprogramthatcreatedithasterminated.Arelationinadatabaseand
tuplesinarelationareexamplesofpersistentdata.Incontrast,theonlypersistent
datathat traditionalprogramming languagesdirectlymanipulateare?les.
Access to a database is only one component of any real-world application.
Whileadata-manipulationlanguagelikeSQLisquiteeffectiveforaccessingdata,
a programming language is required for implementing other components of the
applicationsuchas userinterfacesorcommunication withothercomputers.The
traditional way of interfacing database languages to programming languages is
by embedding SQL withinthe programminglanguage.
22.8 Persistent Programming Languages 965
A persistent programming language is a programming language extended
withconstructstohandlepersistentdata.Persistentprogramminglanguagescan
be distinguishedfromlanguages withembedded SQL inat leasttwo ways:
1. With an embeddedlanguage, the type system of the host language usually
differs from the type system of the data-manipulation language. The pro-
grammerisresponsibleforanytypeconversionsbetweenthehostlanguage
andSQL.Havingtheprogrammercarryoutthistaskhasseveraldrawbacks:
• The code to convert between objects and tuples operates outside the
object-oriented type system, and hence has a higher chance of having
undetectederrors.
• Conversion between the object-oriented format and the relational for-
mat of tuples in the database takes a substantial amount of code. The
formattranslationcode,alongwiththecodeforloadingandunloading
datafromadatabase,canformasigni?cantpercentageofthetotalcode
requiredfor anapplication.
In contrast, in a persistent programming language, the query language is
fully integrated with the host language, and both share the same type sys-
tem. Objects can be createdand stored in the database without any explicit
type or format changes; any format changes requiredare carried out trans-
parently.
2. The programmer using an embedded query language is responsible for
writing explicit code to fetch data from the database into memory. If any
updatesareperformed,theprogrammermustwritecodeexplicitlytostore
the updateddataback inthe database.
In contrast, in a persistent programming language, the programmer can
manipulate persistent data without writing code explicitly to fetch it into
memoryor storeitback to disk.
In this section, we describe how object-oriented programming languages,
such as C++ and Java, can be extended to make them persistent programming
languages. These language features allow programmers to manipulate data di-
rectly from the programming language, without having to go through a data-
manipulation language such as SQL. They thereby provide tighter integration of
theprogramminglanguageswiththedatabasethan,forexample,embeddedSQL.
There are certain drawbacks to persistent programming languages, how-
ever, that we must keep in mind when deciding whether to use them. Since the
programming language is usually a powerful one, it is relatively easy to make
programming errors that damage the database. The complexity of the language
makesautomatichigh-leveloptimization,suchastoreducediskI/O,harder.Sup-
port for declarative querying is important for many applications, but persistent
programminglanguages currentlydonot support declarativequeryingwell.
In this section, we describe a number of conceptual issues that must be ad-
dressedwhenaddingpersistencetoanexistingprogramminglanguage.We?rst
966 Chapter 22 Object-Based Databases
address language-independent issues, and in subsequent sections we discuss is-
sues that are speci?c to the C++ language and to the Java language. However,
wedonotcoverdetailsoflanguageextensions;althoughseveralstandardshave
beenproposed,none has met universalacceptance. Seethe referencesin thebib-
liographical notes to learn more about speci?c language extensions and further
detailsof implementations.
22.8.1 Persistence ofObjects
Object-oriented programming languages already have a concept of objects, a
type system to de?ne object types, and constructs to create objects. However,
these objects are transient—they vanish when the program terminates, just as
variables in a Java or C program vanish when the program terminates. If we
wish to turn such a language into a database programming language, the ?rst
stepistoprovideawaytomakeobjectspersistent.Severalapproacheshavebeen
proposed.
• Persistence by class. The simplest, but least convenient, way is to declare
that aclass ispersistent.Allobjects ofthe classarethenpersistentobjects by
default.Objects ofnonpersistent classesarealltransient.
Thisapproachisnot ?exible,sinceitisoftenusefultohavebothtransient
andpersistentobjectsinasingleclass.Manyobject-orienteddatabasesystems
interpret declaring a class to be persistent as saying that objects in the class
potentially can be made persistent, rather than that all objects in the class
arepersistent.Suchclassesmightmoreappropriatelybecalled “persistable”
classes.
• Persistencebycreation.Inthisapproach,newsyntaxisintroducedtocreate
persistentobjects,byextendingthesyntaxforcreatingtransientobjects.Thus,
an object is either persistent or transient, depending on how it was created.
Severalobject-orienteddatabasesystemsfollowthisapproach.
• Persistence by marking. A variant of the preceding approach is to mark
objectsaspersistentaftertheyarecreated.Allobjectsarecreatedastransient
objects, but, if an object is to persist beyond the execution of the program, it
must be marked explicitly as persistent before the program terminates. This
approach, unlike the previous one, postpones the decision on persistence or
transienceuntil aftertheobject iscreated.
• Persistence by reachability. One or more objects are explicitly declared as
(root)persistentobjects.Allotherobjectsarepersistentif(andonlyif)theyare
reachablefromtherootobjectthroughasequenceofoneormorereferences.
Thus, all objects referenced by (that is, whose object identi?ers are stored
in) the root persistent objects are persistent. But also, all objects referenced
from these objects are persistent, and objects to which they refer are in turn
persistent,andso on.
A bene?t of this scheme is that it is easy to make entire data structures
persistentbymerelydeclaringtherootofsuchstructuresaspersistent.How-
22.8 Persistent Programming Languages 967
ever,thedatabasesystemhastheburdenoffollowingchainsofreferencesto
detectwhich objectsarepersistent,and that can beexpensive.
22.8.2 ObjectIdentity andPointers
In an object-orientedprogramming language that has not been extendedto han-
dle persistence, when an object is created, the system returns a transient object
identi?er. Transient object identi?ers are valid only when the program that cre-
atedthemisexecuting;afterthatprogramterminates,theobjectsaredeleted,and
the identi?er is meaningless. When a persistent object is created, it is assigned a
persistentobject identi?er.
The notion of object identity has an interesting relationship to pointers in
programming languages. A simple way to achieve built-in identity is through
pointers to physical locations in storage. In particular, in many object-oriented
languages such as C++, a transient object identi?er is actually an in-memory
pointer.
However,theassociationofanobjectwithaphysicallocationinstoragemay
change overtime.Thereareseveraldegreesofpermanenceofidentity:
• Intraprocedure. Identity persists only during the execution of a single pro-
cedure. Examples of intraprogram identity are local variables within proce-
dures.
• Intraprogram. Identity persists only during the execution of a single pro-
gram or query. Examples of intraprogram identity are global variables in
programming languages. Main-memory or virtual-memory pointers offer
only intraprogramidentity.
• Interprogram. Identity persists from one program execution to another.
Pointers to ?le-system data on disk offer interprogram identity, but they
maychange ifthe way dataisstoredin the?lesystemischanged.
• Persistent. Identity persists not only among program executions, but also
among structural reorganizations of the data. It is the persistent form of
identitythat isrequiredfor object-orientedsystems.
In persistent extensions of languages such as C++, object identi?ers for per-
sistent objects are implemented as “persistent pointers.” A persistent pointer is
a type of pointer that, unlike in-memory pointers, remains valid even after the
end of a program execution, and across some forms of data reorganization. A
programmer may use a persistent pointer in the same ways that she may use an
in-memory pointer in a programming language. Conceptually, we may think of
a persistentpointerasa pointerto anobject inthedatabase.
22.8.3 StorageandAccessofPersistentObjects
What does it mean to store an object in a database? Clearly, the data part of
an object has to be stored individually for each object. Logically, the code that
968 Chapter 22 Object-Based Databases
implements methods of a class should be stored in the database as part of the
database schema, along with the type de?nitions of the classes. However, many
implementations simply store the code in ?les outside the database, to avoid
havingtointegratesystemsoftwaresuchascompilerswiththedatabasesystem.
There are several ways to ?nd objects in the database. One way is to give
namestoobjects,justaswegivenamesto?les.Thisapproachworksforarela-
tivelysmallnumberofobjects,butdoesnotscaletomillionsofobjects.Asecond
wayistoexposeobjectidenti?ersorpersistentpointerstotheobjects,whichcan
be stored externally. Unlike names, these pointers do not have to be mnemonic,
and theycan evenbe physical pointersintoadatabase.
Athirdwayistostorecollectionsofobjects,andtoallowprogramstoiterate
overthecollectionsto?ndrequiredobjects.Collectionsofobjectscanthemselves
be modeled as objects of a collection type. Collection types include sets, multisets
(thatis,setswithpossiblymanyoccurrencesofavalue),lists,andsoon.Aspecial
caseofacollectionisaclassextent,whichisthecollectionofallobjectsbelonging
to the class. If a class extent is present for a class, then, whenever an object of
the class is created, that object is inserted in the class extent automatically, and,
wheneveranobject isdeleted,that object isremovedfromthe classextent.Class
extentsallowclassestobetreatedlikerelationsinthatwecanexamineallobjects
intheclass, justas wecan examinealltuplesina relation.
Most object-oriented database systems support all three ways of accessing
persistent objects. They give identi?ers to all objects. They usually give names
only to class extents and other collection objects, and perhaps to other selected
objects,butnottomostobjects.Theyusuallymaintainclassextentsforallclasses
that can have persistent objects, but, in many of the implementations, the class
extentscontain only persistentobjectsof theclass.
22.8.4 PersistentC++Systems
Thereareseveralobject-orienteddatabasesbasedonpersistentextensionstoC++
(see the bibliographical notes). There are differences among them in terms of
the system architecture, yet they have many common features in terms of the
programminglanguage.
Several of the object-oriented features of the C++ language provide support
for persistence without changing the language itself. For example, we can de-
clare a class called Persistent Object with attributes and methods to support
persistence; any other class that should be persistent can be made a subclass of
thisclass,andtherebyinheritthesupportforpersistence.TheC++language(like
someothermodernprogramminglanguages)alsoletsusrede?nestandardfunc-
tion names and operators—such as +, ?, the pointer dereference operator ?>,
and so on—according to the types of the operands on which they are applied.
This ability is called overloading; it is used to rede?ne operators to behave in the
requiredmanner when theyareoperatingonpersistentobjects.
Providingpersistencesupportviaclasslibrarieshasthebene?tofmakingonly
minimal changes to C++ necessary; moreover, it is relatively easy to implement.
However, it has the drawback that the programmer has to spend much more
22.8 Persistent Programming Languages 969
time to write a program that handles persistent objects, and it is not easy for
the programmer to specify integrity constraints on the schema or to provide
supportfordeclarativequerying.SomepersistentC++implementationssupport
extensionsto theC++syntax tomakethesetasks easier.
Thefollowingaspectsneedtobeaddressedwhenaddingpersistencesupport
to C++(and otherlanguages):
• Persistentpointers:Anewdatatypehastobede?nedtorepresentpersistent
pointers. For example, the ODMG C++ standard de?nes a template class
d Ref< T > to represent persistent pointers to a class T. The dereference
operatoronthisclassisrede?nedtofetchtheobjectfromdisk(ifnotalready
presentinmemory),anditreturnsanin-memorypointertothebufferwhere
the object has been fetched. Thus if p is a persistent pointer to a class T,one
can use standard syntax such as p?>A or p?>f(v) to access attribute Aof
class T or invokemethod f of class T.
The ObjectStore database system uses a different approach to persistent
pointers.Itusesnormalpointertypestostorepersistentpointers.Thisposes
two problems: (1) in-memory pointer sizes may be only 4 bytes, which is
too small to use with databases larger than 4 gigabytes, and (2) when an
object is moved on disk, in-memory pointers to its old physical location are
meaningless. ObjectStore uses a technique called “hardware swizzling” to
addressboth problems;it prefetchesobjects from the database intomemory,
andreplacespersistentpointerswithin-memorypointers,andwhendataare
storedbackondisk,in-memorypointersarereplacedbypersistentpointers.
Whenondisk,thevaluestoredinthein-memorypointer?eldisnottheactual
persistentpointer;instead,thevalueislookedupinatablethatcontains the
fullpersistentpointervalue.
• Creation of persistent objects:TheC++new operator is used to create per-
sistentobjectsbyde?ningan “overloaded”versionoftheoperatorthattakes
extraargumentsspecifyingthatitshouldbecreatedinthedatabase.Thusin-
steadofnew T(),onewouldcallnew (db) T()tocreateapersistentobject,
wheredbidenti?esthedatabase.
• Class extents: Class extents are created and maintained automatically for
each class. The ODMG C++ standard requires the name of the class to be
passed as an additional parameter to the new operation. This also allows
multipleextentsto be maintainedfora class, bypassing differentnames.
• Relationships: Relationships between classes are often represented by stor-
ing pointers from each object to the objects to which it is related. Objects
related to multiple objects of a given class store a set of pointers. Thus if a
pair of objects is in a relationship, each should store a pointer to the other.
Persistent C++ systems provide a way to specify such integrity constraints
and toenforce themby automaticallycreating anddeletingpointers:Forex-
ample, if a pointer is created from an object a to an object b,apointertoa is
addedautomaticallyto object b.
970 Chapter 22 Object-Based Databases
• Iterator interface: Since programs need to iterate over class members, an
interface is required to iterate over members of a class extent. The iterator
interfacealso allows selectionstobe speci?ed,so that only objectssatisfying
theselectionpredicateneedtobe fetched.
• Transactions:PersistentC++systemsprovidesupportforstartingatransac-
tion,and for committingit orrollingit back.
• Updates:Oneofthegoalsofprovidingpersistencesupportinaprogramming
languageistoallowtransparentpersistence.Thatis,afunctionthatoperates
on an object should not need to know that the object is persistent; the same
functionscanthusbeusedonobjectsregardlessofwhethertheyarepersistent
or not.
However, one resultant problem is that it is dif?cult to detect when
an object has been updated. Some persistent extensions to C++ require the
programmertospecifyexplicitlythatanobjecthasbeenmodi?edbycallinga
functionmark modified().Inadditiontoincreasingprogrammereffort,this
approachincreasesthechancethatprogrammingerrorscanresultinacorrupt
database.Ifaprogrammeromitsacalltomark modified(),itispossiblethat
oneupdatemadebyatransactionmayneverbepropagatedtothedatabase,
whileanotherupdatemadebythesametransactionispropagated,violating
atomicityof transactions.
Other systems, such as ObjectStore, use memory-protection support
provided by the operating system/hardware to detect writes to a block of
memory and mark the block as a dirty block that should be written later to
disk.
• Query language: Iterators provide support for simple selection queries. To
support more complex queries, persistent C++ systems de?ne a query lan-
guage.
A large number of object-oriented database systems based on C++ were de-
velopedinthelate1980sandearly1990s.However,themarketforsuchdatabases
turned out to be much smaller than anticipated, since most application require-
ments are more than met by using SQL through interfaces such as ODBC or JDBC.
As a result, most of the object-oriented database systems developed in that pe-
riod do not exist any longer. In the 1990s, the Object Data Management Group
(ODMG)de?nedstandardsforaddingpersistencetoC++andJava.However,the
group wound up its activities around 2002. ObjectStore and Versant are among
the originalobject-orienteddatabase systemsthat arestillinexistence.
Although object-oriented database systems did not ?nd the commercial suc-
cess that they had hoped for, the motivation for adding persistence to program-
ming language remains. There are several applications with high performance
requirements that run on object-oriented database systems; using SQL would
impose too high a performance overhead for many such systems. With object-
relational database systems now providing support for complex data types, in-
cluding references, it is easier to store programming language objects in an SQL
22.8 Persistent Programming Languages 971
database. A new generation of object-oriented database systems using object-
relationaldatabasesas a backend mayyetemerge.
22.8.5 PersistentJava Systems
TheJavalanguagehasseenanenormousgrowthinusageinrecentyears.Demand
for supportfor persistenceof data inJava programshas growncorrespondingly.
Initial attempts at creating a standard for persistence in Java were led by the
ODMG consortium; the consortium wound up its efforts later, but transferred its
design to the Java Database Objects (JDO) effort, which is coordinated by Sun
Microsystems.
The JDOmodelforobjectpersistenceinJavaprogramsdiffersfromthemodel
for persistencesupportinC++programs.Among itsfeaturesare:
• Persistence by reachability: Objects are not explicitly created in a database.
Explicitly registering an object as persistent (using the makePersistent()
method of the PersistenceManager class) makes the object persistent. In
addition,any object reachable froma persistentobject becomespersistent.
• Byte code enhancement: Instead of declaring a class to be persistent in the
Java code, classes whose objects may be made persistent are speci?ed in
a con?guration ?le (with suf?x .jdo). An implementation-speci?c enhancer
program is executed that reads the con?guration ?le and carries out two
tasks.First,itmaycreatestructuresinadatabasetostoreobjectsoftheclass.
Second,itmodi?esthebytecode(generatedbycompilingtheJavaprogram)
to handle tasks related to persistence. Below are some examples of such
modi?cations:
?
Any code that accesses an object could be changed to check ?rst if the
object isinmemory,and ifnot, takestepstobring itinto memory.
?
Any code that modi?es an object is modi?ed to record additionally that
the object has been modi?ed, and perhaps to save a pre-updated value
used in case the update needs to be undone (that is, if the transaction is
rolledback).
Othermodi?cationstothebytecodemayalsobecarriedout.Suchbytecode
modi?cationispossiblesincethebytecodeisstandardacrossallplatforms,
and includesmuchmoreinformationthan compiledobject code.
• Databasemapping: JDOdoesnotde?nehowdataarestoredintheback-end
database. For example, a common scenario is to store objects in a relational
database. The enhancer program may create an appropriate schema in the
database to store class objects. How exactly it does this is implementation
dependent and not de?ned by JDO.Someattributescouldbemappedto
relational attributes,while others may be storedin a serializedform, treated
as a binary object by the database. JDO implementations may allow existing
relationaldatato beviewedas objectsby de?ning anappropriatemapping.
972 Chapter 22 Object-Based Databases
• Class extents: Class extents are created and maintained automatically for
each class declared to be persistent. All objects made persistent are added
automatically to the class extent corresponding to their class. JDO programs
may access a class extent, and iterate over selected members. TheIterator
interfaceprovidedbyJavacanbeusedtocreateiteratorsonclassextents,and
to step through the members of the class extent. JDO also allows selections
to be speci?ed when an iterator is created on a class extent, and only objects
satisfyingthe selectionarefetched.
• Single reference type: There is no difference in type between a reference to
a transientobject and areferenceto a persistentobject.
One approach to achieving such a uni?cation of pointer types would
be to load the entire database into memory, replacing all persistent pointers
with in-memory pointers. After updates were done, the process would be
reversed, storing updated objects back on disk. Such an approach would be
veryinef?cient for largedatabases.
We now describe an alternative approach that allows persistent objects
to be fetched automatically into memory when required, while allowing all
referencescontainedinin-memoryobjectstobein-memoryreferences.When
an object A is fetched, a hollow object is created for each object B
i
that it
references,andthein-memorycopyof Ahasreferencestothecorresponding
hollowobjectforeach B
i
.Ofcoursethesystemhastoensurethatifanobject
B
i
was fetched already, the reference points to the already fetched object
insteadofcreatinganewhollowobject.Similarly,ifanobject B
i
hasnotbeen
fetched, but is referenced by another object fetched earlier, it would already
haveahollowobjectcreatedforit;thereferencetotheexistinghollowobject
isreused,insteadofcreating anewhollow object.
Thus, for every object O
i
that has been fetched, every reference from O
i
iseithertoanalreadyfetchedobjectortoahollowobject.Thehollowobjects
form a fringe surroundingfetchedobjects.
Whenevertheprogramactuallyaccessesa hollowobject O, theenhanced
byte code detects this and fetches the object from the database. When this
object is fetched, the same process of creating hollow objects is carried out
forallobjectsreferencedby O.Afterthistheaccesstotheobjectisallowedto
proceed.
5
Anin-memoryindexstructuremappingpersistentpointerstoin-memory
references is required to implement this scheme. In writing objects back to
disk, this index would be used to replace in-memory references with persis-
tentpointersinthecopy writtento disk.
5
Thetechniqueusinghollowobjectsdescribedaboveiscloselyrelatedtothehardwareswizzlingtechnique(mentioned
earlierinSection22.8.4).HardwareswizzlingisusedbysomepersistentC++implementationstoprovideasinglepointer
type for persistent and in-memory pointers. Hardware swizzling uses virtual-memory protection techniques provided
bytheoperatingsystemtodetectaccessestopages,andfetchesthepagesfromthedatabase whenrequired.Incontrast,
theJavaversionmodi?esbytecodetocheckforhollowobjects,insteadofusingmemoryprotection,andfetchesobjects
whenrequired, instead offetching wholepagesfrom the database.
22.10 Object-Oriented versus Object-Relational 973
22.9 Object-Relational Mapping
So far we have seen two approaches to integrating object-oriented data models
andprogramminglanguageswithdatabasesystems.Object-relationalmapping
systemsprovideathirdapproachtointegrationofobject-orientedprogramming
languages anddatabases.
Object-relational mapping systems are built on top of a traditional rela-
tional database, and allow a programmer to de?ne a mapping between tuples
in database relations and objects in the programming language. Unlike in per-
sistent programming languages, objects are transient,and thereis no permanent
object identity.
An object, or a set of objects, can be retrieved based on a selection condition
on its attributes; relevant data are retrieved from the underlying database based
ontheselectionconditions,andoneormoreobjectsarecreatedfromtheretrieved
data,basedontheprespeci?edmappingbetweenobjects andrelations.Thepro-
gram can optionally update such objects, create new objects, or specify that an
objectistobedeleted,andthenissueasavecommand;themappingfromobjects
to relations is then used to correspondingly update, insert or deletetuplesin the
database.
Object-relational mapping systems in general, and in particular the widely
used Hibernate system which providesan object-relational mapping to Java, are
describedinmoredetailinSection9.4.2.
The primary goal of object-relational mapping systems is to ease the job of
programmerswhobuildapplications,byprovidingthemanobject-model,while
retaining the bene?ts of using a robust relational database underneath. As an
added bene?t, when operating on objects cached in memory, object-relational
systems can provide signi?cant performance gains over direct access to the un-
derlyingdatabase.
Object-relational mapping systems also provide query languages that allow
programmers to write queries directly on the object model; such queries are
translated into SQL queries on the underlying relational database, and result
objects createdfromthe SQL queryresults.
On the negative side, object-relational mapping systems can suffer from sig-
ni?cant overheads for bulk database updates, and may provide only limited
querying capabilities. However, it is possible to directly update the database,
bypassing the object-relational mapping system, and to write complex queries
directlyin SQL.Thebene?tsorobject-relationalmodelsexceedthedrawbacksfor
manyapplications,andobject-relationalmappingsystemshaveseenwidespread
adoptioninrecentyears.
22.10 Object-OrientedversusObject-Relational
Wehavenowstudiedobject-relationaldatabases,whichareobject-orienteddata-
bases built on top of the relation model, as well as object-oriented databases,
which arebuilt around persistentprogramming languages, and object-relational
974 Chapter 22 Object-Based Databases
mapping systems, which build an object layer on top of a traditional relational
database.
Each of these approaches targets a different market. The declarative nature
and limited power (compared to a programming language) of the SQL language
providesgoodprotectionofdatafromprogrammingerrors,andmakeshigh-level
optimizations, such as reducing I/O, relatively easy. (We covered optimization
ofrelationalexpressionsinChapter13.)Object-relationalsystemsaimatmaking
data modeling and querying easier by using complex data types. Typical ap-
plications include storage and querying of complex data, including multimedia
data.
A declarative language such as SQL, however, imposes a signi?cant perfor-
mance penaltyforcertainkindsofapplicationsthatrunprimarilyinmainmem-
ory, and that perform a large number of accesses to the database. Persistent
programminglanguagestargetsuchapplicationsthathavehighperformancere-
quirements. They provide low-overhead access to persistent data and eliminate
theneedfordatatranslationifthedataaretobemanipulatedbyaprogramming
language. However, they are more susceptible to data corruption by program-
mingerrors,andtheyusuallydonothaveapowerfulqueryingcapability.Typical
applications include CADdatabases.
Object-relationalmappingsystemsallowprogrammerstobuildapplications
usinganobjectmodel,whileusingatraditionaldatabasesystemtostorethedata.
Thus, they combine the robustness of widely used relational database systems,
with the power of object models for writing applications. However, they suffer
from overheads of data conversion between the object model and the relational
modelusedtostoredata.
We can summarize the strengths of the various kinds of database systems in
this way:
• Relationalsystems:Simpledatatypes,powerfulquerylanguages,highpro-
tection.
• Persistentprogramminglanguage–basedOODBs:Complexdatatypes,in-
tegrationwithprogramminglanguage, highperformance.
• Object-relational systems: Complex data types, powerful query languages,
highprotection.
• Object-relational mapping systems: Complex data types integrated with
programming languages, designed as a layer on top of a relational database
system.
Thesedescriptionsholdingeneral,butkeepinmindthatsomedatabasesystems
blurtheboundaries.Forexample,object-orienteddatabasesystemsbuiltaround
apersistentprogramminglanguagecanbeimplementedontopofarelationalor
object-relationaldatabasesystem.Suchsystemsmayprovidelowerperformance
than object-oriented database systems built directly on a storage system, but
providesomeof thestrongerprotectionguarantees ofrelationalsystems.
Review Terms 975
22.11 Summary
• The object-relational data model extends the relational data model by pro-
vidingarichertypesystemincludingcollectiontypesandobjectorientation.
• Collectiontypesincludenestedrelations,sets,multisets,andarrays,andthe
object-relationalmodelpermitsattributesof a tableto becollections.
• Objectorientationprovidesinheritancewithsubtypesandsubtables,aswell
asobject (tuple)references.
• The SQL standard includes extensions of the SQL data-de?nition and query
language to deal with new data types and with object orientation. These
include support for collection-valued attributes, inheritance, and tuple ref-
erences. Such extensions attempt to preserve the relational foundations—
in particular, the declarative access to data—while extending the modeling
power.
• Object-relational database systems (that is, database systems based on the
object-relationmodel)provideaconvenientmigrationpathforusersofrela-
tional databaseswho wish touseobject-orientedfeatures.
• Persistent extensions to C++ and Java integrate persistence seamlessly and
orthogonallywithexistingprogramminglanguageconstructsandsoareeasy
touse.
• The ODMGstandardde?nesclassesand otherconstructsforcreatingandac-
cessing persistentobjects from C++, while the JDO standard providesequiv-
alentfunctionality for Java.
• Object-relational mapping systems provide an object view of data that is
stored in a relational database. Objects are transient, and there is no notion
of persistent object identity. Objects are created on-demand from relational
data, and updates to objects are implemented by updating the relational
data. Object-relational mapping systems have been widely adopted, unlike
themorelimitedadoptionof persistentprogramminglanguages.
• We discussed differences between persistent programming languages and
object-relationalsystems,andwementioncriteriaforchoosingbetweenthem.
ReviewTerms
• Nestedrelations
• Nestedrelationalmodel
• Complextypes
• Collectiontypes
• Largeobjecttypes
• Sets
• Arrays
• Multisets
• Structuredtypes
• Methods
976 Chapter 22 Object-Based Databases
• Row types
• Constructors
• Inheritance
?
Singleinheritance
?
Multipleinheritance
• Typeinheritance
• Most-speci?ctype
• Table inheritance
• Subtable
• Overlappingsubtables
• Reference types
• Scope of a reference
• Self-referentialattribute
• Pathexpressions
• Nestingand unnesting
• SQLfunctions and procedures
• Persistentprogramming
languages
• Persistenceby
?
Class
?
Creation
?
Marking
?
Reachability
• ODMG C++binding
• ObjectStore
• JDO
?
Persistenceby reachability
?
Roots
?
Hollowobjects
• Object-relationalmapping
PracticeExercises
22.1 A car-rental company maintains a database for all vehicles in its current
?eet.Forallvehicles,itincludesthevehicleidenti?cationnumber,license
number, manufacturer, model, date of purchase, and color. Special data
areincludedforcertaintypesofvehicles:
• Trucks:cargo capacity.
• Sportscars:horsepower,renteragerequirement.
• Vans:number of passengers.
• Off-road vehicles: ground clearance, drivetrain (four- or two-wheel
drive).
Construct an SQL schema de?nition for this database. Use inheritance
whereappropriate.
22.2 Consider a database schema with a relation Empwhoseattributesareas
shown below, withtypesspeci?edformultivaluedattributes.
Emp = (ename, ChildrenSet multiset(Children), SkillSet multiset(Skills))
Children = (name, birthday)
Skills = (type, ExamSet setof(Exams))
Exams = (year, city)
PracticeExercises 977
a. De?ne the above schema in SQL, with appropriate types for each
attribute.
b. Usingtheabove schema,writethe following queriesin SQL.
i. Find the names of all employees who have a child born on or
afterJanuary 1,2000.
ii. Findthoseemployeeswhotookanexaminationfortheskilltype
“typing” inthe city “Dayton”.
iii. Listallskilltypesinthe relation Emp.
22.3 Considerthe E-RdiagraminFigure22.5, which contains composite,mul-
tivalued,and derivedattributes.
a. Givean SQL schema de?nition correspondingto the E-R diagram.
b. Giveconstructors for eachof thestructuredtypesde?nedabove.
22.4 Considerthe relationalschema shown inFigure22.6.
a. Giveaschemade?nitioninSQLcorrespondingtotherelationalschema,
but using referencestoexpressforeign-keyrelationships.
b. WriteeachofthequeriesgiveninExercise6.13ontheaboveschema,
using SQL.
instructor
ID
name
?rst_name
middle_inital
last_name
address
street
street_number
street_name
apt_number
city
state
zip
{phone_number}
date_of_birth
age ( )
Figure22.5 E-R diagram with composite, multivalued, and derived attributes.
978 Chapter 22 Object-Based Databases
employee (person name, street, city)
works (person name, company name, salary)
company (company name, city)
manages (person name, manager name)
Figure22.6 Relational database for Practice Exercise 22.4.
22.5 Suppose that you have been hired as a consultant to choose a database
systemforyourclient’sapplication.Foreachofthefollowingapplications,
state what type of database system (relational, persistent programming
language–based OODB, object relational; do not specify a commercial
product)you would recommend.Justifyyour recommendation.
a. Acomputer-aideddesignsystemfor a manufacturer ofairplanes.
b. Asystemtotrackcontributionsmadetocandidatesforpublicof?ce.
c. Aninformationsystemtosupportthe makingof movies.
22.6 Howdoestheconceptofanobjectintheobject-orientedmodeldifferfrom
theconcept of anentityinthe entity-relationshipmodel?
Exercises
22.7 RedesignthedatabaseofPracticeExercise22.2into?rstnormalformand
fourthnormalform.Listanyfunctionalormultivalueddependenciesthat
you assume. Also list all referential-integrity constraints that should be
presentinthe ?rstand fourthnormal form schemas.
22.8 Considerthe schema fromPractice Exercise22.2.
a. Give SQL DDL statements to create a relation EmpA which has the
sameinformationasEmp,butwheremultiset-valuedattributesChil-
drenSet, SkillsSet and ExamsSet are replaced by array-valued at-
tributes ChildrenArray, SkillsArray and ExamsArray.
b. Write a query to convert data from the schema of Emp to that of
EmpA, with the array of children sorted by birthday, the array of
skillsbythe skilltypeand thearray of examsby theyear.
c. Writean SQLstatementtoupdatetheEmprelationbyaddingachild
Jeb, with a birthdate of February 5, 2001, to the employee named
George.
d. Write an SQL statement to perform the same update as above but
on the EmpA relation. Make sure that the array of children remains
sortedby year.
Exercises 979
person
ID
name
address
student
instructor
rank
secretary
hours_per_week
employee
salary tot_credits
Figure22.7 Specialization and generalization.
22.9 Consider the schemas for the table people,andthetablesstudents and
teachers,whichwerecreatedunderpeople,inSection22.4.Givearelational
schema in third normal form that represents the same information. Re-
call the constraints on subtables, and give all constraints that must be
imposed on the relational schema so that every database instance of the
relational schema can also be represented by an instance of the schema
withinheritance.
22.10 Explainthedistinctionbetweenatypexandareferencetyperef(x).Under
what circumstances wouldyou choose to usea referencetype?
22.11 Consider the E-R diagram in Figure 22.7, which contains specializations,
using subtypesandsubtables.
a. Givean SQL schema de?nitionof the E-Rdiagram.
b. Give an SQL query to ?nd the names of all people who are not
secretaries.
c. Give an SQL query to print the names of people who are neither
employeesnor students.
d. Canyoucreateapersonwhoisanemployeeandastudentwiththe
schemayoucreated?Explainhow,orexplainwhyitisnotpossible.
22.12 SupposeaJDOdatabasehadanobject A,whichreferencesobject B,which
inturnreferencesobjectC.Assumeallobjectsareondiskinitially.Suppose
a program ?rst dereferences A, then dereferences B by following the
reference from A, and then ?nally dereferences C. Show the objects that
980 Chapter 22 Object-Based Databases
are represented in memory after each dereference, along with their state
(hollowor ?lled,and valuesintheirreference?elds).
Tools
Thereareconsiderabledifferencesbetweendatabaseproductsintheirsupportfor
object-relationalfeatures.Oracleprobablyhasthemostextensivesupportamong
the major database vendors. The Informix database system provides support
for many object-relational features. Both Oracle and Informix provided object-
relational features before the SQL:1999 standard was ?nalized, and have some
featuresthat arenot partof SQL:1999.
Information about ObjectStore and Versant, including download of trial ver-
sions, may be obtained from their respective Web sites (objectstore.com and ver-
sant.com).TheApacheDBproject( db.apache.org) provides an object-relational
mappingtoolforJavathatsupportsbothanODMGJavaandJDOAPIs.Areference
implementationof JDOmay be obtained from sun.com; use asearchengine to get
the full URL.
BibliographicalNotes
Severalobject-orientedextensions to SQL have beenproposed. POSTGRES (Stone-
brakerandRowe[1986]andStonebraker[1986])wasanearlyimplementationof
anobject-relationalsystem.Otherearlyobject-relationalsystemsincludethe SQL
extensions of O
2
(Bancilhon et al. [1989]) and UniSQL (UniSQL [1991]). SQL:1999
wastheproductofanextensive(andlong-delayed)standardizationeffort,which
originally started off as adding object-oriented features to SQL and ended up
addingmanymorefeatures,suchasproceduralconstructs,whichwesawearlier.
Supportfor multisettypeswas addedas partof SQL:2003.
Melton[2002]concentratesontheobject-relationalfeaturesofSQL:1999.Eisen-
berg et al. [2004] provides an overview of SQL:2003, including its support for
multisets.
A number of object-oriented database systems were developed in the late
1980s and early 1990s. Among the notable commercial ones were ObjectStore
(Lamb et al. [1991]), O
2
(Lecluse et al. [1988]), and Versant. The object database
standard ODMG is described in detail in Cattell [2000]. JDO is described by Roos
[2002], Tyagi etal.[2003], andJordanand Russell[2003].
CHAPTER
23
XML
The Extensible Markup Language (XML) was not designed for database appli-
cations. In fact, like the Hyper-Text Markup Language (HTML)onwhichtheWorld
Wide Web is based, XML has its roots in document management, and is derived
from alanguage for structuring largedocuments known as the Standard General-
ized MarkupLanguage(SGML).However,unlike SGMLand HTML, XMLisdesigned
to represent data. It is particularly useful as a data format when an applica-
tion must communicate with another application, or integrate information from
several other applications. When XML is used in these contexts, many database
issues arise, including how to organize, manipulate, and query the XML data. In
this chapter, we introduce XML and discuss both the management of XML data
withdatabasetechniquesandtheexchangeofdataformattedasXMLdocuments.
23.1 Motivation
To understand XML,itisimportanttounderstanditsrootsasadocumentmarkup
language.Thetermmarkupreferstoanythinginadocumentthatisnotintended
to be part of the printed output. For example, a writer creating text that will
eventually be typeset in a magazine may want to make notes about how the
typesetting should be done. It would be important to type these notes in a way
so that they could be distinguished from the actual content, so that a note like
“set this word in large size, bold font” or “insert a line break here” does not end
up printed in the magazine. Such notes convey extra information about the text.
Inelectronicdocumentprocessing,amarkuplanguageisaformaldescriptionof
whatpartofthedocumentiscontent,whatpartismarkup,andwhatthemarkup
means.
Just as database systems evolved from physical ?le processing to provide a
separate logical view, markup languages evolved from specifying instructions
for how to print parts of the document to specifying the function of the content.
Forinstance,withfunctionalmarkup,textrepresentingsectionheadings(forthis
section,theword “Motivation”)wouldbemarkedupasbeingasectionheading,
instead of being marked up as text to be printed in large size, bold font. From
theviewpointoftypesetting,suchfunctional markupallowsthedocumenttobe
981
982 Chapter 23 XML
formatteddifferentlyindifferentsituations.Italsohelpsdifferentpartsofalarge
document, or different pages in a large Web site, to be formatted in a uniform
manner.Moreimportantly,functionalmarkupalsohelpsrecordwhateachpartof
the text represents semantically, and correspondingly helps automate extraction
ofkeypartsofdocuments.
For the family of markup languages that includes HTML, SGML,andXML,
the markup takes the form of tags enclosed in angle brackets, <>.T agsare
used in pairs, with <tag> and </tag> delimiting the beginning and the end of
the portion of the document to which the tag refers. For example, the title of a
document mightbe markedup as follows:
<title>Database System Concepts</title>
Unlike HTML, XMLdoesnotprescribethesetoftagsallowed,andthesetmay
be chosen as needed by each application. This feature is the key to XML’s major
role in data representation and exchange, whereas HTML is used primarily for
document formatting.
<university>
<department>
<dept name> Comp. Sci. </dept name>
<building> Taylor </building>
<budget> 100000 </budget>
</department>
<department>
<dept name> Biology </dept name>
<building> Watson </building>
<budget> 90000 </budget>
</department>
<course>
<course id> CS-101 </course id>
<title> Intro. to Computer Science</title>
<dept name> Comp. Sci</dept name>
<credits> 4 </credits>
</course>
<course>
<course id> BIO-301 </course id>
<title> Genetics</title>
<dept name> Biology </dept name>
<credits> 4 </credits>
</course>
continued in Figure 23.2
Figure 23.1 XML representation of (part of) university information.
23.1 Motivation 983
<instructor>
<IID> 10101 </IID>
<name> Srinivasan</name>
<dept name> Comp. Sci. </dept name>
<salary> 65000 </salary>
</instructor>
<instructor>
<IID> 83821 </IID>
<name> Brandt </name>
<dept name> Comp. Sci. </dept name>
<salary> 92000 </salary>
</instructor>
<instructor>
<IID> 76766 </IID>
<name> Crick </name>
<dept name> Biology </dept name>
<salary> 72000 </salary>
</instructor>
<teaches>
<IID> 10101 </IID>
<course id> CS-101 </course id>
</teaches>
<teaches>
<IID> 83821 </IID>
<course id> CS-101 </course id>
</teaches>
<teaches>
<IID> 76766 </IID>
<course id> BIO-301 </course id>
</teaches>
</university>
Figure 23.2 Continuation of Figure 23.1.
For example, in our running university application, department, course and
instructor information can be representedas part of an XML document as in Fig-
ures23.1and23.2.Observetheuseoftagssuchasdepartment,course,instructor,
and teaches. To keep the example short, we use a simpli?ed version of the uni-
versity schema that ignores section information for courses. We have also used
thetag IIDtodenotethe identi?eroftheinstructor,for reasons weshall seelater.
These tags provide context for each value and allow the semantics of the
value to be identi?ed. For this example, the XML data representation does not
provideanysigni?cantbene?toverthetraditionalrelationaldatarepresentation;
however,weusethisexampleasourrunningexamplebecause ofitssimplicity.
984 Chapter 23 XML
<purchase order>
<identi?er> P-101 </identi?er>
<purchaser>
<name> Cray Z. Coyote </name>
<address> Mesa Flats, Route 66, Arizona 12345, USA </address>
</purchaser>
<supplier>
<name> Acme Supplies </name>
<address> 1 Broadway, New York, NY, USA </address>
</supplier>
<itemlist>
<item>
<identi?er> RS1 </identi?er>
<description> Atom powered rocket sled </description>
<quantity> 2 </quantity>
<price> 199.95 </price>
</item>
<item>
<identi?er> SG2 </identi?er>
<description> Superb glue </description>
<quantity> 1 </quantity>
<unit-of-measure> liter</unit-of-measure>
<price> 29.95 </price>
</item>
</itemlist>
<total cost> 429.85 </total cost>
<payment terms> Cash-on-delivery </payment terms>
<shipping mode> 1-second-delivery</shipping mode>
</purchaseorder>
Figure 23.3 XML representation of a purchase order.
Figure 23.3, which shows how information about a purchase order can be
represented in XML, illustrates a more realistic use of XML. Purchase orders are
typically generated by one organization and sent to another. Traditionally they
wereprintedonpaperbythepurchaserandsenttothesupplier;thedatawouldbe
manually re-entered into a computer system by the supplier. This slow process
can be greatly sped up by sending the information electronically between the
purchaser and supplier. The nested representation allows all information in a
purchase orderto be representednaturally in a single document. (Real purchase
orders have considerably more information than that depicted in this simpli?ed
example.)XMLprovidesastandardwayoftaggingthedata;thetwoorganizations
must of course agree on what tags appear in the purchase order, and what they
mean.
23.1 Motivation 985
Compared to storage of data in a relational database, the XML representa-
tion may be inef?cient, since tag names are repeated throughout the document.
However,inspiteofthisdisadvantage,an XMLrepresentationhassigni?cantad-
vantageswhenitisusedtoexchangedatabetweenorganizations,andforstoring
complexstructuredinformationin?les:
• First, the presence of the tags makes the message self-documenting;thatis,
a schema need not be consulted to understand the meaning of the text. We
canreadilyreadthefragmentabove,forexample.
• Second,theformatofthedocumentisnotrigid.Forexample,ifsomesender
addsadditionalinformation,suchasataglast accessednotingthelastdate
onwhichanaccountwasaccessed,therecipientofthe XMLdatamaysimply
ignore the tag. As another example, in Figure 23.3, the item with identi?er
SG2hasatagcalledunit-of-measurespeci?ed,whichthe?rstitemdoesnot.
Thetagisrequiredforitemsthatareorderedbyweightorvolume,andmay
beomittedfor itemsthat aresimplyorderedbynumber.
The ability to recognize and ignore unexpected tags allows the format
of the data to evolve over time, without invalidating existing applications.
Similarly, the ability to have multiple occurrences of the same tag makes it
easytorepresentmultivaluedattributes.
• Third,XMLallowsnestedstructures.ThepurchaseordershowninFigure23.3
illustratesthebene?ts ofhavinganestedstructure.Eachpurchase orderhas
a purchaser and a list of items as two of its nested structures. Each item in
turnhasanitemidenti?er,descriptionandapricenestedwithinit,whilethe
purchaserhas aname and addressnestedwithinit.
Such information would have been split into multiple relations in a
relationalschema. Iteminformation would have beenstoredinone relation,
purchaser information in a second relation, purchase orders in a third, and
therelationshipbetweenpurchaseorders,purchasers,anditemswouldhave
beenstoredinafourthrelation.
The relational representation helps to avoid redundancy; for example,
item descriptions would be stored only once for each item identi?er in a
normalized relational schema. In the XML purchase order, however, the de-
scriptions may be repeated in multiple purchase orders that order the same
item. However, gathering all information related to a purchase order into a
single nested structure, even at the cost of redundancy, is attractive when
informationhas tobeexchanged withexternalparties.
• Finally, since the XML format is widely accepted, a wide variety of tools are
availabletoassistinitsprocessing,includingprogramminglanguage APIsto
createandtoread XMLdata,browser software,and databasetools.
We describe several applications for XML data later, in Section 23.7. Just as
SQL is the dominant language for querying relational data, XML has become the
dominant format for dataexchange.
986 Chapter 23 XML
23.2 Structure of XML Data
The fundamental construct in an XML document is the element. An element is
simplyapairofmatchingstart-andend-tagsandallthetextthatappearsbetween
them.
XML documents must have a single root element that encompasses all other
elementsinthedocument.IntheexampleinFigure23.1,the<university>element
formstherootelement.Further,elementsinanXMLdocumentmustnestproperly.
For instance:
<course>...<title>...</title>...< /course>
isproperlynested,whereas:
<course>...<title>...</course>...< /title>
isnotproperlynested.
Whilepropernestingisanintuitiveproperty,wemayde?neitmoreformally.
Textissaidtoappearinthecontextofanelementifitappearsbetweenthestart-
tag and end-tag of that element. Tags are properly nested if every start-tag has a
uniquematching end-tagthat isinthecontext of thesame parentelement.
Note that text may be mixed with the subelements of an element, as in Fig-
ure23.4.Aswithseveralotherfeaturesof XML,thisfreedommakesmoresensein
adocument-processingcontextthaninadata-processingcontext,andisnotpar-
ticularly useful for representing more-structured data such as database content
in XML.
The ability to nest elements within other elements provides an alternative
way to represent information. Figure 23.5 shows a representation of part of the
universityinformationfromFigure23.1,butwithcourseelementsnestedwithin
departmentelements.Thenestedrepresentationmakesiteasyto?ndallcourses
offered by a department. Similarly, identi?ers of courses taught by an instruc-
tor are nested within the instructor elements. If an instructor teaches more than
one course, there would be multiple course id elements within the correspond-
...
<course>
This course is being offered for the ?rst time in 2009.
<course id> BIO-399 </course id>
<title> Computational Biology </title>
<dept name> Biology </dept name>
<credits> 3 </credits>
</course>
...
Figure 23.4 Mixture of text with subelements.
23.2 Structure ofXMLData 987
<university-1>
<department>
<dept name> Comp. Sci. </dept name>
<building> Taylor </building>
<budget> 100000 </budget>
<course>
<course id> CS-101 </course id>
<title> Intro. to Computer Science</title>
<credits> 4 </credits>
</course>
<course>
<course id> CS-347 </course id>
<title> Database System Concepts </title>
<credits> 3 </credits>
</course>
</department>
<department>
<dept name> Biology </dept name>
<building> Watson </building>
<budget> 90000 </budget>
<course>
<course id> BIO-301 </course id>
<title> Genetics</title>
<credits> 4 </credits>
</course>
</department>
<instructor>
<IID> 10101 </IID>
<name> Srinivasan </name>
<dept name> Comp. Sci. </dept name>
<salary> 65000. </salary>
<course id> CS-101 </coursr id>
</instructor>
</university-1>
Figure 23.5 Nested XML representation of university information.
ing instructor element. Details of instructors Brandt and Crick are omitted from
Figure23.5for lack ofspace,but aresimilarinstructuretothat for Srinivasan.
Although nested representations are natural in XML, they may lead to re-
dundant storage of data. For example, suppose details of courses taught by an
instructorarestorednestedwithintheinstructorelementasshowninFigure23.6.
If a course is taught by more than one instructor, course information such as ti-
tle, department, and credits would be stored redundantly with every instructor
associatedwiththecourse.
988 Chapter 23 XML
<university-2>
<instructor>
<ID> 10101 </ID>
<name> Srinivasan</name>
<dept name> Comp. Sci.</dept name>
<salary> 65000 </salary>
<teaches>
<course>
<course id> CS-101 </course id>
<title> Intro. to Computer Science</title>
<dept name> Comp. Sci. </dept name>
<credits> 4 </credits>
</course>
</teaches>
</instructor>
<instructor>
<ID> 83821 </ID>
<name> Brandt </name>
<dept name> Comp. Sci.</dept name>
<salary> 92000 </salary>
<teaches>
<course>
<course id> CS-101 </course id>
<title> Intro. to Computer Science</title>
<dept name> Comp. Sci. </dept name>
<credits> 4 </credits>
</course>
</teaches>
</instructor>
</university-2>
Figure 23.6 Redundancy in nested XML representation.
NestedrepresentationsarewidelyusedinXMLdatainterchangeapplications
toavoidjoins.Forinstance,apurchaseorderwouldstorethefulladdressofsender
and receiver redundantly on multiple purchase orders, whereas a normalized
representationmayrequireajoinofpurchaseorderrecordswithacompany address
relationtogetaddressinformation.
Inadditiontoelements,XMLspeci?esthenotionofanattribute.Forinstance,
the course identi?er of a course can be represented as an attribute, as shown in
Figure 23.7. The attributes of an element appear as name=value pairs before the
closing “>” of a tag. Attributes are strings and do not contain markup. Further-
more, attributes can appear only once in a given tag, unlike subelements, which
mayberepeated.
23.2 Structure ofXMLData 989
...
<course course id= “CS-101”>
<title> Intro. to Computer Science</title>
<dept name> Comp. Sci. </dept name>
<credits> 4 </credits>
</course>
...
Figure 23.7 Use of attributes.
Notethatinadocumentconstructioncontext,thedistinctionbetweensubele-
ment and attribute is important—an attribute is implicitly text that does not
appear in the printed or displayed document. However, in database and data
exchange applications of XML, this distinction is less relevant, and the choice of
representing data as an attribute or a subelement is frequently arbitrary. In gen-
eral, it is advisable to use attributes only to represent identi?ers, and to store all
otherdataassubelements.
One?nalsyntacticnoteisthatanelementoftheform<element></element>
thatcontainsnosubelementsortextcanbeabbreviatedas<element/>;abbrevi-
atedelementsmay,however,contain attributes.
Since XML documents are designed to be exchanged between applications,
a namespace mechanism has been introduced to allow organizations to specify
globally unique names to be used as element tags in documents. The idea of a
namespaceistoprependeachtagorattributewithauniversalresourceidenti?er
(for example, a Web address). Thus, for example, if Yale University wanted to
ensure that XML documents it created would not duplicate tags used by any
business partner’s XML documents, it could prepend a unique identi?er with a
colon toeachtagname.The universitymayuse aWeb URLsuch as:
http://www.yale.edu
asauniqueidenti?er.Usinglonguniqueidenti?ersineverytagwouldberather
inconvenient,sothenamespacestandardprovidesawaytode?neanabbreviation
for identi?ers.
InFigure23.8,therootelement(university)hasanattributexmlns:yale,which
declares that yale is de?ned as an abbreviation for the URL given above. The
abbreviationcanthenbeusedinvariouselementtags,asillustratedinthe?gure.
Adocumentcanhavemorethanonenamespace,declaredaspartoftheroot
element.Differentelementscan thenbe associatedwith differentnamespaces.A
defaultnamespacecanbede?nedbyusingtheattributexmlnsinsteadofxmlns:yale
in the root element. Elements without an explicit namespace pre?x would then
belong tothe defaultnamespace.
Sometimes we need to store values containing tags without having the tags
interpretedas XMLtags.Sothat wecan doso, XMLallows this construct:
<![CDATA[<course> ···</course>]]>
990 Chapter 23 XML
<university xmlns:yale=“http://www.yale.edu”>
...
<yale:course>
<yale:course id> CS-101 </yale:course id>
<yale:title> Intro. to Computer Science</yale:title>
<yale:dept name> Comp. Sci. </yale:dept name>
<yale:credits> 4 </yale:credits>
</yale:course>
...
</university>
Figure 23.8 Unique tag names can be assigned by using namespaces.
Because it is enclosed within CDATA,thetext<course> is treated as normal
textdata,not as atag.The term CDATAstandsforcharacter data.
23.3 XML Document Schema
Databases have schemas, which are used to constrain what information can be
stored in the database and to constrain the data types of the stored information.
In contrast, by default, XML documents can be created without any associated
schema: an element may then have any subelement or attribute. While such
freedom may occasionally be acceptable given the self-describing nature of the
data format, it is not generally useful when XML documents must be processed
automatically as part of an application, or even when large amounts of related
dataaretobe formattedin XML.
Here,wedescribethe?rstschema-de?nitionlanguageincludedaspartofthe
XML standard, the Document Type De?nition, as well as its more recently de?ned
replacement, XML Schema.AnotherXMLschema-de?nitionlanguagecalledRelax
NGisalsoinuse,butwedonotcoverithere;formoreinformationonRelaxNG
seethereferencesinthebibliographical notes section.
23.3.1 Document Type De?nition
Thedocumenttypede?nition(DTD)isanoptionalpartofanXMLdocument.The
main purpose of a DTD is much like that of a schema: to constrain and type the
informationpresentinthedocument.However,theDTDdoesnotinfactconstrain
types in the sense of basic types like integer or string. Instead, it constrains only
the appearance of subelements and attributes within an element. The DTD is
primarily a list of rules for what pattern of subelements may appear within an
element.Figure23.9showsapartofanexampleDTDforauniversityinformation
document;the XMLdocumentinFigure23.1conforms tothis DTD.
Eachdeclarationisintheformofaregularexpressionforthesubelementsof
an element. Thus, in the DTD in Figure 23.9, a university element consists of one
or more course, department, or instructor elements; the | operator speci?es “or”
23.3 XMLDocumentSchema 991
<!DOCTYPE university [
<!ELEMENT university ( (department|course|instructor|teaches)+)>
<!ELEMENT department ( dept name, building, budget)>
<!ELEMENT course ( course id, title, dept name, credits)>
<!ELEMENT instructor (IID, name, dept name, salary)>
<!ELEMENT teaches (IID, course id)>
<!ELEMENT dept name( #PCDATA )>
<!ELEMENT building( #PCDATA )>
<!ELEMENT budget( #PCDATA )>
<!ELEMENT course id ( #PCDATA )>
<!ELEMENT title ( #PCDATA )>
<!ELEMENT credits( #PCDATA )>
<!ELEMENT IID( #PCDATA )>
<!ELEMENT name( #PCDATA )>
<!ELEMENT salary( #PCDATA )>
] >
Figure 23.9 Example of a DTD.
while the + operator speci?es “one or more.” Although not shown here, the ?
operatorisusedtospecify “zero or more,”whilethe?operatorisusedtospecify
anoptional element(that is, “zeroor one”).
The course element contains subelements course id, title, dept name,and
credits(inthatorder).Similarly,departmentandinstructorhavetheattributesof
theirrelationalschemade?nedassubelementsinthe DTD.
Finally,theelementscourse id,title,dept name,credits,building,budget,IID,
name,andsalary are all declared to be of type #PCDATA. The keyword #PCDATA
indicatestextdata;itderivesitsname,historically,from “parsedcharacterdata.”
Two other special type declarations are empty, which says that the element has
no contents, and any, which says that there is no constraint on the subelements
of the element; that is, any elements, even those not mentioned in the DTD,can
occurassubelementsoftheelement.Theabsenceofadeclarationforanelement
isequivalenttoexplicitlydeclaringthetypeas any.
TheallowableattributesforeachelementarealsodeclaredintheDTD.Unlike
subelements,noorderisimposedonattributes.Attributesmaybespeci?edtobe
of type CDATA, ID, IDREF,orIDREFS;thetypeCDATA simply says that the attribute
containscharacterdata,whiletheotherthreearenotsosimple;theyareexplained
in more detail shortly. For instance, the following line from a DTD speci?es that
element course has an attribute of type course id, and a value must be present
for thisattribute:
<!ATTLIST course course id CDATA #REQUIRED>
Attributesmusthaveatypedeclarationandadefaultdeclaration.Thedefault
declarationcanconsistofadefaultvaluefortheattributeor#REQUIRED,meaning
992 Chapter 23 XML
<!DOCTYPE university-3 [
<!ELEMENT university ( (department|course|instructor)+)>
<!ELEMENT department ( building, budget )>
<!ATTLIST department
dept name ID #REQUIRED >
<!ELEMENT course (title, credits )>
<!ATTLIST course
course id ID #REQUIRED
dept name IDREF #REQUIRED
instructors IDREFS #IMPLIED >
<!ELEMENT instructor ( name, salary )>
<!ATTLIST instructor
IID ID #REQUIRED >
dept name IDREF #REQUIRED >
···declarations for title, credits, building,
budget, name and salary ···
] >
Figure 23.10 DTD with ID and IDREFS attribute types.
that a value must be speci?ed for the attribute in each element, or #IMPLIED,
meaning that no default value has been provided, and the document may omit
this attribute. If an attribute has a default value, for every element that does not
specifyavalueforthe attribute,the defaultvalueis?lledinautomaticallywhen
the XMLdocumentisread.
An attribute of type ID provides a unique identi?er for the element; a value
that occurs in an ID attribute of an element must not occur in any other element
in the same document. At most one attribute of an element is permitted to be of
type ID. (We renamed the attribute ID of the instructor relation to IID in the XML
representation,inordertoavoidconfusion withthe type ID.)
An attribute of type IDREF is a reference to an element; the attribute must
containavaluethatappearsinthe IDattributeofsomeelementinthedocument.
The type IDREFSallowsalistofreferences,separatedbyspaces.
Figure23.10showsanexampleDTDinwhichidenti?ersofcourse,department
andinstructorarerepresentedbyIDattributes,andrelationshipsbetweenthemare
representedbyIDREFandIDREFSattributes.Thecourseelementsusecourse idas
theiridenti?erattribute;todoso,course idhasbeenmadeanattributeofcourse
insteadofasubelement.Additionally,eachcourseelementalsocontainsan IDREF
ofthedepartmentcorrespondingtothecourse,andanIDREFSattributeinstructors
identifying the instructors who teach the course. The department elements have
anidenti?erattributecalleddept name.Theinstructorelementshaveanidenti?er
attributecalled IID,andanIDREFattributedept nameidentifyingthedepartment
towhich theinstructor belongs.
Figure 23.11 shows an example XML document based on the DTD in Fig-
ure23.10.
23.3 XMLDocumentSchema 993
<university-3>
<department dept name=“Comp. Sci.”>
<building> Taylor </building>
<budget> 100000 </budget>
</department>
<department dept name=“Biology”>
<building> Watson </building>
<budget> 90000 </budget>
</department>
<course course id=“CS-101” dept name=“Comp. Sci”
instructors=“10101 83821”>
<title> Intro. to Computer Science</title>
<credits> 4 </credits>
</course>
<course course id=“BIO-301” dept name=“Biology”
instructors=“76766”>
<title> Genetics </title>
<credits> 4 </credits>
</course>
<instructor IID=“10101” dept name=“Comp. Sci.”>
<name> Srinivasan </name>
<salary> 65000 </salary>
</instructor>
<instructor IID=“83821” dept name=“Comp. Sci.”>
<name> Brandt </name>
<salary> 72000 </salary>
</instructor>
<instructor IID=“76766” dept name=“Biology”>
<name> Crick </name>
<salary> 72000 </salary>
</instructor>
</university-3>
Figure 23.11 XMLdatawithIDandIDREFattributes.
The ID and IDREF attributes serve the same role as reference mechanisms in
object-oriented and object-relational databases, permitting the construction of
complexdatarelationships.
Documenttypede?nitionsarestronglyconnectedtothedocumentformatting
heritage of XML. Because of this, they are unsuitable in many ways for serving
as the type structure of XML for data-processing applications. Nevertheless, a
numberofdataexchangeformatshavebeende?nedintermsof DTDs,sincethey
were part of the original standard. Here are some of the limitations of DTDsasa
schemamechanism:
994 Chapter 23 XML
• Individualtextelementsandattributescannotbetypedfurther.Forinstance,
the element balance cannot be constrained to be a positive number. The
lack of such constraints is problematic for data processing and exchange
applications, which must then contain code to verify the types of elements
and attributes.
• It is dif?cult to use the DTD mechanism to specify unordered sets of subele-
ments. Order is seldom important for data exchange (unlike document lay-
out,whereitiscrucial).Whilethecombinationofalternation(the |operation)
andthe ?orthe +operationasinFigure23.9permitsthespeci?cationofun-
ordered collections of tags, it is much more dif?cult to specify that each tag
mayonlyappearonce.
• There is a lack of typing in IDsandIDREFSs. Thus, there is no way to specify
thetypeofelementtowhichanIDREF or IDREFS attribute should refer. As a
result, the DTDinFigure23.10doesnotpreventthe “dept name”attributeof
acourse elementfromreferringtoothercourses,eventhoughthismakesno
sense.
23.3.2 XML Schema
An effort toredressthe de?ciencies of the DTDmechanism resultedin the devel-
opment of a more sophisticated schema language, XML Schema.W eprovidea
briefoverviewof XMLSchema, and then we listsome areas inwhich it improves
DTDs.
XMLSchemade?nesanumberofbuilt-intypessuchasstring,integer,decimal
date,andboolean.Inaddition,itallowsuser-de?nedtypes;thesemaybesimple
types with added restrictions, or complex types constructed using constructors
such as complexTypeand sequence.
Figures 23.12 and 23.13 show how the DTDin Figure 23.9 can be represented
byXMLSchema;wedescribebelowXMLSchemafeaturesillustratedbythe?gures.
The?rstthingtonoteisthatschemade?nitionsinXMLSchemaarethemselves
speci?edin XMLsyntax,usingavarietyoftagsde?nedby XMLSchema.Toavoid
con?ictswithuser-de?nedtags,wepre?xtheXMLSchematagwiththenamespace
pre?x “xs:”; this pre?x is associated with the XML Schema namespace by the
xmlns:xs speci?cationintherootelement:
<xs:schema xmlns:xs=“http://www.w3.org/2001/XMLSchema”>
Notethatanynamespacepre?xcouldbeusedinplaceofxs;thuswecouldreplace
all occurrences of “xs:” in the schema de?nition with “xsd:” without changing
the meaning of the schema de?nition. All types de?nedby XML Schema must be
pre?xedbythisnamespacepre?x.
The ?rst element is the root element university, whose type is speci?ed to be
UniversityType, which is declared later. The example then de?nes the types of
elements department, course, instructor,andteaches. Note that each of these
23.3 XMLDocumentSchema 995
<xs:schema xmlns:xs=“http://www.w3.org/2001/XMLSchema”>
<xs:element name=“university” type=“universityType” />
<xs:element name=“department”>
<xs:complexType>
<xs:sequence>
<xs:element name=“dept name” type=“xs:string”/>
<xs:element name=“building” type=“xs:string”/>
<xs:element name=“budget” type=“xs:decimal”/>
</xs:sequence>
</xs:complexType>
</xs:element>
<xs:element name=“course”>
<xs:element name=“course id” type=“xs:string”/>
<xs:element name=“title” type=“xs:string”/>
<xs:element name=“dept name” type=“xs:string”/>
<xs:element name=“credits” type=“xs:decimal”/>
</xs:element>
<xs:element name=“instructor”>
<xs:complexType>
<xs:sequence>
<xs:element name=“IID” type=“xs:string”/>
<xs:element name=“name” type=“xs:string”/>
<xs:element name=“dept name” type=“xs:string”/>
<xs:element name=“salary” type=“xs:decimal”/>
</xs:sequence>
</xs:complexType>
</xs:element>
continued in Figure 23.13.
Figure 23.12 XML Schema version of DTD from Figure 23.9.
is speci?ed by an element with tag xs:element, whose body contains the type
de?nition.
The type of department is de?ned to be a complex type, which is further
speci?ed to consist of a sequence of elements dept name, building,andbudget.
Anytypethathaseitherattributesornestedsubelementsmustbespeci?edtobe
acomplextype.
Alternatively,thetypeofanelementcanbespeci?edtobeaprede?nedtype
bytheattributetype;observehowtheXMLSchematypesxs:stringandxs:decimal
areusedtoconstrain thetypesofdataelementssuch asdept nameand credits.
Finally the example de?nes the type UniversityType as containing zero or
more occurrences of each of department, course, instructor,andteaches.Note
theuseofreftospecifytheoccurrenceofanelementde?nedearlier. XMLSchema
cande?netheminimumandmaximumnumberofoccurrencesofsubelementsby
996 Chapter 23 XML
<xs:element name=“teaches”>
<xs:complexType>
<xs:sequence>
<xs:element name=“IID” type=“xs:string”/>
<xs:element name=“course id” type=“xs:string”/>
</xs:sequence>
</xs:complexType>
</xs:element>
<xs:complexType name=“UniversityType”>
<xs:sequence>
<xs:element ref=“department” minOccurs=“0”
maxOccurs=“unbounded”/>
<xs:element ref=“course” minOccurs=“0”
maxOccurs=“unbounded”/>
<xs:element ref=“instructor” minOccurs=“0”
maxOccurs=“unbounded”/>
<xs:element ref=“teaches” minOccurs=“0”
maxOccurs=“unbounded”/>
</xs:sequence>
</xs:complexType>
</xs:schema>
Figure 23.13 Continuation of Figure 23.12.
usingminOccursandmaxOccurs.Thedefaultforbothminimumandmaximum
occurrences is 1, so these have to be speci?ed explicitly to allow zero or more
department, course, instructor,andteacheselements.
Attributesarespeci?edusingthexs:attributetag.Forexample,wecouldhave
de?ned dept nameas anattributeby adding:
<xs:attribute name = “dept name”/>
within the declaration of the department element. Adding the attribute use =
“required” to the above attribute speci?cation declaresthat the attribute must be
speci?ed, whereas the default value of use is optional. Attribute speci?cations
would appear directly under the enclosing complexType speci?cation, even if
elementsarenestedwithinasequencespeci?cation.
We can use the xs:complexType elementto create named complextypes;the
syntax is the same as that used for the xs:complexType element in Figure 23.12,
exceptthatweaddanattributename=typeNametothexs:complexTypeelement,
where typeNameisthenamewewishtogivetothetype.W ecanthenusethe
namedtypetospecifythetypeofanelementusingthetype attribute, just as we
usedxs:decimaland xs:stringinourexample.
Inadditiontode?ningtypes,arelationalschemaalsoallowsthespeci?cation
of constraints. XML Schema allows the speci?cation of keys and key references,
23.3 XMLDocumentSchema 997
corresponding to the primary-key and foreign-key de?nition in SQL.InSQL,a
primary-key constraint or unique constraint ensures that the attribute values
do not recur within the relation. In the context of XML, we need to specify a
scope within which values are unique and form a key. The selector is a path
expressionthatde?nesthescopefortheconstraint,and?elddeclarationsspecify
the elements or attributes that form the key.
1
To specify that dept name forms
akeyfordepartment elements under the root university element, we add the
following constraint speci?cationtothe schemade?nition:
<xs:key name = “deptKey”>
<xs:selector xpath = “/university/department”/>
<xs:?eld xpath = “dept name”/>
</xs:key>
Correspondingly a foreign-key constraint from course to department may be
de?nedasfollows:
<xs: name = “courseDeptFKey” refer=“deptKey”>
<xs:selector xpath = “/university/course”/>
<xs:?eld xpath = “dept name”/>
</xs:keyref>
Notethatthereferattributespeci?esthenameofthekeydeclarationthatisbeing
referenced, while the ?eldspeci?cationidenti?esthe referringattributes.
XML Schema offers several bene?ts over DTDs, and is widely used today.
Among thebene?ts that wehave seeninthe examplesabovearethese:
• Itallowsthetextthatappearsinelementstobeconstrainedtospeci?ctypes,
suchasnumerictypesinspeci?cformatsorcomplextypessuchassequences
ofelementsofothertypes.
• Itallowsuser-de?nedtypestobecreated.
• Itallows uniquenessand foreign-keyconstraints.
• It is integrated with namespaces to allow different parts of a document to
conform todifferentschemas.
In addition to the features we have seen, XML Schema supports several other
featuresthat DTDsdonot,suchasthese:
• It allows types to be restricted to create specialized types, for instance by
specifyingminimumand maximum values.
• Itallows complextypestobe extendedbyusingaformofinheritance.
1
We use simple path expressions here that are in a familiar syntax. XML has a rich syntax for path expressions, called
XPath,whichweexploreinSection23.4.2.
998 Chapter 23 XML
Our description of XML Schema is just an overview; to learn more about XML
Schema, seethereferencesinthebibliographical notes.
23.4 Querying and Transformation
Giventhe increasing number of applications that use XML to exchange, mediate,
andstoredata,toolsforeffectivemanagementof XMLdataarebecomingincreas-
inglyimportant.Inparticular,toolsforqueryingandtransformationof XMLdata
areessentialtoextractinformationfromlargebodiesof XMLdata,andtoconvert
data between different representations (schemas) in XML. Just as the output of a
relationalqueryisarelation,theoutputofanXMLquerycanbeanXMLdocument.
Asaresult,queryingand transformationcan becombined intoasingletool.
Inthissection, wedescribethe XPathand XQuerylanguages:
• XPath is a language for path expressions and is actually a building block for
XQuery.
• XQuery is the standard language for querying XML data. It is modeled after
SQL but is signi?cantly different, since it has to deal with nested XML data.
XQueryalsoincorporates XPathexpressions.
The XSLT language is another language designed for transforming XML.How-
ever, it is used primarily in document-formatting applications, rather in data-
management applications,sowedonot discuss itinthisbook.
The tools section at the end of this chapter provides references to software
that can beusedtoexecutequerieswrittenin XPathand XQuery.
23.4.1 Tree Model of XML
A tree model of XML data is used in all these languages. An XML document is
modeledasatree,withnodescorrespondingtoelementsandattributes.Element
nodescanhavechildnodes,whichcanbesubelementsorattributesoftheelement.
Correspondingly, each node (whether attribute or element), other than the root
element, has a parent node, which is an element. The order of elements and
attributes in the XML document is modeled by the ordering of children of nodes
ofthetree.Thetermsparent,child,ancestor,descendant,andsiblingsareusedin
the treemodelof XMLdata.
The text content of an element can be modeled as a text-node child of the
element. Elements containing text broken up by intervening subelements can
have multiple text-node children. For instance, an element containing “this is a
<bold>wonderful</bold>book”wouldhaveasubelementchildcorresponding
to the element bold and two text node children corresponding to “this is a” and
“book.” Since such structures are not commonly used in data representation,we
shall assumethatelementsdonotcontain both textand subelements.
23.4 Querying andTransformation 999
23.4.2 XPath
XPath addresses parts of an XML document by means of path expressions. The
languagecanbeviewedasanextensionofthesimplepathexpressionsinobject-
orientedandobject-relationaldatabases(seeSection22.6).Thecurrentversionof
the XPathstandardis XPath2.0,andour descriptionisbasedonthis version.
A path expression in XPath is a sequence of location steps separated by “/”
(instead of the “.” operator that separates location steps in SQL). The result of a
path expression is a set of nodes. For instance, on the document in Figure 23.11,
the XPathexpression:
/university-3/instructor/name
returnstheseelements:
<name>Srinivasan</name>
<name>Brandt</name>
Theexpression:
/university-3/instructor/name/text()
returnsthesame names,but without theenclosing tags.
Path expressions are evaluated from left to right. Like a directory hierarchy,
theinitial’/’indicatestherootofthedocument.Notethatthisisanabstractroot
“above”<university-3>that isthedocument tag.
Asapathexpressionisevaluated,theresultofthepathatanypointconsistsof
anorderedsetofnodesfromthedocument.Initially,the“current”setofelements
containsonlyonenode,theabstractroot.Whenthenextstepinapathexpression
isanelementname,suchasinstructor,theresultofthestepconsistsofthenodes
corresponding to elementsof the speci?ed name that are childrenof elements in
the currentelementset.Thesenodesthenbecome thecurrentelementsetforthe
nextstepofthepathexpressionevaluation.Thus, theexpression:
/university-3
returnsasinglenodecorrespondingtothe:
<university-3>
tag,while:
/university-3/instructor
returnsthetwo nodes correspondingto the:
1000 Chapter 23 XML
instructor
elementsthatarechildrenofthe:
university-3
node.
The result of a path expression is then the set of nodes after the last step of
pathexpressionevaluation.The nodesreturnedbyeach stepappearin thesame
orderastheirappearance inthe document.
Since multiple children can have the same name, the number of nodes in
the node set can increase or decrease with each step. Attribute values may also
beaccessed,usingthe“@”symbol.Forinstance,/university-3/course/@course id
returns a set of all values of course id attributes of course elements. By default,
IDREF linksarenot followed;we shallseehowtodealwith IDREFslater.
XPathsupportsanumber ofotherfeatures:
• Selection predicates may follow any step in a path, and are contained in
squarebrackets.Forexample,
/university-3/course[credits>= 4]
returnscourseelementswithacreditsvaluegreaterthanorequalto4,while:
/university-3/course[credits>= 4]/@course id
returnsthecourse identi?ersofthose courses.
Wecantesttheexistenceofasubelementbylistingitwithoutanycompar-
ison operation; for instance, if we removed just “>= 4” from the above, the
expression would return course identi?ers of all courses that have a credits
subelement,regardlessofitsvalue.
• XPath provides several functions that can be used as part of predicates, in-
cluding testing the position of the current node in the sibling order and the
aggregate function count(), which counts the number of nodes matched by
theexpressiontowhichitisapplied.Forexample,onthe XMLrepresentation
inFigure23.6, thepathexpression:
/university-2/instructor[count(./teaches/course)> 2]
returns instructors who teach more than two courses. Boolean connectives
and and or can be usedin predicates,while the function not(...) can be used
for negation.
• The function id(“foo”) returns the node (if any) with an attribute of type ID
and value “foo”. The function id can even be applied on sets of references,
23.4 Querying andTransformation 1001
or even strings containing multiple references separated by blanks, such as
IDREFS.Forinstance,thepath:
/university-3/course/id(@dept name)
returns all department elements referred to from the dept name attribute of
courseelements,while:
/university-3/course/id(@instructors)
returnstheinstructorelementsreferredtointheinstuctorsattributeofcourse
elements.
• The | operator allows expression results to be unioned. For example, given
datausingtheschemafromFigure23.11,wecould?ndtheunionofComputer
Scienceand Biologycoursesusing theexpression:
/university-3/course[@dept name=“Comp. Sci”] |
/university-3/course[@dept name=“Biology”]
However, the | operator cannot be nested inside other operators. It is also
worth noting that the nodes in the union are returned in the order in which
theyappearinthedocument.
• An XPath expression can skip multiple levels of nodes by using “//”.For
instance,theexpression/university-3//name?ndsallnameelementsanywhere
under the /university-3 element, regardless of the elements in which they
are contained, and regardless of how many levels of enclosing elements are
presentbetweentheuniversity-3andnameelements.Thisexampleillustrates
theabilityto?nd requireddatawithout fullknowledgeofthe schema.
• A step in the path need not just select from the children of the nodes in the
current node set. In fact, this is just one of several directions along which
a step in the path may proceed, such as parents, siblings, ancestors, and
descendants. We omit details, but note that “//”, described above, is a short
formfor specifying “all descendants,”while “..”speci?estheparent.
• The built-in function doc(name) returns the root of a named document; the
name could be a ?le name or a URL. The root returned by the function can
then be used in a path expression to access the contents of the document.
Thus, a path expression can be applied on a speci?ed document, instead of
beingappliedon thecurrentdefaultdocument.
Forexample,iftheuniversitydatainouruniversityexampleiscontained
in a ?le “university.xml”, the following path expression would return all
departmentsatthe university:
doc(“university.xml”)/university/department
1002 Chapter 23 XML
Thefunctioncollection(name)issimilartodoc,butreturnsacollectionofdoc-
umentsidenti?edbyname.Thefunctioncollectioncanbeused,forexample,
toopenan XMLdatabase,whichcanbeviewedasacollectionofdocuments;
the following element in the XPath expression would select the appropriate
document(s) fromthe collection.
Inmostofourexamples,weassumethattheexpressionsareevaluatedinthe
context of adatabase, which implicitlyprovidesacollection of “documents”
on which XPath expressions are evaluated. In such cases, we do not need to
usethe functions doc and collection.
23.4.3 XQuery
The World Wide Web Consortium (W3C) has developed XQuery as the standard
query language for XML. Our discussion is based on XQuery 1.0, which was
releasedas aW3C recommendationon23January 2007.
23.4.3.1 FLWORExpressions
XQuery queries are modeled after SQL queries, but differ signi?cantly from SQL.
Theyare organizedinto ?vesections: for,let, where, orderby,andreturn.They
arereferredtoas “FLWOR”(pronounced “?ower”)expressions,withthelettersin
FLWOR denotingthe?ve sections.
AsimpleFLWOR expression that returns course identi?ers of courses with
greaterthan3credits,shownbelow,isbasedontheXMLdocumentofFigure23.11,
which uses IDand IDREFS:
for $x in /university-3/course
let $courseId := $x/@course id
where $x/credits > 3
return <course id> { $courseId } </course id>
TheforclauseislikethefromclauseofSQL,andspeci?esvariablesthatrange
over the results of XPath expressions. When more than one variable is speci?ed,
the resultsinclude the Cartesianproduct of the possiblevalues the variables can
take,justas the SQL fromclause does.
The let clause simply allows the results of XPath expressions to be assigned
tovariablenamesforsimplicityofrepresentation.Thewhereclause,likethe SQL
where clause, performs additional tests on the joined tuples from the for clause.
The order by clause, like the SQL order by clause, allows sorting of the output.
Finally,thereturnclauseallows theconstruction ofresultsin XML.
A FLWOR query need not contain all the clauses; for example a query may
contain just the for and return clauses, and omit the let, where,andorder by
clauses. The preceding XQueryquery didnot contain an orderbyclause. In fact,
since this query is simple, we can easily do away with the let clause, and the
variable $courseId in the return clause could be replaced with $x/@course id.
Note further that, since the for clause uses XPath expressions, selections may
23.4 Querying andTransformation 1003
occur within the XPath expression.Thus, an equivalentquerymayhave only for
and returnclauses:
for $x in /university-3/course[credits> 3]
return <course id> { $x/@course id } </course id>
However,the let clause helps simplify complexqueries. Note also that variables
assignedbyletclausesmaycontainsequenceswithmultipleelementsorvalues,
if the path expression on the right-hand side returns a sequence of multiple
elementsorvalues.
Observe the use of curly brackets (“{}”)inthereturn clause. When XQuery
?ndsanelementsuchas<course id>startinganexpression,ittreatsitscontents
asregularXMLtext,exceptforportionsenclosedwithincurlybrackets,whichare
evaluated as expressions. Thus, if we omitted the curly brackets in the above re-
turnclause,theresultwouldcontainseveralcopiesofthestring“$x/@course id”
each enclosed in a course id tag. The contents within the curly brackets are,
however, treated as expressions to be evaluated. Note that this convention ap-
pliesevenifthecurlybracketsappearwithinquotes.Thus,wecouldmodifythe
above query to return an element with tag course, with the course identi?er as
anattribute,by replacingthereturnclausewiththe following:
return <course course id=“{$x/@course id}” />
XQueryprovidesanotherwayofconstructingelementsusingtheelementand
attribute constructors. For example, if the return clause in the previous query is
replacedbythefollowingreturnclause,thequerywouldreturncourseelements
with course idand dept nameas attributesand titleand creditsassubelements.
return element course {
attribute course id {$x/@course id},
attribute dept name {$x/dept name},
element title {$x/title},
element credits {$x/credits}
}
Notethat,asbefore,thecurlybracketsarerequiredtotreatastringasanexpres-
siontobeevaluated.
23.4.3.2 Joins
Joinsarespeci?edinXQuerymuchastheyareinSQL.Thejoinofcourse,instructor,
and teacheselementsinFigure23.1can bewrittenin XQuerythis way:
1004 Chapter 23 XML
for $c in /university/course,
$i in /university/instructor,
$t in /university/teaches
where $c/course id= $t/course id
and $t/IID = $i/IID
return <course instructor> { $c $i } </course instructor>
Thesamequerycanbeexpressedwiththeselectionsspeci?edas XPathselec-
tions:
for $c in /university/course,
$i in /university/instructor,
$t in /university/teaches[ $c/course id= $t/course id
and $t/IID = $i/IID]
return <course instructor> { $c $i } </course instructor>
PathexpressionsinXQueryarethesameaspathexpressionsinXPath2.0.Path
expressions may return a single value or element, or a sequence of values or
elements. In the absence of schema information, it may not be possible to infer
whether a path expression returns a single value or a sequence of values. Such
pathexpressionsmayparticipateincomparisonoperationssuchas =,<,and>=.
XQueryhasaninterestingde?nitionofcomparisonoperationsonsequences.
Forexample,theexpression$x/credits>3wouldhavetheusualinterpretationif
the resultof $x/creditsisasinglevalue,but iftheresultisasequencecontaining
multiple values, the expression evaluates to true if at least one of the values is
greater than 3. Similarly, the expression $x/credits = $y/credits evaluates to true
ifanyoneofthevaluesreturnedbythe?rstexpressionisequaltoanyoneofthe
valuesreturnedbythesecondexpression.Ifthisbehaviorisnotappropriate,the
operatorseq, ne, lt, gt, le, gecanbeusedinstead.Theseraiseanerrorifeitherof
theirinputsisasequencewithmultiplevalues.
23.4.3.3 Nested Queries
XQueryFLWORexpressionscanbenestedinthereturnclause,inordertogenerate
elementnestingsthatdonotappearinthesourcedocument.Forinstance,theXML
structure shown in Figure 23.5, with course elements nested within department
elements,canbegeneratedfromthestructureinFigure23.1bythequeryshown
inFigure23.14.
The query also introduces the syntax $d/*, which refers to all the children
of the node (or sequence of nodes) bound to the variable $d. Similarly, $d/text()
givesthe textcontent ofanelement,withoutthetags.
XQuery provides a variety of aggregate functions such as sum() and count()
that can be applied on sequences of elements or values. The function distinct-
values() applied on a sequence returns a sequence without duplication. The se-
quence(collection)ofvaluesreturnedbyapathexpressionmayhavesomevalues
repeated because they are repeated in the document, although an XPath expres-
23.4 Querying andTransformation 1005
<university-1>
{
for $d in /university/department
return
<department>
{ $d/* }
{ for $c in /university/course[dept name = $d/dept name]
return $c }
</department>
}
{
for $i in /university/instructor
return
<instructor>
{ $i/* }
{ for $c in /university/teaches[IID = $i/IID]
return $c/course id }
</instructor>
}
</university-1>
Figure 23.14 Creating nested structures in XQuery
sion result can contain at most one occurrence of each node in the document.
XQuery supports many other functions; see the references in the bibliographical
notes for more information. These functions are actually common to XPath 2.0
and XQuery,andcanbeusedinanyXPathpathexpression.
Toavoidnamespace con?icts, functions areassociatedwithanamespace:
http://www.w3.org/2005/xpath-functions
whichhasadefaultnamespacepre?xoffn.Thus,thesefunctionscanbereferred
tounambiguously as fn:sumor fn:count.
While XQuery does not provide a group by construct, aggregate queries can
bewrittenbyusingtheaggregatefunctionsonpathor FLWORexpressionsnested
withinthereturnclause.Forexample,thefollowingqueryontheuniversity XML
schema?nds thetotal salaryofallinstructors ineach department:
for $d in /university/department
return
<department-total-salary>
<dept name> { $d/dept name } </dept name>
<total salary> { fn:sum(
for $i in /university/instructor[dept name = $d/dept name]
return $i/salary
) } </total salary>
</department-total-salary>
1006 Chapter 23 XML
23.4.3.4 SortingofResults
Results can be sorted in XQuery by using the order by clause. For instance, this
queryoutputsallinstructor elementssortedbythe namesubelement:
for $i in /university/instructor
orderby $i/name
return <instructor> { $i/* } </instructor>
Tosortindescendingorder,wecan useorderby $i/name descending.
Sorting can be done at multiple levels of nesting. For instance, we can get
a nested representation of university information with departments sorted in
departmentname order,withcourses sortedbycourse identi?ers,asfollows:
<university-1> {
for $d in /university/department
orderby $d/dept name
return
<department>
{ $d/* }
{ for $c in /university/course[dept name = $d/dept name]
orderby $c/course id
return <course> { $c/* } </course> }
</department>
} </university-1>
23.4.3.5 FunctionsandTypes
XQuery provides a variety of built-in functions, such as numeric functions and
string matching and manipulationfunctions. Inaddition, XQuerysupportsuser-
de?nedfunctions.Thefollowinguser-de?nedfunctiontakesasinputaninstruc-
toridenti?er,and returnsalistofallcoursesofferedbythedepartmenttowhich
the instructorbelongs:
declarefunction local:dept courses($iid as xs:string) as element(course)* {
for $i in /university/instructor[IID = $iid],
$c in /university/courses[dept name = $i/dept name]
return $c
}
The namespace pre?x xs: used in the above example is prede?ned by XQuery
to be associated with the XML Schema namespace, while the namespace local: is
prede?nedtobe associatedwith XQuerylocal functions.
Thetypespeci?cationsforfunctionargumentsandreturnvaluesareoptional,
and may be omitted. XQuery uses the type system of XML Schema. The type
element allows elements with any tag, while element(course) allows elements
23.4 Querying andTransformation 1007
with the tag course. Types can be suf?xed with a * to indicate a sequence of
valuesofthattype;forexample,thede?nitionoffunctiondept coursesspeci?es
thereturnvalueas asequenceof courseelements.
The following query, which illustrates function invocation, prints out the
departmentcourses forthe instructor(s)namedSrinivasan:
for $i in /university/instructor[name = “Srinivasan”],
returnlocal:inst dept courses($i/IID)
XQuery performs type conversion automatically whenever required. For ex-
ample,ifanumericvaluerepresentedbyastringiscomparedtoanumerictype,
type conversion from string to the numeric type is done automatically. When an
element is passed to a function that expects a string value, type conversion to a
string is done by concatenating all the text values contained (nested) within the
element.Thus,thefunctioncontains(a,b),whichchecksifstringacontainsstring
b, can be used with its ?rst argument set to an element, in which case it checks
if the element a contains the string b nested anywhere inside it. XQuery also
provides functions to convert between types. For instance, number(x) converts a
stringtoanumber.
23.4.3.6 OtherFeatures
XQuery offers a variety of other features, such as if-then-else constructs that can
be used within return clauses, and existential and universal quanti?cation that
canbeusedinpredicatesinwhereclauses.Forexample,existentialquanti?cation
can beexpressedinthewhereclauseby using:
some $e in path satis?es P
where path is a path expression and P is a predicate that can use $e. Universal
quanti?cation can beexpressedby using everyinplaceofsome.
Forexample,to?nddepartmentswhereeveryinstructorhasasalarygreater
than $50,000,we canuse thefollowing query:
for $d in /university/department
where every $i in /university/instructor[dept name=$d/dept name]
satis?es $i/salary > 50000
return $d
Note, however, that if a department has no instructor, it will trivially satisfy the
abovecondition. Anextraclause:
and fn:exists(/university/instructor[dept name=$d/dept name])
1008 Chapter 23 XML
can be used to ensure that there is at least one instructor in the department. The
built-in function exists() used in the clause returns true if its input argument is
nonempty.
The XQJ standard provides an API to submit XQuery queries to an XML
database system and to retrieve the XML results. Its functionality is similar to
the JDBC API.
23.5 Application Program Interfaces to XML
With the wide acceptance of XML as a data representation and exchange format,
software tools are widely available for manipulation of XML data. There are two
standard models for programmatic manipulation of XML, each available for use
withanumberofpopularprogramminglanguages.BoththeseAPIscanbeusedto
parseanXMLdocumentandcreateanin-memoryrepresentationofthedocument.
They are used for applications that deal with individual XML documents. Note,
however, that they are not suitable for querying large collections of XML data;
declarative querying mechanisms such as XPath and XQuery are better suited to
thistask.
OneofthestandardAPIsformanipulatingXMLisbasedonthedocumentobject
model (DOM), which treats XML content as a tree, with each element represented
by a node, called a DOMNode. Programs may access parts of the document in a
navigational fashion, beginning withthe root.
DOM libraries are available for most common programming languages and
are even present in Web browsers, where they may be used to manipulate the
document displayed to the user. We outline here some of the interfaces and
methodsintheJava APIfor DOM, togivea?avorof DOM.
• TheJava DOM APIprovidesaninterfacecalledNode,andinterfacesElement
and Attribute,which inherit fromthe Nodeinterface.
• TheNodeinterfaceprovidesmethodssuchasgetParentNode(),getFirstChild(),
and getNextSibling(),tonavigatethe DOMtree,startingwiththe rootnode.
• Subelements of an element can be accessed by name, using getElementsBy-
TagName(name),whichreturnsalistofallchildelementswithaspeci?edtag
name; individual members of the list can be accessed by the method item(i),
which returnsthe ith elementinthe list.
• Attribute values of an element can be accessed by name, using the method
getAttribute(name).
• The text value of an element is modeled as a Text node, which is a child of
the element node; an element node with no subelements has only one such
childnode.The method getData()onthe Text nodereturnsthe textcontents.
DOM also provides a variety of functions for updating the document by adding
and deleting attribute and element children of a node, setting node values, and
soon.
23.6 Storage ofXMLData 1009
Many more details are required for writing an actual DOM program; see the
bibliographical notesforreferencestofurtherinformation.
DOMcanbeusedtoaccessXMLdatastoredindatabases,andanXMLdatabase
can be builtwith DOM asits primaryinterfacefor accessing and modifying data.
However,the DOMinterfacedoesnot supportanyformofdeclarativequerying.
The second commonly used programming interface, the Simple API for XML
(SAX)isaneventmodel,designedtoprovideacommoninterfacebetweenparsers
and applications. This API is built on the notion of event handlers, which consist
of user-speci?ed functions associated with parsing events. Parsing events corre-
spondtotherecognitionofpartsofadocument;forexample,aneventisgenerated
whenthestart-tagisfound foranelement,andanothereventisgeneratedwhen
the end-tag is found. The pieces of a document are always encountered in order
from startto?nish.
The SAX application developercreates handler functions for each event, and
registers them. When a document is read in by the SAX parser, as each event
occurs,thehandlerfunctioniscalledwithparametersdescribingtheevent(such
as element tag or text contents). The handler functions then carry out their task.
Forexample,toconstructatreerepresentingthe XMLdata,thehandlerfunctions
for an attribute or element start event could add a node (or nodes) to a partially
constructed tree. The start- and end-tag event handlers would also have to keep
track of the current node in the tree to which new nodes must be attached; the
element start event would set the new element as the node that is the point
where further child nodes must be attached. The corresponding element end
event would set the parent of the node as the current node where further child
nodesmust beattached.
SAXgenerallyrequiresmoreprogrammingeffortthanDOM,butithelpsavoid
the overhead of creating a DOM tree in situations where the application needs to
createitsowndatarepresentation.If DOMwereusedforsuchapplications,there
would beunnecessary spaceand timeoverheadfor constructing the DOM tree.
23.6 Storage of XML Data
Many applications require storage of XML data. One way to store XML data is to
storeitasdocumentsina?lesystem,whileasecondistobuildaspecial-purpose
database to store XML data. Another approach is to convert the XML data to a
relationalrepresentationandstoreitinarelationaldatabase.Severalalternatives
for storing XMLdataarebrie?youtlinedinthissection.
23.6.1 Nonrelational Data Stores
There are several alternatives for storing XML data in nonrelational data-storage
systems:
• Storein?at?les.SinceXMLisprimarilya?leformat,anaturalstoragemech-
anismissimplya?at?le.Thisapproachhasmanyofthedrawbacks,outlined
1010 Chapter 23 XML
in Chapter 1, of using ?le systems as the basis for database applications. In
particular, it lacks data isolation, atomicity, concurrent access, and security.
However, the wide availability of XML tools that work on ?le data makes it
relativelyeasytoaccessandqueryXMLdatastoredin?les.Thus,thisstorage
format maybe suf?cient forsome applications.
• Create an XML database. XML databases are databases that use XML as their
basic data model. Early XML databases implemented the Document Object
Model on a C++-based object-oriented database. This allows much of the
object-orienteddatabase infrastructure to be reused,while providinga stan-
dard XML interface. The addition of XQuery or other XML query languages
provides declarative querying. Other implementations have built the entire
XML storage and querying infrastructure on top of a storage manager that
providestransactional support.
Althoughseveraldatabasesdesignedspeci?callytostoreXMLdatahavebeen
built,buildingafull-featureddatabasesystemfromgroundupisaverycomplex
task. Such a database must support not only XML data storage and querying but
alsootherdatabasefeaturessuchastransactions,security,supportfordataaccess
from clients, and a variety of administration facilities. It makes sense to instead
use an existing database system to provide these facilities and implement XML
datastorageandqueryingeitherontopoftherelationalabstraction,orasalayer
paralleltotherelationalabstraction.WestudytheseapproachesinSection23.6.2.
23.6.2 Relational Databases
Sincerelationaldatabasesarewidelyusedinexistingapplications,thereisagreat
bene?t to be had instoring XML datainrelationaldatabases, sothat the datacan
be accessedfromexistingapplications.
Converting XMLdatatorelationalformisusuallystraightforwardifthedata
weregeneratedfromarelationalschemainthe?rstplaceand XMLisusedmerely
as a data exchange format for relational data. However, there are many appli-
cations where the XML data are not generated from a relational schema, and
translating the datatorelationalform for storage may not bestraightforward. In
particular,nestedelementsandelementsthatrecur(correspondingtoset-valued
attributes)complicatestorageofXMLdatainrelationalformat.Severalalternative
approachesareavailable,which we describebelow.
23.6.2.1 StoreasString
SmallXMLdocumentscanbestoredasstring(clob)valuesintuplesinarelational
database.LargeXMLdocumentswiththetop-levelelementhavingmanychildren
can be handled by storing each child element as a string in a separate tuple.
For instance, the XML data in Figure 23.1 could be stored as a set of tuples in
arelationelements(data), with the attribute data of each tuple storing one XML
element(department, course, instructor,orteaches)instringform.
23.6 Storage ofXMLData 1011
While the above representation is easy to use, the database system does not
know the schema of the stored elements. As a result, it is not possible to query
the data directly. In fact, it is not even possible to implement simple selections
suchas?ndingalldepartmentelements,or?ndingthedepartmentelementwith
department name “Comp. Sci.”, without scanning all tuples of the relation and
examining thestringcontents.
A partial solution to this problem is to store different types of elements in
differentrelations,andalsostorethevaluesofsomecriticalelementsasattributes
oftherelationtoenableindexing.Forinstance,inourexample,therelationswould
be department elements, course elements, instructor elements,andteaches elements,
each with an attribute data. Each relation may have extra attributes to store the
values of some subelements, such as dept name, course id,orname.Thus,aquery
that requires department elements with a speci?ed department name can be
answeredef?cientlywiththisrepresentation.Suchanapproachdependsontype
informationabout XMLdata,such as the DTDofthe data.
Somedatabasesystems,suchasOracle,supportfunctionindices,whichcan
helpavoidreplicationofattributesbetweentheXMLstringandrelationattributes.
Unlike normal indices, which are on attribute values, function indices can be
built on the result of applying user-de?ned functions on tuples. For instance, a
function index can be built on a user-de?ned function that returns the value of
the dept name subelement of the XML string in a tuple. The index can then be
usedinthe samewayas anindexona dept nameattribute.
The above approaches have the drawback that a large part of the XML in-
formation is stored within strings. It is possible to store all the information in
relationsinoneofseveralwaysthat we examinenext.
23.6.2.2 TreeRepresentation
Arbitrary XMLdatacan bemodeledasatreeand storedusingarelation:
nodes(id, parent id, type, label, value)
Each element and attribute in the XML data is given a unique identi?er. A tuple
inserted in the nodes relation for each element and attribute with its identi?er
(id), the identi?er of its parent node (parent id), the type of the node (attribute or
element), the name of the element or attribute (label), and the text value of the
elementorattribute(value).
If order information of elements and attributes must be preserved, an extra
attributepositioncanbeaddedtothenodesrelationtoindicatetherelativeposition
of the child among the children of the parent. As an exercise, you can represent
the XMLdataofFigure23.1byusingthis technique.
Thisrepresentationhastheadvantagethatall XMLinformationcanbe repre-
sented directly in relational form, and many XML queries can be translated into
relational queries and executed inside the database system. However, it has the
drawbackthateachelementgetsbrokenupintomanypieces,andalargenumber
ofjoinsarerequiredtoreassemblesubelementsintoanelement.
1012 Chapter 23 XML
23.6.2.3 Map toRelations
Inthisapproach, XMLelementswhoseschemaisknownaremappedtorelations
and attributes. Elements whose schema is unknown are stored as strings or as a
tree.
A relation is created for each element type (including subelements) whose
schemaisknownandwhosetypeisacomplextype(thatis,containsattributesor
subelements). The root element of the document can be ignored in this step if it
doesnothaveanyattributes.Theattributesoftherelationarede?nedasfollows:
• All attributes of these elements are stored as string-valued attributes of the
relation.
• Ifasubelementoftheelementisasimpletype(thatis,cannothaveattributes
orsubelements),anattributeisaddedtotherelationtorepresentthesubele-
ment. The type of the relation attribute defaults to a string value, but if the
subelementhadanXMLSchematype,acorrespondingSQLtypemaybeused.
For example, when applied to the element department in the schema
(DTD or XML Schema) of the data in Figure 23.1, the subelements dept name,
buildingandbudgetoftheelementdepartmentallbecomeattributesofarelation
department. Applying this procedure to the remaining elements,we get back
theoriginalrelationalschemathatwehaveusedinearlierchapters.
• Otherwise, a relation is created corresponding to the subelement (using the
samerulesrecursivelyonitssubelements).Further:
?
An identi?er attribute is added to the relations representing the element.
(Theidenti?erattributeisaddedonlyonceevenifanelementhasseveral
subelements.)
?
Anattributeparent idisaddedtotherelationrepresentingthesubelement,
storingtheidenti?erofitsparentelement.
?
Iforderingistobepreserved,anattribute position isaddedtotherelation
representingthe subelement.
Forexample,ifweapplytheaboveproceduretotheschemacorresponding
tothedatainFigure23.5,wegetthe following relations:
department(id, dept name, building, budget)
course(parent id, course id, dept name, title, credits)
Variantsofthisapproacharepossible.Forexample,therelationscorrespond-
ingtosubelementsthat can occurat mostonce canbe “?attened”intotheparent
relation by moving all their attributes into the parent relation. The bibliograph-
ical notes provide references to different approaches to represent XML data as
relations.
23.6 Storage ofXMLData 1013
23.6.2.4 PublishingandShreddingXMLData
When XML is used to exchange data between business applications, the data
most often originates in relational databases. Data in relational databases must
be published, that is, converted to XML form, for export to other applications.
Incoming datamust be shredded, that is, convertedback from XMLto normalized
relation form and stored in a relational database. While application code can
performthepublishingandshreddingoperations,theoperationsaresocommon
that the conversions should be done automatically, without writing application
code, where possible. Database vendors have spent a lot of effort to XML-enable
theirdatabaseproducts.
An XML-enabled database supports an automatic mechanism for publishing
relational data as XML. The mapping used for publishing data may be simple or
complex. A simple relation to XML mapping might create an XML element for
every row of a table, and make each column in that row a subelement of the
XML element. The XML schema in Figure 23.1 can be created from a relational
representationofuniversityinformation,usingsuchamapping.Suchamapping
isstraightforwardtogenerateautomatically.Suchan XMLviewofrelationaldata
canbetreatedasavirtualXMLdocument,andXMLqueriescanbeexecutedagainst
thevirtual XMLdocument.
A more complicated mapping would allow nested structures to be created.
Extensions of SQL with nested queries in the select clause have been developed
to allow easy creation of nested XML output. We outline these extensions in
Section23.6.3.
Mappings also have to be de?ned to shred XML data into a relational rep-
resentation. For XML data created from a relational representation, the mapping
required to shred the data is a straightforward inverse of the mapping used to
publishthedata.Forthegeneralcase,amappingcanbegeneratedasoutlinedin
Section23.6.2.3.
23.6.2.5 NativeStoragewithinaRelationalDatabase
SomerelationaldatabasessupportnativestorageofXML.SuchsystemsstoreXML
dataasstringsorinmoreef?cientbinaryrepresentations,withoutconvertingthe
datatorelationalform.AnewdatatypexmlisintroducedtorepresentXMLdata,
although the CLOB and BLOB data types may provide the underlying storage
mechanism. XML query languages such as XPath and XQuery are supported to
query XMLdata.
A relation with an attribute of type xml can be used to store a collection of
XML documents; each document is stored as a value of type xml in a separate
tuple.Special-purposeindicesarecreatedtoindexthe XMLdata.
Severaldatabasesystemsprovidenativesupportfor XMLdata.Theyprovide
an xml data type and allow XQuery queries to be embeddedwithin SQL queries.
AnXQueryquerycanbeexecutedonasingleXMLdocumentandcanbeembedded
within an SQL query to allow it to execute on each of a collection of documents,
witheachdocumentstoredinaseparatetuple.Forexample,seeSection30.11for
moredetailsonnative XMLsupportinMicrosoft SQL Server2005.
1014 Chapter 23 XML
<university>
<department>
<row>
<dept name> Comp. Sci. </dept name>
<building> Taylor </building>
<budget> 100000 </budget>
</row>
<row>
<dept name> Biology </dept name>
<building> Watson </building>
<budget> 90000 </budget>
</row>
</department>
<course>
<row>
<course id> CS-101 </course id>
<title> Intro. to Computer Science</title>
<dept name> Comp. Sci </dept name>
<credits> 4 </credits>
</row>
<row>
<course id> BIO-301 </course id>
<title> Genetics </title>
<dept name> Biology </dept name>
<credits> 4 </credits>
</row>
<course>
</university>
Figure 23.15 SQL/XML representation of (part of) university information.
23.6.3 SQL/XML
While XML is used widely for data interchange, structured data is still widely
stored in relational databases. There is often a need to convert relational data to
XMLrepresentation.TheSQL/XMLstandard,developedtomeetthisneed,de?nes
a standard extension of SQL, allowing the creation of nested XML output. The
standard has several parts, including a standard way of mapping SQL types to
XMLSchematypes,andastandardwaytomaprelationalschemastoXMLschemas,
as wellas SQL querylanguage extensions.
For example, the SQL/XML representation of the department relation would
haveanXMLschemawithoutermostelementdepartment,witheachtuplemapped
toan XMLelement row,andeachrelationattributemappedtoan XMLelementof
the same name (with some conventions to resolve incompatibilities with special
characters in names). An entire SQL schema, with multiple relations, can also be
mappedto XMLinasimilarfashion.Figure23.15showsthe SQL/XMLrepresenta-
23.6 Storage ofXMLData 1015
tionof (part of)the university datafrom 23.1, containing therelations department
and course.
SQL/XML adds several operators and aggregate operations to SQL to allow
the construction of XMLoutput directlyfromthe extended SQL.Thexmlelement
function can be usedto create XML elements,while xmlattributescan be used to
createattributes,as illustratedby the following query.
select xmlelement(name “course”,
xmlattributes(course id as course id, dept nameas dept name),
xmlelement(name “title”, title),
xmlelement(name “credits”, credits))
from course
The above query creates an XML element for each course, with the course
identi?er and department name represented as attributes, and title and credits
as subelements. The result would look like the course elements shown in Fig-
ure23.11,butwithouttheinstructorattribute.Thexmlattributesoperatorcreates
theXMLattributenameusingtheSQLattributename,whichcanbechangedusing
anasclause as shown.
Thexmlforestoperatorsimpli?estheconstructionof XMLstructures.Itssyn-
tax and behavior are similar to those of xmlattributes,exceptthatitcreatesa
forest (collection) of subelements, instead of a list of attributes. It takes multi-
ple arguments, creating an element for each argument, with the attribute’s SQL
name used as the XML element name. The xmlconcat operator can be used to
concatenate elementscreatedby subexpressionsintoaforest.
When the SQL value used to construct an attribute is null, the attribute is
omitted.Nullvaluesareomittedwhenthebodyofanelementisconstructed.
SQL/XML also provides an aggregate function xmlagg that creates a forest
(collection) of XML elements from the collection of values on which it is applied.
The following query creates an element for each department with a course, con-
taining as subelements all the courses in that department. Since the query has
aclausegroup by dept name, the aggregate function is applied on all courses in
eachdepartment,creatingasequenceof course id elements.
select xmlelement(name “department”,
dept name,
xmlagg (xmlforest(course id)
orderby course id))
from course
groupby dept name
SQL/XMLallowsthesequencecreatedbyxmlaggtobeordered,asillustrated
intheprecedingquery.Seethebibliographicalnotesforreferencestomoreinfor-
mationon SQL/XML.
1016 Chapter 23 XML
23.7 XML Applications
We now outline several applications of XML for storing and communicating (ex-
changing) dataandfor accessingWebservices(informationresources).
23.7.1 Storing Data with Complex Structure
Many applicationsneedtostoredatathatarestructured,butarenoteasilymod-
eled as relations. Consider, for example, user preferences that must be stored by
anapplicationsuchasabrowser.Thereareusuallyalargenumberof?elds,such
ashomepage,securitysettings,languagesettings,anddisplaysettings,thatmust
berecorded.Someofthe?eldsaremultivalued,forexample,alistoftrustedsites,
or maybe ordered lists, for example, a list of bookmarks. Applications tradition-
allyusedsometypeoftextualrepresentationtostoresuchdata.Today,amajority
ofsuchapplicationsprefertostoresuchcon?gurationinformationinXMLformat.
Theadhoctextualrepresentationsusedearlierrequireefforttodesignandeffort
to create parsers that can read the ?le and convert the data into a form that a
programcan use.The XMLrepresentationavoidsboththesesteps.
XML-basedrepresentationsarenowwidelyusedforstoringdocuments,spre-
adsheetdataandotherdatathatarepartofof?ceapplicationpackages.TheOpen
Document Format (ODF), supported by the Open Of?ce software suite as well
as other of?ce suites, and the Of?ce Open XML (OOXML) format, supported by
theMicrosoftOf?cesuite,aredocumentrepresentationstandardsbasedon XML.
Theyarethetwomostwidelyusedformatsforeditabledocumentrepresentation.
XML is also used to represent data with complex structure that must be ex-
changedbetweendifferentpartsofanapplication.Forexample,adatabasesystem
mayrepresentaqueryexecutionplan(arelational-algebraexpressionwithextra
informationonhowtoexecuteoperations)byusing XML.Thisallowsonepartof
the system to generate the query execution plan and another part to display it,
without using a shared data structure. For example, the data may be generated
at aserversystemand senttoaclient systemwhere thedataaredisplayed.
23.7.2 Standardized Data Exchange Formats
XML-basedstandardsforrepresentationofdatahavebeendevelopedforavariety
of specialized applications, ranging from business applications such as banking
and shipping to scienti?c applications such as chemistry and molecular biology.
Someexamples:
• The chemical industry needs information about chemicals, such as their
molecularstructure,andavarietyofimportantproperties,suchasboilingand
melting points, calori?c values, and solubility in various solvents. ChemML
isastandardfor representingsuch information.
• In shipping, carriers of goods and customs and tax of?cials need shipment
recordscontainingdetailedinformationaboutthegoodsbeingshipped,from
23.7 XMLApplications 1017
whom and to where they were sent, to whom and to where they are being
shipped,themonetary valueofthe goods,and soon.
• Anonlinemarketplaceinwhichbusinesscanbuyandsellgoods[aso-called
business-to-business(B2B)market]requiresinformationsuchasproductcata-
logs,includingdetailedproductdescriptionsandpriceinformation,product
inventories, quotes for a proposed sale, and purchase orders. For example,
theRosettaNetstandardsfore-businessapplicationsde?ne XMLschemasand
semanticsfor representingdataas wellasstandardsfor messageexchange.
Using normalized relational schemas to model such complex data require-
mentswouldresultinalargenumberofrelationsthatdonotcorresponddirectly
totheobjectsthatarebeingmodeled.Therelationswouldoftenhavelargenum-
bers of attributes; explicit representation of attribute/elementnames along with
values in XML helps avoid confusion between attributes. Nested element repre-
sentations help reduce the number of relations that must be represented,as well
as the number of joins required to get required information, at the possible cost
ofredundancy.Forinstance,inouruniversityexample,listingdepartmentswith
course elementsnestedwithin departmentelements,as in Figure 23.5, resultsin
aformatthatismorenaturalforsomeapplications—inparticular,forhumansto
read—than isthenormalizedrepresentationinFigure23.1.
23.7.3 Web Services
Applicationsoftenrequiredatafromoutsideoftheorganization,orfromanother
department in the same organization that uses a different database. In many
such situations, the outside organization or department is not willing to allow
direct access to its database using SQL, but is willing to provide limited forms of
informationthrough prede?nedinterfaces.
When the information is to be used directly by a human, organizations pro-
vide Web-based forms, where users can input values and get back desired in-
formation in HTML form. However, there are many applications where such in-
formation needs to be accessed by software programs, rather than by end users.
Providing the results of a query in XML form is a clear requirement. In addition,
itmakessensetospecifytheinput valuestothequeryalsoin XMLformat.
Ineffect,theprovideroftheinformationde?nesprocedureswhoseinputand
output are both in XML format. The HTTP protocol is used to communicate the
inputandoutputinformation,sinceitiswidelyusedandcangothrough?rewalls
that institutionsusetokeepout unwanted traf?c fromtheInternet.
The SimpleObject Access Protocol (SOAP)de?nesa standard for invoking
procedures, using XML for representing the procedure input and output. SOAP
de?nes a standard XML schema for representing the name of the procedure, and
resultstatusindicatorssuchasfailure/errorindicators.Theprocedureparameters
andresultsareapplication-dependent XMLdataembeddedwithinthe SOAP XML
headers.
Typically, HTTP is used as the transport protocol for SOAP, but a message-
based protocol (such as email over the SMTP protocol) may also be used. The
1018 Chapter 23 XML
SOAP standard is widely used today. For example, Amazon and Google provide
SOAP-basedprocedurestocarryoutsearchandotheractivities.Theseprocedures
can be invoked by other applications that provide higher-level services to users.
The SOAP standard is independent of the underlying programming language,
and it is possible for asite running one language, such as C#, to invoke a service
that runs onadifferentlanguage,suchas Java.
A site providing such a collection of SOAP procedures is called a Web ser-
vice. Several standards have been de?ned to support Web services. The Web
Services Description Language (WSDL) is a language used to describe a Web
service’s capabilities. WSDL provides facilities that interface de?nitions (or func-
tionde?nitions)provideinatraditionalprogramminglanguage,specifyingwhat
functionsareavailableandtheirinputandoutputtypes.InadditionWSDLallows
speci?cation of the URL and network port number to be used to invoke the Web
service. There is also a standard called Universal Description, Discovery, and
Integration (UDDI) that de?nes how a directory of available Web services may
be created and how a program may search in the directory to ?nd a Web service
satisfyingitsrequirements.
The following example illustrates the value of Web services. An airline may
de?neaWebserviceprovidingasetofproceduresthatcanbeinvokedbyatravel
Website;thesemayincludeproceduresto?nd?ightschedulesandpricinginfor-
mation,aswellastomake?ightbookings.ThetravelWebsitemayinteractwith
multiple Web services, provided by different airlines, hotels, and other compa-
nies,toprovidetravelinformationtoacustomerandtomaketravelbookings.By
supporting Web services, the individual companies allow a useful service to be
constructed on top, integrating the individualservices.Userscan interact with a
singleWebsitetomaketheirtravelbookings,withouthavingtocontactmultiple
separateWebsites.
To invoke a Web service, a client must prepare an appropriate SOAP XML
message and send it to the service; when it gets the result encoded in XML,the
clientmustthenextractinformationfromthe XMLresult.Therearestandard APIs
in languages such as Java and C# to create and extract information from SOAP
messages.
See the bibliographical notes for references to more information on Web ser-
vices.
23.7.4 Data Mediation
Comparison shopping is an example of a mediation application, in which data
aboutitems,inventory,pricing,andshippingcostsareextractedfromavarietyof
Websitesofferingaparticularitemforsale.Theresultingaggregatedinformation
issigni?cantlymorevaluablethantheindividualinformationofferedbyasingle
site.
A personal ?nancial manager is a similar application in the context of bank-
ing. Consider a consumer with a variety of accounts to manage, such as bank
accounts, credit-card accounts, and retirement accounts. Suppose that these ac-
counts may be held at different institutions. Providing centralized management
23.8 Summary 1019
forallaccountsofacustomerisamajorchallenge.XML-based mediation ad-
dressesthe problem by extracting an XML representationof account information
from the respective Web sites of the ?nancial institutions where the individual
holdsaccounts.Thisinformationmaybeextractedeasilyiftheinstitutionexports
itinastandard XMLformat,forexample,asaWebservice.Forthose thatdonot,
wrapper software is used to generate XML data from HTML Web pages returned
by the Web site. Wrapper applications need constant maintenance, since they
depend on formatting details of Web pages, which change often. Nevertheless,
thevalueprovidedbymediationoftenjusti?estheeffortrequiredtodevelopand
maintainwrappers.
Once the basic tools are available to extract information from each source, a
mediator application is used to combine the extracted information under a single
schema. This may requirefurther transformation of the XML datafrom each site,
since different sites may structure the same information differently. They may
also use different names for the same information (for instance, acct number
and account id), or may even use the same name for different information. The
mediatormustdecideonasingleschemathatrepresentsallrequiredinformation,
andmustprovidecodetotransformdatabetweendifferentrepresentations.Such
issues are discussed in more detail in Section 19.8, in the context of distributed
databases. XMLquerylanguagessuchas XSLTand XQueryplayanimportantrole
inthetaskoftransformation betweendifferent XMLrepresentations.
23.8 Summary
• Like the Hyper-Text Markup Language (HTML)onwhichtheWebisbased,
theExtensibleMarkupLanguage(XML)isderivedfromtheStandardGener-
alizedMarkupLanguage(SGML). XMLwasoriginallyintendedforproviding
functional markup for Web documents, but has now become the de facto
standarddataformat fordataexchange betweenapplications.
• XML documents contain elements with matching starting and ending tags
indicating the beginning and end of an element.Elementsmay have subele-
ments nested within them, to any level of nesting. Elements may also have
attributes. The choice between representing information as attributes and
subelementsis oftenarbitrary inthe context ofdatarepresentation.
• Elements may have an attribute of type ID that stores a unique identi?er for
the element. Elements may also store references to other elements by using
attributesoftypeIDREF.AttributesoftypeIDREFScanstorealistofreferences.
• Documentsoptionallymayhavetheirschemaspeci?edbyaDocumentType
Declaration (DTD). The DTD of a document speci?es what elements may
occur,howtheymaybenested,andwhatattributeseachelementmayhave.
Although DTDs are widely used, they have several limitations. For instance,
theydonot provideatypesystem.
• XMLSchemaisnowthestandardmechanismforspecifyingtheschemaofan
XMLdocument.Itprovidesalargesetofbasictypes,aswellasconstructsfor
1020 Chapter 23 XML
creating complex types and specifying integrity constraints, including key
constraints and foreign-key(keyref)constraints.
• XMLdatacan berepresentedastreestructures,withnodescorrespondingto
elements and attributes. Nesting of elements is re?ected by the parent-child
structureofthe treerepresentation.
• Path expressions can be used to traverse the XML tree structure and locate
data. XPathisastandardlanguageforpathexpressions,andallowsrequired
elements to be speci?ed by a ?le-system-like path, and additionally allows
selections and other features. XPath also forms part of other XML query lan-
guages.
• The XQuerylanguageisthestandardlanguageforquerying XMLdata.Ithas
astructurenotunlike SQL,withfor,let,where,orderby,andreturnclauses.
However, it supports many extensions to deal with the tree nature of XML
andtoallowforthetransformationof XMLdocumentsintootherdocuments
withasigni?cantlydifferentstructure. XPathpathexpressionsformapartof
XQuery. XQuerysupportsnestedqueriesand user-de?nedfunctions.
• TheDOMandSAX APIsarewidelyusedforprogrammaticaccesstoXMLdata.
These APIs are available from a variety of programming languages.
• XML data can be stored in any of several different ways. XML data may also
bestoredin?lesystems,orinXMLdatabases,whichuseXMLastheirinternal
representation.
• XML data can be stored as strings in a relational database. Alternatively,
relations can represent XML data as trees. As another alternative, XML data
canbemappedtorelationsinthesameway that E-Rschemasaremappedto
relationalschemas.NativestorageofXMLinrelationaldatabasesisfacilitated
byaddinganxmldatatypeto SQL.
• XML is used in a variety of applications, such as storing complex data, ex-
changeofdatabetweenorganizationsinastandardizedform,datamediation,
and Web services. Web services provide a remote-procedure call interface,
with XMLasthemechanismforencodingparametersaswellasresults.
Review Terms
• ExtensibleMarkup Language
(XML)
• Hyper-TextMarkupLanguage
(HTML)
• StandardGeneralizedMarkup
Language
• Markuplanguage
• Tags
• Self-documenting
• Element
• Root element
• Nestedelements
• Attribute
PracticeExercises 1021
• Namespace
• Defaultnamespace
• Schemade?nition
• Document TypeDe?nition
(DTD)
?
ID
?
IDREF and IDREFS
• XMLSchema
?
Simpleand complextypes
?
Sequencetype
?
Keyand keyref
?
Occurrence constraints
• Tree model of XMLdata
• Nodes
• Queryingand transformation
• Pathexpressions
• XPath
• XQuery
?
FLWOR expressions
null for
null let
null where
null orderby
null return
?
Joins
?
Nested FLWOR expression
?
Sorting
• XML API
• Document Object Model(DOM)
• Simple API for XML(SAX)
• Storageof XMLdata
?
Innonrelational datastores
?
Inrelationaldatabases
null Storeasstring
null Treerepresentation
null Map torelations
null Publishand shred
null XML-enableddatabase
null Nativestorage
null SQL/XML
• XMLapplications
?
Storingcomplexdata
?
Exchange ofdata
?
Datamediation
?
SOAP
?
Webservices
Practice Exercises
23.1 Give an alternative representation of university information containing
thesamedataasinFigure23.1,butusingattributesinsteadofsubelements.
Alsogivethe DTDor XMLSchemaforthisrepresentation.
23.2 Give the DTD or XML Schema for an XML representation of the following
nested-relationalschema:
Emp = (ename, ChildrenSetsetof(Children), SkillsSet setof(Skills))
Children = (name, Birthday)
Birthday = (day, month, year)
Skills = (type, ExamsSetsetof(Exams))
Exams = (year, city)
1022 Chapter 23 XML
<!DOCTYPE bibliography [
<!ELEMENT book (title, author+, year, publisher, place?)>
<!ELEMENT article (title, author+, journal, year, number, volume, pages?)>
<!ELEMENT author ( last name, ?rst name)>
<!ELEMENT title ( #PCDATA )>
···similar PCDATA declarations for year, publisher, place, journal, year,
number, volume, pages, last name and ?rst name
] >
Figure 23.16 DTD for bibliographical data.
23.3 Write a query in XPath on the schema of Practice Exercise 23.2 to list all
skilltypesin Emp.
23.4 Write a query in XQueryonthe XMLrepresentationinFigure23.11to?nd
thetotal salaryofallinstructors ineach department.
23.5 Write a query in XQuery on the XML representation in Figure 23.1 to
computetheleftouterjoinofdepartmentelementswithcourseelements.
(Hint:Useuniversalquanti?cation.)
23.6 Write queries in XQuery to output department elements with associated
course elements nested within the department elements, given the uni-
versityinformationrepresentationusing IDand IDREFSinFigure23.11.
23.7 Give a relational schema to represent bibliographical information speci-
?edaccordingtothe DTDfragmentinFigure23.16.Therelationalschema
mustkeeptrackoftheorderofauthorelements.Youcanassumethatonly
booksand articlesappearastop-levelelementsin XMLdocuments.
23.8 Show the tree representation of the XML data in Figure 23.1, and the
representation of the tree using nodes and child relations described in
Section23.6.2.
23.9 Considerthe followingrecursive DTD:
<!DOCTYPE parts [
<!ELEMENT part (name, subpartinfo*)>
<!ELEMENT subpartinfo (part, quantity)>
<!ELEMENT name ( #PCDATA )>
<!ELEMENT quantity ( #PCDATA )>
] >
a. Giveasmallexampleofdatacorrespondingtothis DTD.
b. Show how to map this DTD to a relational schema. You can assume
that part names are unique; that is, wherever a part appears, its
subpart structurewillbe thesame.
c. Createaschemain XMLSchema correspondingtothis DTD.
Exercises 1023
Exercises
23.10 Show, by giving a DTD, how to represent the non-1NF books relation from
Section22.2,using XML.
23.11 WritethefollowingqueriesinXQuery,assumingtheschemafromPractice
Exercise23.2.
a. Findthenamesofallemployeeswhohaveachildwhohasabirthday
inMarch.
b. Find those employees who took an examination for the skill type
“typing”inthecity “Dayton”.
c. Listall skilltypesin Emp.
23.12 Consider the XML data shown in Figure 23.3. Suppose we wish to ?nd
purchaseordersthatorderedtwoormorecopiesofthepartwithidenti?er
123. Considerthefollowingattempttosolvethisproblem:
for $p in purchaseorder
where $p/part/id = 123 and $p/part/quantity >= 2
return $p
Explainwhythequerymayreturnsomepurchaseordersthatorderless
thantwocopies ofpart 123.Giveacorrectversionoftheabovequery.
23.13 Give a query in XQuery to ?ip the nesting of data from Exercise 23.10.
That is, at the outermost level of nesting the output must have elements
correspondingtoauthors,andeachsuchelementmusthavenestedwithin
ititemscorrespondingtoallthebooks writtenby theauthor.
23.14 Givethe DTDforan XMLrepresentationoftheinformationinFigure7.29.
Create a separate element type to represent each relationship, but use ID
and IDREF toimplementprimaryand foreignkeys.
23.15 Givean XMLSchema representationofthe DTDfromExercise23.14.
23.16 WritequeriesinXQueryonthebibliographyDTDfragmentinFigure23.16,
todothefollowing:
a. Find all authors who have authored a book and an article in the
sameyear.
b. Displaybooksand articlessortedbyyear.
c. Displaybooks withmorethanone author.
d. Findallbooksthatcontaintheword “database”intheirtitleandthe
word “Hank” inanauthor’s name (whether?rst or last).
1024 Chapter 23 XML
23.17 Give a relational mapping of the XML purchase order schema illustrated
in Figure 23.3, using the approach described in Section 23.6.2.3. Suggest
how to remove redundancy in the relational schema, if item identi?ers
functionallydeterminethedescriptionandpurchaseandsuppliernames
functionally determinethepurchase and supplieraddress,respectively.
23.18 Write queries in SQL/XML to convert university data from the relational
schemawehaveusedinearlierchapterstotheuniversity-1anduniversity-2
XMLschemas.
23.19 As in Exercise 23.18, write queries to convert university data to the
university-1anduniversity-2XMLschemas,butthistimebywritingXQuery
queriesonthe default SQL/XML databaseto XMLmapping.
23.20 One way to shred an XML document is to use XQuery to convert the
schema to an SQL/XML mapping of the corresponding relational schema,
andthenusetheSQL/XMLmappinginthebackwarddirectiontopopulate
therelation.
As an illustration, give an XQuery query to convert data from the
university-1 XMLschematothe SQL/XML schemashown inFigure23.15.
23.21 Considertheexample XMLschemafromSection23.3.2,andwrite XQuery
queriestocarryout the following tasks:
a. Checkifthekeyconstraint shown inSection23.3.2holds.
b. Checkifthekeyrefconstraint shown inSection23.3.2holds.
23.22 Consider Practice Exercise 23.7, and suppose that authors could also ap-
pear as top-level elements. What change would have to be done to the
relational schema?
Tools
A number of toolsto deal with XML are available in the public domain. The W3C
Web site www.w3.org has pages describing the various XML-related standards, as
wellaspointerstosoftwaretoolssuchaslanguageimplementations.Anextensive
list of XQuery implementations is available at www.w3.org/XML/Query.SaxonD
(saxon.sourceforge.net)andGalax( http://www.galaxquery.org/) are useful as learning
tools, although not designed to handle large databases. Exist (exist-db.org)isan
open source XML database, supporting a variety of features. Several commercial
databases, including IBM DB2, Oracle, and Microsoft SQL Server support XML
storage,publishingusing various SQL extensions,and queryingusing XPathand
XQuery.
Bibliographical Notes
The World Wide Web Consortium (W3C)actsasthestandardsbodyforW eb-
relatedstandards,includingbasic XMLandallthe XML-relatedlanguagessuchas
Bibliographical Notes 1025
XPath, XSLT,andXQuery. A large number of technical reports de?ning the XML-
relatedstandardsareavailableat www.w3.org.Thissitealsocontainstutorialsand
pointerstosoftwareimplementingthe variousstandards.
The XQuery language derives from an XML query language called Quilt;
Quilt itself included features from earlier languages such as XPath, discussed
in Section 23.4.2, and two other XML query languages, XQL and XML-QL. Quilt is
described in Chamberlin et al. [2000]. Deutsch et al. [1999] describes the XML-QL
language. The W3C issued a candidate recommendation for an extension of XQuery
inmid-2009that includesupdates.
Katzetal.[2004]providesdetailedtextbookcoverageof XQuery.The XQuery
speci?cation may be found at www.w3.org/TR/xquery. Speci?cations of XQuery ex-
tensions,includingtheXQueryUpdatefacilityandtheXQueryScriptingExtension
arealsoavailableatthissite.IntegrationofkeywordqueryingintoXMLisoutlined
by Florescuetal.[2000] andAmer-Yahiaetal. [2004].
Funderburk et al. [2002a], Florescu and Kossmann [1999], Kanne and Mo-
erkotte [2000], and Shanmugasundaram et al. [1999] describe storage of XML
data. Eisenberg and Melton [2004a] provides an overview of SQL/XML, while
Funderburketal.[2002b]providesoverviewsofSQL/XMLandXQuery.SeeChap-
ters28through30formoreinformationonXMLsupportincommercialdatabases.
Eisenberg and Melton [2004b] provides an overview of the XQJ API for XQuery,
whilethe standardde?nitionmay befound online at http://www.w3.org/TR/xquery.
XML Indexing, Query Processing and Optimization: Indexing of XML data, and
query processing and optimization of XML queries, has been an area of great
interest in the past few years. A large number of papers have been published in
thisarea.Oneofthechallengesinindexingisthatqueriesmayspecifyaselection
on a path, such as /a/b//c[d=“CSE”]; the index must support ef?cient retrieval of
nodesthatsatisfythepathspeci?cationandthevalueselection.Workonindexing
of XMLdataincludesPaletal.[2004]andKaushiketal.[2004].Ifdataisshredded
and stored in relations, evaluating a path expression maps to computation of a
join.Severaltechniqueshavebeenproposedforef?cientlycomputingsuchjoins,
in particular when the path expression speci?es any descendant (//). Several
techniques for numbering of nodes in XML data have been proposed that can be
used to ef?ciently check if a node is a descendant of another; see, for example,
O’Neil et al. [2004]. Work on optimizationof XML queriesincludes McHugh and
Widom [1999], Wuetal.[2003]and Krishnaprasadetal. [2004].
This page intentionally left blank 
PART
8
ADVANCEDTOPICS
Chapter 24 covers a number of advanced topics in application development,
starting with performance tuning to improveapplication speed.It then discusses
standard benchmarks that are used as measures of commercial database-system
performance. Issues in application development, such as application testing and
applicationmigrationarediscussednext.Thechapterconcludeswithanoverview
of the standardization process and existing database-language standards.
Chapter 25 describes spatial and temporal data types, and multimedia data,
andtheissuesinstoringsuchdataindatabases.Databaseissuesrelatedtomobile
computing systems are also described in this chapter.
Finally, Chapter 26 describes several advanced transaction-processing tech-
niques, including transaction-processing monitors, transactional work?ows, and
transaction processing issues in electronic commerce. The chapter then discusses
main-memorydatabasesystemsandreal-timetransactionsystems,andconcludes
with a discussion of long-duration transactions.
1027
This page intentionally left blank 
CHAPTER
24
Advanced Application
Development
Thereareanumberoftasksinapplicationdevelopment.WesawearlierinChap-
ters7to9howtodesignandbuildanapplication.Oneoftheaspectsofapplication
designistheperformanceoneexpectsoutoftheapplication.Infact,itiscommon
to ?nd that once an application has been built, it runs slower than the designers
wanted, or handles fewer transactions per second than they required. An appli-
cation that takes an excessive amount of time to perform requested actions can
cause userdissatisfactionat bestand becompletelyunusable at worst.
Applications can be made to run signi?cantly faster by performance tuning,
which consists of ?nding and eliminating bottlenecks and adding appropriate
hardwaresuchasmemoryordisks.Therearemanythings anapplicationdevel-
oper can do to tune the application, and there are things that a database-system
administratorcan dotospeedup processingfor anapplication.
Benchmarks are standardized sets of tasks that help to characterize the per-
formanceofdatabasesystems.Theyareusefultogetaroughideaofthehardware
andsoftwarerequirementsofanapplication,evenbeforetheapplicationisbuilt.
Applications must be tested as they are being developed. Testing requires
generationofdatabasestatesandtestinputs,andverifyingthattheoutputsmatch
the expected outputs. We discuss issues in application testing. Legacy systems
areapplicationsystemsthat areoutdatedandusuallybasedonolder-generation
technology.However,theyareoftenatthecoreoforganizations,andrunmission-
criticalapplications.Weoutlineissuesininterfacingwithandissuesinmigrating
away from legacysystems,replacingthem withmoremodernsystems.
Standards are very important for application development, especially in the
age of the Internet, since applications need to communicate with each other
to perform useful tasks. A variety of standards have been proposed that affect
database-applicationdevelopment.
24.1 PerformanceTuning
Tuning the performance of a system involves adjusting various parameters and
design choices to improve its performance for a speci?c application. Various
1029
1030 Chapter 24 Advanced Application Development
aspects of a database-system design—ranging from high-level aspects such as
the schema and transaction design to database parameters such as buffer sizes,
down to hardware issues such as number of disks—affect the performance of
an application. Each of these aspects can be adjusted so that performance is
improved.
24.1.1 ImprovingSetOrientation
When SQL queries are executed from an application program, it is often the case
thataqueryisexecutedfrequently,butwithdifferentvaluesforaparameter.Each
callhasanoverheadofcommunicationwiththeserver,inadditiontoprocessing
overheadsattheserver.
For example, consider a program that steps through each department, in-
voking an embedded SQL query to ?nd the total salary of all instructors in the
department:
select sum(salary)
frominstructor
wheredept name=?
If the instructor relation does not have a clustered index on dept name,eachsuch
querywillresultinascanoftherelation.Evenifthereissuchanindex,arandom
I/O operationwill berequiredfor eachdept name value.
Instead, we can use a single SQLqueryto?ndtotalsalaryexpensesofeach
department:
select dept name,sum(salary)
frominstructor
groupbydept name;
Thisquerycanbeevaluatedwithasinglescanoftheinstructorrelation,avoiding
randomI/Oforeachdepartment.Theresultscanbefetchedtotheclientsideusing
a single round of communication, and the client program can then step through
the results to ?nd the aggregate for each department. Combining multiple SQL
queries into a single SQL query as above can reduce execution costs greatly in
many cases–for example, if the instructor relation is very large and has a large
number of departments.
TheJDBCAPIalsoprovidesafeaturecalledbatchupdatethatallowsanumber
of inserts to be performed using a single communication with the database.
Figure24.1illustratestheuseofthisfeature.Thecodeshowninthe?gurerequires
only one round of communication with the database, when the executeBatch()
method is executed,in contrast to similar code without the batch update feature
thatwesawearlierinFigure5.2.Intheabsenceofbatchupdate,asmanyrounds
of communication with the database are required as there are instructors to be
inserted.Thebatchupdatefeaturealsoenablesthedatabasetoprocessabatchof
24.1 PerformanceTuning 1031
PreparedStatement pStmt = conn.prepareStatement(
"insert into instructor values(?,?,?,?)");
pStmt.setString(1, "88877");
pStmt.setString(2, "Perry");
pStmt.setInt(3, "Finance");
pStmt.setInt(4, 125000);
pStmt.addBatch( );
pStmt.setString(1, "88878");
pStmt.setString(2, "Thierry");
pStmt.setInt(3, "Physics");
pStmt.setInt(4, 100000);
pStmt.addBatch( ); pStmt.executeBatch( );
Figure24.1 Batch update in JDBC.
insertsatonce,whichcanpotentiallybedonemuchmoreef?cientlythanaseries
of singlerecordinserts.
Anothertechniqueusedwidelyinclient–serversystemstoreducethecostof
communication and SQL compilation is to use stored procedures, where queries
are stored at the server in the form of procedures, which may be precompiled.
Clients can invoke these stored procedures, rather than communicate a series of
queries.
Another aspect of improving set orientation lies in rewriting queries with
nested subqueries. In the past, optimizers on many database systems were not
particularlygood,sohowaquerywaswrittenwouldhaveabigin?uenceonhow
itwasexecuted,andthereforeontheperformance.Today’sadvancedoptimizers
can transform even badly written queries and execute them ef?ciently, so the
needfor tuningindividualqueriesislessimportantthan itusedtobe.However,
complex queries containing nested subqueries are not optimized very well by
many optimizers.
We saw techniques for nested subquery decorrelation in Section 13.4.4. If a
subquery isnot decorrelated,it getsexecutedrepeatedly,potentially resultingin
agreatdealofrandom I/O.Incontrast,decorrelationallowsef?cientset-oriented
operationssuchasjoinstobeused,minimizingrandomI/O.Mostdatabasequery
optimizers incorporate some forms of decorrelation, but some can handle only
very simple nested subqueries. The execution plan chosen by the optimizer can
be found as described earlier in Chapter 13. If the optimizer has not succeeded
indecorrelatinganestedsubquery,the querycan be decorrelatedbyrewritingit
manually.
24.1.2 TuningofBulkLoadsandUpdates
When loading a large volume of data into a database (called a bulk load oper-
ation), performance is usually very poor if the inserts are carried out a separate
SQL insert statements. One reason is the overhead of parsing each SQL query; a
more important reason is that performing integrity constraint checks and index
1032 Chapter 24 Advanced Application Development
updatesseparatelyforeachinsertedtupleresultsinalargenumberofrandomI/O
operations. If the inserts were done as a large batch, integrity-constraint check-
ingandindexupdatecanbedoneinamuchmoreset-orientedfashion,reducing
overheadsgreatly;performanceimprovementsofanorder-of-magnitudeormore
are not uncommon.
To support bulk load operations, most database systems provide a bulk im-
port utility, and a corresponding bulk export utility. The bulk-import utility
readsdatafroma?le,andperformsintegrityconstraintcheckingaswellasindex
maintenance in a very ef?cient manner. Common input and output ?le format
supported by such bulk import/exportutilitiesinclude text ?les with characters
such as commas or tabs separating attribute values, with each record in a line of
itsown(such?leformatsarereferredtoascomma-separatedvaluesortab-separated
valuesformats).Databasespeci?cbinaryformats,aswellas XMLformatsarealso
supportedbybulkimport/exportutilities.Thenamesofthebulkimport/export
utilitiesdifferbydatabase.In PostgreSQL,theutilitiesarecalledpg dumpandpg
restore(PostgreSQLalsoprovidesanSQLcommandcopywhichprovidessimilar
functionality).Thebulkimport/exportutilityinOracleiscalled SQL*Loader,the
utilityin DB2iscalledload,andthe utilityin SQL Serveriscalledbcp(SQL Server
alsoprovidesan SQL command calledbulkinsert).
Wenowconsiderthecaseoftuningofbulkupdates.Supposewehavearela-
tionfunds received(dept name,amount)thatstoresfundsreceived(say,byelectronic
funds transfer) for each of a set of departments. Suppose now that we want to
add the amounts to the balances of the corresponding department budgets. In
order to use the SQL update statement to carry out this task, we have to perform
a look up on the funds received relation for each tuple in the department relation.
We can use subqueries in the update clause to carry out this task, as follows: We
assume for simplicity that the relation funds received contains at most one tuple
for eachdepartment.
updatedepartmentset budget=budget +
(select amount
fromfunds received
wherefunds received.dept name=department.dept name)
whereexists(
select *
fromfunds received
wherefunds received.dept name=department.dept name);
Note that the condition in the where clause of the update ensures that only
accounts with corresponding tuples infunds received are updated, while the sub-
query within the set clause computes the amount to be added to each such
department.
There are many applications that require updates such as that illustrated
above.Typically,thereisatable,whichweshallcallthemastertable,andupdates
to the master table are received as a batch. Now the master table has to be
24.1 PerformanceTuning 1033
correspondinglyupdated.SQL:2003providesaspecialconstruct,calledthemerge
construct, to simplify the task of performing such merging of information. For
example,theabove updatecan beexpressedusingmerge asfollows:
mergeintodepartmentas A
using (select *
fromfunds received)as F
on(A.dept name=F.dept name)
whenmatched then
updateset budget=budget +F.amount;
When a record from the subquery in the using clause matches a record in the
department relation, the when matched clause is executed, which can execute an
updateontherelation;inthiscase,thematchingrecordinthedepartmentrelation
isupdatedas shown.
The merge statement can also have a when not matched then clause, which
permits insertion of new records into the relation. In the above example, when
there is no matching department for a funds received tuple, the insertion action
could create a new department record (with a null building) using the following
clause:
whennotmatchedthen
insertvalues(F.dept name,null, F.budget)
Although not very meaningful in this example,
1
the when not matched then
clause can be quite useful in other cases. For example, suppose the local rela-
tion is a copy of a master relation, and we receive updated as well as newly in-
sertedrecordsfromthemasterrelation.Themergestatementcanupdatematched
records (these would be updated old records) and insert records that are not
matched(thesewould benewrecords).
Not all SQL implementations support the merge statement currently; see the
respectivesystemmanuals for furtherdetails.
24.1.3 Location ofBottlenecks
Theperformanceofmostsystems(atleastbeforetheyaretuned)isusuallylimited
primarily by the performance of one or a few components, called bottlenecks.
For instance, a program may spend 80 percent of its time in a small loop deep
in the code, and the remaining 20 percent of the time on the rest of the code; the
small loop then is a bottleneck. Improving the performance of a component that
is not a bottleneck does little to improve the overall speed of the system; in the
example, improving the speed of the rest of the code cannot lead to more than a
1
A better action here would have been to insert these records into an error relation, but that cannot be done with the
merge statement.
1034 Chapter 24 Advanced Application Development
concurrency-control 
manager
disk manager
CPU manager
transaction
manager
transaction
monitor
transaction
source
bu?er
manager
lock
grant
lock
request
page
reply
page
request
page
reply
page
request
…
…
Figure24.2 Queues in a database system.
20 percent improvement overall, whereas improving the speed of the bottleneck
loopcouldresultinanimprovementofnearly80percentoverall,inthebestcase.
Hence, when tuning a system, we must ?rst try to discover what the bot-
tlenecks are and then eliminate them by improving the performance of system
componentscausingthebottlenecks.Whenonebottleneckisremoved,itmayturn
out that another component becomes the bottleneck. In a well-balanced system,
no single component is the bottleneck. If the system contains bottlenecks, com-
ponents that are not part of the bottleneck are underutilized, and could perhaps
havebeenreplacedbycheapercomponents withlower performance.
For simple programs, the time spent in each region of the code determines
the overall execution time. However, database systems are much more complex,
and can be modeled as queueing systems. A transaction requests various ser-
vices from the database system, starting from entry into a server process, disk
reads during execution, CPU cycles, and locks for concurrency control. Each of
these services has a queue associated with it, and small transactions may spend
mostoftheirtimewaitinginqueues—especiallyindisk I/Oqueues—insteadof
executingcode.Figure24.2illustratessomeof the queuesina databasesystem.
Asaresultofthenumerousqueuesinthedatabase,bottlenecksinadatabase
system typically show up in the form of long queues for a particular service, or,
equivalently, in high utilizations for a particular service. If requests are spaced
exactly uniformly, and the time to service a request is less than or equal to the
24.1 PerformanceTuning 1035
time before the next requestarrives,theneach requestwill ?nd the resourceidle
and can therefore start execution immediately without waiting. Unfortunately,
the arrival of requests in a database system is never so uniform and is instead
random.
Ifaresource,suchasadisk,hasalowutilization,then,whenarequestismade,
theresourceislikelytobeidle,inwhichcasethewaitingtimefortherequestwill
be0.Assuminguniformlyrandomlydistributedarrivals,thelengthofthequeue
(and correspondingly the waiting time) go up exponentially with utilization; as
utilization approaches 100 percent, the queue length increases sharply, resulting
in excessively long waiting times. The utilization of a resource should be kept
low enough that queue length is short. As a rule of the thumb, utilizations of
around 70 percent are considered to be good, and utilizations above 90 percent
areconsideredexcessive,sincetheywillresultinsigni?cantdelays.Tolearnmore
about the theory of queueing systems, generally referredto as queueingtheory,
you canconsult thereferencescitedinthebibliographical notes.
24.1.4 TunableParameters
Database administrators can tune a database system at three levels. The lowest
level is at the hardware level. Options for tuning systems at this level include
adding disks or using a RAID system if disk I/O is a bottleneck, adding more
memory if the disk buffer size is a bottleneck, or moving to a faster processor if
CPU useisabottleneck.
The second level consists of the database-system parameters, such as buffer
sizeandcheckpointingintervals.Theexactsetofdatabase-systemparametersthat
can be tuned depends on the speci?c database system. Most database-system
manuals provide information on what database-system parameters can be ad-
justed, and how you should choose values for the parameters. Well-designed
database systems perform as much tuning as possible automatically, freeing the
user or database administrator from the burden. For instance, in many database
systems the buffer size is ?xed but tunable. If the system automatically adjusts
thebuffersizebyobservingindicatorssuchaspage-faultrates,thenthedatabase
administratorwillnot haveto worry about tuning the buffersize.
The third level is the highest level. It includes the schema and transactions.
Theadministratorcantunethedesignoftheschema,theindicesthatarecreated,
and the transactions that are executed, to improve performance. Tuning at this
leveliscomparativelysystemindependent.
The three levels of tuning interact with one another; we must consider them
togetherwhen tuning a system.For example,tuning at a higher levelmay result
in the hardware bottleneck changing from the disk system to the CPU,orvice
versa.
24.1.5 TuningofHardware
Even in a well-designed transaction processing system, each transaction usually
hastodoatleastafewI/O operations, if the data required by the transaction
1036 Chapter 24 Advanced Application Development
are on disk. An important factor in tuning a transaction processing system is to
makesurethatthedisksubsystemcanhandletherateatwhichI/Ooperationsare
required. For instance, consider a disk that supports an access time of about 10
milliseconds,andaveragetransferrateof25to100megabytespersecond(afairly
typical disk today). Such a disk would support a little under 100 random-access
I/O operations of 4 kilobytes each per second. If each transaction requires just 2
I/O operations, a single disk would support at most 50 transactions per second.
Theonlywaytosupportmoretransactionspersecondistoincreasethenumberof
disks.Ifthe systemneedstosupportntransactions persecond,each performing
2 I/O operations, data must be striped (or otherwise partitioned) across at least
n/50 disks(ignoring skew).
Notice here that the limiting factor is not the capacity of the disk, but the
speed at which random data can be accessed (limited in turn by the speed at
which the disk arm can move). The number of I/O operations per transaction
can be reduced by storing more data in memory. If all data are in memory, there
will be no disk I/O except for writes. Keeping frequently used data in memory
reducesthenumberofdisk I/Os,andisworththeextracostofmemory.Keeping
veryinfrequentlyuseddatainmemorywouldbeawaste,sincememoryismuch
moreexpensivethandisk.
Thequestionis,foragivenamountofmoneyavailableforspendingondisks
or memory, what is the best way to spend the money to achieve the maximum
number of transactions persecond.A reductionof 1 I/O persecond saves:
(price perdisk drive)/(access persecondperdisk)
Thus, if a particular page is accessed n times per second, the saving due to
keepingitinmemoryisntimestheabovevalue.Storingapageinmemorycosts:
(price per megabyteofmemory)/(pages permegabyte ofmemory)
Thus, thebreak-evenpointis:
n ?
price per diskdrive
access persecondperdisk
=
price permegabyte ofmemory
pages permegabyteof memory
Wecanrearrangetheequationandsubstitutecurrentvaluesforeachoftheabove
parameterstogetavalueforn;ifapageisaccessedmorefrequentlythanthis,itis
worth buying enough memory to store it. Current disk technology and memory
and disk prices (which we assume to be about $50 per disk, and $0.020 per
megabyte) give a value of n around 1/6400 times per second (or equivalently,
once in nearly 2 hours) for pages that are randomly accessed; with disk and
memorycost and speedsas ofsome yearsago,the correspondingvaluewas in5
minutes.
Thisreasoningiscapturedbytheruleofthumbthatwasoriginallycalledthe
5-minuterule:ifapageisusedmorefrequentlythanoncein5minutes,itshould
becachedinmemory.Inotherwords,someyearsago,therulesuggestedbuying
24.1 PerformanceTuning 1037
enough memory to cache all pages that are accessed at least once in 5 minutes
on average. Today, it is worth buying enough memory to cache all pages that
are accessed at least once in 2 hours on average. For data that are accessed less
frequently,buy enough disksto supporttherateof I/O required for the data.
For data that are sequentially accessed, signi?cantly more pages can be read
persecond.Assuming1megabyteofdataisreadatatime,someyearsagowehad
the1-minuterule,whichsaidthatsequentiallyaccesseddatashouldbecachedin
memoryiftheyareusedatleastoncein1minute.Thecorrespondingnumber,with
current memory and disk costs from our earlier example, is around 30 seconds.
Surprisingly, this ?gure has not changed all that much over the years, since disk
transfer rates have increased greatly, even though the price of a megabyte of
memoryhas reducedgreatlycomparedtothepriceof adisk.
Clearly the amount of data read per I/O operation greatly affects the time
above; in fact the 5-minute rule still holds if about 100 kilobytes of data are read
or writtenper I/O operation.
The 5-minute rule of thumb and its variants take only the number of I/O
operationsintoaccount,anddonotconsiderfactorssuchasresponsetime.Some
applications need to keep even infrequently used data in memory, to support
responsetimesthat arelessthanor comparable todisk-accesstime.
With the wide availability of ?ash memory, and “solid-state disks” based on
?ash memory,systemdesignerscan now choose to storefrequentlyused data in
?ash storage, instead of storing it on disk. Alternatively, in the ?ash-as-buffer
approach, ?ash storage is used as a persistent buffer, with each block having a
permanent location on disk, but stored in ?ash instead of being written to disk
as long as it is frequently used. When ?ash storage is full, a block that is not
frequentlyusedisevicted,and ?ushed back todiskifitwas updatedafter being
readfromdisk.
The ?ash-as-buffer approach requires changes in the database system itself.
Even if a database system does not support ?ash as a buffer, a database admin-
istrator can control the mapping of relations or indices to disks, and allocate
frequently used relations/indices to ?ash storage. The tablespace feature, sup-
portedbymostdatabasesystems,canbeusedtocontrolthemapping,bycreating
a tablespace on ?ash storage and assigning desired relations and indices to that
tablespace.Controllingthemappingata?nerlevelofgranularitythanarelation,
however,requireschanges tothe database-systemcode.
The “5-minute” rule has been extended to the case where data can be stored
on ?ash, in addition to main memory and disk. See the bibliographical notes for
references to more information.
Another aspect of tuning is whether to use RAID1orRAID 5. The answer
depends on how frequently the data are updated, since RAID5ismuchslower
than RAID1onrandomwrites:RAID 5 requires 2 reads and 2 writes to execute
a single random write request. If an application performs r random reads and
w random writes per second to support a particular throughput rate, a RAID 5
implementationwouldrequirer +4w I/Ooperationspersecondwhereasa RAID
1 implementation would requirer +2w I/O operations per second. We can then
calculatethenumberofdisksrequiredtosupporttherequiredI/Ooperationsper
1038 Chapter 24 Advanced Application Development
secondbydividingtheresultofthecalculationby100 I/Ooperationspersecond
(for current-generation disks). For many applications, r and w are large enough
that the (r + w)/100 disks can easily hold two copies of all the data. For such
applications, if RAID 1 is used, the required number of disks is actually less than
therequirednumberofdisksifRAID5isused!ThusRAID5isusefulonlywhenthe
data storage requirements are very large, but the update rates, and particularly
random updaterates,aresmall,that is,for verylargeand very “cold” data.
24.1.6 TuningoftheSchema
Within the constraints of the chosen normal form, it is possible to partition rela-
tions vertically.For example,considerthecourse relation,withthe schema:
course(course id,title, dept name,credits)
forwhichcourse idisakey.Withintheconstraintsofthenormalforms(BCNFand
thirdnormal forms),we canpartitionthecourserelationinto two relations:
course credit (course id,credits)
course title dept (course id,title, dept name)
Thetwo representationsarelogicallyequivalent,since course id isakey,but they
havedifferentperformance characteristics.
If most accesses to course information look at only the course id and credits,
then they can be run against the course credit relation, and access is likely to be
somewhat faster, since the title and dept name attributes are not fetched. For the
samereason,moretuplesof course credit will?t inthebufferthancorresponding
tuples of course, again leading to faster performance. This effect would be par-
ticularly marked if the title and dept name attributeswere large. Hence, a schema
consisting of course credit and course title deptwouldbepreferabletoaschema
consisting of the courserelationinthiscase.
On the other hand, if most accesses to course information require both dept
name and credits,usingthecourse relation would be preferable, since the cost of
the join of course credit and course title dept would be avoided. Also, the storage
overheadwouldbelower,sincetherewouldbeonlyonerelation,andtheattribute
course id would not bereplicated.
The column store approach to storing data is based on vertical partitioning,
but takes it to the limit by storing each attribute (column) of the relation in a
separate ?le. Column stores have been shown to perform well for several data-
warehouse applications.
Anothertricktoimproveperformanceistostoreadenormalizedrelation,suchas
ajoinofinstructoranddepartment,wheretheinformationaboutdept name,building,
and budget is repeated for every instructor. More effort has to be expended to
makesuretherelationisconsistentwheneveranupdateiscarriedout.However,
a query that fetches the names of the instructors and the associated buildings
will be speeded up, since the join of instructor and department will have been
24.1 PerformanceTuning 1039
precomputed.Ifsuch aqueryis executedfrequently,and has tobe performedas
ef?cientlyas possible,the denormalizedrelationcouldbe bene?cial.
Materializedviewscanprovidethebene?tsthatdenormalizedrelationspro-
vide, at the cost of some extra storage; we describe performance tuning of ma-
terialized views in Section 24.1.8. A major advantage to materialized views over
denormalized relations is that maintaining consistency of redundant data be-
comes the job of the database system, not the programmer. Thus, materialized
viewsarepreferable,whenevertheyaresupportedby the databasesystem.
Anotherapproach tospeedupthe computationof thejoinwithout material-
izing it, is to cluster records that would match in the join on the same disk page.
Wesaw suchclustered?leorganizations inSection10.6.2.
24.1.7 TuningofIndices
We cantune theindicesina databasesystemto improveperformance.If queries
are the bottleneck, we can often speed them up by creating appropriate indices
onrelations.Ifupdatesarethebottleneck,theremaybetoomany indices,which
havetobeupdatedwhentherelationsareupdated.Removingindicesmayspeed
up certainupdates.
The choice of the type of index also is important. Some database systems
support different kinds of indices, such as hash indices and B-tree indices. If
rangequeriesarecommon,B-treeindicesarepreferabletohashindices.Whether
tomakeanindexaclusteredindexisanothertunableparameter.Onlyoneindex
on a relation can be made clustered, by storing the relation sorted on the index
attributes. Generally, the index that bene?ts the greatest number of queries and
updatesshould be madeclustered.
To help identify what indices to create, and which index (if any) on each
relation should be clustered, most commercial database systems provide tuning
wizards; these are described in more detail in Section 24.1.9. These tools use the
pasthistoryofqueriesandupdates(calledtheworkload)toestimatetheeffectsof
variousindicesontheexecutiontimeofthequeriesandupdatesintheworkload.
Recommendations onwhat indicestocreatearebasedontheseestimates.
24.1.8 UsingMaterializedViews
Maintaining materializedviews can greatlyspeedup certaintypes of queries,in
particularaggregatequeries.RecalltheexamplefromSection13.5wherethetotal
salary for each department (obtained by summing the salary of each instructor
in the department) is required frequently. As we saw in that section, creating a
materialized view storing the total salary for each department can greatly speed
up suchqueries.
Materializedviewsshouldbeusedwithcare,however,sincethereisnotonly
spaceoverheadforstoringthembut,moreimportant,thereisalsotimeoverhead
formaintainingmaterializedviews.Inthecaseofimmediateviewmaintenance,
iftheupdatesofatransactionaffectthematerializedview,thematerializedview
must be updated as part of the same transaction. The transaction may therefore
runslower.Inthecaseofdeferred view maintenance, the materialized view is
1040 Chapter 24 Advanced Application Development
updatedlater;untilitisupdated,thematerializedviewmaybeinconsistentwith
the database relations. For instance, the materialized view may be brought up-
to-datewhenaqueryusestheview,orperiodically.Usingdeferredmaintenance
reduces the burden on update transactions.
The database administrator is responsible for the selection of materialized
views and for view-maintenance policies. The database administrator can make
the selection manually by examining the types of queries in the workload, and
?nding out which queries need to run faster and which updates/queries may
beexecutedmoreslowly.Fromtheexamination,thedatabaseadministratormay
choose an appropriate set of materialized views. For instance, the administrator
may?ndthatacertainaggregateisusedfrequently,andchoosetomaterializeit,or
may ?nd that a particularjoiniscomputedfrequently,and choose to materialize
it.
However, manual choice is tedious for even moderately large sets of query
types,andmakingagoodchoicemaybedif?cult,sinceitrequiresunderstanding
thecostsofdifferentalternatives;onlythequeryoptimizercanestimatethecosts
with reasonable accuracy, without actually executing the query. Thus a good set
of views may be found only by trial and error—that is, by materializing one
or more views, running the workload, and measuring the time taken to run the
queriesintheworkload.Theadministratorrepeatstheprocessuntilasetofviews
isfound that givesacceptable performance.
A better alternative is to provide support for selecting materialized views
within the database system itself, integrated with the query optimizer. This ap-
proach isdescribedinmoredetailinSection24.1.9.
24.1.9 Automated TuningofPhysicalDesign
Most commercial database systems today provide tools to help the database
administratorwithindexandmaterializedviewselection,andothertasksrelated
to physical database design such as how to partition data in a parallel database
system.
These tools examine the workload (the history of queries and updates) and
suggest indices and views to be materialized. The database administrator may
specifytheimportanceofspeedingupdifferentqueries,whichthetooltakesinto
account when selecting views to materialize. Often tuning must be done before
theapplicationisfullydeveloped,andtheactualdatabasecontentsmaybesmall
on the development database, but expected to be much larger on a production
database.Thus,sometuningtoolsalsoallowthedatabaseadministratortospecify
informationabout theexpectedsizeof thedatabaseand relatedstatistics.
Microsoft’s Database Tuning Assistant, for example, allows the user to ask
“what if” questions, whereby the user can pick a view, and the optimizer then
estimatestheeffectofmaterializingtheviewonthetotalcostoftheworkloadand
ontheindividualcostsofdifferenttypesofqueriesandupdatesintheworkload.
The automatic selection of indices and materialized views is usually imple-
mented by enumerating different alternatives and using the query optimizer to
estimate the costs and bene?ts of selecting each alternative by using the work-
24.1 PerformanceTuning 1041
load.Sincethenumberofdesignalternativesmaybeextremelylarge,asalsothe
workload,the selectiontechniquesmustbe designedcarefully.
The ?rst step is to generate a workload. This is usually done by recording
allthequeriesandupdatesthatareexecutedduringsometimeperiod.Next,the
selection tools perform workload compression, that is, create a representation
of the workload using a small number of updates and queries. For example,
updates of the same form can be represented by a single update with a weight
correspondingtohowmanytimestheupdateoccurred.Queriesofthesameform
canbesimilarlyreplacedbyarepresentativewithappropriateweight.Afterthis,
queries that are very infrequent and do not have a high cost may be discarded
from consideration. The most expensive queries may be chosen to be addressed
?rst.Suchworkload compressionis essentialfor largeworkloads.
With the help of the optimizer, the tool would come up with a set of indices
andmaterializedviewsthatcouldhelpthequeriesandupdatesinthecompressed
workload.Differentcombinationsoftheseindicesandmaterializedviewscanbe
triedoutto?ndthebestcombination.However,anexhaustiveapproachwouldbe
totallyimpractical,since thenumber of potentialindicesandmaterializedviews
isalreadylarge,andeachsubsetoftheseisapotentialdesignalternative,leading
toanexponentialnumberofalternatives.Heuristicsareusedtoreducethespace
of alternatives,that is,to reducethenumber of combinations considered.
Greedy heuristics for index and materialized view selection operate as fol-
lows:Theyestimatethebene?tsofmaterializingdifferentindicesorviews(using
the optimizer’s cost estimation functionality as a subroutine). They then choose
theindexorviewthatgiveseitherthemaximumbene?torthemaximumbene?t
per unit space (that is, bene?t divided by the space required to store the index
or view). The cost of maintaining the index or view must be taken into account
whencomputingthebene?t.Oncetheheuristichasselectedanindexorview,the
bene?tsofotherindicesorviewsmayhavechanged,sotheheuristicrecomputes
these, and chooses the next best index or view for materialization. The process
continues until eitherthe available diskspace for storing indices or materialized
views is exhausted, or the cost of maintaining the remaining candidates is more
than thebene?t toqueriesthat could usetheindicesor views.
Real-world index and materialized-view selection tools usually incorporate
some elementsofgreedyselection,but use other techniques toget betterresults.
Theyalsosupportotheraspectsofphysicaldatabasedesign,suchasdecidinghow
topartitionarelationinaparalleldatabase,orwhatphysicalstoragemechanism
to usefora relation.
24.1.10 TuningofConcurrentTransactions
Concurrent execution of different types of transactions can sometimes lead to
poor performance because of contention on locks. We ?rst consider the case of
read-write contention, which is more common, and then consider the case of
write-writecontention.
As an example of read-write contention, consider the following situation
on a banking database. During the day, numerous small update transactions are
1042 Chapter 24 Advanced Application Development
executedalmostcontinuously.Supposethatalargequerythatcomputesstatistics
on branches is run at the same time. If the query performs a scan on a relation,
it may block out all updates on the relation while it runs, and that can have a
disastrouseffectontheperformance of thesystem.
Severaldatabasesystems—Oracle, PostgreSQL,andMicrosoft SQLServer,for
example— supportsnapshot isolation,whereby queriesareexecutedona snap-
shot of the data, and updates can go on concurrently. (Snapshot isolation is de-
scribed in detail in Section 15.7.) Snapshot isolation should be used, if available,
for large queries, to avoid lock contention in the above situation. In SQL Server,
executingthe statement
settransaction isolation levelsnapshot
at the beginning of a transaction resultsinsnapshot isolationbeing used for that
transaction. In Oracle and PostgreSQL,usingthekeywordserializable in place
of the keyword snapshot in the above command has the same effect, since both
these systems actually use snapshot isolation when the isolation level is set to
serializable.
If snapshot isolation isnot available,an alternativeoptionis to executelarge
queries at times when updates are few or nonexistent. However, for databases
supportingWebsites,theremay beno such quietperiodforupdates.
Another alternative is to use weaker levels of consistency, such as the read
committedisolationlevel,wherebyevaluationofthequeryhasaminimalimpact
on concurrent updates, but the query results are not guaranteed to be consis-
tent. The application semantics determine whether approximate (inconsistent)
answers areacceptable.
We now consider the case of write-write contention. Data items that are
updatedveryfrequentlycanresultinpoorperformancewithlocking,withmany
transactions waiting for locks on those data items. Such data items are called
update hot spots. Update hot spots can cause problems even with snapshot
isolation, causing frequent transaction aborts due to write validation failures. A
commonly occurring situation that results in an update hot spot is as follows:
transactions need to assign unique identi?ers to data items being inserted into
the database, and to doso they read and increment a sequence counter stored in
atupleinthedatabase.Ifinsertsarefrequent,andthesequencecounterislocked
inatwo-phasemanner,thetuplecontainingthesequencecounterbecomesahot
spot.
Onewaytoimproveconcurrencyistoreleasethelockonthesequencecounter
immediatelyafteritisreadandincremented;however,afterdoingso,evenifthe
transactionaborts,theupdatetothesequencecountershouldnotberolledback.
To understand why, suppose T
1
increments the sequence counter, and then T
2
incrementsthesequencecounterbeforeT
1
commits;ifT
1
thenaborts,rollingback
itsupdate,eitherbyrestoringthecountertotheoriginalvalue,orbydecrementing
the counter, will result in the sequence value used by T
2
getting reused by a
subsequenttransaction.
24.1 PerformanceTuning 1043
Most databases provide a special construct for creating sequence counters
that implement early, non-two-phase, lock release, coupled with special case
treatment of undo logging so that updates to the counter are not rolled back if
thetransactionaborts.The SQL standardallowsasequencecounter tobecreated
using the command:
createsequencecounter1;
Intheabovecommand,counter1isthenameofthesequence;multiplesequences
canbecreatedwithdifferentnames.Thesyntaxtogetavaluefromthesequence
isnotstandardized;inOracle,counter1.nextvalwouldreturnthenextvalueofthe
sequence, after incrementing it, while the function call nextval(’counter1’) would
havethesameeffectinPostgreSQL,andDB2usesthesyntaxnextvalforcounter1.
TheSQLstandardprovidesanalternativetousinganexplicitsequencecounter,
which isusefulwhenthegoal istogiveuniqueidenti?erstotuplesinsertedinto
a relation. To do so, the keyword identity can be added to the declaration of an
integer attribute of a relation (usually this attribute would also be declared as
the primary key). If the value for that attribute is left unspeci?ed in an insert
statement for that relation, a unique new value is created automatically for each
newly inserted tuple. A non-two-phase locked sequence counter is used inter-
nally to implement the identity declaration, with the counter incremented each
time a tuple is inserted.Severaldatabases including DB2 and SQL Server support
the identity declaration, although the syntax varies. PostgreSQL supports a data
type called serial, which provides the same effect; the PostgreSQL type serial is
implementedby transparentlycreatinganon-two-phase lockedsequence.
Itisworthnotingthatsincetheacquisitionofasequencenumberbyatransac-
tioncannotberolledbackifthetransactionaborts(forreasonsdiscussedearlier),
transaction aborts may result in gaps in the sequence numbers in tuple inserted in
the database. For example, there may be tuples with identi?er value 1001 and
1003, but no tuple with value 1002, if the transaction that acquired the sequence
number1002didnotcommit.Suchgapsarenotacceptableinsomeapplications;
for example, some ?nancial applications require that there be no gaps in bill or
receipt numbers. Database provided sequences and automatically incremented
attributesshouldnotbeusedforsuchapplications,sincetheycanresultingaps.A
sequencecounterstoredinnormaltuple,whichislockedinatwo-phasemanner,
wouldnotbesusceptibletosuchgapssinceatransactionabortwouldrestorethe
sequence counter value, and the next transaction would get the same sequence
number, avoidingagap.
Longupdatetransactionscancauseperformanceproblemswithsystemlogs,
and can increase the time taken to recover from system crashes. If a transaction
performs many updates, the system log may become full even before the trans-
action completes, in which case the transaction will have to be rolled back. If an
update transaction runs for a long time (even with few updates), it may block
deletionofoldpartsofthelog,iftheloggingsystemisnotwelldesigned.Again,
this blocking could leadto thelog getting?lledup.
1044 Chapter 24 Advanced Application Development
To avoid such problems, many database systems impose strict limits on the
numberofupdatesthatasingletransactioncancarryout.Evenifthesystemdoes
not impose such limits, it is often helpful to break up a large update transaction
intoasetofsmallerupdatetransactionswherepossible.Forexample,atransaction
that gives a raise to every employee in a large corporation could be split up into
a series of small transactions, each of which updates a small range of employee-
ids. Such transactions are called minibatch transactions. However, minibatch
transactions must be used with care. First,if thereare concurrent updateson the
setofemployees,theresultofthesetofsmallertransactionsmaynotbeequivalent
to that of the single large transaction. Second, if there is a failure, the salaries of
some of the employees would have been increased by committed transactions,
but salaries of other employeeswould not. To avoid this problem, as soon as the
system recovers from failure, we must execute the transactions remaining in the
batch.
Long transactions, whether read-only or update, can also result in the lock
table becoming full. If a single query scans a large relation, the query optimizer
wouldensurethatarelationlockisobtainedinsteadofacquiringalargenumber
oftuplelocks.However,ifatransactionexecutesalargenumberofsmallqueries
or updates, it may acquire a large number of locks, resulting in the lock table
becoming full.
Toavoidthisproblem,somedatabasesprovideforautomaticlockescalation;
with this technique, if a transaction has acquired a large number of tuple locks,
tuplelocksareupgradedtopagelocks,orevenfullrelationlocks.Recallthatwith
multiple-granularity locking (Section 15.3), once a coarser level lock is obtained,
there is no need to record ?ner-level locks, so tuple lock entries can be removed
from the lock table, freeing up space. On databases that do not support lock
escalation, it is possible for the transaction to explicitly acquire a relation lock,
therebyavoidingtheacquisitionof tuplelocks.
24.1.11 PerformanceSimulation
To test the performance of a database system even before it is installed, we
can create a performance-simulationmodel of the database system.Each service
showninFigure24.2,suchasthe CPU,eachdisk,the buffer,andtheconcurrency
control, is modeled in the simulation. Instead of modeling details of a service,
thesimulationmodelmaycaptureonlysomeaspectsofeachservice,suchasthe
servicetime—thatis,thetimetakento?nishprocessingarequestonceprocessing
has begun. Thus, the simulation can model a disk access from just the average
disk-accesstime.
Sincerequestsforaservicegenerallyhavetowaittheirturn,eachservicehas
anassociatedqueueinthesimulationmodel.Atransactionconsistsofaseriesof
requests.Therequestsarequeuedupastheyarrive,andareservicedaccordingto
thepolicyforthatservice,suchas?rstcome,?rstserved.Themodelsforservices
suchas CPUandthedisksconceptuallyoperateinparallel,toaccountforthefact
that thesesubsystemsoperateinparallelina realsystem.
24.2 PerformanceBenchmarks 1045
Once the simulation model for transaction processing is built, the system
administrator can run a number of experiments on it. The administrator can use
experiments with simulated transactions arriving at different rates to ?nd how
thesystemwouldbehaveundervariousloadconditions.Theadministratorcould
runotherexperimentsthatvarytheservicetimesforeachserviceto?ndouthow
sensitive the performance is to each of them. System parameters, too, can be
varied,sothat performancetuning canbe doneonthe simulationmodel.
24.2 PerformanceBenchmarks
Asdatabaseserversbecomemorestandardized,thedifferentiatingfactoramong
the products of different vendors is those products’ performance. Performance
benchmarks are suites of tasks that are used to quantify the performance of
softwaresystems.
24.2.1 SuitesofTasks
Since most software systems, such as databases, are complex, there is a good
deal of variation in their implementation by different vendors. As a result, there
is a signi?cant amount of variation in their performance on different tasks. One
system may be the most ef?cient on a particular task; another may be the most
ef?cientonadifferenttask.Hence,asingletaskisusuallyinsuf?cienttoquantify
theperformanceofthesystem.Instead,theperformanceofasystemismeasured
by suitesofstandardizedtasks,calledperformance benchmarks.
Combiningtheperformancenumbersfrommultipletasksmustbedonewith
care.Supposethatwehavetwotasks,T
1
andT
2
,andthatwemeasurethethrough-
put of a system as the number of transactions of each type that run in a given
amountoftime—say,1second.SupposethatsystemAruns T
1
at99transactions
persecondand T
2
at1 transactionpersecond.Similarly,letsystemBrunboth T
1
and T
2
at 50 transactions per second. Suppose also that a workload has an equal
mixtureof the two typesof transactions.
Ifwetooktheaverageofthetwopairsofnumbers(thatis,99and1,versus50
and50),itmightappearthatthetwosystemshaveequalperformance.However,
it is wrong to take the averages in this fashion—if we ran 50 transactions of each
type,systemAwouldtakeabout50 .5secondsto?nish,whereassystemBwould
?nish injust 2seconds!
The example shows that a simple measure of performance is misleading if
there is more than one type of transaction. The right way to average out the
numbers is to take the time to completion for the workload, rather than the
average throughput for each transaction type. We can then compute system per-
formance accurately in transactions per second for a speci?ed workload. Thus,
systemAtakes50.5/100,whichis0.505secondspertransaction,whereassystem
Btakes0 .02 seconds pertransaction, on average.Intermsof throughput, system
Arunsatanaverageof1.98transactionspersecond,whereassystemBrunsat50
transactions per second. Assuming that transactions of all the types are equally
1046 Chapter 24 Advanced Application Development
likely, the correct way to average out the throughputs on different transaction
typesistotaketheharmonicmean of the throughputs. The harmonic meanof n
throughputs t
1
,...,t
n
is de?nedas:
n
1
t
1
+
1
t
2
+···+
1
t
n
Forourexample,theharmonicmeanforthethroughputsinsystemAis1.98.
ForsystemB,itis50.Thus,systemBisapproximately25timesfasterthansystem
A on a workload consisting of an equal mixture of the two example types of
transactions.
24.2.2 Database-Application Classes
Online transaction processing (OLTP)anddecision support, including online
analytical processing (OLAP), are two broad classes of applications handled by
database systems. These two classes of tasks have different requirements. High
concurrency and clever techniques to speed up commit processing are required
forsupportingahighrateofupdatetransactions.Ontheotherhand,goodquery-
evaluationalgorithmsandqueryoptimizationarerequiredfordecisionsupport.
Thearchitectureofsomedatabasesystemshasbeentunedtotransactionprocess-
ing; that of others, such as the Teradata series of parallel database systems, has
beentunedtodecisionsupport.Othervendorstrytostrikeabalancebetweenthe
two tasks.
Applications usually have a mixture of transaction-processing and decision-
support requirements. Hence, which database system is best for an application
dependsonwhat mixof thetwo requirementsthe applicationhas.
Supposethatwehavethroughputnumbersforthetwoclassesofapplications
separately,andtheapplicationathandhasamixoftransactionsinthetwoclasses.
We must be careful even about taking the harmonic mean of the throughput
numbers,becauseofinterferencebetweenthetransactions.Forexample,along-
runningdecision-supporttransactionmayacquireanumberoflocks,whichmay
prevent all progress of update transactions. The harmonic mean of throughputs
should beusedonly ifthe transactions do not interferewithone another.
24.2.3 TheTPCBenchmarks
The Transaction Processing Performance Council (TPC) has de?ned a series of
benchmark standardsfor databasesystems.
The TPC benchmarks are de?ned in great detail. They de?ne the set of rela-
tionsandthesizesofthetuples.Theyde?nethenumberoftuplesintherelations
not as a ?xed number, but rather as a multiple of the number of claimed trans-
actions per second, to re?ect that a larger rate of transaction execution is likely
to be correlated with a larger number of accounts. The performance metric is
throughput, expressed as transactions per second (TPS). When its performance
is measured,the system must providea response timewithin certain bounds, so
thatahighthroughputcannotbeobtainedatthecostofverylongresponsetimes.
24.2 PerformanceBenchmarks 1047
Further, for business applications, cost is of great importance. Hence, the TPC
benchmarkalsomeasuresperformanceintermsofpriceper TPS.Alargesystem
may have a high number of transactions per second, but may be expensive(that
is,haveahighpriceper TPS).Moreover,acompanycannotclaim TPCbenchmark
numbers for its systems without an external audit that ensures that the system
faithfully follows the de?nition of the benchmark, including full support for the
ACID propertiesof transactions.
The ?rst in the series was the TPC-A benchmark,which was de?ned in 1989.
This benchmark simulates a typical bank application by a single type of trans-
action that models cash withdrawal and deposit at a bank teller.The transaction
updatesseveralrelations—suchasthebankbalance,theteller’sbalance,andthe
customer’sbalance—andaddsarecordtoanaudittrailrelation.Thebenchmark
also incorporates communication with terminals, to model the end-to-end per-
formance of the system realistically. The TPC-B benchmark was designed to test
the core performance of the database system, along with the operating system
onwhichthesystemruns.Itremovesthepartsofthe TPC-Abenchmarkthatdeal
with users, communication, and terminals, to focus on the back-end database
server.Neither TPC-Anor TPC-Bisinusetoday.
The TPC-C benchmark was designed to model a more complex system than
the TPC-A benchmark. The TPC-C benchmark concentrates on the main activities
inanorder-entryenvironment,suchasenteringanddeliveringorders,recording
payments, checking status of orders, and monitoring levels of stock. The TPC-C
benchmark is still widely used for benchmarking online transaction processing
(OLTP)systems.Themorerecent TPC-Ebenchmarkisalsoaimedat OLTPsystems,
butisbasedonamodelofabrokerage?rm,withcustomerswhointeractwiththe
?rm and generate transactions. The ?rm in turn interacts with ?nancial markets
to executetransactions.
The TPC-D benchmark was designed to test the performance of database
systemsondecision-supportqueries.Decision-supportsystemsarebecomingin-
creasingly important today. The TPC-A, TPC-B,andTPC-C benchmarks measure
performance on transaction-processing workloads, and should not be used as a
measure of performance on decision-support queries. The D in TPC-D stands for
decisionsupport.TheTPC-Dbenchmarkschemamodelsasales/distributionap-
plication,withparts,suppliers,customers,andorders,alongwithsomeauxiliary
information. The sizes of the relations are de?ned as a ratio, and database size is
the total size of all the relations, expressed in gigabytes. TPC-D at scale factor 1
represents the TPC-D benchmark on a 1-gigabyte database, while scale factor 10
represents a 10-gigabyte database. The benchmark workload consists of a set of
17 SQL queries modeling common tasks executed on decision-support systems.
Some of the queries make use of complex SQL features, such as aggregation and
nestedqueries.
The benchmark’s users soon realized that the various TPC-D queries could
be signi?cantly speeded up by using materialized views and other redundant
information. There are applications, such as periodic reporting tasks, where the
queries are known in advance and materialized view can be carefully selected
1048 Chapter 24 Advanced Application Development
to speed up the queries. It is necessary, however, to account for the overhead of
maintaining materializedviews.
The TPC-H benchmark (where H represents ad hoc) is a re?nement of the
TPC-D benchmark. The schema is the same, but there are 22 queries, of which
16 are from TPC-D. In addition, there are two updates, a set of inserts, and a set
ofdeletes. TPC-Hprohibitsmaterializedviewsandotherredundantinformation,
and permits indices only on primary and foreign keys. This benchmark models
adhocqueryingwherethequeriesarenotknownbeforehand,soitisnotpossible
tocreateappropriatematerializedviewsaheadoftime.Avariant,TPC-R(whereR
standsfor“reporting”),whichisnolongerinuse,allowedtheuseofmaterialized
viewsand otherredundantinformation.
TPC-Hmeasuresperformanceinthisway:Thepowertestrunsthequeriesand
updates one at a time sequentially, and 3600 seconds divided by the geometric
meanoftheexecutiontimesofthequeries(inseconds)givesameasureofqueries
perhour.Thethroughputtestrunsmultiplestreamsinparallel,witheachstream
executingall22queries.Thereisalsoaparallelupdatestream.Herethetotaltime
for theentirerunis usedto computethe number ofqueriesperhour.
The composite query per hour metric, which is the overall metric, is then
obtained as the square root of the product of the power and throughput metrics.
A composite price/performance metric is de?ned by dividing the system price
by the compositemetric.
The TPC-W Web commerce benchmark is an end-to-end benchmark that
models Web sites having static content (primarily images) and dynamic content
generatedfromadatabase.Cachingofdynamiccontentisspeci?callypermitted,
since it is very useful for speeding up Web sites. The benchmark models an
electronicbookstore,and likeother TPC benchmarks, providesfor differentscale
factors.TheprimaryperformancemetricsareWebinteractionspersecond(WIPS)
and priceper WIPS. However,the TPC-Wbenchmark is no longerin use.
24.3 OtherIssuesinApplication Development
In this section, we discuss two issues in application development: testing of
applications,and migrationofapplications.
24.3.1 TestingApplications
Testing of programs involves designing a test suite, that is, a collection of test
cases.Testingisnotaone-timeprocess,sinceprogramsevolvecontinuously,and
bugs may appear as an unintended consequence of a change in the program;
such a bug is referred to as program regression. Thus, after every change to
a program, the program must be tested again. It is usually infeasible to have
a human perform tests after every change to a program. Instead, expected test
outputsarestoredwitheachtestcase ina testsuite.Regression testinginvolves
running the program on each test case in a test suite, and checking that the
programgeneratestheexpectedtestoutput.
24.3 Other IssuesinApplication Development 1049
In the context of database applications, a test case consists of two parts: a
databasestate,and aninputto a speci?cinterfaceof theapplication.
SQL queries can have subtle bugs that can be dif?cult to catch. For example,
aquerymayexecuter   s, when it should have actually performed r   s.The
difference between these two queries would be found only if the test database
had an r tuple with no matching s tuple. Thus, it is important to create test
databases that can catch commonly occurring errors. Such errors are referred to
as mutations, since they are usually small changes to a query (or program). A
test case that produces different outputs on an intended query and a mutant of
the query is said to kill the mutant. A test suite should have test cases that kill
(most) commonly occurring mutants.
If a test case performs an update on the database, to check that it executed
properly one must verify that the contents of the database match the expected
contents. Thus, the expected output consists not only of data displayed on the
user’sscreen,but also(updatesto) thedatabase state.
Since the database state can be rather large, multiple test cases would share
a common database state. Testing is complicated by the fact that if a test case
performs an update on the database, the results of other test cases run subse-
quentlyonthesamedatabasemaynotmatchtheexpectedresults.Theothertest
caseswouldthenbeerroneouslyreportedashavingfailed.Toavoidthisproblem,
whenever a test case performs an update, the database state must be restored to
itsoriginal stateafterrunning thetest.
Testing can also be used to ensure that an application meets performance
requirements. To carry out such performance testing, the test database must be
of the same size as the real database would be. In some cases, there is already
existing data on which performance testing can be carried out. In other cases,
a test database of the required size must be generated; there are several tools
availableforgeneratingsuchtestdatabases.Thesetoolsensurethatthegenerated
data satis?es constraints such as primary and foreign key constraints. They may
additionally generate data that looks meaningful, for example, by populating a
name attribute using meaningful names instead of random strings. Some tools
also allow data distributions to be speci?ed; for example, a university database
may requirea distributionwith moststudentsin therange of 18 to 25 years,and
mostfaculty inthe rangeof 25 to65 years.
Even if there is an existing database, organizations usually do not want to
reveal sensitive data to an external organization that may be carrying out the
performance tests. In such a situation, a copy of the real database may be made,
and the values in the copy may be modi?ed in such a way that any sensitive
data, such as credit-card numbers, social-security numbers, or dates of birth, are
obfuscated. Obfuscation is done in most cases by replacing a real value with a
randomlygeneratedvalue(takingcaretoalsoupdateallreferencestothatvalue,
incasethevalueisaprimarykey).Ontheotherhand,iftheapplicationexecution
depends on the value, such as the date of birth in an application that performs
differentactionsbasedonthedateofbirth,obfuscationmaymakesmallrandom
changes inthe valueinsteadof replacingitcompletely.
1050 Chapter 24 Advanced Application Development
24.3.2 ApplicationMigration
Legacy systems are older-generation application systems that are in use by an
organization,butthattheorganizationwishestoreplacewithadifferentapplica-
tion.Forexample,manyorganizationsdevelopedapplicationsinhouse,butmay
decidetoreplacethemwithacommercialproduct.Insomecases,alegacysystem
may use old technology that is incompatible with current-generation standards
andsystems.Somelegacysystemsinoperationtodayareseveraldecadesoldand
are based on technologies such as databases that use the network or hierarchical
datamodels,oruseCoboland?lesystemswithoutadatabase.Suchsystemsmay
stillcontain valuabledata,and may supportcriticalapplications.
Replacing legacy applications with new applications is often costly in terms
of both time and money, since they are often very large, consisting of millions of
lines of code developed by teams of programmers, often over several decades.
They contain large amounts of data that must be ported to the new application,
which may use a completely different schema. Switchover from an old to a new
application involves retraining large numbers of staff. Switchover must usually
be done without any disruption, with data entered in the old system available
through thenewsystemaswell.
Many organizations attempt to avoid replacing legacy systems, and instead
trytointeroperatethemwithnewersystems.Oneapproachusedtointeroperate
between relational databases and legacy databases is to build a layer, called a
wrapper, on top of the legacy systems that can make the legacy system appear
to be a relational database.The wrappermay providesupport for ODBC or other
interconnectionstandardssuchasOLE-DB,whichcanbeusedtoqueryandupdate
the legacy system. The wrapper is responsible for converting relational queries
and updatesinto queriesand updateson thelegacysystem.
Whenanorganizationdecidestoreplacealegacysystemwithanewsystem,
itmay followa processcalledreverseengineering,which consists of going over
the code of the legacy system to come up with schema designs in the required
data model (such as an E-R model or an object-oriented data model). Reverse
engineering also examines the code to ?nd out what procedures and processes
were implemented, in order to get a high-level model of the system. Reverse
engineering is needed because legacy systems usually do not have high-level
documentation of their schema and overall system design. When coming up
with the design of a new system, developers review the design, so that it can be
improved rather than just reimplemented as is. Extensive coding is required to
support all the functionality (such as user interface and reporting systems) that
wasprovidedbythelegacysystem.Theoverallprocessiscalledre-engineering.
Whenanewsystemhasbeenbuiltandtested,thesystemmustbepopulated
withdatafromthelegacysystem,andallfurtheractivitiesmustbecarriedouton
thenewsystem.However,abruptlytransitioningtoanewsystem,whichiscalled
thebig-bangapproach,carriesseveralrisks.First,usersmaynotbefamiliarwith
the interfaces of the new system. Second, there may be bugs or performance
problems in the new system that were not discovered when it was tested. Such
problems may lead to great losses for companies, since their ability to carry out
24.4 Standardization 1051
criticaltransactionssuchassalesandpurchasesmaybeseverelyaffected.Insome
extreme cases the new system has even been abandoned, and the legacy system
reused,afteranattemptedswitchover failed.
An alternative approach, called the chicken-little approach, incrementally
replaces the functionality of the legacy system. For example, the new user inter-
faces may be used with the old system in the back end, or vice versa. Another
option is to use the new system only for some functionality that can be decou-
pled from the legacy system. In either case, the legacy and new systems coexist
for some time. There is therefore a need for developing and using wrappers
on the legacy system to provide required functionality to interoperate with the
new system. This approach therefore has a higher development cost associated
withit.
24.4 Standardization
Standardsde?netheinterfaceofasoftwaresystem;forexample,standardsde?ne
the syntax and semantics of a programming language, or the functions in an
application-program interface, or evena data model (such as the object-oriented
databasestandards).Today,databasesystemsarecomplex,andareoftenmadeup
ofmultipleindependentlycreatedpartsthatneedtointeract.Forexample,client
programs may be created independently of back-end systems, but the two must
be able to interact with each other. A company that has multiple heterogeneous
databasesystemsmayneedtoexchangedatabetweenthedatabases.Givensuch
a scenario,standardsplayan importantrole.
Formal standards are those developedby a standards organization or by in-
dustrygroups,througha publicprocess.Dominant productssometimesbecome
de facto standards, in that they become generally accepted as standards with-
outanyformalprocessofrecognition.Someformalstandards,likemanyaspects
of the SQL-92 and SQL:1999 standards, are anticipatory standards that lead the
marketplace; they de?ne features that vendors then implement in products. In
othercases,thestandards,orpartsofthestandards,arereactionarystandards,in
that theyattempttostandardizefeaturesthat somevendorshavealreadyimple-
mented,andthatmayevenhavebecomedefactostandards. SQL-89wasinmany
ways reactionary, since it standardized features, such as integrity checking, that
were already present in the IBM SAA SQL standardand inotherdatabases.
Formalstandardscommitteesaretypicallycomposedofrepresentativesofthe
vendors and of members from user groups and standards organizations such as
theInternationalOrganizationforStandardization(ISO)ortheAmericanNational
StandardsInstitute(ANSI),orprofessionalbodies,suchastheInstituteofElectrical
andElectronicsEngineers(IEEE).Formalstandardscommitteesmeetperiodically,
and members present proposals for features to be added to or modi?ed in the
standard. After a (usually extended) period of discussion, modi?cations to the
proposal,andpublicreview,membersvoteonwhethertoacceptorrejectafeature.
Sometimeafterastandardhasbeende?nedandimplemented,itsshortcomings
become clear and new requirements become apparent. The process of updating
1052 Chapter 24 Advanced Application Development
the standard then begins, and a new version of the standard is usually released
after a few years. This cycle usually repeats every few years, until eventually
(perhaps many years later) the standard becomes technologically irrelevant, or
losesitsuserbase.
The DBTGCODASYLstandardfornetworkdatabases,formulatedbytheData-
base Task Group, was one of the early formal standards for databases. IBM
databaseproductsformerlyestablisheddefactostandards,sinceIBMcommanded
muchofthedatabasemarket.Withthegrowthofrelationaldatabasescameanum-
berofnewentrantsinthedatabasebusiness;hence,theneedforformalstandards
arose. In recent years, Microsoft has created a number of speci?cations that also
have become defacto standards.A notable exampleis ODBC, which is now used
in non-Microsoft environments. JDBC, whose speci?cation was created by Sun
Microsystems,is another widelyuseddefacto standard.
This section givesa veryhigh-level overviewof different standards, concen-
trating on the goals of the standard. The bibliographical notes at the end of the
chapter provide references to detailed descriptions of the standards mentioned
inthis section.
24.4.1 SQLStandards
Since SQListhemostwidelyusedquerylanguage,muchworkhasbeendoneon
standardizing it. ANSI and ISO, with the various database vendors, have played
a leading role in this work. The SQL-86 standard was the initial version. The IBM
Systems Application Architecture (SAA)standardforSQL was released in 1987.
As people identi?ed the need for more features, updated versions of the formal
SQL standardweredeveloped,called SQL-89 and SQL-92.
The SQL:1999 version of the SQL standard added a variety of features to SQL.
We have seen many of these features in earlier chapters. The SQL:2003 version
of the SQL standard is a minor extension of the SQL:1999 standard. Some features
suchastheSQL:1999OLAPfeatures(Section5.6.3)werespeci?edasanamendment
to the earlier version of the SQL:1999 standard, instead of waiting for the release
of SQL:2003.
The SQL:2003 standardwas brokeninto severalparts:
• Part 1: SQL/Framework providesanoverviewof thestandard.
• Part 2: SQL/Foundation de?nes the basics of the standard: types, schemas,
tables, views, query and update statements, expressions, security model,
predicates,assignment rules,transactionmanagement, andso on.
• Part 3: SQL/CLI (Call LevelInterface) de?nes applicationprogram interfaces
to SQL.
• Part 4: SQL/PSM (Persistent Stored Modules) de?nes extensions to SQL to
makeitprocedural.
• Part 9: SQL/MED (Management of External Data) de?nes standards or in-
terfacing an SQL system to external sources. By writing wrappers, system
24.4 Standardization 1053
designers can treat external data sources, such as ?les or data in nonrela-
tional databases,as ifthey were “foreign” tables.
• Part 10: SQL/OLB (Object Language Bindings) de?nes standards for embed-
ding SQL inJava.
• Part 11: SQL/Schemata (Information and De?nition Schema) de?nes a stan-
dardcatalog interface.
• Part 13: SQL/JRT (Java Routines and Types) de?nes standards for accessing
routinesand typesinJava.
• Part 14: SQL/XML de?nes XML-RelatedSpeci?cations.
Themissingnumberscoverfeaturessuchastemporaldata,distributedtrans-
action processing, and multimedia data, for which there is as yet no agreement
onthe standards.
The latest versions of the SQL standard are SQL:2006, which added several
features related to XML,andSQL:2008, which introduces a number of extensions
to the SQL language.
24.4.2 DatabaseConnectivity Standards
The ODBCstandardisawidelyusedstandardforcommunicationbetweenclient
applicationsanddatabasesystems.ODBCisbasedontheSQLCallLevelInterface
(CLI)standardsdevelopedbytheX/OpenindustryconsortiumandtheSQLAccess
Group, but it has several extensions. The ODBC API de?nes a CLI,anSQL syntax
de?nition, and rules about permissible sequences of CLI calls. The standard also
de?nes conformance levelsfor the CLI and the SQL syntax. For example,the core
level of the CLI has commands to connect to a database, to prepare and execute
SQL statements, to get back results or status values, and to manage transactions.
The next level of conformance (level 1) requires support for catalog information
retrievalandsomeotherfeaturesoverandabovethecore-levelCLI;level2requires
further features, such as ability to send and retrieve arrays of parameter values
and toretrievemoredetailedcatalog information.
ODBC allows a client to connect simultaneously to multipledata sources and
toswitchamongthem,buttransactionsoneachareindependent; ODBCdoesnot
supporttwo-phase commit.
A distributed system provides a more general environment than a client–
server system. The X/Open consortium has also developed the X/Open XA
standards for interoperation of databases. These standards de?ne transaction-
management primitives (such as transaction begin, commit, abort, and prepare-
to-commit) that compliant databases should provide; a transaction manager can
invoketheseprimitivestoimplementdistributedtransactionsbytwo-phasecom-
mit. The XA standards are independent of the data model and of the speci?c in-
terfacesbetweenclientsanddatabasestoexchangedata.Thus,wecanusethe XA
protocolstoimplementadistributedtransactionsysteminwhichasingletransac-
1054 Chapter 24 Advanced Application Development
tioncanaccessrelationalaswellasobject-orienteddatabases,yetthetransaction
manager ensuresglobalconsistency viatwo-phase commit.
Therearemanydatasourcesthatarenotrelationaldatabases,andinfactmay
notbedatabasesatall.Examplesare?at?lesandemailstores.Microsoft’sOLE-DB
is a C++ API with goals similar to ODBC, but for nondatabase data sources that
may provide only limited querying and update facilities. Just like ODBC, OLE-DB
provides constructs for connecting to a data source, starting a session, executing
commands, and getting back results in the form of a rowset, which is a set of
result rows.
However,OLE-DBdiffersfrom ODBCinseveralways.Tosupportdatasources
with limited feature support, features in OLE-DB are divided into a number of
interfaces, and a data source may implement only a subset of the interfaces. An
OLE-DB program can negotiate with a data source to ?nd what interfaces are
supported.In ODBCcommandsarealwaysin SQL.InOLE-DB,commandsmaybe
in any language supported by the data source; while some sources may support
SQL,oralimitedsubsetofSQL,othersourcesmayprovideonlysimplecapabilities
such as accessing data in a ?at ?le, without any query capability. Another major
differenceof OLE-DBfrom ODBCisthatarowsetisanobjectthatcanbesharedby
multiple applications through shared memory. A rowset object can be updated
by one application, and other applications sharing that object will get noti?ed
about the change.
The Active Data Objects (ADO) API, also created by Microsoft, provides
an easy-to-use interface to the OLE-DB functionality, which can be called from
scripting languages, such as VBScript and JScript. The newer ADO.NET API is
designed for applications written in the .NET languages such as C# and Visual
Basic.NET.Inadditiontoprovidingsimpli?edinterfaces,itprovidesanabstraction
calledthe DataSet that permitsdisconnecteddata access.
24.4.3 ObjectDatabaseStandards
Standards in the area of object-oriented databases have so far been driven pri-
marilybyOODBvendors.TheObjectDatabaseManagementGroup(ODMG)was
agroupformedbyOODB vendors to standardize the data model and language
interfaces to OODBs. The C++ language interface speci?ed by ODMG was brie?y
outlined in Chapter 22. ODMG is no longer active. JDO is a standard for adding
persistenceto Java.
TheObjectManagementGroup(OMG)isaconsortiumofcompanies,formed
with the objective ofdevelopingastandard architecture for distributedsoftware
applicationsbasedontheobject-orientedmodel.OMGbroughtouttheObjectMan-
agement Architecture (OMA) reference model. The Object Request Broker (ORB)isa
componentoftheOMAarchitecturethatprovidesmessagedispatchtodistributed
objects transparently, so the physical location of the object is not important. The
CommonObjectRequestBrokerArchitecture(CORBA)providesadetailedspec-
i?cationoftheORB,andincludesanInterfaceDescriptionLanguage(IDL),which
is used to de?ne the data types used for data interchange. The IDL helps to sup-
24.4 Standardization 1055
portdataconversionwhendataareshippedbetweensystemswithdifferentdata
representations.
Microsoft introduced the Entity data model, which incorporates ideas from
theentity-relationshipandobject-orienteddatamodels,andanapproachtointe-
grating querying with the programming language, called Language Integrated
Queryingor LINQ. Thesearelikelyto becomedefacto standards.
24.4.4 XML-BasedStandards
A wide variety of standards based on XML (see Chapter 23) have been de-
?ned for a wide variety of applications. Many of these standards are related
toe-commerce.Theyinclude standardspromulgatedby nonpro?t consortia and
corporate-backedeffortsto createdefacto standards.
RosettaNet, which falls into the former category, is an industry consortium
thatusesXML-basedstandardstofacilitatesupply-chainmanagementinthecom-
puter and information technology industries. Supply-chain management refers
to the purchases of material and services that an organization needs to function.
In contrast, customer-relationship management refers to the front end of a com-
pany’s interaction, dealing with customers. Supply-chain management requires
standardizationof a varietyof things suchas:
• Globalcompanyidenti?er:RosettaNetspeci?esasystemforuniquelyiden-
tifying companies, using a 9-digit identi?er called Data Universal Numbering
System(DUNS).
• Global product identi?er: RosettaNet speci?es a 14-digit Global Trade Item
Number(GTIN) for identifyingproductsand services.
• Global class identi?er: This is a 10-digit hierarchical code for classifying
products and servicescalled the United Nations/Standard Product and Services
Code (UN/SPSC).
• Interfaces between trading partners: RosettaNet Partner Interface Processes
(PIPs)de?nebusinessprocessesbetweenpartners. PIPsaresystem-to-system
XML-based dialogs: They de?ne the formats and semantics of business doc-
uments involved in a process and the steps involved in completing a trans-
action. Examples of steps could include getting product and service infor-
mation, purchase orders, order invoicing, payment, order status requests,
inventory management, post-sales support including service warranty, and
soon.Exchangeofdesign,con?guration,process,andqualityinformationis
alsopossibleto coordinatemanufacturing activitiesacross organizations.
Participantsinelectronicmarketplacesmaystoredatainavarietyofdatabase
systems. These systems may use different data models, data formats, and data
types. Furthermore, there may be semantic differences (metric versus English
measure, distinct monetary currencies, and so forth) in the data. Standards for
electronic marketplaces include methods for wrapping each of these heteroge-
1056 Chapter 24 Advanced Application Development
neous systems with an XML schema. These XML wrappers form the basis of a
uni?ed viewof dataacross all ofthe participantsinthemarketplace.
Simple Object Access Protocol (SOAP) is a remote procedure call standard
that uses XML to encode data (both parameters and results), and uses HTTP as
the transport protocol; that is, a procedure call becomes an HTTP request. SOAP
is backed by the World Wide Web Consortium (W3C) and has gained wide ac-
ceptance in industry. SOAP can be used in a variety of applications. For instance,
in business-to-business e-commerce, applications running at one site can access
datafrom and executeactions atother sitesthrough SOAP.
SOAP and WebservicesweredescribedinmoredetailinSection23.7.3.
24.5 Summary
• Tuningofthedatabase-systemparameters,aswellasthehigher-leveldatabase
design—such as the schema, indices, and transactions—is important for
good performance. Queries can be tuned to improve set-orientation, while
bulk-loading utilitiescan greatlyspeedup dataimportinto a database.
Tuning is best done by identifying bottlenecks and eliminating them.
Databasesystemsusuallyhaveavarietyoftunableparameters,suchasbuffer
sizes,memorysize,andnumberofdisks.Thesetofindicesandmaterialized
views can be appropriately chosen to minimize overall cost. Transactions
can be tuned to minimize lock contention; snapshot isolation, and sequence
numberingfacilitiessupportingearlylockreleaseareusefultoolsforreducing
read-writeand write-writecontention.
• Performancebenchmarksplayanimportantroleincomparisonsofdatabase
systems, especially as systems become more standards compliant. The TPC
benchmarksuitesarewidelyused,andthedifferentTPCbenchmarksareuse-
fulfor comparing the performanceof databasesunderdifferentworkloads.
• Applicationsneedtobetestedextensivelyastheyaredeveloped,andbefore
they are deployed. Testing is used to catch errors, as well as to ensure that
performancegoals aremet.
• Legacy systems are systems based on older-generation technologies such as
nonrelational databases or even directly on ?le systems. Interfacing legacy
systems with new-generation systems is often important when they run
mission-critical systems. Migrating from legacy systems to new-generation
systems must be done carefully to avoid disruptions, which can be very
expensive.
• Standards are important because of the complexity of database systems and
their need for interoperation. Formal standards exist for SQL. De facto stan-
dards, such as ODBC and JDBC, and standards adopted by industry groups,
suchas CORBA,haveplayedanimportantroleinthegrowthofclient–server
databasesystems.
PracticeExercises 1057
ReviewTerms
• Performance tuning
• Set-orientation
• Batchupdate(JDBC)
• Bulkload
• Bulkupdate
• Mergestatement
• Bottlenecks
• Queueingsystems
• Tunable parameters
• Tuning of hardware
• Five-minuterule
• One-minuterule
• Tuning of the schema
• Tuning of indices
• Materializedviews
• Immediateviewmaintenance
• Deferredviewmaintenance
• Tuning of transactions
• Lockcontention
• Sequences
• Minibatch transactions
• Performance simulation
• Performance benchmarks
• Servicetime
• Timetocompletion
• Database-applicationclasses
• The TPC benchmarks
?
TPC-A
?
TPC-B
?
TPC-C
?
TPC-D
?
TPC-E
?
TPC-H
• Webinteractions persecond
• Regressiontesting
• Killingmutants
• Legacysystems
• Reverseengineering
• Re-engineering
• Standardization
?
Formalstandards
?
Defacto standards
?
Anticipatorystandards
?
Reactionary standards
• Database connectivity standards
?
ODBC
?
OLE-DB
?
X/Open XAstandards
• Object databasestandards
?
ODMG
?
CORBA
• XML-basedstandards
PracticeExercises
24.1 Many applications need to generate sequence numbers for each transac-
tion.
a. If a sequence counter is locked in two-phase manner, it can become
aconcurrency bottleneck.Explainwhythis maybe thecase.
1058 Chapter 24 Advanced Application Development
b. Many databasesystemssupportbuilt-insequencecountersthat are
not locked in two-phase manner; when a transaction requests a se-
quencenumber, thecounter islocked,incrementedand unlocked.
i. Explainhow such counters canimproveconcurrency.
ii. Explainwhytheremaybegapsinthesequencenumbersbelong-
ingto the?nal setofcommittedtransactions.
24.2 Supposeyouaregivenarelationr(a,b,c).
a. Give an example of a situation under which the performance of
equality selection queries on attribute a can be greatly affected by
howr isclustered.
b. Supposeyoualsohadrangeselectionqueriesonattributeb.Canyou
clusterr insuchawaythattheequalityselectionqueriesonr.a and
the range selection queries on r.b can both be answered ef?ciently?
Explainyour answer.
c. If clustering as above is not possible, suggest how both types of
queriescanbeexecutedef?cientlybychoosingappropriateindices,
assumingyourdatabasesupportsindex-onlyplans(thatis,ifallin-
formationrequiredforaqueryisavailableinanindex,thedatabase
can generatea planthat usesthe indexbut doesnot access the rela-
tion).
24.3 Suppose that a database application does not appear to have a single
bottleneck;thatis,CPUanddiskutilizationarebothhigh,andalldatabase
queues are roughly balanced. Does that mean the application cannot be
tunedfurther?Explainyour answer.
24.4 Suppose a system runs three types of transactions. Transactions of type
A run at the rate of 50 per second, transactions of type B run at 100 per
second, and transactions of type C run at 200 per second. Suppose the
mix of transactions has 25 percent of type A, 25 percent of type B, and 50
percentoftypeC.
a. Whatistheaveragetransactionthroughputofthesystem,assuming
thereisnointerferencebetweenthetransactions?
b. What factors may result in interference between the transactions of
differenttypes,leadingtothecalculatedthroughputbeingincorrect?
24.5 List some bene?ts and drawbacks of an anticipatory standard compared
to areactionary standard.
Exercises
24.6 Findoutallperformanceinformationyourfavoritedatabasesystempro-
vides.Lookforatleastthefollowing:whatqueriesarecurrentlyexecuting
Bibliographical Notes 1059
or executed recently, what resources each of them consumed (CPU and
I/O), what fraction of page requests resulted in buffer misses (for each
query,ifavailable),andwhatlockshaveahighdegreeofcontention.You
may also be able to get information about CPU and I/O utilization from
theoperatingsystem.
24.7 a. What are the three broad levels at which a database system can be
tunedtoimproveperformance?
b. Givetwoexamplesofhowtuningcanbedoneforeachofthelevels.
24.8 When carrying out performance tuning, should you try to tune your
hardware (by adding disks or memory) ?rst, or should you try to tune
yourtransactions(byaddingindicesormaterializedviews)?rst?Explain
your answer.
24.9 Suppose that your application has transactions that each access and up-
dateasingletupleinaverylargerelationstoredinaB
+
-tree?leorganiza-
tion.AssumethatallinternalnodesoftheB
+
-treeareinmemory,butonly
a very small fraction of the leaf pages can ?t in memory. Explain how to
calculate the minimum number of disks required to support a workload
of 1000 transactions per second. Also calculate the required number of
disks,using valuesfor diskparametersgiveninSection10.2.
24.10 Whatisthemotivationforsplittingalongtransactionintoaseriesofsmall
ones?Whatproblemscouldariseasaresult,andhowcantheseproblems
beaverted?
24.11 Suppose the price of memory falls by half, and the speed of disk access
(number of accesses per second) doubles, while all other factors remain
thesame.Whatwouldbetheeffectofthischangeonthe5-minuteand
1-minuterules?
24.12 List at least four features of the TPC benchmarks that help make them
realisticand dependablemeasures.
24.13 Why was the TPC-D benchmark replaced by the TPC-H and TPC-R bench-
marks?
24.14 Explainwhatapplicationcharacteristicswouldhelpyoudecidewhichof
TPC-C, TPC-H,orTPC-Rbest modelstheapplication.
BibliographicalNotes
Theclassictextonqueueingtheoryis Kleinrock[1975].
An early proposal for a database-system benchmark (the Wisconsin bench-
mark) was made by Bitton et al. [1983]. The TPC-A, -B, and -C benchmarks are
described in Gray [1991]. An online version of all the TPC benchmark descrip-
tions,aswellasbenchmarkresults,isavailableontheWorldWideWebattheURL
1060 Chapter 24 Advanced Application Development
www.tpc.org; the site also contains up-to-date information about new benchmark
proposals.TheOO1benchmarkforOODBsisdescribedinCattellandSkeen[1992];
the OO7 benchmark isdescribedinCareyetal.[1993].
Shasha and Bonnet [2002] provides detailed coverage of database tuning.
O’NeilandO’Neil[2000]providesaverygoodtextbookcoverageofperformance
measurementandtuning.The5-minuteand1-minuterulesaredescribedinGray
andGraefe[1997],andmorerecentlyextendedtoconsidercombinationsofmain
memory,?ash, and disk,inGraefe[2008].
Index selection and materialized view selection are addressed by Ross et al.
[1996], Chaudhuri and Narasayya [1997], Agrawal et al. [2000], and Mistry et al.
[2001].Zilioetal.[2004],Dagevilleetal.[2004],andAgrawaletal.[2004]describe
tuning supportinIBMDB2,Oracleand Microsoft SQL Server.
Information about ODBC, OLE-DB, ADO,andADO.NET can be found on the
Web site www.microsoft.com/data and in a number of books on the subject that
can be found through www.amazon.com. ACM Sigmod Record, which is published
quarterly,has a regularsectiononstandardsindatabases.
AwealthofinformationonXML-basedstandardsandtoolsisavailableonline
on the Web site www.w3c.org. Information about RosettaNet can be found on the
Web at www.rosettanet.org.
Businessprocessre-engineeringiscoveredbyCook[1996].Umar[1997]cov-
ersre-engineeringand issuesindealingwithlegacysystems.
CHAPTER
25
Spatial and Temporal Data and
Mobility
For most of the history of databases, the types of data stored in databases were
relatively simple, and this was re?ected in the rather limited support for data
types in earlier versions of SQL. Over time, however, there developedincreasing
need for handling more complex datatypesin databases, such as temporaldata,
spatialdata, and multimediadata.
Another major trend has created its own set of issues: the growth of mobile
computers,startingwithlaptopcomputersandpocketorganizersandextending
in more recent years to mobile phones with built-in computers and a variety of
wearable computers thatare increasingly used incommercial applications.
Inthischapter,westudyseveraldatatypesandotherdatabaseissuesdealing
with these applications.
25.1 Motivation
Before we address each of the topics in detail, we summarize the motivation for,
and some importantissuesin dealingwith, each of these typesofdata.
• Temporal data. Mostdatabase systemsmodelthe current state ofthe world,
for instance, current customers, current students, and courses currently be-
ing offered. In many applications, it is very important to store and retrieve
information about past states. Historical information can be incorporated
manually into a schema design. However, the task is greatly simpli?ed by
database supportfor temporaldata,which we studyin Section25.2.
• Spatial data. Spatial data include geographic data,suchasmapsandasso-
ciated information, and computer-aided-design data,suchasintegrated-
circuit designs or building designs. Applications of spatial data initially
stored data as ?les in a ?le system, as did early-generation business ap-
plications. But as the complexityand volume of the data, and the number of
users, have grown, ad hoc approaches to storing and retrievingdata in a ?le
1061
1062 Chapter25 SpatialandTemporalDataandMobility
system have proved insuf?cient for the needs of many applications that use
spatialdata.
Spatial-dataapplicationsrequirefacilitiesofferedbyadatabase system—
in particular, the ability to store and query large amounts of data ef?ciently.
Some applications may also require other database features, such as atomic
updates to parts of the stored data, durability, and concurrency control. In
Section25.3,westudytheextensionsneededintraditionaldatabasesystems
to supportspatialdata.
• Multimediadata.InSection25.4,westudythefeaturesrequiredindatabase
systemsthatstoremultimediadatasuchasimage,video,andaudiodata.The
maindistinguishingfeatureofvideoandaudiodataisthatthedisplayofthe
data requires retrieval at a steady, predetermined rate; hence, such data are
calledcontinuous-mediadata.
• Mobiledatabases.InSection25.5,westudythedatabaserequirementsofmo-
bilecomputingsystems,suchaslaptopandnetbookcomputersandhigh-end
cell phones that are connected to base stations via wireless digital commu-
nication networks. Such computers may need to be able to operate while
disconnectedfromthenetwork,unlikethedistributeddatabasesystemsdis-
cussed in Chapter 19. They also have limited storage capacity, and thus
requirespecialtechniques formemorymanagement.
25.2 Time in Databases
A database modelsthe state of some aspectof the real world outside itself.Typi-
cally, databases model only one state—the current state—of the real world, and
do not store information about past states, except perhaps as audit trails. When
the state of the real world changes, the database gets updated, and information
about the old state gets lost. However, in many applications, it is important to
store and retrieveinformation about past states.For example,a patient database
muststoreinformationaboutthemedicalhistoryofapatient.Afactorymonitor-
ing system may store information about current and past readings of sensors in
the factory, for analysis. Databases that store information about states of the real
world across time are calledtemporaldatabases.
Whenconsideringtheissueoftimeindatabasesystems,wemustdistinguish
betweentimeasmeasuredbythesystemandtimeasobservedintherealworld.
The valid time for a fact is the set of time intervals during which the fact is
true in the real world. Thetransactiontime for a fact is the time interval during
which the fact is current within the database system. This latter time is based on
the transaction serialization order and is generated automatically by the system.
Note that valid-time intervals, being a real-world concept, cannot be generated
automatically and mustbe providedtothe system.
A temporal relation is one where each tuple has an associated time when
it is true; the time may be either valid time or transaction time. Of course, both
valid time and transaction time can be stored, in which case the relation is said
25.2 TimeinDatabases 1063
ID name dept name salary from to
10101 Srinivasan Comp. Sci. 61000 2007/1/1 2007/12/31
10101 Srinivasan Comp. Sci. 65000 2008/1/1 2008/12/31
12121 Wu Finance 82000 2005/1/1 2006/12/31
12121 Wu Finance 87000 2007/1/1 2007/12/31
12121 Wu Finance 90000 2008/1/1 2008/12/31
98345 Kim Elec.Eng. 80000 2005/1/1 2008/12/31
Figure 25.1 Atemporalinstructor relation.
tobeabitemporalrelation.Figure25.1showsanexampleofatemporalrelation.
To simplify the representation, each tuple has only one time interval associated
withit;thus,a tupleis representedonce for everydisjointtimeintervalinwhich
it is true. Intervals are shown here as a pair of attributes from and to;anactual
implementation would have a structured type, perhaps called Interval,thatcon-
tains both ?elds. Note that some of the tuples have a “*” in the to time column;
theseasterisksindicatethatthetupleistrueuntilthevalueinthetotimecolumn
is changed; thus, the tuple is true at the current time. Although times are shown
in textual form, they are stored internally in a more compact form, such as the
number of seconds since some ?xed time on a ?xed date (such as 12:00 A.M.,
January 1, 1900) that can be translated back to the normal textual form.
25.2.1 Time Speci?cation in SQL
The SQL standard de?nes the types date, time,andtimestamp as we saw in
Chapter4.Thetypedatecontainsfourdigitsfortheyear(1–9999),twodigitsfor
themonth(1–12),andtwodigitsforthedate(1–31).Thetypetimecontainstwo
digits for the hour, two digits for the minute, and two digits for the second, plus
optional fractional digits. The seconds ?eld can go beyond 60, to allow for leap
seconds that are added during some years to correct for small variations in the
speed of rotation of Earth. The type timestamp contains the ?elds of date and
time, with six fractional digitsfor the seconds ?eld.
Since different places in the world have different local times, there is often a
needforspecifyingthetimezonealongwiththetime.TheUniversalCoordinated
Time (UTC) is a standard reference point for specifying time, with local times
de?ned as offsets from UTC. (The standard abbreviation is UTC, rather than UCT,
since it is an abbreviation of “Universal Coordinated Time”writteninFrenchas
universeltempscoordonn´ e.) SQLalsosupportstwotypes,timewithtimezone,and
timestampwithtimezone, which specify the time as a local time plus the offset
of the local time from UTC.Forinstance,thetimecouldbeexpressedintermsof
U.S. Eastern Standard Time, with an offset of ?6:00, since U.S. Eastern Standard
time is6 hours behind UTC.
SQL supports a type called interval, which allows us to refer to a period of
timesuchas“1day”or“2daysand5hours,”withoutspecifyingaparticulartime
when this period starts. This notion differs from the notion of interval we used
1064 Chapter25 SpatialandTemporalDataandMobility
previously, which refers to an interval of time with speci?c starting and ending
times.
1
25.2.2 Temporal Query Languages
Adatabaserelationwithouttemporalinformationissometimescalledasnapshot
relation,sinceitre?ectsthestateinasnapshotoftherealworld.Thus,asnapshot
of a temporal relation at a point in time t is the set of tuples in the relation that
are true at time t, with the time-interval attributes projected out. The snapshot
operation on a temporal relation gives the snapshot of the relation at a speci?ed
time (orthe currenttime, ifthe time isnot speci?ed).
Atemporalselectionisaselectionthatinvolvesthetimeattributes;atempo-
ralprojectionisaprojectionwherethetuplesintheprojectioninherittheirtimes
fromthetuplesintheoriginalrelation.Atemporaljoinisajoin,withthetimeof
atupleintheresultbeingtheintersectionofthetimesofthetuplesfromwhichit
isderived.Ifthe timesdonot intersect,the tuple isremovedfromthe result.
Thepredicatesprecedes,overlaps,andcontainscanbeappliedonintervals;their
meaningsshouldbeclear.Theintersectoperationcanbeappliedontwointervals,
to give a single (possibly empty) interval. However, the union of two intervals
may ormay not be asingle interval.
Functional dependencies must be used with care in a temporal relation, as
wesawinSection8.9.Althoughtheinstructor IDmayfunctionallydeterminethe
salary at any given point in time, obviously the salary can change over time. A
temporalfunctionaldependency X
  ? Y holds on a relation schema R if, for all
legalinstancesr of R,allsnapshotsofr satisfythefunctionaldependency X ? Y.
Several proposals have been made for extending SQL to improve its support
of temporal data, but at least until SQL:2008, SQL has not provided any special
supportfor temporaldatabeyond the time-relateddatatypesand operations.
25.3 Spatial and Geographic Data
Spatial data support in databases is important for ef?ciently storing, indexing,
and queryingof data on the basis of spatial locations. For example,suppose that
we want to store a set of polygons in a database and to query the database to
?nd all polygons that intersect a given polygon. We cannot use standard index
structures, such as B-trees or hash indices, to answer such a query ef?ciently.
Ef?cient processing of the above query would require special-purpose index
structures,such asR-trees(which we studylater)for the task.
Two typesof spatialdataare particularlyimportant:
• Computer-aided-design(CAD)data,whichincludesspatialinformationabout
howobjects—suchasbuildings,cars,oraircraft—areconstructed.Otherim-
1
Many temporaldatabase researchersfeelthistypeshouldhave beencalledspan sinceitdoesnotspecifyanexactstart
orend time, only the time span betweenthe two.
25.3 SpatialandGeographicData 1065
portant examplesof computer-aided-designdatabases are integrated-circuit
and electronic-devicelayouts.
• Geographicdatasuchasroadmaps,land-usagemaps,topographicelevation
maps,politicalmapsshowingboundaries,land-ownershipmaps,andsoon.
Geographicinformationsystems arespecial-purpose databasestailoredfor
storinggeographic data.
Support for geographic data has been added to many database systems, such as
the IBM DB2SpatialExtender,theInformixSpatialDatablade,andOracleSpatial.
25.3.1 Representation of Geometric Information
Figure 25.2 illustrates how various geometric constructs can be represented in a
database,inanormalizedfashion.Westressherethatgeometricinformationcan
be representedinseveraldifferentways,only someof which we describe.
A line segmentcan be representedby the coordinatesof its endpoints.For ex-
ample,inamapdatabase,thetwocoordinatesofapointwouldbeitslatitudeand
longitude. Apolyline (also called alinestring) consists of a connected sequence
oflinesegmentsandcanberepresentedbyalistcontainingthecoordinatesofthe
endpointsofthesegments,insequence.Wecanapproximatelyrepresentanarbi-
trary curve by polylines, by partitioning the curve into a sequence of segments.
Thisrepresentationisusefulfortwo-dimensionalfeaturessuchasroads;here,the
width of the road is small enough relative to the size of the full map that it can
be considered to be a line. Some systems also support circular arcs as primitives,
allowing curvesto berepresentedassequencesof arcs.
We can represent a polygon by listing its vertices in order, as in Figure 25.2.
2
The list of vertices speci?es the boundary of a polygonal region. In an alterna-
tive representation, a polygon can be dividedinto a set of triangles, as shown in
Figure25.2.Thisprocessiscalledtriangulation,andanypolygoncanbetriangu-
lated. The complex polygon can be given an identi?er, and each of the triangles
into which it is divided carries the identi?er of the polygon. Circles and ellipses
canberepresentedbycorrespondingtypes,orcanbeapproximatedbypolygons.
List-based representations of polylines or polygons are often convenient for
query processing. Such non-?rst-normal-form representations are used when
supported by the underlying database. So that we can use ?xed-size tuples (in
?rstnormalform)forrepresentingpolylines,wecangivethepolylineorcurvean
identi?er,andcanrepresenteachsegmentasaseparatetuplethatalsocarrieswith
ittheidenti?erofthepolylineorcurve.Similarly,thetriangulatedrepresentation
of polygons allowsa ?rst normal formrelationalrepresentationof polygons.
The representation of points and line segments in three-dimensional space
is similar to their representation in two-dimensional space, the only difference
beingthatpointshaveanextrazcomponent.Similarly,therepresentationofpla-
nar?gures—suchastriangles,rectangles,andotherpolygons—doesnotchange
2
Some referencesuse the term closedpolygon to referto what we call polygons,and refer to polylinesas openpolygons.
1066 Chapter25 SpatialandTemporalDataandMobility
1
2
1
1
3
3
4
5
2
2
1
3
4
5
2
line segment
triangle
polygon
polygon
{(x1,y1), (x2,y2)}
{(x1,y1), (x2,y2), (x3,y3)}
{(x1,y1), (x2,y2), (x3,y3), (x4,y4), (x5,y5)}
{(x1,y1), (x2,y2), (x3,y3), ID1}
{(x1,y1), (x3,y3), (x4,y4), ID1}
{(x1,y1), (x4,y4), (x5,y5), ID1}
object representation
Figure 25.2 Representation of geometric constructs.
muchwhenwemovetothreedimensions.Tetrahedronsandcuboidscanberep-
resented in the same way as triangles and rectangles. We can representarbitrary
polyhedra by dividing them into tetrahedrons, just as we triangulate polygons.
Wecanalsorepresentthembylistingtheirfaces,eachofwhichisitselfapolygon,
along with an indicationof which side ofthe face isinsidethe polyhedron.
25.3.2 Design Databases
Computer-aided-design(CAD)systemstraditionallystoreddatainmemorydur-
ing editing or other processing, and wrote the data back to a ?le at the end of a
session of editing. The drawbacks of such a scheme include the cost (program-
ming complexity, as well as time cost) of transforming data from one form to
another,andtheneedtoreadinanentire?leevenifonlypartsofitarerequired.
For large designs, such as the design of a large-scale integrated circuit or the
design of an entire airplane, it may be impossible to hold the complete design in
25.3 SpatialandGeographicData 1067
memory. Designers of object-oriented databases were motivated in large part by
the database requirements of CAD systems. Object-oriented databases represent
components of the design as objects, and the connections between the objects
indicate how the designisstructured.
The objects stored in a design database are generally geometric objects. Sim-
pletwo-dimensionalgeometricobjectsincludepoints,lines,triangles,rectangles,
and,ingeneral,polygons.Complextwo-dimensionalobjectscanbeformedfrom
simple objects by means of union, intersection, and difference operations. Simi-
larly,complexthree-dimensionalobjectsmaybeformedfromsimplerobjectssuch
as spheres, cylinders, and cuboids, by union, intersection, and difference opera-
tions, as in Figure 25.3. Three-dimensional surfaces may also be represented by
wireframemodels,whichessentiallymodelthesurfaceasasetofsimplerobjects,
such asline segments,triangles, and rectangles.
Design databases also store nonspatial information about objects, such as
the material from which the objects are constructed. We can usually model such
information by standard data-modeling techniques. We concern ourselves here
with only the spatialaspects.
Various spatial operations must be performed on a design. For instance, the
designer may want to retrieve that part of the design that corresponds to a par-
ticular region of interest. Spatial-index structures, discussed in Section 25.3.5,
are useful for such tasks. Spatial-index structures are multidimensional, dealing
with two- and three-dimensional data, rather than dealing with just the simple
one-dimensionalorderingprovidedby theB
+
-trees.
Spatial-integrity constraints, such as “two pipes should not be in the same
location,” are important in design databases to prevent interference errors. Such
errorsoftenoccurifthedesignisperformedmanually,andaredetectedonlywhen
aprototypeisbeingconstructed.Asaresult,theseerrorscanbeexpensiveto?x.
Database support for spatial-integrity constraints helps people to avoid design
(a) Di?erence of cylinders (b) Union of cylinders
Figure 25.3 Complex three-dimensional objects.
1068 Chapter25 SpatialandTemporalDataandMobility
errors,therebykeepingthedesignconsistent.Implementingsuchintegritychecks
again dependson the availabilityof ef?cientmultidimensionalindexstructures.
25.3.3 Geographic Data
Geographicdataarespatialinnature,butdifferfromdesigndataincertainways.
Maps and satellite images are typical examples of geographic data. Maps may
provide not only location information—about boundaries, rivers, and roads, for
example—but also much more detailed information associated with locations,
such aselevation,soiltype,land usage,and annual rainfall.
25.3.3.1 ApplicationsofGeographicData
Geographic databases have a variety of uses, including online map services;
vehicle-navigation systems; distribution-network information for public-service
utilities such as telephone, electric-power, and water-supply systems; and land-
usage information forecologistsand planners.
Web-based road map services form a very widely used application of map
data. At the simplest level, these systems can be used to generate online road
mapsofadesiredregion.Animportantbene?tofonlinemapsisthatitiseasyto
scale the maps to the desired size—that is, to zoom in and out to locate relevant
features.Roadmapservicesalsostoreinformationaboutroadsandservices,such
asthelayoutofroads,speedlimitsonroads,roadconditions,connectionsbetween
roads,andone-wayrestrictions.Withthisadditionalinformationaboutroads,the
maps can be used for getting directions to go from one place to another and for
automatic trip planning. Users can query online information about services to
locate, for example, hotels, gas stations, or restaurants with desired offerings
and price ranges. In recent years, several Web-based map services have de?ned
APIs that allow programmers to create customized maps that include data from
the map service along with data from other sources. Such customized maps can
be used to display, for example, houses available for sale or rent, or shops and
restaurants, in aparticular area.
Vehicle-navigationsystemsaresystemsthataremountedinautomobilesand
provideroadmapsandtrip-planningservices.TheyincludeaGlobalPositioning
System (GPS) unit, which uses information broadcast from GPS satellitesto ?nd
the current location with an accuracy of tens of meters. With such a system, a
driver can never
3
get lost—the GPS unit ?nds the location in terms of latitude,
longitude, and elevation and the navigation system can query the geographic
databaseto ?nd whereand onwhich road thevehicleiscurrentlylocated.
Geographic databases for public-utility information have become very im-
portant as the network of buried cables and pipes has grown. Without detailed
maps, work carried out by one utility may damage the cables of another utility,
resultinginlarge-scaledisruptionofservice.Geographicdatabases,coupledwith
accurate location-?nding systems,help avoidsuch problems.
3
Well,hardly ever!
25.3 SpatialandGeographicData 1069
25.3.3.2 RepresentationofGeographicData
Geographicdata canbe categorizedinto two types:
• Raster data. Such data consist of bit maps or pixel maps, in two or more
dimensions.Atypicalexampleofatwo-dimensionalrasterimageisasatellite
imageofanarea.Inadditiontotheactualimage,thedataincludesthelocation
oftheimage,speci?edforexamplebythelatitudeandlongitudeofitscorners,
and the resolution, speci?ed either by the total number of pixels, or, more
commonly in the context of geographic data, by the area covered by each
pixel.
Raster data is often representedastiles, each covering a ?xed sized area.
A larger area can be displayed by displaying all the tiles that overlap with
the area. To allow the display of data at different zoom levels, a separate set
of tiles is created for each zoom level. Once the zoom level is set by the user
interface(forexampleaWebbrowser),tilesatthatzoomlevel,whichoverlap
theareabeing displayed,areretrievedand displayed.
Raster data can be three-dimensional—for example, the temperature
at different altitudes at different regions, again measured with the help of
a satellite. Time could form another dimension—for example, the surface
temperaturemeasurementsat differentpointsin time.
• Vector data. Vector data are constructed from basic geometric objects, such
as points, line segments, polylines, triangles, and other polygons in two
dimensions,andcylinders,spheres,cuboids,andotherpolyhedronsinthree
dimensions.Inthecontextofgeographicdata,pointsareusuallyrepresented
by latitude and longitude, and where the height is relevant, additionally by
elevation.
Map data are often represented in vector format. Roads are often repre-
sentedaspolylines.Geographicfeatures,suchaslargelakes,orevenpolitical
features such as states and countries, are represented as complex polygons.
Some features, such as rivers, may be represented either as complex curves
oras complexpolygons, dependingonwhether theirwidthisrelevant.
Geographic information related to regions, such as annual rainfall, can be
represented as an array—that is, in raster form. For space ef?ciency, the array
can be stored in a compressed form. In Section 25.3.5.2, we study an alternative
representationof such arrays by a datastructurecalled a quadtree.
As another alternative, we can represent region information in vector form,
usingpolygons,whereeachpolygonisaregionwithinwhichthearrayvalueisthe
same. The vector representation is more compact than the raster representation
in some applications. It is also more accurate for some tasks, such as depicting
roads, where dividing the region into pixels (which may be fairly large) leads to
a loss of precision in location information. However, the vector representation is
unsuitable for applications where the data are intrinsically raster based, such as
satelliteimages.
1070 Chapter25 SpatialandTemporalDataandMobility
Topographical information, that is information about the elevation (height)
of each point on a surface, can be representedin raster form. Alternativelyit can
be representedinvector form by dividingthesurface into polygons covering re-
gionsof(approximately)equalelevation,withasingleelevationvalueassociated
with each polygon. As another alternative, the surface can be triangulated (that
is, divided into triangles), with each triangle represented by the latitude, longi-
tude, and elevation of each of its corners. The latter representation, called the
triangulatedirregularnetwork(TIN)representation,isacompactrepresentation
which isparticularlyuseful forgeneratingthree-dimensionalviewsof an area.
Geographicinformationsystemsusuallycontainbothrasterandvectordata,
and can merge the two kinds of data when displaying results to users. For ex-
ample, maps applications usually contain both satellite images and vector data
about roads,buildingand otherlandmarks.A mapdisplayusuallyoverlays dif-
ferent kinds of information; for example, road information can be overlaid on
a background satellite image, to create a hybrid display. In fact, a map typically
consistsofmultiplelayers,whicharedisplayedinbottom-to-toporder;datafrom
higher layersappearson topof datafrom lowerlayers.
It is also interesting to note that even information that is actually stored in
vector form may be converted to raster form before it is sent to a user interface
such as a Web browser. One reason is that even Web browsers that do not sup-
port scripting languages (required to interpretand display vector data) can then
display map data; a second reason may be to prevent end users from extracting
and using the vectordata.
MapservicessuchasGoogleMapsandYahoo!Mapsprovide APIsthatallow
users to create specialized map displays, containing application speci?c data
overlaidontopofstandardmapdata.Forexample,aWebsitemayshowamapof
anareawithinformationaboutrestaurantsoverlaidonthemap.Theoverlayscan
be constructed dynamically, displaying only restaurants with a speci?c cuisine,
for example, or allowing users to change the zoom level,or pan the display. The
maps APIs for a speci?c language (typically JavaScript or Flash) are built on top
of a Web servicethat providesthe underlyingmap data.
25.3.4 Spatial Queries
There are a number oftypesof queriesthat involvespatiallocations.
• Nearness queries request objects that lie near a speci?ed location. A query
to ?nd all restaurants that lie within a given distance of a given point is an
exampleofanearnessquery.Thenearest-neighborqueryrequeststheobject
that is nearest to a speci?ed point. For example, we may want to ?nd the
nearestgasolinestation.Notethatthisquerydoesnothavetospecifyalimit
onthedistance,andhence wecanaskitevenifwehaveno ideahowfarthe
nearestgasoline station lies.
• Region queries deal with spatial regions. Such a query can ask for objects
that lie partially or fully inside a speci?ed region. A query to ?nd all retail
shops within thegeographic boundaries ofa giventown is an example.
25.3 SpatialandGeographicData 1071
• Queries may also requestintersections andunions of regions. For example,
given region information, such as annual rainfall and population density, a
query may request all regions with a low annual rainfall as well as a high
populationdensity.
Queriesthatcomputeintersectionsofregionscanbethoughtofascomputing
the spatial join of two spatial relations—for example, one representing rainfall
andtheotherrepresentingpopulationdensity—withthelocationplayingtherole
of join attribute. In general, given two relations, each containing spatial objects,
thespatialjoinofthetworelationsgenerateseitherpairsofobjectsthatintersect,
or the intersectionregionsof such pairs.
Several join algorithms ef?ciently compute spatial joins on vector data. Al-
though nested-loop join and indexed nested-loop join (with spatial indices) can
be used, hash joins and sort–merge joins cannot be used on spatial data. Re-
searchers have proposed join techniques based on coordinated traversal of spa-
tial index structures on the two relations. See the bibliographical notes for more
information.
In general, queries on spatial data may have a combination of spatial and
nonspatialrequirements.Forinstance,wemaywantto?ndthenearestrestaurant
that has vegetarianselectionsand that charges lessthan $10 for ameal.
Sincespatialdataareinherentlygraphical,weusuallyquerythembyusinga
graphicalquerylanguage.Resultsofsuchqueriesarealsodisplayedgraphically,
rather than in tables. The user can invoke various operations on the interface,
such as choosing an area to be viewed (for example, by pointing and clicking
on suburbs west of Manhattan), zooming in and out, choosing what to display
on the basis of selection conditions (for example, houses with more than three
bedrooms), overlay of multiple maps (for example, houses with more than three
bedrooms overlaid on a map showing areas with low crime rates), and so on.
The graphical interface constitutes the front end. Extensions of SQL have been
proposed to permitrelational databases to store and retrievespatial information
ef?ciently, and also to allow queries to mix spatial and nonspatial conditions.
Extensions include allowing abstract data types, such as lines, polygons, and bit
maps, and allowing spatial conditions, such as contains or overlaps.
25.3.5 Indexing of Spatial Data
Indices are required for ef?cient access to spatial data. Traditional index struc-
tures, such as hash indices and B-trees, are not suitable, since they deal only
with one-dimensional data, whereas spatial data are typically of two or more
dimensions.
25.3.5.1 k-dTrees
To understand how to index spatial data consisting of two or more dimensions,
weconsider?rsttheindexingofpointsinone-dimensionaldata.Treestructures,
such as binary trees and B-trees, operate by successively dividing space into
smaller parts. For instance, each internal node of a binary tree partitions a one-
1072 Chapter25 SpatialandTemporalDataandMobility
313
2
33
2
Figure 25.4 Division of space by a k-d tree.
dimensional interval in two. Points that lie in the left partition go into the left
subtree;pointsthatlieintherightpartitiongointotherightsubtree.Inabalanced
binary tree, the partition is chosen so that approximately one-half of the points
stored in the subtree fall in each partition. Similarly, each level of a B-tree splits a
one-dimensional interval into multiple parts.
We can use that intuition to create tree structures for two-dimensional space,
aswellasinhigher-dimensionalspaces.Atreestructurecalledak-dtreewasone
oftheearlystructuresusedforindexinginmultipledimensions.Eachlevelofak-d
tree partitions the space into two. The partitioning is done along one dimension
at the node at the top level of the tree, along another dimension in nodes at the
next level, and so on, cycling through the dimensions. The partitioning proceeds
in such a way that, at each node, approximately one-half of the points stored in
thesubtreefallononesideand one-halffallontheother.Partitioningstopswhen
a node has less than a given maximum number of points. Figure 25.4 shows a
set of points in two-dimensional space, and a k-d tree representation of the set
of points. Each line corresponds to a node in the tree, and the maximum number
of points in a leaf node has been set at 1. Each line in the ?gure (other than the
outside box) corresponds to a node in the k-d tree. The numbering of the lines in
the?gureindicatesthelevelofthetreeatwhichthecorrespondingnodeappears.
The k-d-B tree extends the k-d tree to allow multiple child nodes for each
internal node, just as a B-tree extends a binary tree, to reduce the height of the
tree. k-d-B trees are better suited for secondary storage than k-d trees.
25.3.5.2 Quadtrees
Analternativerepresentationfortwo-dimensionaldataisaquadtree.Anexample
of the division of space by a quadtree appears in Figure 25.5. The set of points
25.3 SpatialandGeographicData 1073
Figure 25.5 Division of space by a quadtree.
is the same as that in Figure 25.4. Each node of a quadtree is associated with a
rectangularregionofspace.Thetopnodeisassociatedwiththeentiretargetspace.
Eachnonleafnodeinaquadtreedividesitsregionintofourequal-sizedquadrants,
and correspondingly each such node has four child nodes corresponding to the
fourquadrants.Leafnodeshavebetweenzeroandsome?xedmaximumnumber
of points. Correspondingly, if the region corresponding to a node has more than
the maximum number of points, child nodes are created for that node. In the
examplein Figure 25.5, the maximum number ofpoints in a leafnode issetto 1.
ThistypeofquadtreeiscalledaPRquadtree,toindicate itstorespoints, and
thatthedivisionofspaceisdividedbasedonregions,ratherthanontheactualset
ofpointsstored.Wecanuseregionquadtreestostorearray(raster)information.
Anodeinaregionquadtreeisaleafnodeifallthearrayvaluesintheregionthat
it covers are the same. Otherwise, it is subdivided further into four children of
equal area, and is therefore an internal node. Each node in the region quadtree
correspondstoasubarrayofvalues.Thesubarrayscorrespondingtoleaveseither
contain just a single array element or have multiple array elements, all of which
have the same value.
Indexing of line segments and polygons presents new problems. There are
extensions of k-d trees and quadtrees for this task. However, a line segment or
polygonmaycrossapartitioningline.Ifitdoes,ithastobesplitandrepresented
in each of the subtrees in which its pieces occur. Multiple occurrences of a line
segmentorpolygoncanresultininef?cienciesinstorage,aswellasinef?ciencies
in querying.
25.3.5.3 R-Trees
A storage structure called an R-tree is useful for indexing of objects such as
points, line segments, rectangles, and other polygons. An R-tree is a balanced
1074 Chapter25 SpatialandTemporalDataandMobility
tree structure with the indexed objects stored in leaf nodes, much like a B
+
-tree.
However, instead of a range of values, a rectangularboundingbox is associated
with each tree node. The bounding box of a leaf node is the smallest rectangle
paralleltotheaxesthatcontainsallobjectsstoredintheleafnode.Thebounding
box of internal nodes is, similarly, the smallest rectangle parallel to the axes that
contains the bounding boxes of its child nodes. The bounding box of an object
(such as a polygon) is de?ned, similarly, as the smallest rectangle parallel to the
axesthat contains the object.
Each internal node stores the bounding boxes of the child nodes along with
the pointers to the child nodes. Each leaf node stores the indexed objects, and
mayoptionallystoretheboundingboxesoftheobjects;theboundingboxeshelp
speedupchecksforoverlapsoftherectanglewiththeindexedobjects—ifaquery
rectangle does not overlap with the bounding box of an object, it cannot overlap
with the object, either.(If the indexedobjects are rectangles,thereis of course no
need tostore bounding boxes, since they are identicalto the rectangles.)
Figure 25.6 shows an example of a set of rectangles(drawn with a solid line)
and the bounding boxes(drawn with a dashedline)of the nodesofan R-tree for
the set of rectangles. Note that the bounding boxes are shown with extra space
inside them, to make them stand out pictorially. In reality, the boxes would be
smaller and ?t tightly on the objects that they contain; that is, each side of a
bounding box B would touch at least one of the objects or bounding boxes that
are contained in B.
The R-tree itself is at the right side of Figure 25.6. The ?gure refers to the
coordinatesof bounding box i as BB
i
in the ?gure.
We shall now see how to implement search, insert, and delete operations on
anR-tree.
BB
1
BB
2
BB
BC AE FH I
A
B
C
I
E
F
H
1
2
3
D
G
DG
3
Figure 25.6 An R-tree.
25.3 SpatialandGeographicData 1075
• Search. As the ?gure shows, the bounding boxes associated with sibling
nodes may overlap; in B
+
-trees, k-d trees, and quadtrees, in contrast, the
ranges do not overlap. A search for objects containing a point therefore has
tofollowallchildnodeswhoseassociatedboundingboxescontainthepoint;
asaresult,multiplepathsmayhavetobesearched.Similarly,aqueryto?nd
all objects that intersect a givenobject has to go down everynode where the
associated rectangle intersectsthe givenobject.
• Insert. When we insert an object into an R-tree,we selecta leafnode to hold
the object. Ideally we should pick a leaf node that has space to hold a new
entry, and whose bounding box contains the bounding box of the object.
However, such a node may not exist; evenif it did,?nding the node may be
very expensive, since it is not possible to ?nd it by a single traversal down
from the root. At each internal node we may ?nd multiple children whose
bounding boxes contain the bounding box of the object, and each of these
childrenneedstobeexplored.Therefore,asaheuristic,inatraversalfromthe
root, if any of the child nodes has a bounding box containing the bounding
boxoftheobject,theR-treealgorithmchoosesoneofthemarbitrarily.Ifnone
of the children satisfy this condition, the algorithm chooses a child node
whose bounding box has the maximum overlap with the bounding box of
theobject for continuing thetraversal.
Oncetheleafnodehasbeenreached,ifthenodeisalreadyfull,thealgorithm
performs node splitting (and propagates splitting upward if required) in a
mannerverysimilartoB
+
-treeinsertion.JustaswithB
+
-treeinsertion,theR-
treeinsertionalgorithmensuresthatthetreeremainsbalanced.Additionally,
it ensures that the bounding boxes of leaf nodes, as well as internal nodes,
remainconsistent;thatis,boundingboxesofleavescontainallthebounding
boxes of the objects stored at the leaf, while the bounding boxes for internal
nodes contain all the bounding boxes ofthe childrennodes.
The main difference of the insertion procedure from the B
+
-tree insertion
procedure lies in how the node is split. In a B
+
-tree, it is possible to ?nd a
valuesuchthathalftheentriesarelessthanthemidpointandhalfaregreater
than the value. This property does not generalize beyond one dimension;
that is, for more than one dimension, it is not always possible to split the
entriesintotwosetssothat theirbounding boxesdonot overlap.Instead,as
a heuristic, the set of entries S can be split into two disjoint sets S
1
and S
2
so
that the bounding boxes of S
1
and S
2
have the minimum total area; another
heuristic would be to split the entries into two sets S
1
and S
2
in such a way
that S
1
and S
2
haveminimumoverlap.Thetwonodesresultingfromthesplit
wouldcontaintheentriesin S
1
and S
2
,respectively.Thecostof?ndingsplits
withminimumtotalareaoroverlapcanitselfbelarge,socheaperheuristics,
suchasthequadraticsplitheuristic,areused.(Theheuristicgetsisnamefrom
the fact that ittakestime quadraticin the number of entries.)
The quadratic split heuristic works this way: First, it picks a pair of
entriesa andb from Ssuchthatputtingtheminthesamenodewouldresult
in a bounding box with the maximum wasted space; that is, the area of the
1076 Chapter25 SpatialandTemporalDataandMobility
minimumboundingboxofa andbminusthesumoftheareasofa andbisthe
largest.Theheuristicplacestheentriesa andb insets S
1
and S
2
,respectively.
It then iteratively adds the remaining entries, one entry per iteration, to
one of the two sets S
1
or S
2
. At each iteration, for each remaining entry e,let
i
e,1
denote the increase in the size of the bounding box of S
1
if e is added to
S
1
and let i
e,2
denote the corresponding increase for S
2
.Ineachiteration,the
heuristic chooses one of the entries with the maximum difference of i
e,1
and
i
e,2
andaddsitto S
1
ifi
e,1
islessthani
e,2
,andtoS
2
otherwise.Thatis,anentry
with “maximum preference” for one of S
1
or S
2
is chosen at each iteration.
The iteration stops when all entries have been assigned, or when one of the
sets S
1
or S
2
hasenoughentriesthatallremainingentrieshavetobeaddedto
the othersetsothe nodesconstructed from S
1
and S
2
both have therequired
minimum occupancy. The heuristic then adds all unassigned entries to the
setwithfewerentries.
• Deletion. Deletion can be performed like a B
+
-tree deletion, borrowing en-
tries from sibling nodes, or merging sibling nodes if a node becomes under-
full. An alternative approach redistributes all the entries of underfull nodes
to sibling nodes, with the aim of improving the clustering of entries in the
R-tree.
See the bibliographical references for more details on insertion and deletion op-
erationson R-trees,as wellason variants of R-trees,calledR
?
-treesor R
+
-trees.
The storage ef?ciency of R-trees is better than that of k-d trees or quadtrees,
since an object is stored only once, and we can ensure easily that each node is
at least half full. However, querying may be slower, since multiple paths have
to be searched. Spatial joins are simpler with quadtrees than with R-trees, since
all quadtrees on a region are partitioned in the same manner. However, because
of their better storage ef?ciency, and their similarity to B-trees, R-trees and their
variantshave provedpopularin database systemsthat supportspatialdata.
25.4 Multimedia Databases
Multimedia data, such as images, audio, and video—an increasingly popular
form of data—are today almost always stored outside the database, in ?le sys-
tems. This kind of storage is not a problem when the number of multimedia
objects is relatively small, since features provided by databases are usually not
important.
However,databasefeaturesbecomeimportantwhenthenumberofmultime-
diaobjectsstoredislarge.Issuessuchastransactionalupdates,queryingfacilities,
and indexingthen become important.Multimediaobjects oftenhave descriptive
attributes, such as those indicating when they were created, who created them,
and to what category theybelong. One approach to buildinga database for such
multimedia objects is to use databases for storing the descriptive attributes and
for keepingtrack of the ?les inwhich the multimediaobjectsare stored.
25.4 MultimediaDatabases 1077
However,storingmultimediaoutsidethedatabasemakesithardertoprovide
database functionality, such as indexing on the basis of actual multimedia data
content. It can also lead to inconsistencies, such as a ?le that is noted in the
database, but whose contents are missing, or vice versa. It is therefore desirable
to storethe datathemselvesinthe database.
Several issues must be addressed if multimedia data are to be stored in a
database.
• The database must support large objects, since multimedia data such as
videos can occupy up to a few gigabytes of storage. Many database sys-
temsdonotsupportobjectslargerthanafewgigabytes.Largerobjectscould
be split into smaller pieces and stored in the database. Alternatively, the
multimedia object may be stored in a ?le system, but the database may con-
tain a pointer to the object; the pointer would typically be a ?le name. The
SQL/MED standard (MED stands for Management of External Data) allows
external data, such as ?les, to be treated as if they are part of the database.
With SQL/MED, the object would appear to be part of the database, but can
be stored externally.We discussmultimediadataformatsin Section 25.4.1.
• Theretrievalofsometypesofdata,suchasaudioandvideo,hastherequire-
mentthatdatadeliverymustproceedataguaranteedsteadyrate.Suchdata
are sometimes called isochronous data,orcontinuous-media data.Forex-
ample,ifaudiodataarenotsuppliedintime,therewillbegapsinthesound.
If the data are supplied too fast, system buffers may over?ow, resulting in
lossof data. We discusscontinuous-media data in Section25.4.2.
• Similarity-based retrieval is needed in many multimedia database applica-
tions. For example, in a database that stores ?ngerprint images, a query
?ngerprint image is provided, and ?ngerprints in the database that are sim-
ilar to the query ?ngerprint must be retrieved. Index structures such as B
+
-
trees and R-trees cannot be used for this purpose; special index structures
needto be created.We discusssimilarity-basedretrievalinSection25.4.3.
25.4.1 Multimedia Data Formats
Because of the large number of bytes required to represent multimedia data, it
is essential that multimedia data be stored and transmitted in compressed form.
Forimage data, the mostwidelyusedformatis JPEG,named afterthe standards
body that created it, the Joint Picture Experts Group.W ecanstorevideodataby
encoding each frame of video in JPEG format, but such an encoding is wasteful,
since successive frames of a video are often nearly the same. The Moving Picture
ExpertsGrouphasdevelopedtheMPEGseriesofstandardsforencodingvideoand
audio data; these encodings exploit commonalities among a sequence of frames
toachieveagreaterdegreeofcompression.The MPEG-1standardstoresaminute
of 30-frame-per-second videoand audio in approximately12.5 megabytes(com-
pared to approximately 75 megabytes for video in only JPEG). However, MPEG-1
encoding introduces some loss of video quality, to a level roughly comparable
1078 Chapter25 SpatialandTemporalDataandMobility
to that of VHS videotape. The MPEG-2 standard is designed for digital broadcast
systems and digital video disks (DVDs); it introduces only a negligible loss of
videoquality. MPEG-2 compresses1 minute of videoand audioto approximately
17megabytes.MPEG-4providestechniquesforfurthercompressionofvideo,with
variablebandwidthtosupportdeliveryofvideodataovernetworkswithawide
range of bandwidths. Severalcompeting standards are used for audio encoding,
including MP3,whichstandsforMPEG-1 Layer 3, RealAudio, Windows Media
Audio,andotherformats.High-de?nitionvideowithaudioisencodedinseveral
variantsof MPEG-4 that include MPEG-4 AVC and AVCHD.
25.4.2 Continuous-Media Data
Themostimportanttypesofcontinuous-mediadataarevideoandaudiodata(for
example,adatabaseofmovies).Continuous-mediasystemsarecharacterizedby
theirreal-timeinformation-deliveryrequirements:
• Data must be delivered suf?ciently fast that no gaps in the audio or video
result.
• Data must be delivered at a rate that does not cause over?ow of system
buffers.
• Synchronizationamongdistinctdatastreamsmustbemaintained.Thisneed
arises, for example, when the video of a person speaking must show lips
movingsynchronously with the audioof the person speaking.
To supply data predictably at the right time to a large number of consumers
ofthedata,thefetchingofdatafromdiskmustbecoordinatedcarefully.Usually,
dataarefetchedinperiodiccycles.Ineachcycle,sayofnseconds,nseconds’worth
ofdataisfetchedforeachconsumerandstoredinmemorybuffers,whilethedata
fetched in the previous cycle is being sent to the consumers from the memory
buffers. The cycle period is a compromise: A short period uses less memory but
requires more disk-arm movement, which is a waste of resources, while a long
periodreducesdisk-armmovementbutincreasesmemoryrequirementsandmay
delay initial delivery of data. When a new request arrives, admission control
comes into play: That is, the system checks if the request can be satis?ed with
available resources(ineach period);if so,itisadmitted;otherwise itisrejected.
Extensiveresearchondeliveryofcontinuous-mediadatahasdealtwithsuch
issues as handling arrays of disks and dealing with disk failure. See the biblio-
graphical referencesfor details.
Several vendors offer video-on-demand servers. Current systems are based
on?lesystems,becauseexistingdatabasesystemsdonotprovidethereal-timere-
sponsethattheseapplicationsneed.Thebasicarchitectureofavideo-on-demand
systemcomprises:
• Video server.Multimediadataarestoredonseveraldisks(usuallyina RAID
con?guration). Systems containing a large volume of data may use tertiary
storage for lessfrequentlyaccessed data.
25.5 MobilityandPersonalDatabases 1079
• Terminals. People view multimedia data through various devices, collec-
tively referred to as terminals. Examples are personal computers and televi-
sions attached to a small,inexpensivecomputercalled aset-topbox.
• Network. Transmission of multimedia data from a server to multiple termi-
nals requiresa high-capacity network.
Video-on-demandserviceovercable networks iswidelyavailable.
25.4.3 Similarity-Based Retrieval
In many multimedia applications, data are described only approximately in the
database.Anexampleisthe?ngerprintdatainSection25.4.Otherexamplesare:
• Pictorialdata.Twopicturesorimagesthatareslightlydifferentasrepresented
inthedatabasemaybeconsideredthesamebyauser.Forinstance,adatabase
maystoretrademarkdesigns.Whenanewtrademarkistoberegistered,the
systemmay need?rst to identifyall similartrademarksthat wereregistered
previously.
• Audiodata.Speech-baseduserinterfacesarebeingdevelopedthatallowthe
user to give a command or identify a data item by speaking.The input from
the user must then be tested for similarity to those commands or data items
storedin the system.
• Handwrittendata.Handwritteninputcanbeusedtoidentifyahandwritten
data item or command stored in the database. Here again, similarity testing
isrequired.
The notion of similarity is often subjective and user speci?c. However, sim-
ilarity testing is often more successful than speech or handwriting recognition,
because the input can be compared to data already in the system and, thus, the
setof choices available tothe systemis limited.
Severalalgorithmsexistfor?ndingthebestmatchestoagiveninputbysim-
ilarity testing. Many voice-activated systems have been deployed commercially,
particularly for phone applications and in-vehicle controls. See the bibliographi-
cal notesfor references.
25.5 Mobility and Personal Databases
Large-scale,commercialdatabaseshavetraditionallybeenstoredincentralcom-
puting facilities. In distributed database applications, there has usually been
strong central database and network administration. Several technology trends
havecombinedtocreateapplicationsinwhichthisassumptionofcentralcontrol
and administrationisnot entirelycorrect:
• The widespreaduse of laptop,notebook, or netbook computers.
• The widespreaduse of cellphones with the capabilitiesof acomputer.
1080 Chapter25 SpatialandTemporalDataandMobility
• The development of a relatively low-cost wireless digital communication
infrastructure, based on wireless local-area networks, cellular digital packet
networks, and othertechnologies.
Mobile computing has proved useful in many applications. Many business
travelers use laptop computers so that they can work and access data en route.
Deliveryservicesusemobilecomputerstoassistinpackagetracking.Emergency-
response services use mobile computers at the scene of disasters, medical emer-
gencies, and the like to access information and to enter data pertaining to the
situation. Cell phones are increasingly becoming devices that provide not only
phone services, but are also mobile computers allowing email and Web access.
New applicationsof mobile computerscontinue toemerge.
Wirelesscomputing createsa situation where machines no longerhave ?xed
locationsandnetworkaddresses.Location-dependentqueriesareaninteresting
class of queries that are motivated by mobile computers; in such queries, the
location of the user (computer) is a parameter of the query. The value of the
locationparameterisprovidedbyaglobalpositioningsystem(GPS).Anexample
isatraveler’sinformationsystemthatprovidesdataonhotels,roadsideservices,
and the like to motorists. Processing of queries about services that are ahead on
thecurrentroutemustbebasedonknowledgeoftheuser’slocation,directionof
motion, and speed. Increasingly, navigational aids are being offered as a built-in
feature inautomobiles.
Energy (battery power) is a scarce resource for most mobile computers. This
limitationin?uencesmanyaspectsofsystemdesign.Amongthemoreinteresting
consequencesoftheneedforenergyef?ciencyisthatsmallmobiledevicesspend
most of their time sleeping, waking up for a fraction of a second every second
or so to check for incoming data and to send outgoing data. This behavior has a
signi?cantimpactonprotocolsusedtocommunicatewithmobiledevices.Theuse
of scheduled data broadcasts to reduce the need for mobile systems to transmit
queriesisanother way toreduceenergyrequirements.
Increasing amounts of data may reside on machines administered by users,
rather than by database administrators. Furthermore, these machines may, at
times,bedisconnectedfromthenetwork.Inmanycases,thereisacon?ictbetween
the user’s need to continue to work while disconnected and the need for global
dataconsistency.
A user is likely to use more than one mobile device. Such users need to be
able to view their data in its most up-to-date version regardless of which device
isbeingused ata giventime.Often,thiscapabilityissupportedbysome variant
of cloud computing, which we discussedin Section19.9.
In Sections 25.5.1 through 25.5.4, we discuss techniques in use and under
developmentto dealwith the problemsof mobilityand personal computing.
25.5.1 A Model of Mobile Computing
The mobile-computingenvironmentconsistsofmobilecomputers,referredtoas
mobile hosts, and a wired network of computers. Mobile hosts communicate
25.5 MobilityandPersonalDatabases 1081
with the wired network via computers referred to as mobile support stations.
Eachmobilesupportstationmanagesthosemobilehostswithinitscell—thatis,
the geographical area that it covers. Mobile hosts may move between cells, thus
necessitating a handoff of control from one mobile support station to another.
Sincemobilehostsmay,attimes,bepowereddown,ahostmayleaveonecelland
rematerialize later at some distant cell. Therefore, moves between cells are not
necessarilybetweenadjacentcells.Withinasmallarea,suchasabuilding,mobile
hosts may be connected by a wireless local-area network (LAN)thatprovides
lower-costconnectivitythanwouldawide-areacellularnetwork,andthatreduces
the overhead of handoffs.
It is possible for mobile hosts to communicate directly without the interven-
tion of a mobile support station. However, such communication can occur only
betweennearby hosts. Such directformsof communication often useBluetooth,
ashort-rangedigitalradiostandardthatallowswirelessconnectivitywithina10-
meterrangeathighspeed(upto721kilobitspersecond).Initiallyconceivedasa
replacement for cables, Bluetooth’s greatest bene?t is in easy ad hoc connection
of mobile computers, PDAs, mobile phones, and so-called intelligentappliances.
Wirelesslocal-areanetworksystemsbasedonthe801.11(a/b/g/n)standards
are verywidelyused today, and systems based on the 802.16 (Wi-Max) standard
arebeing deployed.
Thenetworkinfrastructureformobilecomputingconsistsinlargepartoftwo
technologies: wireless local-area networks and packet-based cellular telephony
networks. Early cellular systems used analog technology and were designed for
voice communication. Second-generation digital systems retained the focus on
voiceapplications.Third-generation(3G)andso-called2.5Gsystemsusepacket-
based networking and are more suited to data applications. In these networks,
voice is just one of many applications (albeit an economically important one).
Fourth-generation(4G) technologies include Wi-Max as wellas severalcompeti-
tors.
Bluetooth,wirelessLANs,and2.5Gand3Gcellularnetworksmakeitpossible
forawidevarietyofdevicestocommunicateatlowcost.Whilesuchcommunica-
tionitselfdoesnot?tthedomainofausualdatabaseapplication,theaccounting,
monitoring, and management data pertaining to this communication generate
huge databases. The immediacyof wirelesscommunication generatesaneed for
real-timeaccesstomanyofthesedatabases.Thisneedfortimelinessaddsanother
dimensiontotheconstraintsonthesystem—amatterweshalldiscussfurtherin
Section26.4.
The size and power limitations of many mobile computers have led to al-
ternative memory hierarchies. Instead of, or in addition to, disk storage, ?ash
memory,whichwediscussedinSection10.1,maybeincluded.Ifthemobilehost
includesaharddisk,thediskmaybeallowedtospindownwhenitisnotinuse,
tosaveenergy.Thesameconsiderationsofsizeandenergylimitthetypeandsize
of the display used in a mobile device. Designers of mobile devices often create
special-purpose user interfaces to work within these constraints. However, the
needtopresentWeb-baseddatahasnecessitatedthecreationofpresentationstan-
dards. Wireless application protocol (WAP) is a standard for wireless Internet
1082 Chapter25 SpatialandTemporalDataandMobility
access. WAP-based browsers access special Web pages that usewirelessmarkup
language (WML), an XML-based language designed for the constraints of mobile
and wirelessWebbrowsing.
25.5.2 Routing and Query Processing
Theroutebetweenapairofhostsmaychangeovertimeifoneofthetwohostsis
mobile.Thissimplefacthasadramaticeffectatthenetworklevel,sincelocation-
based network addressesare no longerconstants within the system.
Mobilityalsodirectlyaffectsdatabasequeryprocessing.AswesawinChap-
ter19, we mustconsiderthe communication costswhen we choose adistributed
query-processing strategy. Mobility results in dynamically changing communi-
cation costs, thus complicating the optimization process. Furthermore, there are
competing notions of cost toconsider:
• Usertimeis ahighly valuable commodity inmany business applications.
• Connectiontimeistheunitbywhichmonetarychargesareassignedinsome
cellularsystems.
• Number of bytes, or packets, transferred is the unit by which charges are
computed in some digitalcellularsystems.
• Time-of-day-basedchargesvary,dependingonwhethercommunicationoc-
curs duringpeakor off-peakperiods.
• Energy is limited. Often, battery power is a scarce resource whose use must
be optimized. A basic principle of radio communication is that it requires
less energy to receive than to transmit radio signals. Thus, transmission and
receptionof dataimpose differentpower demandson the mobile host.
25.5.3 Broadcast Data
It is often desirable for frequently requested data to be broadcast in a contin-
uous cycle by mobile support stations, rather than transmitted to mobile hosts
on demand. A typical application of such broadcast data is stock-market price
information. There are two reasons for using broadcast data. First, the mobile
host avoidsthe energycost fortransmitting datarequests.Second,the broadcast
data can be received by a large number of mobile hosts at once, at no extra cost.
Thus, the available transmission bandwidth isutilizedmore effectively.
A mobile host can then receive data as they are transmitted, rather than
consuming energy by transmitting a request. The mobile host may have local
nonvolatile storage available to cache the broadcast data for possible later use.
Givenaquery,themobilehostmayoptimizeenergycostsbydeterminingwhether
itcanprocessthatquerywithonlycacheddata.Ifthecacheddataareinsuf?cient,
there are two options: wait for the data to be broadcast, or transmit a request for
data. To make this decision, the mobile host must know when the relevant data
will be broadcast.
25.5 MobilityandPersonalDatabases 1083
Broadcastdatamaybetransmittedaccordingtoa?xedscheduleorachange-
ableschedule.Intheformercase,themobilehostusestheknown ?xedschedule
to determine when the relevant data will be transmitted. In the latter case, the
broadcastschedulemustitselfbebroadcastatawell-knownradiofrequencyand
at well-known time intervals.
Ineffect,thebroadcastmediumcanbemodeledasadiskwithahighlatency.
Requests for data can be thought of as being serviced when the requested data
are broadcast. The transmission schedules behave like indices on the disk. The
bibliographical notes list recent research papers in the area of broadcast data
management.
25.5.4 Disconnectivity and Consistency
Since wireless communication may be paid for on the basis of connection time,
there is an incentive for certain mobile hosts to be disconnected for substantial
periods. Mobile computers without wireless connectivity are disconnected most
ofthetimewhentheyarebeingused,exceptperiodicallywhentheyareconnected
to theirhost computers,eitherphysicallyor through a computernetwork.
During these periods of disconnection, the mobile host may remain in oper-
ation. The user of the mobile host may issue queries and updates on data that
resideorarecachedlocally.Thissituationcreatesseveralproblems,inparticular:
• Recoverability: Updates entered on a disconnected machine may be lost
if the mobile host experiences a catastrophic failure. Since the mobile host
representsa single point offailure, stable storage cannot be simulatedwell.
• Consistency: Locally cached data may become out-of-date, but the mobile
host cannot discover this situation until it is reconnected. Likewise, updates
occurringinthemobilehostcannotbepropagateduntilreconnectionoccurs.
We explored the consistency problem in Chapter 19, where we discussed
network partitioning, and we elaborate on it here. In wired distributed systems,
partitioningisconsideredtobeafailuremode;inmobilecomputing,partitioning
viadisconnectionispartofthenormalmodeofoperation.Itisthereforenecessary
toallowdataaccesstoproceeddespitepartitioning,evenatthe riskofsomeloss
of consistency.
For data updated by only the mobile host, it is a simple matter to propagate
theupdateswhenthemobilehostreconnects.However,ifthemobilehostcaches
read-only copies of data that may be updated by other computers, the cached
datamaybecome inconsistent. When the mobile host isconnected, itcan be sent
invalidation reports that inform it of out-of-date cache entries. However, when
the mobile host is disconnected, it may miss an invalidation report. A simple
solution to this problem is to invalidate the entire cache on reconnection, but
suchanextremesolutionishighlycostly.Severalcachingschemesarecitedinthe
bibliographical notes.
Ifupdatescanoccuratboththemobilehostandelsewhere,detectingcon?ict-
ing updates is more dif?cult.Version-numbering-based schemes allow updates
1084 Chapter25 SpatialandTemporalDataandMobility
of shared ?les from disconnected hosts. These schemes do not guarantee that
the updates will be consistent. Rather, they guarantee that, if two hosts inde-
pendently update the same version of a document, the clash will be detected
eventually, when the hosts exchange information either directly or through a
common host.
The version-vector scheme detects inconsistencies when copies of a docu-
ment are independently updated. This scheme allows copies of a document to be
stored at multiple hosts. Although we use the term document, the scheme can be
appliedto any otherdata items,such as tuplesof a relation.
The basic idea is for each host i to store, with its copy of each document d,
a version vector—that is, a set of version numbers {V
d,i
[j]},withoneentryfor
each other host j on which the document could potentially be updated. When a
hosti updatesa document d, it increments the versionnumber V
d,i
[i]byone.
Whenevertwohostsi and j connectwitheachother,theyexchangeupdated
documents, so that both obtain new versions of the documents. However, be-
fore exchanging documents, the hosts have to discover whether the copies are
consistent:
1. Iftheversionvectorsarethesameonbothhosts—thatis,foreachk,V
d,i
[k] =
V
d,j
[k]—then the copiesof document d are identical.
2. If,foreachk, V
d,i
[k] ? V
d,j
[k]andtheversionvectorsarenotidentical,then
the copy of document d at host i is older than the one at host j.Thatis,the
copyofdocumentd athost j wasobtainedbyoneormoremodi?cationsof
the copy of the document at host i.Hosti replaces its copy of d,aswellas
itscopy of the versionvectorfor d, with the copiesfrom host j.
3. If there are a pair of hosts k and m such that V
d,i
[k]< V
d,j
[k]andV
d,i
[m]>
V
d,j
[m], then the copies are inconsistent;thatis,thecopyofd at i contains
updatesperformedby host k that have not been propagatedto host j,and,
similarly,thecopyofd at j containsupdatesperformedbyhostmthathave
not been propagated to host i. Then, the copies of d are inconsistent, since
two or more updates have been performed on d independently. Manual
interventionmay berequiredto mergethe updates.
The version-vector scheme was initially designed to deal with failures in
distributed?lesystems.Theschemegainedimportancebecausemobilecomput-
ersoftenstorecopiesof?lesthatarealsopresentonserversystems,ineffectcon-
stitutingadistributed?lesystemthatisoftendisconnected.Anotherapplication
of the scheme is in groupware systems, where hosts are connected periodically,
ratherthan continuously, and mustexchange updateddocuments.
The version-vector scheme also has applications in replicated databases,
whereitcanbeappliedtoindividualtuples.Forexample,ifacalendaroraddress
book is maintained on a mobile device as well as on a host, inserts, deletes and
updates can happen either on the mobile device or on the host. By applying the
version-vectorschemetoindividualcalendarentriesorcontacts,itiseasytohan-
dle situations where a particular entry has been updated on the mobile device
25.6 Summary 1085
while a different one has been updated on the host; such a situation would not
beconsideredacon?ict.However,ifthesameentryisupdatedindependentlyat
both places,a con?ict wouldbe detectedby theversion-vectorscheme.
The version-vector scheme, however, fails to address the most dif?cult and
most important issue arising from updates to shared data—the reconciliation of
inconsistent copies of data. Many applications can perform reconciliation auto-
matically by executing in each computer those operations that had performed
updates on remote computers during the period of disconnection. This solution
works if update operations commute—that is, they generate the same result,
regardless of the order in which they are executed. Alternative techniques may
be available in certain applications; in the worst case, however, it must be left
tothe userstoresolvethe inconsistencies. Dealingwith such inconsistency auto-
matically,andassistingusersinresolvinginconsistenciesthatcannotbehandled
automatically, remainsan areaof research.
Anotherweaknessisthattheversion-vectorschemerequiressubstantialcom-
munication between a reconnecting mobile host and that host’s mobile support
station. Consistency checks can be delayed until the data are needed, although
this delaymay increase the overallinconsistency of the database.
Thepotentialfordisconnectionandthecostofwirelesscommunicationlimit
the practicality of transaction-processing techniques discussed in Chapter 19 for
distributed systems. Often, it is preferable to let users prepare transactions on
mobile hosts, but to require that, instead of executing the transactions locally,
they submit transactions to a server for execution. Transactions that span more
thanonecomputerandthatincludeamobilehostfacelong-termblockingduring
transaction commit, unlessdisconnectivityisrare or predictable.
25.6 Summary
• Time plays an important role in database systems. Databases are models of
the real world. Whereas most databases model the state of the real world at
a point in time (at the current time), temporal databases model the states of
the realworld across time.
• Factsintemporalrelationshaveassociatedtimeswhentheyarevalid,which
canberepresentedasaunionofintervals.Temporalquerylanguagessimplify
modelingof time,as wellas time-relatedqueries.
• Spatial databases are ?nding increasing use today to store computer-aided-
designdata aswellas geographic data.
• Design data are stored primarily as vector data; geographic data consist
of a combination of vector and raster data. Spatial-integrity constraints are
importantfor designdata.
• Vector data can be encoded as ?rst-normal-form data, or they can be stored
using non-?rst-normal-form structures, such as lists. Special-purpose index
structures are particularly important for accessing spatial data, and for pro-
cessingspatialqueries.
1086 Chapter25 SpatialandTemporalDataandMobility
• R-trees are a multidimensional extension of B-trees; with variants such as
R+-trees and R
?
-trees, they have provedpopular in spatial databases. Index
structuresthatpartitionspaceinaregularfashion,suchasquadtrees,helpin
processingspatialjoin queries.
• Multimedia databases are growing in importance. Issues such as similarity-
based retrievaland deliveryof data at guaranteed rates are topics of current
research.
• Mobilecomputingsystemshavebecomecommon,leadingtointerestindata-
basesystemsthatcanrunonsuchsystems.Queryprocessinginsuchsystems
mayinvolvelookupsonserverdatabases.Thequerycostmodelmustinclude
thecostofcommunication, includingmonetarycostandbattery-powercost,
which isrelativelyhigh for mobile systems.
• Broadcast is much cheaper per recipient than is point-to-point communica-
tion, and broadcast of data such as stock-market data helps mobile systems
to pickup datainexpensively.
• Disconnected operation,use ofbroadcastdata, and caching ofdataarethree
importantissuesbeing addressedinmobile computing.
Review Terms
• Temporal data
• Validtime
• Transaction time
• Temporalrelation
• Bitemporalrelation
• Universalcoordinatedtime( UTC)
• Snapshot relation
• Temporalquerylanguages
• Temporalselection
• Temporalprojection
• Temporal join
• Spatialand geographic data
• Computer-aided-design(CAD)
data
• Geographic data
• Geographic information systems
• Triangulation
• Designdatabases
• Geographic data
• Raster data
• Vector data
• Global positioning system(GPS)
• Spatialqueries
• Nearnessqueries
• Nearest-neighborqueries
• Region queries
• Spatialjoin
• Indexingof spatialdata
• k-dtrees
• k-d-Btrees
• Quadtrees
?
PR quadtree
?
Region quadtree
• R-trees
?
Boundingbox
PracticeExercises 1087
?
Quadratic split
• Multimediadatabases
• Isochronous data
• Continuous-mediadata
• Similarity-basedretrieval
• Multimediadataformats
• Videoservers
• Mobile computing
?
Mobile hosts
?
Mobile supportstations
?
Cell
?
Handoff
• Location-dependentqueries
• Broadcastdata
• Consistency
?
Invalidationreports
?
Version-vectorscheme
Practice Exercises
25.1 What are the two types of time, and how are they different? Why does it
make sense to have both typesof timeassociated with a tuple?
25.2 Suppose you have a relation containing the x,y coordinates and names
of restaurants. Suppose also that the only queries that will be asked are
of the following form: The query speci?es a point, and asks if there is a
restaurantexactlyatthatpoint.Whichtypeofindexwouldbepreferable,
R-treeorB-tree?Why?
25.3 Suppose you have a spatial database that supports region queries (with
circular regions) but not nearest-neighborqueries.Describean algorithm
to ?nd the nearestneighbor by making use of multipleregionqueries.
25.4 SupposeyouwanttostorelinesegmentsinanR-tree.Ifalinesegmentis
notparalleltothe axes,thebounding box foritcan be large,containing a
large emptyarea.
• Describe the effect on performance of having large bounding boxes
onqueriesthat askfor linesegmentsintersectingagivenregion.
• Brie?ydescribeatechniquetoimproveperformanceforsuchqueries
andgiveanexampleofitsbene?t.Hint:Youcandividesegmentsinto
smallerpieces.
25.5 Give a recursive procedure to ef?ciently compute the spatial join of two
relations with R-tree indices. (Hint: Use bounding boxes to check if leaf
entriesundera pairof internal nodesmayintersect.)
25.6 Describe how the ideas behind the RAID organization (Section 10.3) can
be used in a broadcast-data environment, where there may occasionally
benoise that preventsreceptionof partof the databeing transmitted.
25.7 De?ne a model of repeatedly broadcast data in which the broadcast
mediumismodeledasavirtualdisk.Describehowaccesstimeanddata-
1088 Chapter25 SpatialandTemporalDataandMobility
transferrateforthisvirtualdiskdifferfromthe correspondingvaluesfor
a typicalhard disk.
25.8 Consider a database of documents in which all documents are kept in a
central database. Copiesof some documents are kepton mobile comput-
ers.SupposethatmobilecomputerAupdatesacopyofdocument1while
it is disconnected, and, at the same time, mobile computer B updates a
copyofdocument2whileitisdisconnected.Showhowtheversion-vector
scheme can ensure proper updating of the central database and mobile
computerswhen amobile computerreconnects.
Exercises
25.9 Will functional dependencies be preserved if a relation is converted to a
temporalrelationbyaddingatimeattribute?Howistheproblemhandled
in a temporaldatabase?
25.10 Considertwo-dimensional vectordatawherethe dataitemsdonotover-
lap.Isitpossibletoconvertsuchvectordatatorasterdata?Ifso,whatare
thedrawbacksofstoringrasterdataobtainedbysuchconversion,instead
of the original vectordata?
25.11 Studythesupportforspatialdataofferedbythedatabasesystemthatyou
use, and implementthe following:
a. A schema to represent the geographic location of restaurants along
with features such as the cuisine served at the restaurant and the
levelofexpensiveness.
b. Aqueryto?ndmoderatelypricedrestaurantsthatserveIndianfood
and are within 5 milesof your house (assume any location for your
house).
c. A query to ?nd for each restaurant the distance from the nearest
restaurant serving the same cuisine and with the same level of ex-
pensiveness.
25.12 Whatproblemscanoccurinacontinuous-mediasystemifdataaredeliv-
eredeithertoo slowly ortoo fast?
25.13 Listthreemainfeaturesofmobilecomputingoverwirelessnetworksthat
are distinctfrom traditionaldistributedsystems.
25.14 List three factors that need to be considered in query optimization for
mobilecomputingthatarenotconsideredintraditionalqueryoptimizers.
25.15 Giveanexampletoshowthattheversion-vectorschemedoesnotensure
serializability. (Hint: Use the example from Practice Exercise 25.8, with
the assumption that documents 1 and 2 are available on both mobile
BibliographicalNotes 1089
computersAandB,andtakeintoaccountthepossibilitythatadocument
maybereadwithoutbeingupdated.)
Bibliographical Notes
StamandSnodgrass[1988]andSoo[1991]providesurveysontemporaldataman-
agement. Jensen et al. [1994] presents a glossary of temporal-database concepts,
aimedatunifyingtheterminology.Tanseletal.[1993]isacollectionofarticleson
differentaspects of temporaldatabases. Chomicki [1995] presentstechniques for
managing temporalintegrityconstraints.
Heywood et al. [2002] provides textbook coverage of geographical informa-
tionsystems.Samet[1995b]providesanoverviewofthelargeamountofworkon
spatial indexstructures. Samet[1990] and Samet[2006] providesa textbook cov-
erageofspatialdatastructures.Anearlydescriptionofthequadtreeisprovided
byFinkelandBentley[1974].Samet[1990]andSamet[1995b]describenumerous
variantsofquadtrees.Bentley[1975]describesthe k-dtree,and Robinson [1981]
describes the k-d-B tree. The R-tree was originally presented in Guttman [1984].
Extensions of the R-tree are presented by Sellis et al. [1987], which describes the
R
+
tree,and Beckmann etal. [1990], which describesthe R
?
tree.
Brinkhoff et al. [1993] discusses an implementation of spatial joins using R-
trees.LoandRavishankar[1996]andPatelandDeWitt[1996]presentpartitioning-
based methods for computation of spatial joins. Samet and Aref [1995] provides
an overview of spatial data models, spatial operations, and the integration of
spatial and nonspatial data.
Revesz[2002]providestextbookcoverageoftheareaofconstraintdatabases;
temporal intervals and spatial regions can be thought of as special cases of con-
straints.
Samet[1995a]describesresearchissuesinmultimediadatabases.Indexingof
multimediadataisdiscussed inFaloutsos and Lin[1995].
Dashtietal.[2003]providesatextbookdescriptionofstreamingmediaserver
design, including extensive coverage of data organization on disk subsystems.
VideoserversarediscussedinAndersonetal.[1992],Ranganetal.[1992],Ozden
etal.[1994],FreedmanandDeWitt[1995],andOzdenetal.[1996b].Faulttolerance
isdiscussedin Bersonetal. [1995] and Ozden etal. [1996a].
Informationmanagementinsystemsthatincludemobilecomputersisstudied
in Alonso and Korth [1993] and Imielinski and Badrinath [1994]. Imielinski and
Korth [1996] presents an introduction to mobile computing and a collection of
researchpaperson the subject.
Theversion-vectorschemefordetectinginconsistencyindistributed?lesys-
temsisdescribedby Popeketal. [1981] and Parker etal. [1983].
This page intentionally left blank 
CHAPTER
26
Advanced Transaction
Processing
InChapters14,15,and16,weintroducedtheconceptofatransaction,aprogram
unit that accesses—and possibly updates—various data items, and whose ex-
ecution ensures the preservation of the ACID properties. We discussed in those
chapters a varietyof techniques for ensuring the ACID propertiesin an environ-
mentwherefailurecanoccur,andwherethetransactionsmayrunconcurrently.
In this chapter, we go beyond the basic schemes discussed previously, and
coveradvancedtransaction-processingconcepts,includingtransaction-processing
monitors, transactional work?ows, and transaction processing in the context of
electroniccommerce.Wealsocovermain-memorydatabases,real-timedatabases,
long-durationtransactions,andnestedtransactions.
26.1 Transaction-Processing Monitors
Transaction-processingmonitors(TPmonitors)aresystemsthatweredeveloped
inthe1970sand1980s,initiallyinresponsetoaneedtosupportalargenumberof
remoteterminals(such as airline-reservationterminals)from a singlecomputer.
Theterm TP monitorinitiallystoodforteleprocessing monitor.
TP monitors have since evolved to provide the core support for distributed
transactionprocessing,andthetermTPmonitorhasacquireditscurrentmeaning.
The CICS TP monitor from IBM was one of the earliest TP monitors, and has
beenverywidelyused.Other TPmonitorsincludeOracleTuxedoandMicrosoft
TransactionServer.
Web application server architectures, including servlets, which we studied
earlierinSection9.3,supportmanyofthefeaturesof TPmonitorsandaresome-
times referred to as “TP lite.” Web application servers are in widespread use,
and have supplanted traditional TP monitors for many applications. However,
theconceptsunderlyingthem,whichwestudyinthissection,areessentiallythe
same.
1091
1092 Chapter26 AdvancedTransactionProcessing
26.1.1 TP-Monitor Architectures
Large-scaletransaction-processingsystemsarebuiltaroundaclient–serverarchi-
tecture. One way of building such systems is to have a server process for each
client;theserverperformsauthentication,andthenexecutesactionsrequestedby
theclient.Thisprocess-per-clientmodelisillustratedinFigure26.1a.Thismodel
presents several problems with respect to memory utilization and processing
speed:
  Per-process memory requirements are high. Even if memory for program
codeissharedbyallprocesses,eachprocessconsumesmemoryforlocaldata
and open ?le descriptors, as well as for operating-system overhead, such as
pagetablestosupportvirtualmemory.
  The operating system divides up available CPU time among processes by
switching among them; this technique is called multitasking.Eachcontext
switch between one process and the next has considerable CPU overhead;
even on today’s fast systems, a context switch can take hundreds of mi-
croseconds.
The above problems can be avoided by having a single-server process to
which all remote clients connect; this model is called the single-server model,
remote
clients
server files remote
clients
(b) Single-server model (a) Process-per-client model
server files
remote
clients
router servers files remote
clients
(d) Many-server, many-router model (c) Many-server, single-router model
routers
monitor
servers files
Figure 26.1 TP-monitor architectures.
26.1 Transaction-ProcessingMonitors 1093
illustrated in Figure 26.1b. Remote clients send requests to the server process,
which then executes those requests.This model is also used in client–server en-
vironments, where clients send requests to a single-server process. The server
processhandlestasks,suchasuserauthentication, that wouldnormallybe han-
dled by the operating system. To avoid blocking other clients when processing
a long request for one client, the server process is multithreaded: The server
processhasathreadofcontrolforeachclient,and,ineffect,implementsitsown
low-overhead multitasking. It executes code on behalf of one client for a while,
thensavestheinternalcontextandswitchestothecodeforanotherclient.Unlike
the overhead of full multitasking, the cost of switching between threads is low
(typicallyonlyafewmicroseconds).
Systemsbasedonthesingle-servermodel,suchastheoriginalversionofthe
IBM CICS TP monitor and ?le serverssuch as Novel’s NetWare,successfully pro-
videdhightransactionrateswithlimitedresources.However,theyhadproblems,
especiallywhenmultipleapplicationsaccessedthesamedatabase:
  Sincealltheapplicationsrunasasingleprocess,thereisnoprotectionamong
them.Abuginoneapplicationcanaffectalltheotherapplicationsaswell.It
wouldbebesttoruneachapplicationasaseparateprocess.
  Suchsystemsarenotsuitedforparallelordistributeddatabases,sinceaserver
processcannotexecuteonmultiplecomputersatonce.(However,concurrent
threadswithinaprocesscanbesupportedinashared-memorymultiproces-
sorsystem.)Thisisaseriousdrawbackinlargeorganizations,whereparallel
processing is critical for handling large workloads, and distributeddata are
becomingincreasinglycommon.
One way to solve these problems is to run multiple application-server pro-
cesses that access a common database, and to let the clients communicate with
theapplicationthroughasinglecommunicationprocessthatroutesrequests.This
modeliscalledthemany-server,single-routermodel,illustratedinFigure26.1c.
Thismodelsupportsindependentserverprocessesformultipleapplications;fur-
ther, each application can have a pool of serverprocesses, any one of which can
handleaclientsession.Therequestcan,forexample,beroutedtothemostlightly
loadedserverinapool.Asbefore,eachserverprocesscanitselfbemultithreaded,
sothatitcanhandlemultipleclientsconcurrently.Asafurthergeneralization,the
applicationserverscanrunondifferentsitesofaparallelordistributeddatabase,
andthecommunicationprocesscanhandlethecoordinationamongtheprocesses.
TheabovearchitectureisalsowidelyusedinWebservers.AWebserverhas
amainprocessthatreceivesHTTPrequests,andthenassignsthetaskofhandling
eachrequesttoaseparateprocess(chosenfromamongapoolofprocesses).Each
oftheprocessesisitselfmultithreaded,sothatitcanhandlemultiplerequests.The
useofsafeprogramminglanguages,suchasJava,C#,orVisualBasic,allowsWeb
application servers to protect threads from errors in other threads. In contrast,
with a language like C or C++, errors such as memory allocation errors in one
threadcancauseotherthreadstofail.
1094 Chapter 26 AdvancedTransactionProcessing
input queue
authorization
output queue
network
lock manager
recovery manager
log manager
application
servers
database and
resource managers 
Figure 26.2 TP-monitor components.
A more general architecture has multiple processes, rather than just one, to
communicatewithclients.Theclientcommunicationprocessesinteractwithone
or more router processes, which route their requests to the appropriate server.
Later-generation TP monitors therefore have a different architecture, called the
many-server,many-routermodel,illustratedinFigure26.1d.Acontrollerprocess
startsuptheotherprocessesandsupervisestheirfunctioning. Veryhighperfor-
manceWeb-serversystemsalsoadoptsuchanarchitecture.Therouterprocesses
are often network routers that direct traf?c addressed to the same Internet ad-
dresstodifferentservercomputers,dependingonwherethetraf?ccomesfrom.
Whatlookslikeasingleserverwithasingleaddresstotheoutsideworldmaybe
acollectionofservers.
The detailed structure of a TP monitor appears in Figure 26.2. A TP monitor
does more than simply pass messages to application servers. When messages
arrive,theymayhavetobequeued;thus,thereisaqueuemanagerforincoming
messages. The queue may be a durable queue, whose entries survive system
failures.Usingadurablequeuehelpsensurethatoncereceivedandstoredinthe
queue, the messages will be processed eventually, regardless of system failures.
Authorizationandapplication-servermanagement(forexample,serverstart-up
and routing of messages to servers) are further functions of a TP monitor. TP
monitors often provide logging, recovery, and concurrency-control facilities, al-
lowingapplicationserverstoimplementthe ACIDtransactionpropertiesdirectly
ifrequired.
Finally,TPmonitorsalsoprovidesupportforpersistentmessaging.Recallthat
persistentmessaging(Section19.4.3)providesaguaranteethatthemessagewill
bedeliveredif(andonlyif)thetransactioncommits.
In addition to these facilities, many TP monitors also provided presentation
facilities to create menus/forms interfaces for dumb clients such as terminals;
26.1 Transaction-ProcessingMonitors 1095
these facilities are no longer important since dumb clients are no longer widely
used.
26.1.2 Application Coordination Using TP monitors
Applications today often have to interact with multiple databases. They may
also have to interact with legacy systems, such as special-purpose data-storage
systems built directly on ?le systems. Finally, they may have to communicate
withusersorotherapplicationsatremotesites.Hence,theyalsohavetointeract
with communication subsystems. It is important to be able to coordinate data
accesses,andtoimplementACIDpropertiesfortransactionsacrosssuchsystems.
ModernTPmonitorsprovidesupportfortheconstructionandadministration
ofsuchlargeapplications,builtupfrommultiplesubsystemssuchasdatabases,
legacy systems, and communication systems. A TP monitor treats each subsys-
tem as a resource manager that provides transactional access to some set of re-
sources.TheinterfacebetweentheTPmonitorandtheresourcemanagerisde?ned
by a set of transaction primitives, such as begin transaction, commit transaction,
abort transaction,andprepare to commit transaction (for two-phase commit). Of
course,theresourcemanagermustalsoprovideotherservices,suchassupplying
data,totheapplication.
Theresource-managerinterfaceisde?nedbytheX/OpenDistributedTrans-
action Processing standard. Many database systems support the X/Open stan-
dards,andcanactasresourcemanagers.TPmonitors—aswellasotherproducts,
such as SQL systems, that support the X/Open standards—can connect to the
resourcemanagers.
Inaddition,servicesprovidedbya TPmonitor,suchaspersistentmessaging
and durable queues, act as resource managers supporting transactions. The TP
monitor can act as coordinator of two-phase commit for transactions that access
theseservicesaswellas databasesystems.Forexample,when aqueuedupdate
transactionisexecuted,anoutputmessageisdelivered,andtherequesttransac-
tionisremovedfromtherequestqueue.Two-phasecommitbetweenthedatabase
andtheresourcemanagersforthedurablequeueandpersistentmessaginghelps
ensurethat,regardlessoffailures,eitheralltheseactionsoccurornoneoccurs.
We can also use TP monitors to administer complex client–server systems
consisting of multiple servers and a large number of clients. The TP monitor
coordinatesactivitiessuchassystemcheckpointsandshutdowns.Itprovidesse-
curityandauthenticationofclients.Itadministersserverpoolsbyaddingservers
or removing servers without interruption of the the database system. Finally, it
controlsthescopeoffailures.Ifaserverfails,theTPmonitorcandetectthisfailure,
abortthetransactionsinprogress,andrestartthetransactions.Ifanodefails,the
TPmonitorcanmigratetransactionstoserversatothernodes,againbackingout
incomplete transactions. When failed nodes restart, the TP monitor can govern
therecoveryofthenode’sresourcemanagers.
TPmonitorscanbeusedtohidedatabasefailuresinreplicatedsystems;remote
backup systems(Section16.9)areanexampleofreplicatedsystems.Transaction
requests are sent to the TP monitor, which relays the messages to one of the
1096 Chapter 26 AdvancedTransactionProcessing
databasereplicas(theprimarysite,incaseofremotebackupsystems).Ifonesite
fails,the TPmonitorcantransparentlyroutemessagestoabackupsite,masking
thefailureofthe?rstsite.
In client–server systems, clients often interact with servers via a remote-
procedure-call(RPC)mechanism,whereaclientinvokesaprocedurecall,which
is actually executed at the server, with the results sent back to the client. As
far as the client code that invokes the RPC is concerned, the call looks like a
localprocedure-callinvocation. TPmonitorsystemsprovideatransactionalRPC
interfacetotheirservices.Insuchaninterface,the RPCmechanismprovidescalls
thatcanbeusedtoencloseaseriesofRPCcallswithinatransaction.Thus,updates
performedbyan RPCarecarriedoutwithinthescopeofthetransaction,andcan
berolledbackifthereisanyfailure.
26.2 Transactional Work?ows
A work?owis an activityinwhich multipletasks are executedina coordinated
way by different processing entities. A task de?nes some work to be done and
can be speci?ed in a number of ways, including a textual description in a ?le
orelectronic-mailmessage,aform,amessage,oracomputerprogram.The pro-
cessing entitythatperformsthetasksmaybeapersonorasoftwaresystem(for
example,amailer,anapplicationprogram,oradatabase-managementsystem).
Figure26.3showsafewexamplesofwork?ows.Asimpleexampleisthatofan
electronic-mailsystem.Thedeliveryofasinglemailmessagemayinvolveseveral
mailsystemsthatreceiveandforwardthemailmessage,untilthemessagereaches
its destination, where it is stored. Other terms used in the database and related
literaturetorefertowork?owsincludetask?owandmultisystemapplications.
Work?owtasksarealsosometimescalledsteps.
In general, work?ows may involve one or more humans. For instance, con-
sidertheprocessingofaloan.Therelevantwork?owappearsinFigure26.4.The
personwhowantsaloan?llsoutaform,whichisthencheckedbyaloanof?cer.
Anemployeewhoprocessesloanapplicationsveri?esthedataintheform,using
sources such as credit-referencebureaus. When all the requiredinformation has
beencollected,theloanof?cermaydecidetoapprovetheloan;thatdecisionmay
Work?ow Typical Typical processing
application task entity
electronic-mail routing electronic-mail message mailers
loan processing form processing
form processing
humans,
application sonullare
purchase-order processing
humans, application
sonullare , DBMSs
Figure 26.3 Examples of work?ows.
26.2 TransactionalWork?ows 1097
customer loan o?cer
veri?cation
superior
o?cer
loan
disbursement
loan
application
reject
accept
Figure 26.4 Work?owinloanprocessing.
then have to be approved by one or more superiorof?cers, after which the loan
canbemade.Eachhumanhereperformsatask;inabankthathasnotautomated
the task of loan processing, the coordination of the tasks is typically carried out
by passing of the loan application, with attached notes and other information,
fromoneemployeetothenext.Otherexamplesofwork?owsincludeprocessing
ofexpensevouchers,ofpurchaseorders,andofcredit-cardtransactions.
Today,alltheinformationrelatedtoawork?owismorethanlikelytobestored
inadigitalformononeormorecomputers,and,withthegrowthofnetworking,
informationcan beeasilytransferredfromonecomputer toanother. Hence,itis
feasiblefororganizationstoautomatetheirwork?ows.Forexample,toautomate
the tasks involved in loan processing, we can store the loan application and
associatedinformationinadatabase.Thework?owitselftheninvolveshanding
ofresponsibilityfromonehumantothenext,andpossiblyeventoprogramsthat
can automatically fetch the required information. Humans can coordinate their
activitiesbymeanssuchaselectronicmail.
Work?owsarebecomingincreasinglyimportantformultiplereasonswithin
aswellasbetweenorganizations.Manyorganizations todayhavemultiplesoft-
ware systemsthat needto worktogether.For example,when an employeejoins
an organization, information about the employee may have to be provided to
thepayrollsystem,tothelibrarysystem,toauthenticationsystemsthatallowthe
usertologontocomputers,toasystemthatmanagescafeteriaaccounts,ansoon.
Updates, such as when the employee changes status or leaves the organization,
alsohavetobepropagatedtoallthesystems.
Organizationsareincreasinglyautomatingtheirservices;forexample,asup-
plier may provide an automated system for customers to place orders. Several
tasks may need to be carried out when an order is placed, including reserving
productiontimetocreatetheorderedproductanddeliveryservicestodeliverthe
product.
We have to address two activities, in general, to automate a work?ow. The
?rst is work?ow speci?cation: detailing the tasks that must be carried out and
de?ning the execution requirements. The second problem is work?ow execu-
tion, which we must do while providing the safeguards of traditional database
1098 Chapter 26 AdvancedTransactionProcessing
systemsrelatedtocomputationcorrectnessanddataintegrityanddurability.For
example,itisnotacceptableforaloanapplicationoravouchertobelost,ortobe
processed more than once, because of a system crash. The idea behind transac-
tionalwork?ows istouseandextendtheconceptsoftransactions tothecontext
ofwork?ows.
Bothactivitiesarecomplicatedbythefactthatmanyorganizationsuseseveral
independentlymanagedinformation-processingsystemsthat,inmostcases,were
developed separately to automate different functions. Work?ow activities may
requireinteractionsamongseveralsuchsystems,eachperformingatask,aswell
asinteractionswithhumans.
A number of work?ow systems have been developed in recent years. Here,
we study properties of work?ow systems at a relatively abstract level, without
goingintothedetailsofanyparticularsystem.
26.2.1 Work?ow Speci?cation
Internalaspectsofataskdonotneedtobemodeledforthepurposeofspeci?cation
and management of a work?ow. In an abstract view of a task, a task may use
parametersstoredinitsinputvariables,mayretrieveandupdatedatainthelocal
system,maystoreitsresultsinitsoutputvariables,andmaybequeriedaboutits
executionstate.Atanytimeduringtheexecution,thework?owstateconsistsof
thecollectionofstatesofthework?ow’sconstituenttasks,andthestates(values)
ofallvariablesinthework?owspeci?cation.
Thecoordinationoftaskscanbespeci?edeitherstaticallyordynamically.A
static speci?cation de?nes the tasks—and dependencies among them—before
the execution of the work?ow begins. Forinstance,thetasksinanexpense-
voucher work?ow may consist of the approvals of the voucher by a secretary, a
manager,andanaccountant,inthatorder,and?nallythedeliveryofacheck.The
dependencies among the tasks may be simple—each task has to be completed
beforethenextbegins.
A generalization of this strategy is to have a precondition for execution of
each task in the work?ow, so that all possible tasks in a work?ow and their
dependencies are known in advance, but only those tasks whose preconditions
aresatis?edareexecuted.Thepreconditionscanbede?nedthroughdependencies
suchasthefollowing:
  Execution statesofothertasks—for example, “task t
i
cannot startuntiltask
t
j
hasended,”or “task t
i
mustabortiftask t
j
hascommitted.”
  Output values of other tasks—for example, “task t
i
can start if task t
j
re-
turnsavaluegreaterthan25,”or “themanager-approvaltaskcanstartifthe
secretary-approvaltaskreturnsavalueofOK.”
  Externalvariablesmodi?edbyexternalevents—forexample,“taskt
i
cannot
be started before 9 A.M.,” or “task t
i
must be started within 24 hours of the
completionoftask t
j
.”
26.2 TransactionalWork?ows 1099
Wecancombinethedependenciesbytheregularlogicalconnectors(or,and,not)
toformcomplexschedulingpreconditions.
An example of dynamic scheduling of tasks is an electronic-mail routing
system.Thenexttasktobescheduledforagivenmailmessagedependsonwhat
thedestinationaddressofthemessageis,andonwhichintermediateroutersare
functioning.
26.2.2 Failure-Atomicity Requirements of a Work?ow
Thework?owdesignermayspecifythefailure-atomicityrequirementsofawork-
?owaccordingtothesemanticsofthework?ow.Thetraditionalnotionoffailure
atomicitywouldrequirethatafailureofanytaskresultinthefailureofthework-
?ow. However, a work?ow can, in many cases, survive the failure of one of its
tasks—for example, by executing a functionally equivalent task at another site.
Therefore,weshouldallowthedesignertode?nefailure-atomicityrequirements
of a work?ow. The system must guarantee that every execution of a work?ow
will terminate in a state that satis?es the failure-atomicity requirementsde?ned
by the designer. We call those states acceptable termination states of a work-
?ow. All other execution states of a work?ow constitute a set of nonacceptable
terminationstates,inwhichthefailure-atomicityrequirementsmaybeviolated.
Anacceptable terminationstatecan be designatedascommittedor aborted.
A committed acceptable termination state is an execution state in which the
objectivesofawork?owhavebeenachieved.Incontrast,an aborted acceptable
termination state is a valid termination state in which a work?ow has failed to
achieveitsobjectives.Ifanabortedacceptableterminationstatehasbeenreached,
all undesirable effects of the partial execution of the work?ow must be undone
inaccordancewiththatwork?ow’sfailure-atomicityrequirements.
A work?ow must reach an acceptable termination state even in the presence
of system failures. Thus, if a work?ow is in a nonacceptable termination state at
the time of failure, during system recovery it must be brought to an acceptable
terminationstate(whetherabortedorcommitted).
For example, in the loan-processing work?ow, in the ?nal state, either the
loanapplicantistoldthataloancannotbemadeortheloanisdisbursed.Incase
of failures such as a long failure of the veri?cation system, the loan application
couldbereturnedtotheloanapplicantwithasuitableexplanation;thisoutcome
would constitute an aborted acceptable termination. A committed acceptable
terminationwouldbeeithertheacceptanceortherejectionoftheloan.
In general, a task can commit and release its resources before the work?ow
reaches a termination state. However, if the multitask transaction later aborts,
its failure atomicity may require that we undo the effects of already completed
tasks(forexample,committedsubtransactions)byexecutingcompensatingtasks
(as subtransactions). The semantics of compensation requires that a compensat-
ing transaction eventually complete its execution successfully, possibly after a
numberofresubmissions.
In an expense-voucher-processing work?ow, for example, a department-
budget balance may be reduced on the basis of an initial approval of a voucher
1100 Chapter 26 AdvancedTransactionProcessing
bythemanager.Ifthevoucherislaterrejected,whetherbecauseoffailureorfor
otherreasons,thebudgetmayhavetoberestoredbyacompensatingtransaction.
26.2.3 Execution of Work?ows
Theexecutionofthetasksmaybecontrolledbyahumancoordinatororbyasoft-
waresystemcalledawork?ow-managementsystem.Awork?ow-management
systemconsistsofascheduler,taskagents,andamechanismtoquerythestateof
thework?owsystem.Ataskagentcontrolstheexecutionofataskbyaprocessing
entity.Aschedulerisaprogramthatprocesseswork?owsbysubmittingvarious
tasksforexecution,monitoringvariousevents,andevaluatingconditionsrelated
tointertaskdependencies.Aschedulermaysubmitataskforexecution(toatask
agent), or may request that a previously submitted task be aborted. In the case
of multidatabase transactions, the tasks are subtransactions, and the processing
entities are local database-management systems. In accordance with the work-
?ow speci?cations, the scheduler enforces the scheduling dependencies and is
responsibleforensuringthattasksreachacceptableterminationstates.
There are three architectural approaches to the developmentof a work?ow-
managementsystem.Acentralizedarchitecturehasasingleschedulerthatsched-
ulesthetasksforallconcurrentlyexecutingwork?ows.Thepartiallydistributed
architecture has one scheduler instantiated for each work?ow. When the issues
ofconcurrentexecutioncanbeseparatedfromtheschedulingfunction,thelatter
optionisanaturalchoice.Afullydistributedarchitecturehasnoscheduler,but
thetaskagentscoordinatetheirexecutionbycommunicatingwithoneanotherto
satisfytaskdependenciesandotherwork?owexecutionrequirements.
The simplest work?ow-execution systems follow the fully distributed ap-
proach just described and are based on messaging. Messaging may be imple-
mented by persistent messaging mechanisms, to provide guaranteed delivery.
Some implementations use email for messaging; such implementations provide
many of the features of persistent messaging, but generally do not guarantee
atomicityofmessagedeliveryandtransactioncommit.Eachsitehasataskagent
that executestasksreceivedthrough messages.Executionmay also involvepre-
senting messages to humans, who have then to carry out some action. When a
task is completed at a site, and needs to be processed at another site, the task
agent dispatches a message to the next site. The message contains all relevant
informationabout thetasktobeperformed.Suchmessage-basedwork?owsys-
temsareparticularlyusefulinnetworksthatmaybedisconnectedforpartofthe
time.
The centralized approach is used in work?ow systems where the data are
storedinacentraldatabase.Theschedulernoti?esvariousagents,suchashumans
or computer programs, that a task has to be carried out, and keeps track of task
completion.Itiseasiertokeeptrackofthestateofawork?owwithacentralized
approachthanitiswithafullydistributedapproach.
The scheduler must guarantee that a work?ow will terminate in one of the
speci?ed acceptable termination states. Ideally, before attempting to execute a
work?ow, the scheduler should examine that work?ow to check whether the
26.2 TransactionalWork?ows 1101
work?ow may terminate in a nonacceptable state. If the scheduler cannot guar-
antee that a work?ow will terminatein anacceptable state,it should rejectsuch
speci?cations without attempting to execute the work?ow. As an example, let
us consider a work?ow consisting of two tasks represented by subtransactions
S
1
and S
2
, with the failure-atomicity requirements indicating that either both or
neither of the subtransactions should be committed. If S
1
and S
2
do not provide
prepared-to-commit states (for a two-phase commit), and further do not have
compensatingtransactions,thenitispossibletoreachastatewhereonesubtrans-
action is committed and the other aborted, and there is no way to bring both to
thesamestate.Therefore,suchawork?owspeci?cationisunsafe,andshouldbe
rejected.
Safetycheckssuchastheonejustdescribedmaybeimpossibleorimpractical
to implement in the scheduler; it then becomes the responsibility of the person
designingthework?owspeci?cationtoensurethatthework?owsaresafe.
26.2.4 Recovery of a Work?ow
Theobjectiveofwork?owrecoveryistoenforcethefailureatomicityofthework-
?ows. The recovery procedures must make sure that, if a failure occurs in any
ofthework?ow-processingcomponents(includingthescheduler),thework?ow
will eventually reach an acceptable termination state (whether aborted or com-
mitted). For example, the scheduler could continue processing after failure and
recovery, as though nothing happened, thus providing forward recoverability.
Otherwise, the scheduler could abort the whole work?ow (that is, reach one
of the global abort states). In either case, some subtransactions may need to be
committedorevensubmittedforexecution(forexample,compensatingsubtrans-
actions).
We assume that the processing entities involved in the work?ow have their
own recovery systems and handle their local failures. To recover the execution-
environmentcontext,thefailure-recoveryroutinesneedtorestorethestateinfor-
mationofthescheduleratthetimeoffailure,includingtheinformationaboutthe
executionstatesofeachtask.Therefore,theappropriatestatusinformationmust
beloggedonstablestorage.
We also need to consider the contents of the message queues. When one
agenthandsoffatasktoanother,thehandoffshouldbecarriedoutexactlyonce:
If the handoff happens twice a task may get executed twice; if the handoff does
not occur, the task may get lost. Persistent messaging (Section 19.4.3) provides
exactlythefeaturestoensurepositive,singlehandoff.
26.2.5 Work?ow-Management Systems
Work?owsareoftenhandcodedaspartofapplicationsystems.Forinstance,en-
terpriseresourceplanning(ERP)systems,whichhelpcoordinateactivitiesacross
anentireenterprise,havenumerouswork?owsbuiltintothem.
Thegoalofwork?ow-managementsystemsistosimplifytheconstructionof
work?owsandmakethemmorereliable,bypermittingthemtobespeci?edina
high-levelmannerandexecutedinaccordancewiththespeci?cation.Therearea
1102 Chapter 26 AdvancedTransactionProcessing
large number of commercial work?ow-management systems;some are general-
purpose work?ow-management systems, while others are speci?c to particular
work?ows,suchasorderprocessingorbug/failurereportingsystems.
In today’s world of interconnected organizations, it is not suf?cient to man-
agework?owsonlywithinanorganization.Work?owsthatcrossorganizational
boundaries are becoming increasingly common. For instance, consider an order
placed by an organization and communicated to another organization that ful-
?llstheorder.Ineachorganizationtheremaybeawork?owassociatedwiththe
order, and it is important that the work?ows be able to interoperate,in order to
minimizehumanintervention.
Thetermbusinessprocess managementisusedtorefertothemanagement
of work?ows related to business processes. Today, applications are increasingly
making their functionality available as services that can be invoked by other
applications,oftenusingaWebservicearchitecture.Asystemarchitecturebased
oninvokingservicesprovidedbymultipleapplicationsisreferredtoasaservice
oriented architecture SOA. Such services are the base layer on top of which
work?owmanagementisimplementedtoday.Theprocesslogicthatcontrolsthe
work?owbyinvokingtheservicesisreferredtoasorchestration.
BusinessprocessmanagementsystemsbasedontheSOAarchitectureinclude
Microsoft’sBizTalkServer,IBMsWebSphereBusinessIntegrationServerFounda-
tion,and BEAsWebLogicProcessEdition,amongothers.
TheWebServicesBusinessProcessExecutionLanguage(WS-BPEL)isanXML
based standard for specifying Web services and business processes (work?ows)
basedontheWebservices,whichcanbeexecutedbyabusinessprocessmanage-
ment system. The Business Process Modeling Notation (BPMN), is a standard
for graphical modeling of business processes in a work?ow, and XML Process
De?nition Language(XPDL)isanXMLbasedrepresentationofbusinessprocess
de?nitions,basedon BPMNdiagrams.
26.3 E-Commerce
E-commerce refers to the process of carrying out various activities related to
commerce, through electronic means, primarily through the Internet. The types
ofactivitiesinclude:
  Presaleactivities,neededtoinformthepotentialbuyerabouttheproductor
servicebeingsold.
  Thesaleprocess,whichincludesnegotiationsonpriceandqualityofservice,
andothercontractualmatters.
  The marketplace: When there are multiple sellers and buyers for a product,
a marketplace, such as a stock exchange, helps in negotiating the price to
be paid for the product. Auctions are used when there is a single seller and
multiplebuyers,and reverseauctions areused when thereisa singlebuyer
andmultiplesellers.
26.3 E-Commerce 1103
  Paymentforthesale.
  Activities related to delivery of the product or service. Some products and
servicescanbedeliveredovertheInternet;forotherstheInternetisusedonly
forprovidingshippinginformationandfortrackingshipmentsofproducts.
  Customersupportandpostsaleservice.
Databases are used extensively to support these activities. For some of the
activities,theuseofdatabasesisstraightforward,butthereareinterestingappli-
cationdevelopmentissuesfortheotheractivities.
26.3.1 E-Catalogs
Any e-commerce site provides users with a catalog of the products and services
that the site supplies.The servicesprovidedby an e-catalog may varyconsider-
ably.
Attheminimum,ane-catalogmustprovidebrowsingandsearchfacilitiesto
helpcustomers?ndtheproductforwhichtheyarelooking.Tohelpwithbrows-
ing, products should be organized into an intuitivehierarchy, so a few clicks on
hyperlinkscanleadcustomerstotheproductsinwhichtheyareinterested.Key-
words provided by the customer (for example, “digital camera” or “computer”)
shouldspeeduptheprocessof?ndingrequiredproducts.E-catalogsshouldalso
provide a means for customers to easily compare alternatives from which to
chooseamongcompetingproducts.
E-catalogs can be customized for the customer. For instance, a retailer may
haveanagreementwithalargecompanytosupplysomeproductsatadiscount.
An employee of the company, viewing the catalog to purchase products for the
company, should see prices with the negotiated discount, instead of the regular
prices.Becauseoflegalrestrictionsonsalesofsometypesofitems,customerswho
areunderage,orfromcertainstatesorcountries,shouldnotbeshownitemsthat
cannot legally be sold to them. Catalogs can also be personalized to individual
users, on the basis of past buying history. For instance, frequent customers may
beofferedspecialdiscountsonsomeitems.
Supportingsuchcustomizationrequirescustomerinformationaswellasspe-
cial pricing/discount information and sales restriction information to be stored
inadatabase.Therearealsochallengesinsupportingveryhightransactionrates,
whichareoftentackledbycachingofqueryresultsorgeneratedWebpages.
26.3.2 Marketplaces
When there are multiple sellers or multiple buyers (or both) for a product, a
marketplace helps in negotiating the price to be paid for the product. There are
severaldifferenttypesofmarketplaces:
  In a reverse auction system a buyer states requirements, and sellers bid for
supplying the item. The supplier quoting the lowest price wins. In a closed
bidding system, the bids are not made public, whereas in an open bidding
systemthebidsaremadepublic.
1104 Chapter 26 AdvancedTransactionProcessing
  In an auction there are multiple buyers and a single seller. For simplicity,
assumethatthereisonlyoneinstanceofeachitembeingsold.Buyersbidfor
theitemsbeingsold,andthehighestbidderforanitemgetstobuytheitem
atthebidprice.
Whentherearemultiplecopiesofanitem,thingsbecomemorecomplicated:
Supposetherearefouritems,andonebiddermaywantthreecopiesfor$10
each,whileanotherwantstwocopiesfor$13each.Itisnotpossibletosatisfy
both bids. If the items will be of no value if they are not sold (for instance,
airline seats, which must be sold before the plane leaves), the seller simply
picksasetofbidsthatmaximizestheincome.Otherwisethedecisionismore
complicated.
  In an exchange, such as a stock exchange, there are multiple sellers and
multiple buyers. Buyers can specify the maximum price they are willing to
pay, while sellers specify the minimum price they want. There is usually a
market maker who matches buy and sell bids, deciding on the price for each
trade(forinstance,atthepriceofthesellbid).
Thereareothermorecomplextypesofmarketplaces.
Amongthedatabaseissuesinhandlingmarketplacesarethese:
  Biddersneedtobeauthenticatedbeforetheyareallowedtobid.
  Bids(buyorsell)needtoberecordedsecurelyinadatabase.Bidsneedtobe
communicatedquicklytootherpeopleinvolvedinthemarketplace(suchas
allthebuyersorallthesellers),whomaybenumerous.
  Delaysinbroadcastingbidscanleadto?nanciallossestosomeparticipants.
  Thevolumesoftradesmaybeextremelylargeattimesofstockmarketvolatil-
ity, or toward the end of auctions. Thus, very high performance databases
withlargedegreesofparallelismareusedforsuchsystems.
26.3.3 Order Settlement
After items have been selected (perhaps through an electronic catalog) and the
price determined (perhaps by an electronic marketplace), the order has to be
settled.Settlementinvolvespaymentforgoodsandthedeliveryofthegoods.
A simple but unsecure way of paying electronically is to send a credit-card
number.Therearetwomajorproblems.First,credit-cardfraudispossible.Whena
buyerpaysforphysicalgoods,companiescanensurethattheaddressfordelivery
matches the cardholder’s address, so no one else can receive the goods, but for
goodsdeliveredelectronicallynosuchcheckispossible.Second,thesellerhasto
betrustedtobillonlyfortheagreed-onitemandtonotpassonthecardnumber
tounauthorizedpeoplewhomaymisuseit.
Severalprotocolsareavailableforsecurepaymentsthatavoidboththeprob-
lemslistedabove.Inaddition,theyprovideforbetterprivacy,wherebytheseller
may not be given any unnecessary details about the buyer, and the credit-card
26.4 Main-MemoryDatabases 1105
company is not provided any unnecessary information about the items pur-
chased.Allinformationtransmittedmustbeencryptedsothatanyoneintercept-
ing the data on the network cannot ?nd out the contents. Public-/private-key
encryptioniswidelyusedforthistask.
Theprotocolsmustalsopreventperson-in-the-middleattacks,wheresome-
onecanimpersonatethebankorcredit-cardcompany,oreventheseller,orbuyer,
and steal secret information. Impersonation can be perpetrated by passing off a
fakekeyassomeoneelse’spublickey(thebank’sorcredit-cardcompany’s,orthe
merchant’sorthebuyer’s).Impersonationispreventedbyasystemofdigitalcer-
ti?cates, wherebypublickeysaresignedby acerti?cationagency,whose public
keyiswellknown(orwhichinturnhasitspublickeycerti?edbyanothercerti-
?cationagencyandsoonuptoakeythatiswellknown).Fromthewell-known
publickey,thesystemcanauthenticatetheotherkeysbycheckingthecerti?cates
inreversesequence.Digitalcerti?catesweredescribedearlier,inSection9.8.3.2.
SeveralnovelpaymentsystemsweredevelopedintheearlydaysoftheWeb.
OneofthesewasasecurepaymentprotocolcalledtheSecureElectronicTransaction
(SET) protocol. The protocol requiresseveral rounds of communication between
the buyer, seller, and the bank, in order to guarantee safety of the transaction.
There were also systemsthat providefor greateranonymity, similar to that pro-
videdbyphysicalcash.TheDigiCashpaymentsystemwasonesuchsystem.When
a payment is made in such a system, it is not possible to identify the purchaser.
Incontrast,identifyingpurchasersisveryeasywithcreditcards,andeveninthe
case of SET, it is possible to identify the purchaser with the cooperation of the
credit-card company or bank. However, none of these systems was successful
commercially,forbothtechnicalandnon-technicalreasons.
Today, many banks provide secure payment gateways which allow a pur-
chasertopayonlineatthebanksWebsite,withoutexposingcreditcardorbank
accountinformationtotheonlinemerchant.Whenmakingapurchaseatanonline
merchant,thepurchaser’sWebbrowserisredirectedtothegatewaytocomplete
the payment by providing credit card or bank account information, after which
thepurchaserisagainredirectedbacktothemerchant’ssitetocompletethepur-
chase.Unlikethe SETorDigiCashprotocols,thereisnosoftwarerunningonthe
purchasers machine, except a Web browser; as a result this approach has found
widesuccesswheretheearlierapproachesfailed.
An alternative approach which is used by the PayPal system is for both the
purchaserand themerchant tohaveanaccount ona commonplatform,andthe
money transfer happens entirely within the common platform. The purchaser
?rst loads her account with money using a credit card, and can then transfer
money to the merchants account. This approach has been very successful with
smallmerchants,sinceitdoesnotrequireeitherthepurchaserorthemerchantto
runanysoftware.
26.4 Main-Memory Databases
To allow a high rate of transaction processing (hundreds or thousands of trans-
actionspersecond),wemustusehigh-performancehardware,andmustexploit
1106 Chapter 26 AdvancedTransactionProcessing
parallelism.Thesetechniquesalone,however,areinsuf?cienttoobtainverylow
response times, since disk I/O remains a bottleneck—about 10 milliseconds are
required for each I/O, and this number has not decreased at a rate comparable
to the increase in processor speeds.Disk I/Ois oftenthe bottleneck for reads,as
wellasfortransactioncommits.Thelongdisklatencyincreasesnotonlythetime
toaccessadataitem,butalsolimitsthenumberofaccessespersecond.
1
Wecanmakeadatabasesystemlessdiskboundbyincreasingthesizeofthe
databasebuffer.Advancesinmain-memorytechnologyletusconstructlargemain
memories at relatively low cost. Today, commercial 64-bit systems can support
main memories of tens of gigabytes. Oracle TimesTen is a currently available
main-memory database. Additional information on main-memory databases is
giveninthereferencesinthebibliographicalnotes.
For some applications, such as real-timecontrol, it is necessaryto store data
in main memory to meet performance requirements. The memory size required
formostsuchsystemsisnotexceptionallylarge,althoughthereareatleastafew
applicationsthatrequiremultiplegigabytesofdatatobememoryresident.Since
memory sizes have been growing at a very fast rate, an increasing number of
applicationscanbeexpectedtohavedatathat?tintomainmemory.
Large main memories allow faster processing of transactions, since data are
memoryresident.However,therearestilldisk-relatedlimitations:
  Logrecordsmustbewrittentostablestoragebeforeatransactioniscommit-
ted.Theimprovedperformancemadepossiblebyalargemainmemorymay
result in the logging process becoming a bottleneck. We can reduce commit
timeby creating astablelog bufferinmainmemory,usingnonvolatile RAM
(implemented, for example, by battery-backed-up memory). The overhead
imposed by logging can also be reduced by the group-commit technique dis-
cussedlaterinthissection.Throughput(numberoftransactionspersecond)
isstilllimitedbythedata-transferrateofthelogdisk.
  Bufferblocksmarkedasmodi?edbycommittedtransactionsstillhavetobe
writtensothat the amount of log that has tobe replayedat recoverytimeis
reduced.Iftheupdaterateisextremelyhigh,thediskdata-transferratemay
becomeabottleneck.
  If the system crashes, all of main memory is lost. On recovery, the system
hasanemptydatabasebuffer,anddataitemsmustbeinputfromdiskwhen
they are accessed. Therefore, even after recovery is complete, it takes some
time before the database is fully loaded in main memory and high-speed
processingoftransactionscanresume.
Ontheother hand,amain-memorydatabase providesopportunitiesfor op-
timizations:
1
Writelatencyfor?ashdependsonwhetheraneraseoperationmustbedone?rst.
26.4 Main-MemoryDatabases 1107
  Since memory is costlier than disk space, internal data structures in main-
memorydatabaseshavetobedesignedtoreducespacerequirements.How-
ever,datastructurescanhavepointerscrossingmultiplepages,unlikethose
indiskdatabases,wherethecostoftheI/Ostotraversemultiplepageswould
beexcessivelyhigh.Forexample,treestructuresinmain-memorydatabases
can be relatively deep, unlike B
+
-trees, but should minimize space require-
ments.
However, the speed difference between cache memory and main-memory,
and the fact that data is transferred between main-memory and cache in
units of a cache-line (typically about 64 bytes), results in a situation where
the relationship between cache and main-memory is not dissimilar to the
relationship between main-memory and disk (although with smaller speed
differences).Asaresult,B
+
-treeswithsmallnodesthat?tinacachelinehave
beenfoundquiteusefuleveninmain-memorydatabases.
  There is no need to pin buffer pages in memory before data are accessed,
sincebufferpageswillneverbereplaced.
  Query-processing techniques should be designed to minimize space over-
head, so that main-memory limits are not exceeded while a query is being
evaluated; that situation would result in paging to swap area, and would
slowdownqueryprocessing.
  Once the disk I/O bottleneck is removed, operations such as locking and
latching may become bottlenecks. Such bottlenecks must be eliminated by
improvementsintheimplementationoftheseoperations.
  Recoveryalgorithmscanbeoptimized,sincepagesrarelyneedtobewritten
outtomakespaceforotherpages.
TheprocessofcommittingatransactionTrequirestheserecordstobewritten
tostablestorage:
  AlllogrecordsassociatedwithTthathavenotbeenoutputtostablestorage.
  The<Tcommit>logrecord.
Theseoutputoperationsfrequentlyrequiretheoutputofblocksthatareonly
partially ?lled. To ensure that nearly full blocks are output, we use the group-
commit technique. Instead of attempting to commit T when T completes, the
system waits until several transactions have completed, or a certain period of
time has passed since a transaction completed execution. It then commits the
group of transactions that are waiting, together. Blocks written to the log on
stablestoragewouldcontainrecordsofseveraltransactions.Bycarefulchoiceof
group size and maximum waiting time, the system can ensure that blocks are
full when they are written to stable storage without making transactions wait
excessively. This technique results, on average, in fewer output operations per
committedtransaction.
1108 Chapter 26 AdvancedTransactionProcessing
Althoughgroupcommitreducestheoverheadimposedbylogging,itresults
inaslightdelayincommitoftransactionsthatperformupdates.Thedelaycanbe
madequitesmall(say,10milliseconds),whichisacceptableformanyapplications.
These delays can be eliminated if disks or disk controllers support nonvolatile
RAMbuffersforwriteoperations.Transactionscancommitassoonasthewriteis
performedonthenonvolatile RAMbuffer.Inthiscase,thereisnoneedforgroup
commit.
Notethatgroupcommitisusefulevenindatabaseswithdisk-residentdata,
notjustformain-memorydatabases.If?ashstorageisusedinsteadofmagnetic
diskforstoringlogrecords,thecommitdelayissigni?cantlyreduced.However,
groupcommitcanstillbeusefulsinceitminimizesthenumberofpageswritten;
this translates to performance bene?ts in ?ash storage, since pages cannot be
overwritten,andtheeraseoperationisexpensive.(Flashstoragesystemsremap
logical pagestoa pre-erasedphysical page, avoidingdelayat the time apage is
written,buttheeraseoperationmustbeperformedeventuallyaspartofgarbage
collectionofoldversionsofpages.)
26.5 Real-Time Transaction Systems
The integrity constraints that we have considered thus far pertain to the values
storedinthedatabase.Incertainapplications,theconstraints include deadlines
bywhichataskmustbecompleted.Examplesofsuchapplicationsincludeplant
management, traf?c control, and scheduling. When deadlinesare included, cor-
rectness of an execution is no longer solely an issue of database consistency.
Rather, we are concerned with how many deadlines are missed, and by how
muchtimetheyaremissed.Deadlinesarecharacterizedasfollows:
  Hard deadline. Serious problems,such as system crash, may occur ifa task
isnotcompletedbyitsdeadline.
  Firmdeadline.Thetaskhaszerovalueifitiscompletedafterthedeadline.
  Soft deadlines. The task has diminishing value if it is completed after the
deadline,withthevalueapproachingzeroasthedegreeoflatenessincreases.
Systemswithdeadlinesarecalledreal-timesystems.
Transaction management in real-time systems must take deadlines into ac-
count. If the concurrency-control protocol determinesthat a transaction T
i
must
wait, it may cause T
i
to miss the deadline. In such cases, it may be preferable to
pre-emptthetransactionholdingthelock,andtoallowT
i
toproceed.Pre-emption
mustbeusedwithcare,however,becausethetimelostbythepre-emptedtrans-
action(duetorollbackandrestart)maycausethepre-emptedtransactiontomiss
itsdeadline.Unfortunately,itisdif?culttodeterminewhetherrollbackorwaiting
ispreferableinagivensituation.
Amajordif?cultyinsupportingreal-timeconstraintsarisesfromthevariance
intransactionexecutiontime.Inthebestcase,alldataaccessesreferencedatain
26.6 Long-Duration Transactions 1109
the database buffer. In the worst case, each access causes a buffer page to be
written to disk (preceded by the requisite log records), followed by the reading
fromdiskofthepagecontainingthedatatobeaccessed.Becausethetwoormore
disk accesses required in the worst case take several orders of magnitude more
time than the main-memory references required in the best case, transaction
execution time can be estimated only very poorly if data are resident on disk.
Hence,main-memorydatabasesareoftenusedifreal-timeconstraintshavetobe
met.
However, even if data are resident in main memory, variances in execution
timearisefromlockwaits,transactionaborts,andsoon.Researchershavedevoted
considerable effort to concurrency control for real-time databases. They have
extendedlockingprotocolstoprovidehigherpriorityfortransactionswithearly
deadlines.Theyhavefoundthatoptimisticconcurrencyprotocolsperformwellin
real-timedatabases;thatis,theseprotocolsresultinfewermisseddeadlinesthan
eventheextendedlockingprotocols.Thebibliographicalnotesprovidereferences
toresearchintheareaofreal-timedatabases.
In real-time systems, deadlines, rather than absolute speed, are the most
important issue. Designing a real-time system involves ensuring that there is
enough processing power to meet deadlines without requiring excessive hard-
ware resources. Achieving this objective, despite the variance in execution time
resultingfromtransactionmanagement,remainsachallengingproblem.
26.6 Long-Duration Transactions
The transaction concept developed initially in the context of data-processing
applications,inwhichmosttransactionsarenoninteractiveandofshortduration.
Although the techniques presented here and earlier in Chapters 14, 15, and 16
work well in those applications, serious problems arise when this concept is
applied to database systems that involve human interaction. Such transactions
havethesekeyproperties:
  Longduration.Onceahumaninteractswithanactivetransaction,thattrans-
actionbecomesalong-durationtransactionfromtheperspectiveofthecom-
puter,sincehumanresponsetimeisslowrelativetocomputerspeed.Further-
more,indesignapplications,thehumanactivitymayinvolvehours,days,or
anevenlongerperiod.Thus,transactionsmaybeoflongdurationinhuman
terms,aswellasinmachineterms.
  Exposure of uncommitted data. Data generated and displayed to a user
by a long-duration transaction are uncommitted, since the transaction may
abort. Thus, users—and, as a result, other transactions—may be forced to
read uncommitted data. If several users are cooperating on a project, user
transactionsmayneedtoexchangedatapriortotransactioncommit.
  Subtasks.Aninteractivetransactionmayconsistofasetofsubtasksinitiated
bytheuser.Theusermaywishtoabortasubtaskwithoutnecessarilycausing
theentiretransactiontoabort.
1110 Chapter 26 AdvancedTransactionProcessing
  Recoverability.Itisunacceptabletoabortalong-durationinteractivetrans-
action because of a system crash. The active transaction must be recovered
to a state that existedshortly before the crash so that relativelylittle human
workislost.
  Performance. Good performance in an interactive transaction system is de-
?ned as fast response time. This de?nition is in contrast to that in a non-
interactive system, in which high throughput (number of transactions per
second) is the goal. Systems with high throughput make ef?cient use of
system resources. However, in the case of interactive transactions, the most
costlyresourceistheuser.Iftheef?ciencyandsatisfactionoftheuseristobe
optimized,responsetimeshouldbefast(fromahumanperspective).Inthose
caseswhereatasktakesalongtime,responsetimeshouldbepredictable(that
is, the variance in response times should be low), so that users can manage
theirtimewell.
In Sections 26.6.1 through 26.6.5, we shall see why these ?ve properties are in-
compatible with the techniques presented thus far and shall discuss how those
techniques can be modi?ed to accommodate long-duration interactive transac-
tions.
26.6.1 Nonserializable Executions
Thepropertiesthatwediscussedmakeitimpracticaltoenforcetherequirement
usedinearlierchaptersthatonlyserializableschedulesbepermitted.Eachofthe
concurrency-controlprotocolsofChapter15hasadverseeffectsonlong-duration
transactions:
  Two-phaselocking.Whenalockcannotbegranted,thetransactionrequest-
ing the lock is forced to wait for the data item in question to be unlocked.
The duration of this wait is proportional to the duration of the transaction
holding the lock. If the data item is locked by a short-duration transaction,
we expect that the waiting time will be short (except in case of deadlock or
extraordinary system load). However, if the data item is locked by a long-
duration transaction, the wait will be of long duration. Long waiting times
leadtobothlongerresponsetimeandanincreasedchanceofdeadlock.
  Graph-basedprotocols.Graph-basedprotocolsallowforlockstobereleased
earlierthan under the two-phase locking protocols, and they preventdead-
lock.However,theyimposeanorderingonthedataitems.Transactionsmust
lockdataitemsinamannerconsistentwiththisordering.Asaresult,atrans-
actionmayhavetolockmoredatathanitneeds.Furthermore,atransaction
must hold alock until thereis no chance that the lock will be neededagain.
Thus,long-durationlockwaitsarelikelytooccur.
  Timestamp-based protocols. Timestamp protocols never require a transac-
tiontowait.However,theydorequiretransactionstoabortundercertaincir-
cumstances.Ifalong-durationtransactionisaborted,asubstantialamountof
26.6 Long-Duration Transactions 1111
workislost.Fornoninteractivetransactions,thislostworkisaperformance
issue. For interactive transactions, the issue is also one of user satisfaction.
It is highly undesirable for a user to ?nd that several hours’ worth of work
havebeenundone.
  Validation protocols. Like timestamp-based protocols, validation protocols
enforceserializabilitybymeansoftransactionabort.
Thus, it appears that the enforcement of serializability results in long-duration
waits, in abort of long-duration transactions, or in both. There are theoretical
results,citedinthebibliographicalnotes,thatsubstantiatethisconclusion.
Furtherdif?cultieswiththeenforcementofserializabilityarisewhenwecon-
siderrecoveryissues.Wepreviouslydiscussedtheproblemofcascadingrollback,
in which the abort of a transaction may lead to the abort of other transactions.
This phenomenon is undesirable, particularly for long-duration transactions. If
locking is used, exclusive locks must be held until the end of the transaction, if
cascading rollback is to be avoided. This holding of exclusive locks, however,
increasesthelengthoftransactionwaitingtime.
Thus,itappearsthattheenforcementoftransactionatomicitymusteither
lead to an increased probability of long-duration waits or create a possibility of
cascadingrollback.
Snapshotisolation,describedinSection15.7,canprovideapartialsolutionto
theseissues,ascantheoptimisticconcurrencycontrolwithoutreadvalidationprotocol
described in Section 15.9.3. The latter protocol was in fact designed speci?cally
to deal with long duration transactions that involve user interaction. Although
itdoesnotguaranteeserializability,optimisticconcurrencycontrolwithoutread
validationisquitewidelyused.
However, when transactions are of long duration, con?icting updates are
more likely,resulting in additional waits or aborts. These considerations are the
basis for the alternative concepts of correctness of concurrent executions and
transactionrecoverythatweconsiderintheremainderofthissection.
26.6.2 Concurrency Control
The fundamental goal of database concurrency control is to ensure that concur-
rent execution of transactions does not result in a loss of database consistency.
Theconceptofserializabilitycanbeusedtoachievethisgoal,sinceallserializable
schedules preserve consistency of the database. However, not all schedules that
preserve consistency of the database are serializable. For an example, consider
again a bank database consisting of two accounts A and B, with the consistency
requirement that the sum A + B be preserved. Although the schedule of Fig-
ure 26.5 is not con?ict serializable, it nevertheless preserves the sum ofA+B .
It also illustrates two important points about the concept of correctness without
serializability.
  Correctnessdependsonthespeci?cconsistencyconstraintsforthedatabase.
  Correctnessdependsonthepropertiesofoperationsperformedbyeachtrans-
action.
1112 Chapter 26 AdvancedTransactionProcessing
T
1
T
2
read(A)
A:=A ?50
write(A)
read(B)
B:=B ?10
write(B)
read(B)
B:=B +50
write(B)
read(A)
A:=A +10
write(A)
Figure 26.5 A non-con?ict-serializable schedule.
Ingeneralitisnotpossibletoperformanautomaticanalysisoflow-levelopera-
tions by transactions and check their effect on database consistency constraints.
However, there are simpler techniques. One is to use the database consistency
constraintsasthebasisforasplitofthedatabaseintosubdatabasesonwhichcon-
currencycanbemanagedseparately.Anotheristotreatsomeoperationsbesides
read and write as fundamental low-level operations and to extend concurrency
controltodealwiththem.
Thebibliographicalnotesreferenceothertechniquesforensuringconsistency
without requiring serializability. Many of these techniques exploit variants of
multiversion concurrency control (see Section 15.6). For older data-processing
applications that need only one version, multiversion protocols impose a high
space overhead to store the extra versions. Since many of the new database
applications require the maintenance of versions of data, concurrency-control
techniquesthatexploitmultipleversionsarepractical.
26.6.3 Nested and Multilevel Transactions
A long-duration transaction can be viewed as a collection of relatedsubtasks or
subtransactions. By structuring a transaction as a set of subtransactions, we are
abletoenhanceparallelism,sinceitmaybepossibletorunseveralsubtransactions
in parallel. Furthermore, it is possible to deal with failure of a subtransaction
(due to abort, system crash, and so on) without having to roll back the entire
long-durationtransaction.
A nested or multilevel transaction T consists of a set T ={ t
1
, t
2
,..., t
n
} of
subtransactions and a partial order P on T.Asubtransactiont
i
in T may abort
withoutforcingTtoabort.Instead,Tmayeitherrestartt
i
orsimplychoosenotto
runt
i
.Ift
i
commits,thisactiondoesnotmaket
i
permanent(unlikethesituationin
Chapter16).Instead,t
i
commitstoT,andmaystillabort(orrequirecompensation
—see Section 26.6.4) if T aborts. An execution of T must not violate the partial
26.6 Long-Duration Transactions 1113
orderP.Thatis,ifanedget
i
? t
j
appearsintheprecedencegraph,thent
j
? t
i
mustnotbeinthetransitiveclosureofP.
Nestingmaybe severallevelsdeep,representinga subdivisionofa transac-
tionintosubtasks,subsubtasks,andsoon.Atthelowestlevelofnesting,wehave
thestandarddatabaseoperationsreadandwritethatwehaveusedpreviously.
If a subtransaction of T is permitted to release locks on completion, T is
calledamultileveltransaction.Whenamultileveltransactionrepresentsalong-
durationactivity,thetransactionissometimesreferredtoasasaga.Alternatively,
if locks held by a subtransaction t
i
of T are automatically assigned to T on
completionof t
i
, T iscalledanestedtransaction.
Although the main practical value of multilevel transactions arises in com-
plex,long-durationtransactions, we shall use the simpleexampleof Figure26.5
to show how nesting can create higher-level operations that may enhance con-
currency. We rewrite transaction T
1
, using subtransactions T
1,1
and T
1,2
,which
performincrementordecrementoperations:
  T
1
consistsof:
?
T
1,1
,whichsubtracts50fromA.
?
T
1,2
,whichadds50to B.
Similarly, we rewrite transaction T
2
, using subtransactions T
2,1
and T
2,2
, which
alsoperformincrementordecrementoperations:
  T
2
consistsof:
?
T
2,1
,whichsubtracts10fromB.
?
T
2,2
,whichadds10to A.
Noorderingisspeci?edon T
1,1
, T
1,2
, T
2,1
,and T
2,2
.Anyexecutionofthesesub-
transactionswillgenerateacorrectresult.ThescheduleofFigure26.5corresponds
totheschedule< T
1,1
, T
2,1
, T
1,2
, T
2,2
>.
26.6.4 Compensating Transactions
To reduce the frequency of long-duration waiting, we arrange for uncommit-
ted updates to be exposed to other concurrently executing transactions. Indeed,
multilevel transactions may allow this exposure. However, the exposure of un-
committeddatacreatesthepotentialforcascadingrollbacks.Theconceptofcom-
pensatingtransactionshelpsustodealwiththisproblem.
Lettransaction T bedividedintoseveralsubtransactions t
1
,t
2
,...,t
n
.Aftera
subtransactiont
i
commits,itreleasesitslocks.Now,iftheouter-leveltransaction
T has to be aborted, the effect of its subtransactions must be undone. Suppose
thatsubtransactionst
1
,...,t
k
havecommitted,andthatt
k+1
wasexecutingwhen
the decision to abort is made. We can undo the effects of t
k+1
by aborting that
1114 Chapter 26 AdvancedTransactionProcessing
subtransaction. However, it is not possible to abort subtransactions t
1
,...,t
k
,
sincetheyhavecommittedalready.
Instead,weexecuteanewsubtransactionct
i
,calledacompensatingtransaction,
toundotheeffectofasubtransactiont
i
.Eachsubtransactiont
i
isrequiredtohave
acompensatingtransactionct
i
.Thecompensatingtransactionsmustbeexecuted
intheinverseorderct
k
,...,ct
1
.Hereareseveralexamplesofcompensation:
  Consider the schedule of Figure 26.5, which we have shown to be correct,
althoughnotcon?ictserializable.Eachsubtransactionreleasesitslocksonce
itcompletes.Supposethat T
2
failsjust priortotermination,after T
2,2
has re-
leaseditslocks.WethenrunacompensatingtransactionforT
2,2
thatsubtracts
10fromAandacompensatingtransactionfor T
2,1
thatadds10toB.
  Consider a database insert by transaction T
i
that, as a side effect, causes a
B
+
-treeindextobeupdated.Theinsertoperationmayhavemodi?edseveral
nodes of the B
+
-tree index. Other transactions may have read these nodes
in accessing dataother than the record insertedby T
i
. Asmentionedin Sec-
tion16.7,wecanundotheinsertionbydeletingtherecordinsertedbyT
i
.The
result is a correct, consistent B
+
-tree, but is not necessarily one with exactly
the same structure as the one we had before T
i
started. Thus, deletion is a
compensatingactionforinsertion.
  Consider a long-duration transaction T
i
representing a travel reservation.
Transaction T has three subtransactions: T
i,1
, which makes airline reserva-
tions; T
i,2
, which reserves rental cars; and T
i,3
, which reserves a hotel room.
Suppose that the hotel cancels the reservation. Instead of undoing all of T
i
,
wecompensateforthefailureofT
i,3
bydeletingtheoldhotelreservationand
makinganewone.
Ifthesystemcrashesinthemiddleofexecutinganouter-leveltransaction,its
subtransactions mustbe rolled back when it recovers.The techniques described
inSection16.7canbeusedforthispurpose.
Compensation for the failure of a transaction requires that the semantics of
the failed transaction be used. For certain operations, such as incrementation or
insertion into a B
+
-tree, the corresponding compensation is easily de?ned. For
more complex transactions, the application programmers may have to de?ne
the correct form of compensation at the time that the transaction is coded. For
complex interactive transactions, it may be necessary for the system to interact
withtheusertodeterminetheproperformofcompensation.
26.6.5 Implementation Issues
The transaction concepts discussed in this section create serious dif?culties for
implementation.Wepresentafewofthemhere,anddiscusshowwecanaddress
theseproblems.
Long-durationtransactionsmustsurvivesystemcrashes.Wecanensurethat
they will by performing a redo on committed subtransactions, and by perform-
26.7 Summary 1115
ing either an undoor compensation for any short-duration subtransactions that
were active at the time of the crash. However, these actions solve only part of
the problem. In typical database systems, such internal system data as lock ta-
blesandtransactiontimestampsarekeptinvolatilestorage.Foralong-duration
transactiontoberesumedafteracrash,thesedatamustberestored.Therefore,it
isnecessarytolognotonlychangestothedatabase,butalsochangestointernal
systemdatapertainingtolong-durationtransactions.
Loggingofupdatesismademorecomplexwhencertaintypesofdataitems
exist in the database. A data item may be a CAD design, text of a document,
oranotherformofcompositedesign.Suchdataitemsarephysicallylarge.Thus,
storingboththeoldandnewvaluesofthedataiteminalogrecordisundesirable.
Therearetwoapproachestoreducingtheoverheadofensuringtherecover-
abilityoflargedataitems:
  Operation logging. Only the operation performed on the data item and
the data-item name are stored in the log. Operation logging is also called
logical logging. For each operation, an inverse operation must exist. We
perform undo using the inverse operation and redo using the operation
itself. Recovery through operation logging is more dif?cult, since redo and
undoarenotidempotent.Further,usinglogicalloggingforanoperationthat
updatesmultiplepagesisgreatlycomplicatedbythefactthatsome,butnot
all, of the updated pages may have been written to the disk, so it is hard to
applyeitherthe redoorthe undooftheoperationonthediskimageduring
recovery.Usingphysicalredologgingandlogicalundologging,asdescribed
in Section 16.7, provides the concurrency bene?ts of logical logging while
avoidingtheabovepitfalls.
  Loggingandshadowpaging.Loggingisusedformodi?cationstosmalldata
items, but large data items are often made recoverable via a shadowing, or
copy-on-write, technique. When we use shadowing, it is possible to reduce
theoverheadbykeepingcopiesofonlythosepagesthatareactuallymodi?ed.
Regardlessof the technique used, the complexitiesintroduced by long-duration
transactions and large data items complicate the recovery process. Thus, it is
desirabletoallowcertainnoncriticaldatatobeexemptfromlogging,andtorely
insteadonofflinebackupsandhumanintervention.
26.7 Summary
  Work?ows are activities that involve the coordinated execution of multiple
tasks performed by different processing entities. They exist not just in com-
puter applications, but also in almost all organizational activities. With the
growthofnetworks,andtheexistenceofmultipleautonomousdatabasesys-
tems,work?owsprovideaconvenientwayofcarryingouttasksthatinvolve
multiplesystems.
  Although the usual ACID transactional requirements are too strong or are
unimplementableforsuchwork?owapplications,work?owsmustsatisfya
1116 Chapter 26 AdvancedTransactionProcessing
limitedsetoftransactionalpropertiesthatguaranteethataprocessisnotleft
inaninconsistentstate.
  Transaction-processing monitors were initially developed as multithreaded
serversthat could service large numbers of terminals from a single process.
Theyhavesinceevolved,andtodaytheyprovidetheinfrastructureforbuild-
ing and administering complex transaction-processing systems that have a
large number of clientsand multiple servers.They provideservicessuch as
durable queueing of client requests and server responses, routing of client
messagestoservers,persistentmessaging,loadbalancing,andcoordination
oftwo-phasecommitwhentransactionsaccessmultipleservers.
  E-commerce systems have become a core part of commerce. There are sev-
eraldatabaseissuesine-commercesystems.Catalogmanagement,especially
personalization of the catalog, is done with databases. Electronic market-
places help in pricing of products through auctions, reverse auctions, or
exchanges. High-performance database systems are needed to handle such
trading. Orders are settled by electronic payment systems, which also need
high-performancedatabasesystemstohandleveryhightransactionrates.
  Large main memories are exploited in certain systems to achieve high sys-
tem throughput. In such systems, logging is a bottleneck. Under the group-
commitconcept,thenumberofoutputstostablestoragecanbereduced,thus
releasingthisbottleneck.
  The ef?cient management of long-duration interactive transactions is more
complex, because of the long-duration waits and because of the possibil-
ity of aborts. Since the concurrency-control techniques used in Chapter 15
use waits, aborts, or both, alternative techniquesmust be considered.These
techniquesmustensurecorrectnesswithoutrequiringserializability.
  Along-durationtransactionisrepresentedasanestedtransactionwithatomic
databaseoperationsatthelowestlevel.Ifatransactionfails,onlyactiveshort-
duration transactions abort. Active long-duration transactions resume once
anyshort-durationtransactionshaverecovered.Acompensatingtransaction
isneededtoundoupdatesofnestedtransactionsthathavecommitted,ifthe
outer-leveltransactionfails.
  In systems with real-time constraints, correctness of execution involves not
only database consistency but also deadline satisfaction. The wide variance
ofexecutiontimesforreadandwriteoperationscomplicatesthetransaction-
managementproblemfortime-constrainedsystems.
Review Terms
  TPmonitor
  TP-monitorarchitectures
?
Processperclient
?
Singleserver
PracticeExercises 1117
?
Manyserver,singlerouter
?
Manyserver,manyrouter
  Multitasking
  Contextswitch
  Multithreadedserver
  Queuemanager
  Applicationcoordination
?
Resourcemanager
?
Remoteprocedurecall(RPC)
  Transactionalwork?ows
?
Task
?
Processingentity
?
Work?owspeci?cation
?
Work?ow execution
  Work?owstate
?
Executionstates
?
Outputvalues
?
Externalvariables
  Work?owfailureatomicity
  Work?owterminationstates
?
Acceptable
?
Nonacceptable
?
Committed
?
Aborted
  Work?owrecovery
  Work?ow-managementsystem
  Work?ow-managementsystem
architectures
?
Centralized
?
Partiallydistributed
?
Fullydistributed
  Businessprocessmanagement
  Orchestration
  E-commerce
  E-catalogs
  Marketplaces
?
Auctions
?
Reverseauctions
?
Exchange
  Ordersettlement
  Digitalcerti?cates
  Main-memorydatabases
  Groupcommit
  Real-timesystems
  Deadlines
?
Harddeadline
?
Firmdeadline
?
Softdeadline
  Real-timedatabases
  Long-durationtransactions
  Exposureofuncommitteddata
  Nonserializableexecutions
  Nestedtransactions
  Multileveltransactions
  Saga
  Compensatingtransactions
  Logicallogging
Practice Exercises
26.1 Like database systems, work?ow systems also require concurrency and
recoverymanagement.Listthreereasonswhywecannotsimplyapplya
relationaldatabasesystemusing 2PL,physicalundologging,and 2PC.
1118 Chapter 26 AdvancedTransactionProcessing
26.2 Consider a main-memory database system recovering from a system
crash.Explaintherelativemeritsof:
  Loadingtheentiredatabasebackintomainmemorybeforeresuming
transactionprocessing.
  Loadingdataasitisrequestedbytransactions.
26.3 Isahigh-performancetransactionsystemnecessarilyareal-timesystem?
Whyorwhynot?
26.4 Explain why it may be impractical to require serializability for long-
durationtransactions.
26.5 Consideramultithreadedprocessthatdeliversmessagesfromadurable
queue of persistent messages. Different threads may run concurrently,
attemptingtodeliverdifferentmessages.Incaseofadeliveryfailure,the
messagemustberestoredinthequeue.Modeltheactionsthateachthread
carriesoutasamultileveltransaction,sothatlocksonthequeueneednot
behelduntilamessageisdelivered.
26.6 Discuss the modi?cations that need to be made in each of the recovery
schemes covered in Chapter 16 if we allow nested transactions. Also,
explainanydifferencesthatresultifweallowmultileveltransactions.
Exercises
26.7 ExplainhowaTPmonitormanagesmemoryandprocessorresourcesmore
effectivelythanatypicaloperatingsystem.
26.8 Compare TP-monitor features with those provided by Web servers sup-
portingservlets(suchservershavebeennicknamed TP-lite).
26.9 Consider the process of admitting new students at your university (or
newemployeesatyourorganization).
a. Giveahigh-levelpictureofthework?owstartingfromthestudent
applicationprocedure.
b. Indicate acceptable termination states and which steps involve hu-
manintervention.
c. Indicate possible errors (including deadline expiry) and how they
aredealtwith.
d. Studyhowmuchofthework?owhasbeenautomatedatyouruni-
versity.
26.10 Answerthefollowingquestionsregardingelectronicpaymentsystems:
Bibliographical Notes 1119
a. Explain why electronic transactions carried out using credit-card
numbersmaybeinsecure.
b. Analternativeistohaveanelectronicpaymentgatewaymaintained
bythecredit-cardcompany,andthesitereceivingpaymentredirects
customerstothegatewaysitetomakethepayment.
i. Explain what bene?ts such a system offers if the gateway does
notauthenticatetheuser.
ii. Explain what further bene?ts are offered if the gateway has a
mechanismtoauthenticatetheuser.
c. Somecredit-cardcompaniesofferaone-time-usecredit-cardnumber
asamoresecuremethodofelectronicpayment.Customersconnect
tothecredit-cardcompany’sWebsitetogettheone-time-usenum-
ber.Explainwhatbene?tsuchasystemoffers,ascomparedtousing
regularcredit-cardnumbers.Alsoexplainitsbene?tsanddrawbacks
ascomparedtoelectronicpaymentgatewayswithauthentication.
d. Doeseitheroftheabovesystemsguaranteethesameprivacythatis
availablewhenpaymentsaremadeincash?Explainyouranswer.
26.11 If the entire database ?ts in main memory, do we still need a database
systemtomanagethedata?Explainyouranswer.
26.12 Inthegroup-committechnique,howmanytransactionsshouldbepartof
agroup?Explainyouranswer.
26.13 In a database system using write-ahead logging, what is the worst-case
number of disk accesses required to read a data item from a speci?ed
diskpage.Explainwhythispresentsaproblemtodesignersofreal-time
databasesystems.Hint:considerthecasewhenthediskbufferisfull.
26.14 Whatisthepurposeofcompensatingtransactions?Presenttwoexamples
oftheiruse.
26.15 Explain the connections between a work?ow and a long-duration trans-
action.
Bibliographical Notes
Gray and Reuter [1993] providesa detailed(and excellent)textbook description
of transaction-processing systems, including chapters on TP monitors. X/Open
[1991]de?nestheX/Open XAinterface.
Fischer[2006]isahandbookonwork?owsystems,whichispublishedinas-
sociationwiththeWork?owManagementCoalition.TheWebsiteofthecoalition
is www.wfmc.org.Ourdescriptionofwork?owsfollowsthemodelofRusinkiewicz
andSheth[1995].
Loeb[1998]providesadetaileddescriptionofsecureelectronictransactions.
1120 Chapter 26 AdvancedTransactionProcessing
Garcia-Molina and Salem [1992] provides an overview of main-memory
databases.Jagadishetal.[1993]describesarecoveryalgorithmdesignedformain-
memorydatabases.Astoragemanagerformain-memorydatabasesisdescribed
inJagadishetal.[1994].
Real-timedatabasesarediscussedbyLamandKuo[2001].Concurrencycon-
trol and scheduling in real-timedatabases are discussedby Haritsa et al. [1990],
Hong et al. [1993], and Pang et al. [1995]. Ozsoyoglu and Snodgrass [1995] is a
surveyofresearchinreal-timeandtemporaldatabases.
NestedandmultileveltransactionsarepresentedbyMoss[1985],Lynchand
Merritt[1986],Moss[1987],HaerderandRothermel[1987],RothermelandMohan
[1989],Weikumetal.[1990],KorthandSpeegle[1990],Weikum[1991],andKorth
and Speegle [1994], Theoretical aspects of multilevel transactions are presented
inLynchetal.[1988].TheconceptofSagawasintroducedinGarcia-Molinaand
Salem[1987].
PART
9
CASESTUDIES
This part describes how different database systems integrate the various con-
cepts described earlier in the book. We begin by covering a widely used open-
sourcedatabasesystem,PostgreSQL,inChapter27.Threewidelyusedcommercial
database systems—IBM DB2,Oracle,andMicrosoftSQL Server—are covered in
Chapters 28, 29, and 30. These three represent three of the most widely used
commercialdatabasesystems.
Each of these chapters highlights unique features of each database system:
tools, SQL variations and extensions, and system architecture, including storage
organization, query processing, concurrency control and recovery, and replica-
tion.
The chapters cover only key aspects of the database products they describe,
andthereforeshouldnotberegardedasacomprehensivecoverageoftheproduct.
Furthermore, since products are enhanced regularly, details of the product may
change. When using a particular product version, be sure to consult the user
manualsforspeci?cdetails.
Keep in mind that the chapters in this part often use industrial rather than
academicterminology.Forinstance,theyusetableinsteadofrelation,rowinstead
oftuple,andcolumninsteadofattribute.
1121
This page intentionally left blank 
CHAPTER
27
PostgreSQL
Anastasia Ailamaki, Sailesh Krishnamurthy, Spiros
Papadimitriou, Bianca Schroeder,
Karl Schnaitter, and Gavin Sherry
PostgreSQLisanopen-sourceobject-relationaldatabasemanagementsystem.Itis
adescendantofoneoftheearliestsuchsystems,the POSTGRESsystemdeveloped
underProfessorMichaelStonebrakerattheUniversityofCalifornia,Berkeley.The
name“postgres”isderivedfromthenameofapioneeringrelationaldatabasesys-
tem,Ingres,alsodevelopedunderStonebrakeratBerkeley.Currently,PostgreSQL
supports many aspects of SQL:2003 and offers features such as complex queries,
foreign keys, triggers, views, transactional integrity, full-text searching, and lim-
ited data replication. In addition, users can extend PostgreSQL with new data
types, functions, operators, or index methods. PostgreSQL supports a variety of
programming languages (including C, C++, Java, Perl, Tcl, and Python) as well
as the database interfaces JDBC and ODBC. Another notable point of PostgreSQL
is that it, along with MySQL, is one of the two most widely used open-source
relational database systems. PostgreSQL is released under the BSD license, which
grants permission to anyone for the use, modi?cation, and distribution of the
PostgreSQL codeand documentationfor any purposewithout fee.
27.1 Introduction
In the course of two decades, PostgreSQL has undergone several major releases.
The ?rst prototype system, under the name POSTGRES, was demonstrated at the
1988 ACM SIGMODconference.The?rstversion,distributedtousersin1989,pro-
vided features such as extensible data types, a preliminary rule system, and a
query language named POSTQUEL. After the subsequent versions added a new
rule system, support for multiple storage managers, and an improved query ex-
ecutor,thesystemdevelopersfocusedonportabilityandperformanceuntil1994,
when an SQL language interpreter was added. Under a new name, Postgres95,
thesystemwasreleasedtotheWebandlatercommercializedbyIllustraInforma-
1123
1124 Chapter27 PostgreSQL
tion Technologies (later merged into Informix, which is now owned by IBM). By
1996,thenamePostgres95wasreplacedby PostgreSQL,tore?ecttherelationship
betweentheoriginal POSTGRESand the more recent versions with SQLcapability.
PostgreSQL runs under virtually all Unix-like operating systems, including
Linux and AppleMacintosh OS X. Early versions of the PostgreSQL servercan be
rununderMicrosoftWindowsintheCygwinenvironment,whichprovidesLinux
emulation under Windows. Version 8.0, released in January 2005, introduced
nativesupportfor MicrosoftWindows.
Today, PostgreSQLisusedtoimplementseveraldifferentresearchandproduc-
tionapplications(suchasthePostGISsystemforgeographicinformation)andan
educational tool at several universities. The system continues to evolve through
the contributions of a community of about 1000 developers. In this chapter, we
explain how PostgreSQL works, starting from user interfaces and languages and
continuing intotheheartofthesystem(thedatastructuresandtheconcurrency-
control mechanism).
27.2 UserInterfaces
The standard distribution of PostgreSQL comes with command-line tools for
administering the database. However, there is a wide range of commercial and
open-source graphical administration and design tools that support PostgreSQL.
Softwaredevelopersmayalsoaccess PostgreSQL through a comprehensive set of
programminginterfaces.
27.2.1 InteractiveTerminalInterfaces
Like most database systems, PostgreSQL offers command-line tools for database
administration. The main interactive terminal client is psql, which is modeled
aftertheUnixshellandallowsexecutionof SQLcommandsontheserver,aswell
asseveralotheroperations(suchasclient-sidecopying).Someofitsfeaturesare:
• Variables. psql provides variable substitution features, similar to common
Unixcommand shells.
• SQLinterpolation.Theusercansubstitute(“interpolate”)psqlvariablesinto
regular SQL statementsby placing a coloninfront of thevariablename.
• Command-line editing. psql uses the GNU readline library for convenient
lineediting,withtab-completionsupport.
PostgreSQL may also be accessed from a Tcl/Tk shell, which provides a ?exible
scripting language commonly used for rapid prototyping. This functionality is
enabledinTcl/Tkbyloadingthepgtcllibrary,whichisdistributedasanoptional
extensionto PostgreSQL.
27.2.2 GraphicalInterfaces
The standard distribution of PostgreSQL does not contain any graphical tools.
However,severalgraphicaluserinterfacetoolsexist,anduserscanchooseamong
27.2 UserInterfaces 1125
Figure27.1 pgAdmin III: An open-source database administration GUI.
commercialandopen-sourcealternatives.Manyofthesegothroughrapidrelease
cycles;the following listre?ectsthestateof affairsat thetimeof thiswriting.
There are graphical tools for administration, including pgAccess and pgAd-
min,thelatterofwhichisshowninFigure27.1.Toolsfordatabasedesigninclude
TORA and Data Architect,the latterof which is shown in Figure27.2. PostgreSQL
workswithseveralcommercialforms-designandreport-generationtools.Open-
source alternatives include Rekall (shown in Figures 27.3 and 27.4), GNU Report
Generator,anda morecomprehensivetoolsuite, GNU Enterprise.
27.2.3 ProgrammingLanguage Interfaces
PostgreSQL provides native interfaces for ODBC and JDBC,aswellasbindings
for most programming languages, including C, C++, PHP, Perl, Tcl/Tk, ECPG,
Python, and Ruby.
The libpq library provides the C API for PostgreSQL; libpq is also the un-
derlying engine for most programming-language bindings. The libpq library
supports both synchronous and asynchronous execution of SQL commands and
prepared statements, through a reentrant and thread-safe interface. The connec-
tion parameters of libpq may be con?gured in several ?exible ways, such as
1126 Chapter27 PostgreSQL
Figure27.2 Data Architect: A multiplatform database design GUI.
setting environment variables, placing settings in a local ?le, or creating entries
onan LDAPserver.
27.3 SQLVariationsandExtensions
Thecurrentversionof PostgreSQLsupportsalmostallentry-levelSQL-92features,
as well as many of the intermediate- and full-level features. It also supports
many SQL:1999 and SQL:2003 features, including most object-relational features
describedinChapter22andthe SQL/XMLfeaturesforparsedXMLdatadescribed
in Chapter 23. In fact, some features of the current SQL standard (such as arrays,
functions,andinheritance)werepioneeredbyPostgreSQLoritsancestors.Itlacks
OLAP features (most notably,cube androllup), but data from PostgreSQL can be
easilyloadedintoopen-sourceexternal OLAP servers(suchasMondrian)aswell
as commercial products.
27.3.1 PostgreSQLTypes
PostgreSQL has support for several nonstandard types, useful for speci?c appli-
cation domains. Furthermore, users can de?ne new types with the create type
27.3 SQLVariationsandExtensions 1127
Figure27.3 Rekall: Form-design GUI.
command. This includes new low-level base types, typically written in C (see
Section27.3.3.1).
27.3.1.1 ThePostgreSQLTypeSystem
PostgreSQL typesfallinto thefollowing categories:
• Basetypes.Basetypesarealsoknownasabstractdatatypes;thatis,modules
that encapsulate both state and a set of operations. These are implemented
below the SQL level, typically in a language such as C (see Section 27.3.3.1).
Examplesareint4 (alreadyincluded in PostgreSQL)orcomplex (included as
an optional extension type). A base type may represent either an individual
scalar value or a variable-length array of values. For each scalar type that
existsinadatabase,PostgreSQLautomaticallycreatesanarraytypethatholds
valuesofthe samescalar type.
• Composite types. These correspond to table rows; that is, they are a list of
?eldnamesandtheirrespectivetypes.Acompositetypeiscreatedimplicitly
wheneveratable iscreated,but usersmayalsoconstruct themexplicitly.
1128 Chapter27 PostgreSQL
Figure27.4 Rekall: Report-design GUI.
• Domains.Adomaintypeisde?nedbycouplingabasetypewithaconstraint
that values of the type must satisfy. Values of the domain type and the asso-
ciatedbasetypemaybeusedinterchangeably,providedthattheconstraintis
satis?ed.Adomainmayalsohaveanoptionaldefaultvalue,whosemeaning
issimilarto thedefaultvalueof a tablecolumn.
• Enumerated types. These are similar to enum types used in programming
languages such as C and Java. An enumerated type is essentially a ?xed list
of named values. In PostgreSQL, enumerated types may be converted to the
textual representation of their name, but this conversion must be speci?ed
explicitly in some cases to ensure type safety. For instance, values of differ-
ent enumerated types may not be compared without explicit conversion to
compatibletypes.
• Pseudotypes. Currently, PostgreSQL supports the following pseudotypes:
any, anyarray, anyelement, anyenum, anynonarray cstring, internal, opaque, lan-
guage handler, record, trigger,andvoid. These cannot be used in composite
types (and thus cannot be used for table columns), but can be used as argu-
mentand returntypesofuser-de?nedfunctions.
• Polymorphictypes.Fourof thepseudotypes anyelement, anyarray, anynonar-
ray,andanyenumarecollectivelyknownaspolymorphic.Functionswithar-
gumentsofthesetypes(correspondinglycalledpolymorphicfunctions)may
operate on any actual type. PostgreSQL has a simple type-resolution scheme
27.3 SQLVariationsandExtensions 1129
thatrequiresthat:(1)inanyparticularinvocationofapolymorphicfunction,
alloccurrencesofapolymorphictypemustbeboundtothesameactualtype
(that is, a function de?ned as f(anyelement, anyelement) may operate only on
pairsofthesameactualtype),and(2)ifthereturntypeispolymorphic,then
atleastone of theargumentsmust beof the samepolymorphictype.
27.3.1.2 NonstandardTypes
Thetypesdescribedinthissectionareincludedinthestandarddistribution.Fur-
thermore, thanks to the open nature of PostgreSQL, there are several contributed
extensiontypes,such as complexnumbers,and ISBN/ISSNs (seeSection27.3.3).
Geometricdatatypes(point, line, lseg, box, polygon, path, circle)areusedinge-
ographic information systems to represent two-dimensional spatial objects such
as points, line segments, polygons, paths, and circles. Numerous functions and
operators are available in PostgreSQL to perform various geometric operations
such as scaling, translation, rotation, and determining intersections. Further-
more,PostgreSQLsupportsindexingofthesetypesusingR-trees(Sections25.3.5.3
and 27.5.2.1).
Full-text searching is performed in PostgreSQL using the tsvector type that
represents a document and the tsquery type that represents a full-text query. A
tsvectorstoresthedistinctwordsinadocument,afterconvertingvariantsofeach
wordtoacommonnormalform(forexample,removingwordstems).PostgreSQL
providesfunctionstoconvertrawtexttoatsvectorandconcatenatedocuments.A
tsqueryspeci?eswordstosearchforincandidatedocuments,withmultiplewords
connected by Boolean operators. For example, the query ’index & !(tree | hash)’
?nds documents that contain “index” without using the words “tree” or “hash.”
PostgreSQL natively supports operations on full-text types, including language
featuresand indexedsearch.
PostgreSQL offers data types to store network addresses. These data types
allow network-management applications to use a PostgreSQL database as their
data store. For those familiar with computer networking, we provide a brief
summary of this feature here. Separate types exist for IPv4, IPv6, and Media
Access Control (MAC) addresses (cidr, inet and macaddr, respectively). Both inet
and cidr types can store IPv4 and IPv6 addresses, with optional subnet masks.
Their main difference is in input/output formatting, as well as the restriction
thatclasslessInternetdomainrouting(CIDR)addressesdonotacceptvalueswith
nonzero bits to the right of the netmask. The macaddr type is used to store MAC
addresses (typically, Ethernet card hardware addresses). PostgreSQL supports
indexing and sorting on these types, as well as a set of operations (including
subnet testing, and mapping MAC addresses to hardware manufacturer names).
Furthermore, these types offer input-error checking. Thus, they are preferable
overplaintext?elds.
The PostgreSQL bit type can store both ?xed- and variable-length strings
of 1s and 0s. PostgreSQL supports bit-logical operators and string-manipulation
functions for thesevalues.
1130 Chapter27 PostgreSQL
27.3.2 RulesandOtherActive-Database Features
PostgreSQL supports SQL constraints and triggers (and stored procedures; see
Section27.3.3).Furthermore,itfeaturesquery-rewritingrulesthatcanbedeclared
onthe server.
PostgreSQL allows check constraints, not null constraints, and primary-key
and foreign-keyconstraints (withrestrictingandcascading deletes).
Like many other relational database systems, PostgreSQL supports triggers,
which are useful for nontrivial constraints and consistency checking or enforce-
ment.TriggerfunctionscanbewritteninaprocedurallanguagesuchasPL/pgSQL
(see Section 27.3.3.4) or in C, but not in plain SQL. Triggers can execute before or
after insert, update,ordelete operations and either once per modi?ed row, or
once per SQL statement.
ThePostgreSQLrulessystemallowsuserstode?nequery-rewriterulesonthe
databaseserver.Unlikestoredproceduresandtriggers,therulesystemintervenes
betweenthequeryparserandtheplannerandmodi?esqueriesonthebasisofthe
set of rules. After the original query tree has been transformed into one or more
trees,theyarepassedtothequeryplanner.Thus,theplannerhasallthenecessary
information(tablestobescanned,relationshipsbetweenthem,quali?cations,join
information,andsoforth)andcancomeupwithanef?cientexecutionplan,even
when complexrulesareinvolved.
Thegeneralsyntax for declaringrulesis:
createrule rule nameas
on{select|insert|update|delete }
to table [where rule quali?cation ]
do[instead ] {nothing| command|( command; command...)}
The rest of this section provides examples that illustrate the rule system’s capa-
bilities.More details on how rules are matched to query trees and how the latter
aresubsequentlytransformedcanbefoundinthePostgreSQLdocumentation(see
the bibliographical notes). The rule system is implemented in the rewrite phase
of queryprocessingand explainedinSection27.6.1.
First, PostgreSQL usesrulestoimplementviews.Aviewde?nitionsuchas:
createview myviewasselect *from mytab;
isconvertedinto thefollowing rulede?nition:
createtable myview(same column list as mytab);
createrule returnasonselectto myviewdoinstead
select *from mytab;
Queriesonmyviewaretransformedbeforeexecutiontoqueriesontheunderlying
table mytab.Thecreate view syntax is considered better programming form in
this case, since it is more concise and it also prevents creation of views that
27.3 SQLVariationsandExtensions 1131
reference each other (which is possible if rules are carelessly declared, resulting
in potentially confusing runtime errors). However, rules can be used to de?ne
updateactions onviewsexplicitly(createview statementsdonot allowthis).
Asanotherexample,considerthecasewheretheuserwantstologallincreases
ofinstructorsalaries.Thiscould beachievedby arulesuchas:
createrule salary auditasonupdateto instructor
wherenew.salary<>old.salary
doinsertinto salary audit
values (current timestamp,current user,
new.name,old.salary,new.salary);
Finally,wegiveaslightlymorecomplicatedinsert/updaterule.Assumethat
pending salary increases are stored in a table salary increases(name, increase). We
can declare a “dummy” table approved increases with the same ?elds and then
de?nethefollowing rule:
createrule approved increases insert
asoninsertto approved increases
doinstead
update instructor
set salary= salary+new.increase
where name=new.name;
Thenthefollowing query:
insertinto approved increasesselect *from salary increases;
will update all salaries in the instructor table at once. Since theinstead keyword
was speci?ed inthe rule,the approved increasestable isunchanged.
There is some overlap between the functionality provided by rules and per-
rowtriggers.ThePostgreSQLrulesystemcanbeusedtoimplementmosttriggers,
butsomekindsofconstraints(inparticular,foreignkeys)cannotbeimplemented
byrules.Also,triggershavetheaddedabilitytogenerateerrormessagestosignal
constraint violations, whereas a rule may only enforce data integrity by silently
suppressing invalid values. On the other hand, triggers cannot be used for the
updateordelete actions that rulesenable onviews.Since thereis no realdatain
aviewrelation,thetriggerwouldneverbecalled.
An important difference between triggers and views is that a trigger is exe-
cuted iteratively for every affected row. A rule, on the other hand, manipulates
the query treebefore query planning. So if a statement affects many rows, a rule
isfar moreef?cientthan atrigger.
The implementation of triggers and constraints in PostgreSQL is outlined
brie?yinSection27.6.4.
1132 Chapter27 PostgreSQL
27.3.3 Extensibility
Likemostrelationaldatabasesystems,PostgreSQLstoresinformationaboutdata-
bases, tables, columns, and so forth, in what are commonly known as system
catalogs, which appear to the user as normal tables. Other relational database
systemsaretypicallyextendedbychanginghard-codedproceduresinthesource
codeor by loadingspecialextensionmoduleswrittenby the vendor.
Unlikemostrelationaldatabasesystems,PostgreSQLgoesonestepfurtherand
stores much more information in its catalogs: not only information about tables
and columns, but also information about data types, functions, access methods,
and so on. Therefore, PostgreSQL is easy for users to extend and facilitates rapid
prototyping of new applications and storage structures. PostgreSQL can also in-
corporate user-written code into the server, through dynamic loading of shared
objects. This provides an alternative approach to writing extensions that can be
usedwhencatalog-based extensionsarenot suf?cient.
Furthermore,thecontribmoduleofthePostgreSQLdistributionincludesnu-
meroususerfunctions(forexample,arrayiterators,fuzzystringmatching,cryp-
tographicfunctions), base types(for example,encryptedpasswords, ISBN/ISSNs,
n-dimensional cubes) and index extensions (for example, RD-trees, indexing for
hierarchical labels). Thanks to the open nature of PostgreSQL,thereisalarge
communityof PostgreSQLprofessionalsandenthusiastswhoalsoactivelyextend
PostgreSQLonanalmostdailybasis.Extensiontypesareidenticalinfunctionality
tothebuilt-intypes(seealsoSection27.3.1.2);thelatteraresimplyalreadylinked
into the server and preregisteredin the system catalog. Similarly,this is the only
differencebetweenbuilt-inand extensionfunctions.
27.3.3.1 Types
PostgreSQLallowsuserstode?necompositetypes,enumerationtypes,andeven
new base types.
A composite-type de?nition is similar to a table de?nition (in fact, the latter
implicitly does the former). Stand-alone composite types are typically useful for
function arguments.Forexample,the de?nition:
createtype city tas (namevarchar(80), statechar(2));
allows functions to accept and return city t tuples, even if there is no table that
explicitlycontains rows of thistype.
Enumeration types are easy to de?ne, by simply listing the names of the
values.Thefollowingexamplecreatesanenumeratedtypetorepresentthestatus
of a softwareproduct.
createtype status tasenum(’alpha’, ’beta’, ’release’);
The order of listed names is signi?cant in comparing values of an enumerated
type.This can beusefulfor astatementsuch as:
27.3 SQLVariationsandExtensions 1133
select namefrom products
where status> ’alpha’;
which retrievesthenames ofproductsthat havemovedpastthe alphastage.
AddingbasetypestoPostgreSQLisstraightforward;anexamplecanbefound
in complex.sql and complex.c in the tutorials of the PostgreSQL distribution.
Thebase typecanbe declaredinC,for example:
typedef struct Complex {
double x;
double y;
} Complex;
The next step is to de?ne functions to read and write values of the new type in
text format (see Section 27.3.3.2). Subsequently, the new type can be registered
using thestatement:
createtype complex {
internallength =16,
input = complex in,
output = complex out,
alignment =double
};
assuming the text I/O functions have been registered as complex in and complex
out.
The user has the option of de?ning binary I/O functions as well (for more
ef?cient datadumping).Extensiontypescan be usedlikethe existingbase types
of PostgreSQL.Infact,theironlydifferenceisthattheextensiontypesaredynam-
ically loaded and linked into the server. Furthermore, indices may be extended
easilytohandle newbasetypes;seeSection27.3.3.3.
27.3.3.2 Functions
PostgreSQL allows users to de?ne functions that are stored and executed on the
server. PostgreSQL also supports function overloading (that is, functions may be
declared by using the same name but with arguments of different types). Func-
tions can be written as plain SQL statements, or in several procedural languages
(covered in Section 27.3.3.4). Finally, PostgreSQL has an application programmer
interfacefor addingfunctions writteninC(explainedinthissection).
User-de?ned functions can be written in C (or a language with compatible
calling conventions, such as C++). The actual coding conventions are essentially
the same for dynamically loaded, user-de?ned functions, as well as for internal
functions(whicharestaticallylinkedintotheserver).Hence,thestandardinternal
functionlibraryisarichsourceofcodingexamplesforuser-de?nedCfunctions.
1134 Chapter27 PostgreSQL
Once the shared library containing the function has been created, a declaration
such asthe following registersit ontheserver:
createfunction complex out(complex)
returnscstring
as ’shared object ?lename’
languageCimmutablestrict;
The entry point to the shared object ?le is assumed to be the same as the SQL
function name (here, complex out),unless otherwisespeci?ed.
The example here continues the one from Section 27.3.3.1. The application
program interface hides most of PostgreSQL’s internal details. Hence, the actual
Ccode forthe abovetextoutputfunction of complex valuesisquitesimple:
PG FUNCTION INFO V1(complex out);
Datum complex out(pg function args) {
Complex *complex = (Complex *) pg getarg pointer(0);
char *result;
result = (char *) palloc(100);
snprintf(result, 100, "(%g,%g)", complex?>x, complex?>y);
pg return cstring(result);
}
The ?rst line declares the function complex out, and the following lines imple-
ment the output function. The code uses several PostgreSQL-speci?c constructs,
such as the palloc function, which dynamically allocates memory controlled by
PostgreSQL’s memory manager. More details may be found in the PostgreSQL
documentation(seethebibliographical notes).
Aggregate functions in PostgreSQL operate by updating a state value via
a state transition function that is called for each tuple value in the aggregation
group.Forexample,thestatefortheavgoperatorconsistsoftherunningsumand
the count of values. As each tuple arrives, the transition function should simply
add its value to the running sum and increment the count by one. Optionally,
a ?nal function may be called to compute the return value based on the state
information. For example, the ?nal function for avg would simply divide the
running sum by the count and returnit.
Thus, de?ning a new aggregate is as simple as de?ning these two functions.
For the complex type example, if complex add is a user-de?ned function that takes
twocomplexargumentsandreturnstheirsum,thenthesumaggregateoperator
can beextendedtocomplexnumbers using the simpledeclaration:
27.3 SQLVariationsandExtensions 1135
createaggregate sum(
sfunc= complex add,
basetype= complex,
stype= complex,
initcond =’(0,0)’
);
Note the use of function overloading: PostgreSQL will call the appropriate sum
aggregate function, on the basis of the actual type of its argument during invo-
cation. The basetypeistheargumenttypeandstype is the state value type.In this
case,a?nalfunctionisunnecessary,sincethereturnvalueisthestatevalueitself
(that is,therunning sum inbothcases).
User-de?nedfunctionscanalsobeinvokedbyusingoperatorsyntax.Beyond
simple “syntactic sugar” for function invocation, operator declarations can also
providehintstothequeryoptimizerinordertoimproveperformance.Thesehints
may include information about commutativity, restriction and join selectivity
estimation,andvariousother propertiesrelatedtojoinalgorithms.
27.3.3.3 IndexExtensions
PostgreSQL currently supports the usual B-tree and hash indices, as well as two
indexmethods that are unique to PostgreSQL: the GeneralizedSearch Tree(GiST)
and the Generalized Inverted Index (GIN), which is useful for full-text indexing
(theseindexstructuresareexplainedinSection27.5.2.1).Finally, PostgreSQLpro-
vides indexing of two-dimensional spatial objects with an R-tree index, which
is implemented using a GiST index behind the scenes. All of these can be easily
extendedtoaccommodate newbase types.
Adding index extensions for a type requires de?nition of an operator class,
which encapsulatesthe following:
• Index-method strategies. These are a set of operators that can be used as
quali?ersinwhereclauses.Theparticularsetdependsontheindextype.For
example, B-tree indices can retrieve ranges of objects, so the set consists of
?ve operators (<,<=, =,>=,and>), all of which can appear in a where
clause involving a B-tree index. A hash index allows only equality testing
and an R-tree index allows a number of spatial relationships (for example
contained,to-the-left,andso forth).
• Index-methodsupportroutines. The above set of operators is typically not
suf?cient for the operation of the index. For example, a hash index requires
a function to compute the hash value for each object. An R-tree index needs
to be able to compute intersections and unions and to estimate the size of
indexedobjects.
1136 Chapter27 PostgreSQL
Forexample,ifthefollowingfunctionsandoperatorsarede?nedtocompare
the magnitude of complex numbers (see Section 27.3.3.1), then we can make such
objects indexableby the following declaration:
createoperatorclass complex abs ops
defaultfortype complexusingbtreeas
operator1<(complex, complex),
operator2<=(complex, complex),
operator3=(complex, complex),
operator4>=(complex, complex),
operator5>(complex, complex),
function1 complex abs cmp(complex, complex);
Theoperatorstatementsde?nethestrategymethodsandthefunctionstatements
de?nethe supportmethods.
27.3.3.4 ProceduralLanguages
Stored functions and procedures can be written in a number of procedural lan-
guages. Furthermore, PostgreSQL de?nes an application programmer interface
for hooking up any programming language for this purpose. Programming lan-
guages can be registered on demand and are either trusted or untrusted.The
latter allow unlimitedaccess to the DBMS and the ?le system,and writing stored
functions inthemrequiressuperuserprivileges.
• PL/pgSQL. This is a trusted language that adds procedural programming
capabilities(forexample,variablesandcontrol?ow)to SQL.Itisverysimilar
to Oracle’s PL/SQL. Although code cannot be transferred verbatim from one
totheother,porting isusuallysimple.
• PL/Tcl, PL/Perl,andPL/Python. These leverage the power of Tcl, Perl, and
Python to write stored functions and procedures on the server.The ?rst two
come in both trusted and untrusted versions (PL/Tcl, PL/Perl and PL/TclU,
PL/PerlU, respectively), while PL/Python is untrusted at the time of this
writing. Each of these has bindings that allow access to the database system
viaa language-speci?cinterface.
27.3.3.5 ServerProgrammingInterface
The server programming interface (SPI) is an application programmer interface
that allows user-de?ned C functions (see Section 27.3.3.2) to run arbitrary SQL
commandsinsidetheirfunctions.Thisgiveswritersofuser-de?nedfunctionsthe
ability to implement only essential parts in C and easily leverage the full power
of therelationaldatabase systemengineto domost of thework.
27.4 TransactionManagementinPostgreSQL 1137
27.4 TransactionManagementinPostgreSQL
Transaction management in PostgreSQL uses both both snapshot isolation and
two-phaselocking.Whichoneofthetwoprotocolsisuseddependsonthetypeof
statement beingexecuted.For DML statements
1
the snapshot isolationtechnique
presented in Section 15.7 is used; the snapshot isolation scheme is referred to as
themultiversionconcurrencycontrol(MVCC)schemeinPostgreSQL.Concurrency
control for DDL statements, on the other hand, is based on standard two-phase
locking.
27.4.1 PostgreSQLConcurrency Control
Since the concurrency control protocol used by PostgreSQL depends on the isola-
tion levelrequestedbytheapplication,webeginwithanoverviewoftheisolation
levels offered by PostgreSQL. We then describe the key ideas behind the MVCC
scheme,followedbyadiscussionoftheirimplementationinPostgreSQLandsome
oftheimplicationsofMVCC.Weconcludethissectionwithanoverviewoflocking
for DDLstatementsand a discussionofconcurrency control forindices.
27.4.1.1 PostgreSQLIsolationLevels
The SQL standard de?nes three weak levels of consistency, in addition to the
serializable level of consistency, on which most of the discussion in this book is
based.Thepurposeofprovidingtheweakconsistencylevelsistoallowahigher
degree of concurrency for applications that don’t require the strong guarantees
thatserializabilityprovides.Examplesofsuchapplicationsincludelong-running
transactionsthatcollectstatisticsoverthedatabaseandwhoseresultsdonotneed
tobeprecise.
The SQL standard de?nes the different isolation levels in terms of three phe-
nomena that violate serializability. The three phenomena are called dirty read,
nonrepeatable read,andphantom read,and are de?nedas follows:
• Dirtyread. The transaction reads values written by another transaction that
hasn’t committedyet.
• Nonrepeatable read. A transaction reads the same object twice during exe-
cution and ?nds a different value the second time, although the transaction
has not changed the valuein themeantime.
• Phantom read. A transaction re-executes a query returning a set of rows
that satisfy a search condition and ?nds that the set of rows satisfying the
conditionhaschangedasaresultofanotherrecentlycommittedtransaction.
(A more detailed explanation of the phantom phenomenon, including the
1
A DML statement is any statement that updates or reads data within a table, that is,select, insert,update, fetch,and
copy. DDL statements affect entire tables; they can remove a table or change the schema of a table, for example. DDL
statements and some otherPostgreSQL-speci?c statements willbe discussed later inthis section.
1138 Chapter27 PostgreSQL
Isolated level Dirty Read Non repeatable Read Phantom Read
ReadUncommitted Maybe Maybe Maybe
ReadCommitted No Maybe Maybe
RepeatedRead No No Maybe
Serializable No No No
Figure27.5 De?nition of the four standard SQL isolation levels.
concept of a phantom con?ict, can be found in Section 15.8.3; eliminating
phantom readsdoesnot eliminateallphantom con?icts.)
It should be obvious that each of the above phenomena violates transaction
isolation, and hence violates serializability. Figure 27.5 shows the de?nition of
the four SQL isolation levels speci?ed in the SQL standard—read uncommitted,
readcommitted,repeatableread,andserializable—intermsofthesephenomena.
PostgreSQL supports two of the four different isolation levels, read committed
(which is the default isolation level in PostgreSQL) and serializable. However,
the PostgreSQL implementation of the serializable isolation level uses snapshot
isolation, which does not truly ensure serializability as we have seen earlier in
Section15.7.
27.4.1.2 ConcurrencyControlforDMLCommands
The MVCC scheme used in PostgreSQL is an implementation of the snapshot
isolation protocol which we saw in Section 15.7. The key idea behind MVCC is to
maintain different versions of each row that correspond to instances of the row
at different points in time. This allows a transaction to see a consistentsnapshot
of the data, by selectingthe most recent versionof each row that was committed
beforetakingthesnapshot.TheMVCCprotocolusessnapshotstoensurethatevery
transaction sees a consistent view of the database: before executing a command,
thetransactionchoosesasnapshotofthedataandprocessestherowversionsthat
areeitherinthesnapshotorcreatedbyearliercommandsofthesametransaction.
This view of the data is “consistent” since it only takes full transactions into
account, butthesnapshot isnot necessarilyequaltothecurrentstateofthedata.
The motivation for using MVCC is that readers never block writers and vice
versa.Readers access the most recent versionof a row that is part of the transac-
tion’ssnapshot.Writerscreatetheirownseparatecopyoftherowtobeupdated.
Section27.4.1.3showsthattheonlycon?ictthatcausesatransactiontobeblocked
arises if two writers try to update the same row. In contrast, under the standard
two-phase locking approach, both readers and writers might be blocked, since
there is only one version of each data object and both read and write operations
arerequiredto obtaina lock beforeaccessing any data.
The MVCC scheme in PostgreSQL implements the ?rst-updater-wins version
of the snapshot isolation protocol, by acquiring exclusive locks on rows that
are written, but using a snapshot (without any locking) when reading rows;
27.4 TransactionManagementinPostgreSQL 1139
additional validation is done when exclusive locks are obtained, as outlined
earlierinSection15.7.
27.4.1.3 PostgreSQLImplementationofMVCC
AtthecoreofPostgreSQL MVCC is the notion of tuple visibility.APostgreSQL
tuplereferstoaversionofarow.Tuplevisibilityde?neswhichofthepotentially
manyversionsofarowinatableisvalidwithinthecontextofagivenstatement
or transaction. A transaction determines tuple visibility based on a database
snapshot that is chosenbeforeexecutingacommand.
Atupleisvisiblefor atransaction T ifthe following two conditions hold:
1. Thetuplewascreatedbyatransactionthatcommittedbeforetransaction T
took itssnapshot.
2. Updatestothetuple(ifany) wereexecutedby atransaction thatis either
• aborted, or
• startedrunning after T took itssnapshot, or
• was activewhen T took itssnapshot.
Tobeprecise,atupleisalsovisibletoT ifitwascreatedbyT andnotsubsequently
updatedby T.We omitthe detailsofthis specialcasefor simplicity.
The goal of the above conditions is to ensure that each transaction sees a
consistentviewofthedata. PostgreSQLmaintainsthefollowingstateinformation
to check theseconditions ef?ciently:
• A transaction ID,whichatthesametimeservesasatimestamp,isassignedto
every transaction at transaction start time. PostgreSQL uses a logical counter
(as describedinSection15.4.1)for assigning transaction IDs.
• A log ?le called pg clog contains the current status of each transaction. The
statuscan beeitherinprogress,committed,oraborted.
• Eachtupleinatablehas aheaderwiththree?elds: xmin,which contains the
transaction IDofthetransactionthatcreatedthetupleandwhichistherefore
also called the creation-transaction ID; xmax, which contains the transaction
IDofthereplacing/deletingtransaction(or nullifnotdeleted/replaced)and
whichisalsoreferredtoasthe expire-transaction ID;andaforwardlinktonew
versionsofthe samelogicalrow, ifthereareany.
• A SnapshotData data structure is created either at transaction start time or at
query start time, depending on the isolation level (described in more detail
below).Itsmainpurposeistodecidewhetheratupleisvisibletothecurrent
command.TheSnapshotDatastoresinformationaboutthestateoftransactions
atthetimeitiscreated,whichincludesalistofactivetransactionsand xmax,
a value equal to 1+ the highest ID of any transaction that has started so far.
1140 Chapter27 PostgreSQL
Database table
department(dept_name, building, budget) forward xmin xmax 
... ...
...
...
100
102
106
...
...
...
100
102
104
...
...
106
...
...
...
10
10
00
...
...
00
...
...
...
102
106
x
Transaction 104
     select budget
     from department
     where dept_name = ‘Physics’
00  In progress
01  Aborted
10  Committed
Watson
Watson
Watson
70000
64000
68000
.....
.....
Physics
Physics
Physics
.....
pg_clog file 
Status flags XID
Figure27.6 The PostgreSQL data structures used for MVCC.
The value xmax serves as a “cutoff” for transactions that may be considered
visible.
Figure27.6illustratessomeofthisstateinformationthroughasimpleexample
involving a database with only one table, the department table from Figure 27.7.
The departmenttablehasthreecolumns,thenameofthedepartment,thebuilding
where the department is located, and the budget of the department. Figure 27.6
showsafragmentofthedepartmenttablecontainingonlythe(versionsof)therow
correspondingtothePhysicsdepartment.Thetupleheadersindicatethattherow
was originally created by transaction 100, and later updated by transaction 102
and transaction 106. Figure 27.6 also shows a fragment of the corresponding pg
clog ?le. On the basis of the pg clog ?le, transactions 100 and 102 are committed,
while transactions 104 and 106 areinprogress.
Giventheabovestateinformation,thetwoconditionsthatneedtobesatis?ed
for atupletobevisiblecan be rewrittenas follows:
dept name building budget
Biology Watson 90000
Comp.Sci. Taylor 100000
Elec.Eng. Taylor 85000
Finance Painter 120000
History Painter 50000
Music Packard 80000
Physics Watson 70000
Figure27.7 The department relation.
27.4 TransactionManagementinPostgreSQL 1141
1. The creation-transaction ID inthetupleheader
a. isa committedtransactionaccording to the pg clog ?le, and
b. is less than the cutoff transaction ID xmax recorded by SnapshotData,
and
c. isnot one of theactivetransactions storedin SnapshotData.
2. The expire-transaction ID,ifitexists,
a. isan abortedtransactionaccording tothe pg clog ?le, or
b. is greater than or equal to the cutoff transaction ID xmax recorded by
SnapshotData, or
c. isone of theactivetransactions storedin SnapshotData.
Consider the example database in Figure 27.6 and assume that the Snapshot-
Datausedbytransaction104simplyuses103asthecutofftransactionIDxmaxand
does not show any earlier transactions to be active. In this case, the only version
of the row corresponding to the Physics department that is visibleto transaction
104isthesecondversioninthetable,createdbytransaction102.The?rstversion,
createdbytransaction100,isnotvisible,sinceitviolatescondition2:Theexpire-
transaction ID of this tuple is 102, which corresponds to a transaction that is not
aborted and that has a transaction ID less than or equal to 103. The third version
of the Physics tuple is not visible, since it was created by transaction 106, which
has a transaction ID larger than transaction 103, implying that this version had
not beencommittedat the time SnapshotData was created.Moreover,transaction
106 is still in progress, which violates another one of the conditions. The second
versionof therowmeetsall theconditions for tuplevisibility.
The details of how PostgreSQL MVCC interacts with the execution of SQL
statements depends on whether the statement is an insert, select, update,or
delete statement. The simplest case is an insert statement, which may simply
create a new tuple based on the data in the statement, initialize the tuple header
(thecreationID),andinsertthenewtupleintothetable.Unliketwo-phaselocking,
thisdoesnotrequireanyinteractionwiththeconcurrency-controlprotocolunless
the insertion needs to be checked for integrity conditions, such as uniqueness or
foreignkeyconstraints.
Whenthesystemexecutesaselect,update,ordeletestatementtheinteraction
withtheMVCCprotocoldependsontheisolationlevelspeci?edbytheapplication.
Iftheisolationlevelisreadcommitted,theprocessingofanewstatementbegins
with creating a new SnapshotData data structure (independent of whether the
statementstartsanewtransactionorispartofanexistingtransaction).Next,the
systemidenti?estarget tuples,thatis,thetuplesthatarevisiblewithrespecttothe
SnapshotData and that match the search criteria of the statement. In the case of a
select statement,the setoftargettuplesmakeup theresultofthequery.
Inthecaseofanupdateordeletestatementinreadcommittedmode,anextra
step is necessary after identifying the target tuples, before the actual update or
1142 Chapter27 PostgreSQL
delete operation can take place. The reason is that visibility of a tuple ensures
only that the tuple has been created by a transaction that committed before
the update/delete statement in question started. However, it is possible that,
sincequerystart,thistuplehasbeenupdatedordeletedbyanotherconcurrently
executing transaction. This can be detected by looking at the expire-transaction
IDofthetuple.Iftheexpire-transactionIDcorrespondstoatransactionthatisstill
inprogress,itisnecessarytowaitforthecompletionofthistransaction?rst.Ifthe
transaction aborts, the update ordelete statement can proceed and perform the
actualmodi?cation.Ifthetransactioncommits,thesearchcriteriaofthestatement
need tobe evaluatedagain, and only ifthe tuple stillmeetsthese criteriacan the
row be modi?ed. If the row is to be deleted, the main step is to update the
expire-transaction ID of the old tuple.A row update also performs this step, and
additionallycreatesanewversionoftherow,setsitscreation-transaction ID,and
setstheforward linkof the oldtupleto referencethenewtuple.
GoingbacktotheexamplefromFigure27.6,transaction104,whichconsistsof
aselectstatementonly,identi?esthesecondversionofthePhysicsrowasatarget
tuple and returns it immediately. If transaction 104 were an update statement
instead, for example, trying to increment the budget of the Physics department
bysomeamount,itwouldhavetowaitfortransaction106tocomplete.Itwould
then re-evaluate the search condition and, only if it is still met, proceed with its
update.
Using the protocol described above for update and delete statements pro-
vides only the read-committed isolation level. Serializability can be violated in
several ways. First, nonrepeatable reads are possible. Since each query within a
transactionmayseeadifferentsnapshotofthedatabase,aqueryinatransaction
might see the effects of an update command completed in the meantime that
weren’tvisibletoearlierquerieswithinthesametransaction.Followingthesame
lineofthought,phantom readsarepossiblewhenarelationismodi?edbetween
queries.
In order to provide the PostgreSQL serializable isolation level, PostgreSQL
MVCC eliminates violations of serializability in two ways: First, when it is deter-
mining tuple visibility, all queries within a transaction use a snapshot as of the
start of the transaction, rather than the start of the individual query. This way
successivequerieswithinatransactionalways seethesamedata.
Second,thewayupdatesanddeletesareprocessedisdifferentinserializable
mode compared toread-committedmode.As in read-committedmode,transac-
tionswaitafteridentifyingavisibletargetrowthatmeetsthesearchconditionand
is currently updated or deletedby another concurrent transaction. If the concur-
renttransactionthatexecutestheupdateordeleteaborts,thewaitingtransaction
canproceedwithitsownupdate.However,iftheconcurrenttransactioncommits,
there is no way for PostgreSQL to ensure serializability for the waiting transac-
tion. Therefore, the waiting transaction is rolled back and returns with the error
message “could not serializeaccess dueto concurrent update.”
It is up to the application to handle an error message like the above appro-
priately, by aborting the current transaction and restarting the entire transaction
fromthebeginning.Observethatrollbacksduetoserializabilityissuesarepossi-
27.4 TransactionManagementinPostgreSQL 1143
bleonlyforupdateanddeletestatements.Itisstillthecasethatselectstatements
nevercon?ict withany other transactions.
27.4.1.4 ImplicationsofUsingMVCC
Using the PostgreSQL MVCC scheme has implications in three different areas: (1)
extraburdenis placedon thestoragesystem,since itneedstomaintain different
versions of tuples; (2) developing concurrent applications takes some extra care,
since PostgreSQL MVCC can lead to subtle, but important, differences in how
concurrenttransactionsbehave,comparedtosystemswherestandardtwo-phase
lockingisused;(3) PostgreSQLperformancedependsonthecharacteristicsofthe
workload running on it. The implications of PostgreSQL MVCC are described in
moredetailbelow.
Creating and storing multiple versions of every row can lead to excessive
storage consumption. To alleviate this problem, PostgreSQL frees up space when
possible by identifying and deleting versions of tuples that cannot be visible to
any active or future transactions, and are therefore no longer needed. The task
of freeing space is nontrivial, because indices may refer to the location of an
unneeded tuple, so these references need to be deleted before reusing the space.
Tolessenthisissue, PostgreSQL avoidsindexingmultipleversionsofatuplethat
haveidenticalindexattributes.Thisallowsthespacetakenbynonindexedtuples
to befreedef?cientlyby any transaction that ?nds sucha tuple.
Formoreaggressivespacereuse,PostgreSQLprovidesthevacuumcommand,
whichcorrectlyupdatesindicesforeachfreedtuple. PostgreSQLemploysaback-
ground process to vacuum tables automatically, but the command can also be
executed by the user directly. The vacuum command offers two modes of op-
eration: Plain vacuum simply identi?es tuples that are not needed, and makes
theirspaceavailableforreuse.Thisformofthecommandcanoperateinparallel
with normal reading and writing of the table.Vacuumfull does more extensive
processing, including moving of tuples across blocks to try to compact the table
to the minimum number of disk blocks. This form is much slower and requires
anexclusivelock oneach tablewhileitis beingprocessed.
BecauseoftheuseofmultiversionconcurrencycontrolinPostgreSQL,porting
applicationsfromotherenvironmentstoPostgreSQLmightrequiresomeextracare
to ensure data consistency. As an example, consider a transaction T
A
executing
a select statement. Since readers in PostgreSQL don’t lock data, data read and
selected by T
A
can be overwritten by another concurrent transaction T
B
, while
T
A
is still running. As a result some of the data that T
A
returns might not be
current anymore at the time of completion of T
A
. T
A
might return rows that in
themeantimehave beenchanged or deletedby othertransactions. Toensurethe
currentvalidityofarowandprotectitagainstconcurrentupdates,anapplication
must either useselectforshare or explicitly acquire a lock with the appropriate
locktablecommand.
PostgreSQL’s approach to concurrency control performs best for workloads
containing many more reads than updates, since in this case there is a very low
chance that two updates will con?ict and force a transaction to roll back. Two-
1144 Chapter27 PostgreSQL
phase locking may be more ef?cient for some update-intensive workloads, but
thisdependsonmanyfactors,suchasthelengthoftransactionsandthefrequency
of deadlocks.
27.4.1.5 DDLConcurrencyControl
The MVCC mechanisms described in the previous section do not protect trans-
actions against operations that affect entire tables, for example, transactions that
drop a table or change the schema of a table. Toward this end, PostgreSQL pro-
videsexplicitlocksthat DDLcommandsareforcedtoacquirebeforestartingtheir
execution. These locks are always table based (rather than row based) and are
acquiredandreleasedinaccordance withthestrict two-phase locking protocol.
Figure 27.8 lists all types of locks offered by PostgreSQL, which locks they
con?ict with, and somecommands that use them (thecreateindexconcurrently
Figure27.8 Table-level lock modes.
27.4 TransactionManagementinPostgreSQL 1145
command is covered in Section 27.5.2.3). The names of the lock types are often
historicalanddon’tnecessarilyre?ecttheuseofthelock.Forexample,allthelocks
are table-level locks, although some contain the word “row” in the name. DML
commands acquire only locks of the ?rst three types. These three lock types are
compatiblewitheachother,since MVCCtakescareofprotectingtheseoperations
againsteachother.DMLcommandsacquiretheselocksonlyforprotectionagainst
DDLcommands.
While their mainpurposeis providing PostgreSQL internal concurrency con-
trolfor DDLcommands,alllocksinFigure27.8canalsobeacquiredexplicitlyby
PostgreSQL applications throughthelocktable command.
Locks are recorded in a lock table that is implemented as a shared-memory
hash table keyed by a signature that identi?esthe object being locked. If a trans-
action wants to acquire a lock on an object that is held by another transaction in
a con?icting mode, it needs to wait until the lock is released. Lock waits are im-
plementedthroughsemaphores,eachofwhichisassociatedwithauniquetrans-
action. When waiting for a lock, a transaction actually waits on the semaphore
associatedwiththetransactionholdingthelock.Oncethelockholderreleasesthe
lock, it will signal the waiting transaction(s) through the semaphore. By imple-
menting lock waits on a per-lock-holder basis, rather than on a per-object basis,
PostgreSQL requires at most one semaphore per concurrent transaction, rather
than onesemaphoreperlockable object.
Deadlockdetectionin PostgreSQLisbasedontime-outs.Bydefault,deadlock
detectionis triggeredif a transaction has been waiting for a lock for more than 1
second. The deadlock-detection algorithm constructs a wait-for graph based on
theinformationinthelocktableandsearchesthisgraphforcirculardependencies.
Ifit?ndsany,meaningadeadlockwasdetected,thetransactionthattriggeredthe
deadlockdetectionabortsandreturnsanerrortotheuser.Ifnocycleisdetected,
the transaction continues waiting on the lock. Unlike some commercial systems,
PostgreSQL does not tune the lock time-out parameter dynamically, but it allows
theadministratortotuneitmanually.Ideally,thisparametershouldbechosenon
the orderof atransaction lifetime,in ordertooptimize the trade-offbetweenthe
time it takes to detecta deadlock and the work wasted for running the deadlock
detectionalgorithmwhen thereisno deadlock.
27.4.1.6 LockingandIndices
All current types of indices in PostgreSQL allow for concurrent access by multi-
ple transactions. This is typically enabled by page-level locks, so that different
transactions may access the index in parallel if they do not request con?icting
locks on a page. These locks are usually held for a short time to avoid deadlock,
withtheexceptionofhashindices,whichlockpagesforlongerperiodsandmay
participateindeadlock.
27.4.2 Recovery
Historically, PostgreSQLdidnotusewrite-aheadlogging(WAL) forrecovery,and
thereforewasnotabletoguaranteeconsistencyinthecaseofcrash.Acrashcould
1146 Chapter27 PostgreSQL
potentiallyresultininconsistentindexstructuresor,worse,totallycorruptedtable
contents,becauseofpartiallywrittendatapages.Asaresult,startingwithversion
7.1,PostgreSQLemploys WAL-basedrecovery.Theapproachissimilartostandard
recovery techniques such as ARIES (Section 16.8), but recovery in PostgreSQL is
simpli?edinsomeways because of the MVCC protocol.
First,under PostgreSQL, recoverydoesn’t have toundothe effectsofaborted
transactions: anaborting transaction makes anentry inthe pg clog ?le, recording
the fact that it is aborting. Consequently, all versions of rows it leaves behind
willneverbevisibletoanyothertransactions.Theonlycasewherethisapproach
couldpotentiallyleadtoproblemsiswhenatransactionabortsbecauseofacrash
ofthecorrespondingPostgreSQLprocessandthePostgreSQLprocessdoesn’thave
a chance to create the pg clog entry before the crash. PostgreSQL handles this as
follows: Before checking the status of another transaction in the pg clog ?le, it
checks whether the transaction is running on any of the PostgreSQL processes.
If no PostgreSQL process is currently running the transaction and the pg clog ?le
shows the transaction as still running, it is safe to assume that the transaction
crashed and the transaction’s pg clog entryisupdatedto “aborted”.
Second,recoveryissimpli?edbythefactthatPostgreSQLMVCCalreadykeeps
track of some of the information required by WAL logging. More precisely, there
is no need for logging the start, commit, and abort of transactions, since MVCC
logs thestatusofeverytransaction inthe pg clog.
27.5 StorageandIndexing
PostgreSQL’s approach to data layout and storage is aimed at the goals of (1) a
simpleandcleanimplementationand(2)easeofadministration.Asasteptoward
these goals, PostgreSQL relies on “cooked” ?le systems, instead of handling the
physical layout of data on raw disk partitions by itself. PostgreSQL maintains a
listofdirectoriesinthe?lehierarchytouseforstorage,whichareconventionally
referredtoastablespaces.EachPostgreSQLinstallationisinitializedwithadefault
tablespace,andadditionaltablespacesmaybeaddedatanytime.Whencreating
a table, index, or entire database, the user may specify any existing tablespace
in which to store the related ?les. It is particularly useful to create multiple
tablespaces if they reside on different physical devices, so that the faster devices
may be dedicated to data that are in higher demand. Moreover, data that are
storedon separatedisksmaybe accessedinparallelmoreef?ciently.
The design of the PostgreSQL storage system potentially leads to some per-
formance limitations, due to clashes between PostgreSQL and the ?le system.
The use of cooked ?le systems results in double buffering, where blocks are
?rst fetched from disk into the ?le system’s cache (in kernel space) before being
copied to PostgreSQL’s buffer pool. Performance can also be limited by the fact
that PostgreSQL stores data in 8-KB blocks, which may not match the block size
used by the kernel. It is possible to change the PostgreSQL block size when the
serverisinstalled,butthismay haveundesiredconsequences:smallblocks limit
27.5 StorageandIndexing 1147
page header data linp
1
linp
2
linp
3
linp
4
... linp
n
pd_lower
pd_upper
“special space” tuple
1
tuple
2
tuple
3
... tuple
n
Figure27.9 Slotted-page format for PostgreSQL tables.
the ability of PostgreSQL to store large tuples ef?ciently, while large blocks are
wastefulwhen a smallregionof a?le isaccessed.
Ontheotherhand,modernenterprisesincreasinglyuseexternalstoragesys-
tems, such as network-attached storage and storage-area networks, instead of
disks attached to servers. The philosophy here is that storage is a service that is
easily administered and tuned for performance separately. One approach used
bythesesystemsis RAID,whichoffersbothparallelismandredundantstorageas
explainedinSection10.3.PostgreSQLmaydirectlyleveragethesetechnologiesbe-
causeofitsrelianceoncooked?lesystems.Thus,thefeelingofmany PostgreSQL
developers is that, for a vast majority of applications, and indeed PostgreSQL’s
audience, the performance limitations are minimal and justi?ed by the ease of
administrationand management,as wellassimplicityof implementation.
27.5.1 Tables
The primary unit of storage in PostgreSQL is a table. In PostgreSQL,tablesare
stored in heap ?les. These ?les use a form of the standard slotted-page format
describedinSection10.5.The PostgreSQLformatisshowninFigure27.9.Ineach
page,aheaderisfollowedbyanarrayof “linepointers.”Alinepointerholdsthe
offset (relative to the start of the page) and length of a speci?c tuple in the page.
The actual tuples are stored in reverse order of line pointers from the end of the
page.
Arecordin a heap?leisidenti?edby itstupleID(TID).The TIDconsists of a
4-byteblock IDthatspeci?esthepageinthe?lecontainingthetupleanda2-byte
slot ID.TheslotID is an index into the line pointer array that in turn is used to
access thetuple.
Althoughthisinfrastructurepermitstuplesinapagetobedeletedorupdated,
underPostgreSQL’sMVCCapproach,neitheroperationactuallydeletesorreplaces
oldversionsofrowsimmediately.AsexplainedinSection27.4.1.4,expiredtuples
may be physically deleted by later commands, causing holes to be formed in a
1148 Chapter27 PostgreSQL
page. The indirection of accessing tuples through the line pointer array permits
the reuseof suchholes.
The length of a tuple is normally limited by the length of a data page. This
makes it dif?cult to store very long tuples. When PostgreSQL encounters such a
largetuple,ittriesto“toast”individuallargeattributes.Insomecases,toastingan
attribute may be accomplished by compressing the value. If this does not shrink
thetupleenoughto?tinthepage(oftenthecase),thedatainthetoastedattribute
isreplacedwithareferencetoacopythat is storedoutsidethe page.
27.5.2 Indices
A PostgreSQL index is a data structure that provides a dynamic mapping from
search predicates to sequences of tuple IDs from a particular table. The returned
tuples are intended to match the search predicate, although in some cases the
predicatemustberecheckedintheheap?le.PostgreSQLsupportsseveraldifferent
index types, including those that are based on user-extensible access methods.
Although an access method may use a different page format, all the indices
available in PostgreSQL use the same slotted-page format described above in
Section27.5.1.
27.5.2.1 IndexTypes
PostgreSQL supportsthe following typesof indices:
• B-tree.ThedefaultindextypeisaB
+
-treeindexbasedonLehmanandYao’sB-
link trees (B-link trees, described in Section 15.10, support high concurrency
of operations). These indices are useful for equality and range queries on
sortable data and also for certain pattern-matching operations such as the
likeexpression.
• Hash. PostgreSQL’shashindicesareanimplementationoflinearhashing(for
moreinformationonhashindices,seeSection11.6.3).Suchindicesareuseful
onlyforsimpleequalityoperations.ThehashindicesusedbyPostgreSQLhave
been shown to have lookup performance no better than that of B-trees, but
haveconsiderablylargersizeandmaintenancecosts.Moreover,hashindices
aretheonlyindicesin PostgreSQLthatdonotsupportcrashrecovery.Thusit
isalmost always preferabletouseB-treeindicesinsteadofhash indices.
• GiST. PostgreSQL supports a highly extensible index called GiST,orGen-
eralized Search Tree. GiST is a balanced, tree-structured access method that
makes it easy for a domain expert who is well versed in a particular data
type (such as image data) to develop performance-enhancing indices with-
outhavingtodealwiththeinternaldetailsofthedatabasesystem.Examples
of some indices built using GiST include B-trees and R-trees, as well as less
conventional indices for multidimensional cubes and full-text search. New
GiST access methods can be implemented by creating an operator class as
explained in Section 27.3.3.3. Operator classes for GiST are different from B-
trees, as each GiST operator class may have a different set of strategies that
27.5 StorageandIndexing 1149
indicate the search predicates implemented by the index. GiST also relies on
seven support functions for operations such as testing set membership and
splittingsetsof entriesfor pageover?ows.
It is interesting to note that the original PostgreSQL implementation of
R-trees(Section25.3.5.3)wasreplacedby GiSToperatorclassesinversion8.2.
This allowed R-trees to take advantage of the WAL logging and concurrency
capabilities that were added to GiST in version 8.1. Since the original R-
tree implementation did not have these features, this change illustrates the
bene?ts of an extensible indexing method. See the bibliographical notes for
references to more information on the GiST index.
• GIN. The newest type of index in PostgreSQL is the Generalized Inverted
Index (GIN). A GIN index interprets both index keys and search keys as sets,
making the index type appropriate for set-oriented operators. One of the
intended uses of GIN is to index documents for full-text search, which is
implemented by reducing documents and queries to sets of search terms.
Like GiST,aGIN index may be extended to handle any comparison operator
by creatinganoperatorclass withappropriatesupportfunctions.
Toevaluateasearch,GINef?cientlyidenti?esindexkeysthatoverlapthe
search key, and computes a bitmap indicating which searched-for elements
are members of the index key.This is accomplished using support functions
that extract members from a set and compare individual members. Another
support function decides if the search predicate is satis?ed based on the
bitmap and theoriginal predicate.Ifthe searchpredicatecannot be resolved
withoutthefullindexedattribute,thedecisionfunctionmustreportamatch
and recheckthe predicateinthe heap?le.
27.5.2.2 OtherIndexVariations
Forsomeoftheindextypesdescribedabove, PostgreSQLsupportsmorecomplex
variationssuch as:
• Multicolumnindices.Theseareusefulforconjunctsofpredicatesovermul-
tiple columns of a table. Multicolumn indices are only supported for B-tree
and GiST indices.
• Unique indices. Unique and primary-key constraints can be enforced by
using unique indices in PostgreSQL. Only B-tree indices may be de?ned as
beingunique.
• Indicesonexpressions.In PostgreSQL,itispossibletocreateindicesonarbi-
traryscalarexpressionsof columns,and not justspeci?ccolumns, ofa table.
This is especially useful when the expressions in question are “expensive”
—say, involving complicated user-de?ned computation. An example is to
supportcase-insensitivecomparisonsbyde?ninganindexontheexpression
lower(column)andusingthepredicate lower(column)=’value’inqueries.One
disadvantageisthat themaintenance costs ofindicesonexpressionsishigh.
1150 Chapter27 PostgreSQL
• Operatorclasses.Thespeci?ccomparisonfunctionsusedtobuild,maintain,
and use an index on a column are tied to the data type of that column.
Each data type has associated with it a default “operator class” (described
in Section 27.3.3.3) that identi?es the actual operators that would normally
be used for it. While this default operator class is normally suf?cient for
mostuses,somedatatypesmightpossessmultiple“meaningful”classes.For
instance, in dealing with complex numbers, it might be desirable to index
either the real or imaginary component. PostgreSQL provides some built-in
operator classes for pattern-matching operations (such as like)ontextdata
that do not use the standard locale-speci?c collation rules (in other words,
language speci?csort orders).
• Partial indices. These are indices built over a subset of a table de?ned by a
predicate.Theindexcontainsonlyentriesfortuplesthatsatisfythepredicate.
Partial indices are suited for cases where a column might contain a large
number of occurrences of a very small number of values. In such cases, the
common values are not worth indexing, since index scans are not bene?cial
for queries that require a large subset of the base table. A partial index that
excludesthe common valuesis smalland incurs less I/O.The partialindices
arelessexpensivetomaintain,asalargefractionofinsertsdonotparticipate
intheindex.
27.5.2.3 IndexConstruction
An index may be added to the database using the create index command. For
example,thefollowingDDLstatementcreatesaB-treeindexoninstructorsalaries.
createindex inst sal idxon instructor (salary);
Thisstatementisexecutedbyscanningthe instructorrelationto?ndrowversions
that might be visible to a future transaction, then sorting their index attributes
and building the index structure. During this process, the building transaction
holdsalockonthe instructorrelationthatpreventsconcurrentinsert,delete,and
updatestatements.Oncetheprocessis?nished,theindexisreadytouseandthe
tablelock is released.
Thelockacquiredbythecreate index command may present a major in-
convenience for some applicationswhere itis dif?culttosuspendupdateswhile
the index is built. For these cases, PostgreSQL provides thecreate indexconcur-
rentlyvariant,which allowsconcurrentupdatesduringindexconstruction.This
is achieved by a more complex construction algorithm that scans the base table
twice. The ?rst table scan builds an initial version of the index, in a way similar
tonormalindexconstructiondescribedabove.Thisindexmaybemissingtuples
if the table was concurrently updated; however, the index is well formed, so it
is ?agged as being ready for insertions. Finally, the algorithm scans the table a
second time and inserts all tuples it ?nds that still need to be indexed. This scan
mayalsomissconcurrentlyupdatedtuples,butthealgorithmsynchronizeswith
other transactions to guarantee that tuples that are updated during the second
27.6 QueryProcessingandOptimization 1151
scan will be added to the index by the updating transaction. Hence, the index
is ready to use after the second table scan. Since this two-pass approach can be
expensive, the plain create index command is preferred if it is easy to suspend
tableupdatestemporarily.
27.6 QueryProcessingandOptimization
WhenPostgreSQLreceivesaquery,itis?rstparsedintoaninternalrepresentation,
which goes through a series of transformations, resulting in a query plan that is
usedby theexecutor toprocessthequery.
27.6.1 QueryRewrite
The ?rst stage of a query’s transformation is rewrite and it is this stage that
is responsible for the PostgreSQL rules system. As explained in Section 27.3, in
PostgreSQL,userscancreaterulesthatare?redondifferenteventssuchasupdate,
delete, insert,andselect statements. A view is implemented by the system by
converting a view de?nition into a select rule. When a query involving a select
statement on the view is received, the select rule for the view is ?red, and the
queryisrewrittenusing the de?nitionof theview.
A rule is registered in the system using the create rule command, at which
point information on the rule is stored in the catalog. This catalog is then used
duringqueryrewritetouncoverall candidaterulesfor agivenquery.
Therewritephase?rstdealswithallupdate,delete,andinsertstatementsby
?ring all appropriate rules. Such statements might be complicated and contain
select clauses. Subsequently, all the remaining rules involving only select state-
ments are ?red. Since the ?ring of a rule may cause the query to be rewritten to
aformthatmayrequireanotherruletobe?red,therulesarerepeatedlychecked
on each form of the rewritten query until a ?xed point is reached and no more
rulesneedtobe?red.
There exist no default rules in PostgreSQL—only those de?ned explicitly by
usersand implicitlyby thede?nitionof views.
27.6.2 QueryPlanningandOptimization
Oncethequeryhas beenrewritten,itissubjecttotheplanning andoptimization
phase.Here,eachqueryblockistreatedinisolationandaplanisgeneratedforit.
Thisplanningbeginsbottom-upfromtherewrittenquery’sinnermostsubquery,
proceedingtoitsoutermostqueryblock.
The optimizer in PostgreSQL is, for the most part, cost based. The idea is to
generateanaccessplanwhoseestimatedcostisminimal.Thecostmodelincludes
as parameters the I/O cost of sequential and random page fetches, as well as the
CPU costs ofprocessingheap tuples,indextuples,and simplepredicates.
Theactualprocessofoptimizationisbasedononeofthefollowingtwoforms:
1152 Chapter27 PostgreSQL
• Standard planner. The standard planner uses the the bottom-up dynamic
programming algorithm for join order optimization, originally used in Sys-
tem R, the pioneering relational system developed by IBM research in the
1970s. The System R dynamic programming algorithm is describedin detail
inSection13.4.1.Thealgorithm isusedonasinglequeryblock atatime.
• Geneticqueryoptimizer.Whenthenumberoftablesinaqueryblockisvery
large,SystemR’sdynamicprogrammingalgorithmbecomesveryexpensive.
Unlike other commercial systems that default to greedy or rule-based tech-
niques, PostgreSQL uses a more radical approach: a genetic algorithm that
was developed initially to solve traveling-salesman problems. There exists
anecdotal evidence of the successful use of genetic query optimization in
productionsystemsfor querieswitharound 45tables.
Since the planner operates in a bottom-up fashion on query blocks, it is
able to perform certain transformations on the query plan as it is being built.
One example is the common subquery-to-join transformation that is present in
many commercial systems (usually implemented in the rewrite phase). When
PostgreSQLencountersanoncorrelatedsubquery(suchasonecausedbyaquery
onaview),itisgenerallypossibleto“pullup”theplannedsubqueryandmergeit
into the upper-level query block. However, transformations that push duplicate
eliminationintolower-levelqueryblocksaregenerallynotpossibleinPostgreSQL.
Thequery-optimizationphaseresultsinaqueryplanthatisatreeofrelational
operators. Each operator represents a speci?c operation on one or more sets of
tuples. The operators can be unary (for example, sort, aggregation), binary (for
example,nested-loopjoin), or n-ary (for example,setunion).
Crucialtothecostmodelisanaccurateestimateofthetotalnumberoftuples
thatwillbeprocessedateachoperatorintheplan.Thisisinferredbytheoptimizer
onthebasisofstatisticsthataremaintainedoneachrelationinthesystem.These
indicate the total number of tuples for each relation and speci?c information on
each column of a relation, such as the column cardinality, a list of most common
valuesinthetableandthenumberofoccurrences,andahistogramthatdividesthe
column’svaluesintogroupsofequalpopulation(thatis,anequi-depthhistogram,
described in Section 13.3.1). In addition, PostgreSQL also maintains a statistical
correlation between the physical and logical row orderings of a column’s values
—this indicates the cost of an index scan to retrieve tuples that pass predicates
on the column. The DBA must ensure that these statistics are current by running
theanalyzecommand periodically.
27.6.3 QueryExecutor
Theexecutormoduleisresponsibleforprocessingaqueryplanproducedbythe
optimizer. The executor follows the iterator model with a set of four functions
implementedforeachoperator(open,next,rescan,andclose).Iteratorsarealso
discussed as part of demand-driven pipelining in Section 12.7.2.1. PostgreSQL
iteratorshaveanextrafunction,rescan,which isusedtoresetasubplan(sayfor
aninner loop ofajoin) withparameterssuch asindexkeyranges.
27.6 QueryProcessingandOptimization 1153
Someoftheimportantoperatorsoftheexecutorcanbecategorizedasfollows:
1. Access methods. The actual access methods that are used to retrieve data
from on-disk objects in PostgreSQL are sequential scans of heap ?les, index
scans, and bitmap indexscans.
• Sequential scans. The tuples of a relation are scanned sequentially
from the ?rst to last blocks of the ?le. Each tuple is returned to the
calleronlyifitis“visible”accordingtothetransactionisolationrulesin
Section27.4.1.3.
• Indexscans.Givenasearchconditionsuchasarangeorequalitypred-
icate, this access method returns a set of matching tuples from the
associatedheap?le.Theoperatorprocessesonetupleatatime,starting
byreadinganentryfromtheindexandthenfetchingthecorresponding
tuplefromtheheap?le.Thiscanresultinarandompagefetchforeach
tupleinthe worst case.
• Bitmapindexscans. A bitmapindexscan reducesthe dangerof exces-
siverandompagefetchesinindexscans.Thisisachievedbyprocessing
tuples in two phases. The ?rst phase reads all index entries and stores
theheaptuple IDsinabitmap,andthesecondphasefetchesthematch-
ing heap tuples in sequential order. This guarantees that each heap
pageisaccessedonlyonce,andincreasesthechanceofsequentialpage
fetches. Moreover, bitmaps from multiple indexes can be merged and
intersectedtoevaluatecomplexBooleanpredicatesbeforeaccessingthe
heap.
2. Joinmethods. PostgreSQL supports threejoin methods:sortedmergejoins,
nested-loop joins (including index-nested loop variants for the inner), and
ahybrid hashjoin(Section12.5).
3. Sort.ExternalsortingisimplementedinPostgreSQLbyalgorithmsexplained
inSection12.4.Theinputisdividedintosortedrunsthatarethenmergedin
apolyphasemerge.Theinitialrunsareformedusingreplacementselection,
using a priority tree instead of a data structure that ?xes the number of in-
memoryrecords.Thisisbecause PostgreSQLmaydealwithtuplesthatvary
considerablyinsizeandtriestoensurefullutilizationofthecon?guredsort
memoryspace.
4. Aggregation. Grouped aggregation in PostgreSQL can be either sort-based
or hash-based. When the estimatednumber of distinct groups is very large
the formerisusedandotherwisethe hash-based approachis preferred.
27.6.4 TriggersandConstraints
InPostgreSQL(unlikesomecommercialsystems)active-databasefeaturessuchas
triggers and constraints are not implemented in the rewrite phase. Instead they
areimplementedaspartofthequeryexecutor.Whenthetriggersandconstraints
1154 Chapter27 PostgreSQL
are registered by the user, the details are associated with the catalog informa-
tion for each appropriate relation and index. The executor processes an update,
delete,andinsertstatementbyrepeatedlygeneratingtuplechangesforarelation.
For each row modi?cation, the executor explicitly identi?es, ?res, and enforces
candidatetriggersand constraints, beforeor afterthechange asrequired.
27.7 SystemArchitecture
The PostgreSQLsystemarchitecturefollowstheprocess-per-transactionmodel.A
running PostgreSQL site is managed by a central coordinating process, calledthe
postmaster. The postmaster process is responsible for initializing and shutting
downtheserverandalsoforhandlingconnectionrequestsfromnewclients.The
postmaster assigns each new connecting client to a back-end server process that
is responsible for executing the queries on behalf of the client and for returning
the resultstotheclient.Thisarchitectureis depictedinFigure27.10.
Client applications can connect to the PostgreSQL server and submit queries
throughoneofthemanydatabaseapplicationprogrammerinterfacessupported
by PostgreSQL (libpq, JDBC, ODBC,PerlDBD) that are provided as client-side
libraries. An example client application is the command-line psql program, in-
cluded in the standard PostgreSQL distribution. The postmaster is responsible
for handling the initial client connections. For this, it constantly listens for new
connections on a known port. After performing initialization steps such as user
authentication,thepostmasterwillspawnanewback-endserverprocesstohan-
dle the new client. After this initial connection, the client interacts only with the
back-end server process, submitting queries and receiving query results. This is
the essenceof the process-per-connectionmodeladoptedby PostgreSQL.
The back-end server process is responsible for executing the queries submit-
ted by the client by performing the necessary query-execution steps, including
parsing, optimization, and execution. Each back-end server process can handle
library
interface
client 
postmaster
daemon
process
PostgreSQL
server
(back end)
SQL queries
and results read/
write 
shared
tables
shared
disk
bu?ers
disk
storage
client processes
client
application
server processes
create
disk
bu?ers
kernel
shared memory unix system
initial
connection
request
and 
authentication
library API
through
communication
Figure27.10 The PostgreSQL system architecture.
BibliographicalNotes 1155
onlyasinglequeryatatime.Inordertoexecutemorethanonequeryinparallel,
anapplicationmustmaintainmultipleconnections tothe server.
At any given time, there may be multiple clients connected to the system
and thus multiple back-end server processes may be executing concurrently.
The back-end server processes access database data through the main-memory
buffer pool,which isplaced insharedmemory,sothat all the processeshave the
same viewof the data. Shared memory is also used to implement other forms of
synchronization between the server processes, for example, the locking of data
items.
The use of shared memory as a communication medium suggests that a
PostgreSQL server should run on a single machine; a single-server site cannot be
spread across multiple machines without the assistance of third-party packages,
suchastheSlonyreplicationtool.However,itispossibletobuildashared-nothing
paralleldatabasesystemwithaninstanceofPostgreSQLrunningoneachnode;in
fact, several commercial parallel database systems have been built with exactly
this architecture,as describedinSection18.8.
BibliographicalNotes
Thereisextensiveonlinedocumentationof PostgreSQLat www.postgresql.org.This
WebsiteistheauthoritativesourceforinformationonnewreleasesofPostgreSQL,
which occur on a frequentbasis. Until PostgreSQL version8, the only way to run
PostgreSQLunderMicrosoftWindowswasbyusingCygwin.CygwinisaLinux-
like environment that allows rebuilding of Linux applications from source to
rununderWindows.Detailsareat www.cygwin.com.Bookson PostgreSQLinclude
Douglas and Douglas [2003] and Stinson [2002]. Rules as used in PostgreSQL
are presented in Stonebraker et al. [1990]. The GiST structure is described in
Hellersteinetal. [1995].
Manytoolsandextensionsfor PostgreSQLaredocumentedbythepgFoundry
at www.pgfoundry.org. These include the pgtcl library and the pgAccess adminis-
trationtoolmentionedinthischapter.ThepgAdmintoolisdescribedontheWeb
at www.pgadmin.org. The database-design tools, TORA and Data Architect, are de-
scribedattora.sourceforge.netandwww.thekompany.com/products/dataarchitect,respec-
tively. The report-generation tools, GNU Report Generator and GNU Enterprise,
are described at www.gnu.org/software/grg and www.gnuenterprise.org, respectively.
Theopen-source Mondrian OLAP serverisdescribedat mondrian.pentaho.org.
An open-source alternative to PostgreSQL is MySQL, which is available for
noncommercial use under the GNU General Public License. MySQL may be em-
beddedincommercialsoftwarethatdoesnothavefreelydistributedsourcecode,
butthisrequiresaspeciallicensetobepurchased.Comparisonsbetweenthemost
recentversionsof thetwo systemsarereadilyavailableon theWeb.
This page intentionally left blank 
CHAPTER
28
Oracle
Hakan Jakobsson
When Oracle was founded in 1977 as Software Development Laboratories by
Larry Ellison, Bob Miner, and Ed Oates, there were no commercial relational
database products. The company, which was later renamed Oracle, set out to
build a relational database management system as a commercial product, and
became a pioneer of the RDBMS market and has held a leading position in this
market ever since. Over the years, its product and service offerings have grown
beyondtherelationaldatabaseservertoincludemiddlewareandapplications.
Inadditiontoproductsdevelopedinsidethecompany,Oracle’sofferingsin-
cludesoftwarethatwasoriginallydevelopedincompaniesthatOracleacquired.
Oracle’sacquisitionshaverangedfromsmallcompaniestolarge,publiclytraded
ones,includingPeoplesoft,Siebel,Hyperion,and BEA.Asaresultoftheseacqui-
sitions,Oraclehasaverybroadportfolioofenterprisesoftwareproducts.
ThischapterisfocusedonOracle’smainrelationaldatabaseserverandclosely
relatedproducts.Newversionsoftheproductsarebeingdevelopedcontinually,
so all product descriptions are subject to change. The feature set described here
is based on the ?rst release of Oracle11g, which is Oracle’s ?agship database
product.
28.1 DatabaseDesignandQueryingTools
Oracle provides a variety of tools for database design, querying, report gener-
ation and data analysis, including OLAP. These tools, along with various other
applicationdevelopmenttools,arepartofaportfolioofsoftwareproductscalled
OracleFusionMiddleware.ProductsincludebothtraditionaltoolsusingOracle’s
PL/SQLprogramminglanguageandneweronesbasedonJava/J2EEtechnologies.
Thesoftwaresupportsopenstandardssuchas SOAP, XML, BPEL,andUML.
28.1.1 Database and Application DesignTools
The Oracle Application Development Framework (ADF)isanend-to-endJ2EE-
baseddevelopmentframeworkforaModel-View-Controldesignpattern.Inthis
1157
1158 Chapter 28 Oracle
framework, an application consists of multiple layers. The Model and Business
Services layers handle the interaction with the data sources and contains the
businesslogic.TheViewlayerhandlestheuserinterface,andtheControllerlayer
handlesthe?owoftheapplicationandtheinteractionbetweentheotherlayers.
The primary development tool for Oracle ADF is Oracle JDeveloper, which
provides an integrated development environment with support for Java, XML,
PHP,HTML,Javascript,BPEL,SQL,andPL/SQLdevelopment.Ithasbuilt-insupport
for UMLmodeling.
OracleDesignerisadatabasedesigntool,whichtranslatesbusinesslogicand
data ?ows into schema de?nitions and procedural scripts for application logic.
Itsupportssuchmodelingtechniquesas E-Rdiagrams,informationengineering,
andobjectanalysisanddesign.
Oraclealsohasanapplicationdevelopmenttoolfordatawarehousing,Ora-
cle Warehouse Builder. Warehouse Builder is a tool for design and deployment
of all aspects of a data warehouse, including schema design, data mapping and
transformations,dataloadprocessing,andmetadatamanagement.OracleWare-
house Builder supports both 3NF and star schemas and can also import designs
from Oracle Designer. This tool, in conjunction with database features, such as
external tables and table functions, typically eliminates the need for third-party
extraction,transformation,andloading(ETL)tools.
28.1.2 QueryingTools
Oracle provides tools for ad hoc querying, report generation, and data analysis,
including OLAP.
Oracle Business Intelligence Suite (OBI) is a comprehensive suite of tools
sharing a common service-oriented architecture. Components include a Busi-
ness Intelligence server and tools for ad hoc querying, dashboard generation,
reporting, and alerting. The components share infrastructure and services for
dataaccessandmetadatamanagementand haveacommon securitymodeland
administrationtool.
Thecomponentforadhocquerying,Oracle BIAnswers,isaninteractivetool
that presents the user with a logical view of the data hiding the details of the
physicalimplementation.Objectsavailabletotheuseraredisplayedgraphically
andtheusercanbuildaquerywithapoint-and-clickinterface.Thislogicalquery
issenttotheOracleBIServercomponent,whichthengeneratesthephysicalquery
orqueries.Multiplephysicaldatasourcesaresupported,andaquerycouldcom-
bine data stored in relational databases, OLAP sources, and Excel spreadsheets.
Results can be presented as charts, reports, pivot tables, or dashboards that are
drillableandcanbesavedandlatermodi?ed.
28.2 SQLVariationsandExtensions
OraclesupportsallcoreSQL:2003featuresfullyorpartially,withtheexceptionof
features-and-conformanceviews.Inaddition,Oraclesupportsalargenumberof
28.2 SQLVariations andExtensions 1159
otherlanguageconstructs,someofwhichconformwithOptionalFeaturesofSQL
Foundation:2003,whileothersareOracle-speci?cinsyntaxorfunctionality.
28.2.1 Object-Relational Features
Oraclehasextensivesupportforobject-relationalconstructs,including:
  Objecttypes.Asingle-inheritancemodelissupportedfortypehierarchies.
  Collectiontypes.Oraclesupportsvarrays,whicharevariablelengtharrays,
andnestedtables.
  Object tables. These are used to store objects while providing a relational
viewoftheattributesoftheobjects.
  Tablefunctions.Thesearefunctionsthatproducesetsofrowsasoutput,and
can be used in the from clause of a query. Table functions in Oracle can be
nested.Ifatablefunctionisusedtoexpresssomeformofdatatransformation,
nesting multiple functions allows multiple transformations to be expressed
inasinglestatement.
  Object views. These provide a virtual object table view of data stored in a
regularrelationaltable.Theyallowdatatobeaccessedorviewedinanobject-
oriented style even if the data are really stored in a traditional relational
format.
  Methods.Thesecanbewrittenin PL/SQL,Java,orC.
  User-de?ned aggregate functions. These can be used in SQL statements in
thesamewayasbuilt-infunctionssuchas sumandcount.
28.2.2 OracleXMLDB
Oracle XML DB provides in-database storage for XML data and support for a
broad set of XML functionality including XML Schema and XQuery. It is built on
the XMLType abstract data type, which is treated as a native Oracle data type.
XMLDBprovidesseveraloptions forhow dataof thisdatatypearestoredinthe
database,including:
  Structured in object-relational format. This format is usually space ef?cient
andallowstheuseofavarietyofstandardrelationalfeatures,suchasB-tree
indices, but incurs some overhead when mapping XML documents to the
storage format and back. It is mainly suitable for XML data that are highly
structured and the mapping includes a manageable number of relational
tablesandjoins.
  Unstructuredasatextstring.Thisrepresentationdoesnotrequireanymap-
ping and provides high throughput when inserting or retrieving an entire
XML document. However,itis usually not veryspace ef?cient and provides
forlessintelligentprocessingwhenoperatingonpartsofan XMLdocument.
1160 Chapter 28 Oracle
  Binary XMLstorage.Thisrepresentationisapost-parse, XMLSchema-aware,
binary format. It is more space ef?cient than unstructured storage and can
handleoperationsagainstpartsofanXMLdocument.Itisalsobetterthanthe
structuredformatathandlingdatathatarehighlyunstructured,butmaynot
alwaysbeasspaceef?cient.ThisformatmaymaketheprocessingofXQuery
statementslessef?cientthanwhenthestructuredformatisused.
Boththebinaryandunstructuredrepresentationcanbeindexedwithaspecial
type of index called XMLIndex. This type of index allows document fragments
tobeindexedbasedontheircorresponding XPathexpression.
Storing XML data inside the database means that they get the bene?t of Or-
acle’s functionality in areas such as backup, recovery, security, and query pro-
cessing.Itallowsforaccessingrelationaldataaspartofdoing XMLprocessingas
wellasaccessingXMLdataaspartofdoingSQLprocessing.Someveryhigh-level
featuresof XMLDBinclude:
  Supportforthe XQuerylanguage(W3C XQuery1.0Recommendation).
  An XSLTprocessthatlets XSLtransformations beperformedinsidethedata-
base.
  An XPathrewriteoptimizationthatcanspeedupqueriesagainstdatastored
in object-relational representation. By translating an expression used in an
XQuery into conditions directlyon the object-relational columns, regular in-
dicesonthesecolumnscanbeusedtospeedupqueryprocessing.
28.2.3 ProceduralLanguages
Oracle has two main procedural languages, PL/SQL and Java. PL/SQL was Ora-
cle’soriginallanguageforstoredproceduresandithassyntaxsimilartothatused
intheAdalanguage.JavaissupportedthroughaJavavirtualmachineinsidethe
database engine. Oracle provides a package to encapsulate related procedures,
functions,andvariablesintosingleunits.OraclesupportsSQLJ(SQLembeddedin
Java)andJDBC,andprovidesatooltogenerateJavaclassde?nitionscorrespond-
ingtouser-de?neddatabasetypes.
28.2.4 Dimensions
Dimensional modeling is a commonly used design technique for relational star
schemas as well as multidimensional databases. Oracle supports the creation of
dimensions as metadata objects in order to support query processing against
databases designed based on this technique. The metadata objects can be used
to store information about various kinds of attributes of a dimension, but per-
haps more importantly, about hierarchical relationships. See Section 28.3.10 for
examples.
28.2 SQLVariations andExtensions 1161
28.2.5 OLAP
Oracle provides support for analytical database processing in several different
ways. In addition to support for a rich set of analytical SQL constructs (cube,
rollup, grouping sets, window functions, etc.), Oracle provides native multidi-
mensional storage inside the relational database server. The multidimensional
datastructuresallowforarray-basedaccesstothedata,and,intherightcircum-
stances,thistypeofaccesscanbevastlymoreef?cientthantraditionalrelational
access methods. Using these data structures as an integrated part of a relational
database provides a choice of storing data in a relational or multidimensional
format while still taking advantage of Oracle features in areas such as backup
andrecovery,security,andadministrationtools.
Oracleprovidesstoragecontainersformultidimensionaldataknownasana-
lyticworkspaces.Ananalyticworkspacecontainsboththedimensionaldataand
measures(or facts) of an OLAP cube and is storedinside an Oracle table.From a
traditionalrelationalperspective,acubestoredinsideatablewouldbeanopaque
object where the data could not normally be interpreted directly in terms of the
table’srowsandcolumns.However,Oracle’sOLAPserverinsidethedatabasehas
theknowledgetointerpretandaccessthedataandmakesitpossibletogive SQL
accesstoitasifithadbeenstoredinaregulartableformat.Hence,itispossible
to store data in either a multidimensional format or a traditional relational for-
mat, depending on what is optimal, and still be able to join data stored in both
typesofrepresentationsinasingle SQLquery.Amaterializedviewcanuseeither
representation.
In addition to Oracle’s OLAP support inside its relational database, Ora-
cle’s product suite includes Essbase. Essbase is a widely used multidimensional
databasethatcametobepartofOraclewiththeacquisitionofHyperion.
28.2.6 Triggers
Oracleprovidesseveraltypesof triggersand severaloptions for whenand how
theyareinvoked.(SeeSection5.3foranintroductiontotriggersin SQL.)Triggers
canbewrittenin PL/SQLorJavaorasCcallouts.
FortriggersthatexecuteonDMLstatementssuchasinsert,update,anddelete,
Oraclesupports rowtriggersand statement triggers.Rowtriggersexecuteonce
for every row that is affected (updated or deleted, for example) by the DML
operation. A statement trigger is executed just once per statement. In each case,
thetriggercanbede?nedaseithera beforeor aftertrigger,dependingonwhether
itistobeinvokedbeforeorafterthe DMLoperationiscarriedout.
Oracle allows the creation of instead of triggers for views that cannot be
subject to DML operations. Depending on the view de?nition, it may not be
possible for Oracle to translate a DML statement on a view to modi?cations of
theunderlyingbasetablesunambiguously.Hence, DMLoperationsonviewsare
subjecttonumerousrestrictions.Ausercancreateaninsteadoftriggeronaview
to specify manually what operations on the base tables are to be carried out in
responsetoaDMLoperationontheview.Oracleexecutesthetriggerinsteadofthe
1162 Chapter 28 Oracle
DMLoperationandthereforeprovidesamechanismtocircumventtherestrictions
on DMLoperationsagainstviews.
Oracle also has triggers that execute on a variety of other events, such as
database start-up or shutdown, servererror messages,user logon or logoff, and
DDLstatementssuchascreate,alteranddropstatements.
28.3 StorageandIndexing
In Oracle parlance, a database consists of information stored in ?les and is ac-
cessedthroughaninstance,whichisasharedmemoryareaandasetofprocesses
that interact with the data in the ?les. The control ?le is a small ?le that con-
tainssomeveryhigh-levelmetadatarequiredtostartoroperateaninstance.The
storagestructureoftheregulardataandmetadataisdescribedinthenextsection.
28.3.1 TableSpaces
Adatabaseconsistsofoneormorelogicalstorageunitscalledtablespaces.Each
table space, in turn, consists of one or more physical structures called data ?les.
Thesemaybeeitherpartofa?lesystemorrawdevices.
Usually,anOracledatabasehasthefollowingtablespaces:
  Thesystemandtheauxiliarysysauxtablespaces,whicharealwayscreated.
They contain the data dictionary tables and storage for triggers and stored
procedures.
  Table spaces created to store user data. While user data can be stored in the
system table space, it is often desirable to separate the user data from the
system data. Usually, the decision about what other table spaces should be
created is based on performance, availability, maintainability, and ease of
administration. For example,having multiple table spaces can be useful for
partialbackupandrecoveryoperations.
  The undotablespace,whichisusedsolelyforstoringundoinformationfor
transactionmanagementandrecovery.
  Temporarytablespaces.Manydatabaseoperationsrequiresortingthedata,
and the sort routine may have to store data temporarily on disk if the sort
cannot be done inmemory.Temporarytablespacesareallocatedforsorting
andhashing tomakethespacemanagementoperationsinvolvedinspilling
todiskmoreef?cient.
Tablespacescanalsobeusedasameansofmovingdatabetweendatabases.
For example, it is common to move data from a transactional system to a data
warehouseatregularintervals.Oracleallowsmovingallthedatainatablespace
fromonesystemtotheotherbysimplycopyingthedata?lesandexportingand
importinga small amount of data-dictionarymetadata.Theseoperationscan be
much fasterthan unloading the datafromone database andthen usingaloader
28.3 Storage andIndexing 1163
to insert it into the other. ThisOraclefeatureisknownastransportable table
spaces.
28.3.2 Segments
The space in a table space is divided into units, called segments,eachofwhich
containsdataforaspeci?cdatastructure.Therearefourtypesofsegments:
  Data segments. Each table in a table space has its own data segment where
thetabledataarestoredunlessthetableispartitioned;ifso,thereisonedata
segmentperpartition.(PartitioninginOracleisdescribedinSection28.3.9.)
  Index segments. Each index in a table space has its own index segment,
exceptforpartitionedindices,whichhaveoneindexsegmentperpartition.
  Temporarysegments.Thesearesegmentsusedwhenasortoperationneeds
towritedatatodiskorwhendataareinsertedintoatemporarytable.
  Undo segments. These segments contain undo information so that an un-
committedtransactioncanberolledback.Thesesegmentsareautomatically
allocated in a special undo table space. They also play an important role in
Oracle’s concurrency control model and for database recovery, described in
Sections 28.5.1 and 28.5.2. In older implementations of Oracle’s undo man-
agement,theterm “rollbacksegment”wasused.
Belowthelevelof segment,spaceisallocatedatalevelof granularitycalled
an extent.Eachextentconsistsofasetofcontiguousdatabase blocks.Adatabase
block is the lowest level of granularity at which Oracle performs disk I/O.A
databaseblockdoesnothavetobethesameasanoperatingsystemblockinsize,
butshouldbeamultiplethereof.
Oracle provides storage parameters that allow for detailed control of how
spaceisallocatedandmanaged,parameterssuchas:
  Thesizeofanewextentthatistobeallocatedtoprovideroomforrowsthat
areinsertedintoatable.
  The percentage of space utilization at which a database block is considered
full and at which no more rows will be inserted into that block. (Leaving
somefreespaceinablockcanallowtheexistingrowstogrowinsizethrough
updates,withoutrunningoutofspaceintheblock.)
28.3.3 Tables
AstandardtableinOracleisheaporganized;thatis,thestoragelocationofarow
in a table is not based on the values contained in the row, and is ?xed when the
rowisinserted.However,ifthetableispartitioned,thecontentoftherowaffects
thepartitioninwhichitisstored.Thereareseveralfeaturesandvariations.Heap
tablescanoptionallybecompressed,asdescribedinSection28.3.3.2.
1164 Chapter 28 Oracle
Oraclesupportsnestedtables;that is,a tablecanhaveacolumn whosedata
typeisanothertable.Thenestedtableisnotstoredinlineintheparenttable,but
isstoredinaseparatetable.
Oraclesupportstemporarytableswherethedurationofthedataiseitherthe
transactioninwhichthedataareinserted,ortheusersession.Thedataareprivate
tothesessionandareautomaticallyremovedattheendofitsduration.
Aclusterisanotherformof?leorganizationfortabledata,describedearlier
inSection10.6.2whereitiscalledmultitableclustering.Theuseoftheterm“cluster”
inthiscontext,shouldnotbeconfusedwithothermeaningsofthewordcluster,
suchasthoserelatingtohardwarearchitecture.Inacluster?leorganization,rows
from different tables are stored together in the same block on the basis of some
commoncolumns.Forexample,adepartmenttableandanemployeetablecould
be clustered so that each row in the department table is stored together with
all the employee rows for those employees who work in that department. The
primarykey/foreignkeyvaluesareusedtodeterminethestoragelocation.
The cluster organization implies that a row belongs in a speci?c place; for
example, a new employee row must be inserted with the other rows for the
same department. Therefore, an index on the clustering column is mandatory.
Analternativeorganizationisahashcluster.Here,Oraclecomputesthelocation
of a row by applying a hash function to the value for the cluster column. The
hashfunctionmapstherowtoaspeci?cblockinthehashcluster.Sincenoindex
traversal is needed to access a row according to its cluster column value, this
organizationcansavesigni?cantamountsofdisk I/O.
28.3.3.1 Index-OrganizedTables
In an index-organized table (IOT), records are stored in an Oracle B-tree index
instead of in a heap; this ?le organization is described earlier in Section 11.4.1,
where it is called B
+
-tree ?le organization.AnIOT requires that a unique key be
identi?edforuseastheindexkey.Whileanentryinaregularindexcontainsthe
key value and row-id of the indexed row, an IOT replaces the row-id with the
column values for the remaining columns of the row. Compared to storing the
data in a regularheap table and creating an indexon the key columns, using an
IOTcanimprovebothperformanceandspaceutilization.Considerlookingupall
the column values of a row, given its primary key value. For a heap table, that
would require an index probe followed by a table access by row-id. For an IOT,
onlytheindexprobeisnecessary.
Secondaryindicesonnonkeycolumnsofanindex-organizedtablearediffer-
ent from indices on a regular heap table. In a heap table, each row has a ?xed
row-id that does not change. However, a B-tree is reorganized as it grows or
shrinkswhenentriesareinsertedordeleted,andthereisnoguaranteethatarow
willstayina?xedplaceinsidean IOT. Hence,a secondaryindexon an IOT con-
tains not normal row-ids, but logical row-ids instead. A logical row-id consists
of two parts: a physical row-id corresponding to where the row was when the
index was created or last rebuilt and a value for the unique key. The physical
row-id is referred to as a “guess” since it could be incorrect if the row has been
28.3 Storage andIndexing 1165
moved.Ifso,theotherpartofalogicalrow-id,thekeyvaluefortherow,isusedto
accesstherow;however,thisaccessisslowerthaniftheguesshadbeencorrect,
since it involves a traversal of the B-tree for the IOT from the root all the way
to the leaf nodes, potentially incurring several disk I/Os. However, if a table is
highly volatile and a large percentage of the guesses are likely to be wrong, it
canbebettertocreatethesecondaryindexwithonlykeyvalues(asdescribedin
Section11.4.1),sinceusinganincorrectguessmayresultinawasteddisk I/O.
28.3.3.2 Compression
Oracle’s compression feature allows data to be stored in a compressed format,
something that can drastically reduce the amount of space needed to store the
dataandthenumberofI/Ooperationsneededtoretrieveit.Oracle’scompression
methodisalosslessdictionary-basedalgorithmthatcompresseseachblockindi-
vidually. All the information needed to uncompress a block is contained in that
blockitself.Thealgorithmworksbyreplacingrepeatedoccurrencesofavaluein
thatblockwithpointerstoanentryforthatvalueinasymboltable(ordictionary)
intheblock.Entriescanbebasedonrepeatedvaluesforindividualcolumnsora
combinationofcolumns.
Oracle’s original table compression generated the compressed block format
as the data were bulk-loaded into a table and was mainly intended for data
warehousing environments. A newer OLTP compression feature supports com-
pression in conjunction with regular DML operations as well. In the latter case,
Oraclecompressesblocksonlyaftercertainthresholdshavebeenreachedforhow
much data have been written into the block. As a result, only transactions that
causeathresholdtobepassedwilloccuranyoverheadforcompressingablock.
28.3.3.3 DataSecurity
Inadditiontoregularaccesscontrolfeaturessuchaspasswords,userprivileges,
and user roles, Oracle supports several features to protect the data from unau-
thorizedaccess,including:
  Encryption.Oraclecanautomaticallystoretabledatainanencryptedformat
andtransparentlyencryptanddecryptdatausingtheAESor3DESalgorithms.
Encryptioncanbeenabledforanentiredatabaseorjustforindividualtable
columns. The main motivation for this feature is to protect sensitive data
outsidethenormallyprotectedenvironment,suchaswhenbackupmediais
senttoaremotelocation.
  DatabaseVault.Thisfeatureisaimedatprovidingaseparationofdutiesfor
userswithaccesstothedatabase.Adatabaseadministratorisahighlyprivi-
legeduserthattypicallycandoalmostanythingwiththedatabase.However,
it may be inappropriate or illegal to let that person access sensitive corpo-
rate?nancialdataorpersonalinformationaboutotheremployees.Database
vaultincludesavarietyofmechanismsthatcanbeusedtorestrictormonitor
accesstosensitivedatabyhighlyprivilegeddatabaseusers.
1166 Chapter 28 Oracle
  Virtual Private Database. This feature, described earlier in Section 9.7.5,
allows additional predicates to be automatically added to the where clause
of a query that accesses a given table or view. Typically, the feature would
be used so that the additional predicate ?lters out all the rows that the user
doesnothavetherighttosee.Forexample,twouserscouldsubmitidentical
queries to ?nd all the employee information in the entire employee table.
However,ifapolicyexiststhatlimitseachusertoseeingonlytheinformation
forthe employeenumberthat matches the user ID,theautomatically added
predicateswillensurethateachqueryonlyreturnstheemployeeinformation
for the user who submittedthe query.Hence, each userwill be left withthe
impressionof accessing a virtualdatabase that contains only a subset of the
dataofthephysicaldatabase.
28.3.4 Indices
Oraclesupportsseveraldifferenttypesofindices.Themostcommonlyusedtype
isaB-treeindex,createdononeormultiplecolumns.Notethatintheterminology
of Oracle (as also in several other database systems) a B-tree index is what is
referred to as a B
+
-tree index in Chapter 11. Index entries have the following
format: for an index on columns col
1
, col
2
,andcol
3
, each row in the table where
atleastoneofthecolumnshasanonnullvaluewouldresultintheindexentry:
<col
1
><col
2
><col
3
><row-id>
where<col
i
>denotesthe valueforcolumn i and<row-id>isthe row-idforthe
row. Oracle can optionally compress the pre?x of the entry to save space. For
example, if there are many repeated combinations of<col
1
><col
2
> values, the
representation of each distinct<col
1
><col
2
> pre?x can be shared between the
entriesthathavethatcombinationofvalues,ratherthanstoredexplicitlyforeach
suchentry.Pre?xcompressioncanleadtosubstantialspacesavings.
28.3.5 BitmapIndices
Bitmap indices (described in Section 11.9) use a bitmap representation for in-
dex entries, which can lead to substantial space saving (and therefore disk I/O
savings), when the indexed column has a moderate number of distinct values.
BitmapindicesinOracleusethesamekindofB-treestructuretostoretheentries
as a regular index. However, where a regular index on a column would have
entriesoftheform<col
1
><row-id>,abitmapindexentryhastheform:
<col
1
><start row-id><end row-id><compressed bitmap>
The bitmap conceptually represents the space of all possible rows in the table
between the start and end row-id. The number of such possible rows in a block
dependsonhowmanyrowscan?tintoablock,whichisafunctionofthenumber
28.3 Storage andIndexing 1167
of columns in the table and their data types. Each bit in the bitmap represents
one such possible row in a block. If the column value of that row is that of the
index entry, the bit is set to 1. If the row has some other value, or the row does
not actually exist in the table, the bit is set to 0. (It is possible that the row does
notactuallyexistbecauseatableblockmaywellhaveasmallernumberofrows
thanthenumberthatwascalculatedasthemaximumpossible.)Ifthedifference
islarge,theresultmaybelongstringsofconsecutivezerosinthebitmap,butthe
compression algorithm deals with such strings of zeros, so the negative effect is
limited.
The compression algorithm is a variation of a compression technique called
Byte-Aligned Bitmap Compression (BBC). Essentially, a section of the bitmap
where the distance between two consecutive 1s is small enough is stored as
verbatim bitmaps. If the distance between two 1s is suf?ciently large—that is,
thereisasuf?cientnumberofadjacent0sbetweenthem—arunlengthof0s,that
is,thenumberof0s,isstored.
Bitmapindicesallowmultipleindicesonthesametabletobecombinedinthe
sameaccesspathiftherearemultipleconditionsonindexedcolumnsinthewhere
clauseofaquery.Bitmapsfromthedifferentindicesareretrievedandcombined
using Boolean operations corresponding to the conditions in the where clause.
AllBooleanoperationsareperformeddirectlyonthecompressedrepresentation
ofthebitmaps—nodecompressionisnecessary—andtheresulting(compressed)
bitmaprepresentsthoserowsthatmatchallthelogicalconditions.
The ability to use the Boolean operations to combine multiple indices is not
limitedtobitmap indices.Oracle canconvert row-idstothe compressedbitmap
representation,so itcan use a regularB-treeindex anywhere in a Boolean tree of
bitmap operation simply by putting a row-id-to-bitmap operator on top of the
indexaccessintheexecutionplan.
Asaruleofthumb,bitmapindicestendtobemorespaceef?cientthanregular
B-tree indices if the number of distinct key values is less than half the number
of rows in the table. For example, in a table with 1 million rows, an index on
a column with less than 500,000 distinct values would probably be smaller if
it were created as a bitmap index. For columns with a very small number of
distinct values—for example, columns referring to properties such as country,
state, gender, marital status, and various status ?ags—a bitmap index might
require only a small fraction of the space of a regular B-tree index. Any such
space advantagecanalso giveriseto correspondingperformance advantagesin
theformoffewerdisk I/Oswhentheindexisscanned.
28.3.6 Function-Based Indices
Inadditiontocreatingindicesononeormultiplecolumnsofatable,Oracleallows
indices to be created on expressions that involve one or more columns, such as
col
1
+ col
2
?5. For example,by creating an indexon the expression upper(name),
where upperisafunctionthatreturnstheuppercaseversionofastring,and name
isa column, itis possibleto docase-insensitivesearcheson the name column. In
orderto?ndallrowswithname “vanGogh”ef?ciently,thecondition:
1168 Chapter 28 Oracle
upper(name)=’ VAN GOGH’
wouldbeusedinthewhereclauseofthequery.Oraclethenmatchesthecondition
withtheindexde?nitionandconcludesthattheindexcanbeusedtoretrieveall
therowsmatching“vanGogh”regardlessofhowthenamewascapitalizedwhen
it was stored in the database. A function-based index can be created as either a
bitmaporaB-treeindex.
28.3.7 JoinIndices
A join index isan index where the key columns are not in the table that isrefer-
encedbytherow-idsintheindex.Oraclesupportsbitmapjoinindicesprimarily
forusewithstarschemas(seeSection20.2.2).Forexample,ifthereisacolumnfor
productnamesinaproductdimensiontable,abitmapjoinindexonthefacttable
withthiskeycolumncouldbeusedtoretrievethefacttablerowsthatcorrespond
to a product with a speci?c name, although the name is not stored in the fact
table. How the rows in the fact and dimension tables correspond is based on a
joinconditionthatisspeci?edwhentheindexiscreated,andbecomespartofthe
indexmetadata.Whenaqueryisprocessed,theoptimizerwilllookforthesame
join condition in the where clause of the query in order to determine if the join
indexisapplicable.
Oraclecan combineabitmapjoinindexonafacttablewithotherindiceson
thesametable—whetherjoinindicesornot—byusingtheoperatorsforBoolean
bitmapoperations.
28.3.8 Domain Indices
Oracle allows tables to be indexedby index structures that are not native to Or-
acle. This extensibility feature of the Oracle server allows software vendors to
developso-called cartridgeswithfunctionalityforspeci?capplicationdomains,
such as text, spatial data, and images, with indexing functionality beyond that
providedby the standardOracle indextypes.Inimplementingthe logic for cre-
ating,maintaining,andsearchingtheindex,theindexdesignermustensurethat
itadherestoaspeci?cprotocolinitsinteractionwiththeOracleserver.
A domain index must be registeredin the data dictionary, together with the
operatorsitsupports.Oracle’s optimizerconsidersdomainindicesas one ofthe
possibleaccesspathsforatable.Oracleallowscostfunctionstoberegisteredwith
theoperatorssothattheoptimizercancomparethecostofusingthedomainindex
tothoseofotheraccesspaths.
For example, a domain index for advanced text searches may support an
operator contains. Once this operatorhas been registered,the domainindexwill
beconsideredasanaccesspathforaquerylike:
select*
from employees
where contains(resume,’ LINUX’);
28.3 Storage andIndexing 1169
where resume is a text column in the employee table. The domain index can be
storedineitheranexternaldata?leorinsideanOracleindex-organizedtable.
Adomainindexcanbecombinedwithother(bitmaporB-tree)indicesinthe
same access path by converting between the row-id and bitmap representation
andusingBooleanbitmapoperations.
28.3.9 Partitioning
Oraclesupportsvariouskindsofhorizontalpartitioningoftablesandindices,and
thisfeatureplaysamajorroleinOracle’sabilitytosupportverylargedatabases.
Theabilitytopartitionatableorindexhasadvantagesinmanyareas.
  Backupandrecoveryareeasierandfaster,sincetheycanbedoneonindivid-
ualpartitionsratherthanonthetableasawhole.
  Loading operations in a data warehousing environment are less intrusive:
datacanbeaddedtoanewlycreatedpartition,andthenthepartitionadded
toatable,whichisaninstantaneousoperation.Likewise,droppingapartition
withobsoletedatafromatableisveryeasyinadatawarehousethatmaintains
arollingwindowofhistoricaldata.
  Queryperformancebene?ts substantially,sincetheoptimizercan recognize
that only a subset of the partitions of a table needto be accessed in order to
resolveaquery(partitionpruning).Also,theoptimizercanrecognizethatin
ajoin,itisnotnecessarytotrytomatchallrowsinonetablewithallrowsin
the other,butthatthe joinsneedtobe done onlybetweenmatching pairsof
partitions(partitionwisejoin).
Anindexonapartitionedtablecanbeeitheraglobalindexoralocalindex.
Entries in a global index can refer to rows in any partition. A locally indexed
tablehasone physicalindexforeach partitionthatonlycontainsentriesforthat
partition. Unless partition pruning restricts a query to a single partition, a table
accessed through a local index will require multiple individual physical index
probes.However,alocalindexhasadvantagesindatawarehousingenvironments
wherenewdatacanbeloadedintoanewpartitionandindexedwithouttheneed
tomaintainanyexistingindex.(Loadingfollowedbyindexcreationismuchmore
ef?cient than maintaining an existing index while the data are being loaded.)
Similarly,droppinganoldpartitionandthephysicalpartofitslocalindexcanbe
donewithoutcausinganyindexmaintenance.
Each row in a partitioned table is associated with a speci?c partition. This
association is based on the partitioning column or columns that are part of the
de?nition of a partitioned table. There are several ways to map column values
to partitions, giving rise to several types of partitioning, each with different
characteristics:range,hash,list,andcompositepartitioning.
1170 Chapter 28 Oracle
28.3.9.1 RangePartitioning
In range partitioning, the partitioning criteria are ranges of values. This type of
partitioning is especially well suited to date columns, in which case all rows
in the same date range, say a day or a month, belong in the same partition.
In a data warehouse where data are loaded from the transactional systems at
regularintervals,rangepartitioningcanbeusedtoimplementarollingwindow
of historical data ef?ciently. Each data load gets its own new partition, making
theloadingprocessfasterandmoreef?cient.Thesystemactuallyloadsthedata
into a separate table with the same column de?nition as the partitionedtable. It
canthencheckthedataforconsistency,cleansethem,andindexthem.Afterthat,
thesystemcanmaketheseparatetableanewpartitionofthepartitionedtable,by
a simplechange to the metadatain the datadictionary—a nearlyinstantaneous
operation.
Upuntilthemetadatachange,theloadingprocessdoesnotaffecttheexisting
datainthepartitionedtableinanyway.Thereisnoneedtodoanymaintenance
ofexistingindicesaspartoftheloading.Olddatacanberemovedfromatableby
simplydroppingitspartition;thisoperationdoesnotaffecttheotherpartitions.
In addition, queries in a data warehousing environment often contain con-
ditions that restrict them to a certain time period, such as a quarter or month. If
date-range partitioning is used, the query optimizer can restrict the data access
to those partitions that are relevant to the query, and avoid a scan of the entire
table.
Partitions can either be created with explicitly set end points or be de?ned
basedona?xedrange,suchasadayoramonth.Inthelattercase,calledinterval
partitioning,thecreationofthepartitionhappensautomaticallyunderthecovers
whentryingtoinsertarowwithavalueinapreviouslynonexistentinterval.
28.3.9.2 HashPartitioning
In hash partitioning, a hash function maps rows to partitions according to the
values in the partitioning columns. This type of partitioning is primarily useful
when it is important to distribute the rows evenly among partitions or when
partitionwisejoinsareimportantforqueryperformance.
28.3.9.3 ListPartitioning
Inlistpartitioning,thevaluesassociatedwithaparticularpartitionarestatedina
list.Thistypeofpartitioningisusefulifthedatainthepartitioningcolumnhave
arelativelysmallsetof discretevalues.Forinstance,a tablewithastatecolumn
can beimplicitlypartitionedby geographicalregionifeachpartitionlisthas the
statesthatbelonginthesameregion.
28.3.9.4 CompositePartitioning
Incompositepartitioning,tablesthatarerange,interval,orlistpartitionedcanbe
subpartitioned by range, list, or hash. For example, a table may be range parti-
tionedonadatecolumnandhashsubpartitionedonacolumnthatisfrequently
28.3 Storage andIndexing 1171
usedasajoincolumn.Thesubpartitioningallowspartition-wisejoinstobeused
whenthetableisjoined.
28.3.9.5 ReferencePartitioning
In reference partitioning, the partitioning key is resolved based on a foreign-
key constraint with another table. The dependency between the tables allows
maintenanceoperationstobeautomaticallycascaded.
28.3.10 MaterializedViews
Thematerialized-viewfeature(seeSection4.2.3)allowstheresultofanSQLquery
to be stored in a table and used for later query processing. In addition, Oracle
maintainsthematerializedresult,updatingitwhenthetablesthatwerereferenced
in the query are updated. Materialized views are used in data warehousing to
speed up query processing, but the technology is also used for replication in
distributedandmobileenvironments.
Indatawarehousing,acommonusageformaterializedviewsistosummarize
data. For example, a common type of query asks for “the sum of sales for each
quarterduringthelast2years.”Precomputingtheresult,orsomepartialresult,of
suchaquerycanspeedupqueryprocessingdramaticallycomparedtocomputing
itfromscratchbyaggregatingalldetail-levelsalesrecords.
Oracle supports automatic query rewrites that take advantage of any useful
materialized view when resolving a query. The rewrite consists of changing the
query to use the materializedviewinstead of the original tables in the query. In
addition,therewritemayaddadditionaljoinsoraggregateprocessingasmaybe
requiredtogetthecorrectresult.Forexample,ifaqueryneedssalesbyquarter,the
rewritecantakeadvantageofaviewthatmaterializessalesbymonth,byadding
additional aggregation to roll up the months to quarters. Oracle has a type of
metadataobject calleddimensionthat allowshierarchical relationshipsin tables
tobede?ned.Forexample,foratime-dimensiontableinastarschema,Oraclecan
de?neadimensionmetadataobjecttospecifyhowdaysrolluptomonths,months
to quarters, quarters to years, and so forth. Likewise, hierarchical properties
relating to geography can be speci?ed—for example,how sales districtsroll up
toregions.Thequeryrewritelogiclooksattheserelationshipssincetheyallowa
materializedviewtobeusedforwiderclassesofqueries.
The container object for a materialized view is a table, which means that a
materialized view can be indexed, partitioned, or subjected to other controls, to
improvequeryperformance.
When there are changes to the data in the tables referenced in the query
that de?nes a materialized view, the materialized view must be refreshed to
re?ect those changes. Oracle supports both complete refresh of a materialized
viewand fast,incrementalrefresh.Inacompleterefresh,Oraclerecomputesthe
materialized view from scratch, which may be the best option if the underlying
tableshavehadsigni?cantchanges,forexample,changesduetoabulkload.Ina
fastrefresh,Oracle updatesthe viewusing the recordsthat were changed in the
underlying tables. The refresh to the view can be executed on commit as part of
1172 Chapter 28 Oracle
thetransactionthatchanged theunderlyingtablesoratsomelaterpointintime
on demand. Fastrefreshmaybe betterifthenumber ofrowsthatwerechanged is
low.Therearesomerestrictionsontheclassesofqueriesforwhichamaterialized
view can be incrementally refreshed (and others for when a materialized view
canbecreatedatall).
A materialized view is similar to an index in the sense that, while it can
improve query performance, it uses up space, and creating and maintaining it
consumes resources. To help resolve this trade-off, Oracle provides an advisor
that can help a user create the most cost-effective materialized views, given a
particularqueryworkloadasinput.
28.4 QueryProcessingandOptimization
Oracle supports a large variety of processing techniques in its query processing
engine.Someofthemoreimportantonesaredescribedherebrie?y.
28.4.1 Execution Methods
Datacanbeaccessedthroughavarietyofaccessmethods:
  Full table scan. The query processor scans the entire table by getting infor-
mation about the blocks that make up the table from the extent map, and
scanningthoseblocks.
  Index scan. The processor creates a start and/or stop key from conditions
in the query and uses it to scan to a relevant part of the index. If there are
columns that need to be retrieved, that are not part of the index, the index
scanwouldbefollowedbyatableaccessbyindexrow-id.Ifnostartorstop
keyisavailable,thescanwouldbeafullindexscan.
  Index fast full scan. The processor scans the extents the same way as the
table extent in a full table scan. If the index contains all the table columns
that are needed for that table, and there are no good start/stop keys that
would signi?cantly reduce that portion of the index that would be scanned
in a regular index scan, this method may be the fastest way to access the
data. This is because the fast full scan can take full advantage of multiblock
disk I/O.However,unlikearegularfullscan, whichtraversestheindexleaf
blocksinorder,afastfullscandoesnotguaranteethattheoutputpreserves
thesortorderoftheindex.
  Index join.Ifaqueryneedsonlyasmallsubsetofthecolumnsofawide
table,butnosingleindexcontainsallthosecolumns,theprocessorcanusean
indexjointogeneratetherelevantinformationwithoutaccessingthetable,by
joiningseveralindicesthattogethercontaintheneededcolumns.Itperforms
thejoinsashashjoinsontherow-idsfromthedifferentindices.
28.4 QueryProcessing andOptimization 1173
  Clusterandhashclusteraccess.Theprocessoraccessesthedatabyusingthe
clusterkey.
Oracle has several ways to combine information from multiple indices in
a single access path. This ability allows multiple where-clause conditions to be
usedtogethertocomputetheresultsetasef?cientlyaspossible.Thefunctionality
includestheabilitytoperformBooleanoperationsand,or,andminusonbitmaps
representing row-ids. There are also operators that map a list of row-ids into
bitmapsandviceversa,whichallowsregularB-treeindicesandbitmapindicesto
beusedtogetherinthesameaccesspath.Inaddition,formanyqueriesinvolving
count(*)onselectionsonatable,theresultcanbecomputedbyjustcountingthe
bitsthataresetinthebitmapgeneratedbyapplyingthewhereclauseconditions,
withoutaccessingthetable.
Oracle supports several types of joins in the execution engine: inner joins,
outerjoins,semijoins,andantijoins.(AnantijoininOraclereturnsrowsfromthe
left-handsideinputthatdonotmatchanyrowintheright-handsideinput;this
operationiscalledanti-semijoininotherliterature.)Itevaluateseachtypeofjoin
byoneofthreemethods:hashjoin,sort–mergejoin,ornested-loopjoin.
28.4.2 Optimization
Chapter 13 discusses the general topic of query optimization. Here, we discuss
optimizationinthecontextofOracle.
28.4.2.1 QueryTransformations
Oracledoesqueryoptimizationinseveralsteps.Onesuchstepistoperformvar-
iousquerytransformationsandrewritesthatfundamentallychangethestructure
ofthequery.Anotherstepistoperformaccesspathselectiontodetermineaccess
paths, join methods, and join order. Since some transformations are not always
bene?cial, Oracle uses cost-based query transformations where the transforma-
tions and access path selection are interleaved. For each transformation that is
tried, access path selection is performed in order to generate a cost estimate,
andthetransformationisacceptedorrejectedbasedonthecostfortheresulting
executionplan.
SomeofthemajortypesoftransformationsandrewritessupportedbyOracle
areasfollows:
  Viewmerging.Aviewreferenceinaqueryisreplacedbytheviewde?nition.
Thistransformationisnotapplicabletoallviews.
  Complexviewmerging.Oracleoffersthisfeatureforcertainclassesofviews
thatarenotsubjecttoregularviewmergingbecausetheyhaveagroupbyor
select distinctintheviewde?nition.Ifsuchaviewisjoinedtoothertables,
Oracle can commute the joins and the sort or hash operation used for the
groupbyordistinct.
1174 Chapter 28 Oracle
  Subquery?attening.Oraclehasavarietyoftransformationsthatconvertvar-
iousclassesofsubqueriesintojoins,semijoins,orantijoins.Suchconversion
isalsocalled decorrelation,andisdescribedbrie?yinSection13.4.4.
  Materializedviewrewrite.Oraclehastheabilitytorewriteaqueryautomati-
callytotakeadvantageofmaterializedviews.Ifsomepartofthequerycanbe
matchedupwithanexistingmaterializedview,Oraclecanreplacethatpart
ofthequerywithareferencetothetableinwhichtheviewismaterialized.If
needbe,Oracleaddsjoinconditionsor group byoperationstopreservethe
semanticsofthequery.Ifmultiplematerializedviewsareapplicable,Oracle
picks the one that gives the greatest advantage in reducing the amount of
datathathavetobeprocessed.Inaddition,Oraclesubjectsboththerewritten
queryandtheoriginalversiontothefulloptimizationprocessproducingan
executionplanandanassociatedcostestimateforeach.Oraclethendecides
whether to execute the rewritten or the original version of the query on the
basisofthecostestimates.
  Star transformation. Oracle supports a technique for evaluating queries
against star schemas, known as the star transformation. When a query con-
tainsajoinofafacttablewithdimensiontables,andselectionsonattributes
fromthedimensiontables,thequeryistransformedbydeletingthejoincon-
dition between the fact table and the dimension tables, and replacing the
selectionconditiononeachdimensiontablebyasubqueryoftheform:
fact table.fk
i
in
(select pk from dimension table
i
where<conditionson dimension table
i
>)
Onesuchsubqueryisgeneratedforeachdimensionthathassomeconstrain-
ing predicate. If the dimension has a snow?ake schema (see Section 20.2),
the subquery will contain a join of the applicable tables that make up the
dimension.
Oracle uses the valuesthat are returnedfrom each subquery to probe an
index on the corresponding fact table column, getting a bitmap as a result.
Thebitmapsgeneratedfromdifferentsubqueriesarecombined byabitmap
andoperation.Theresultantbitmapcanbeusedtoaccessmatchingfacttable
rows.Hence,onlythoserowsinthefacttablethatsimultaneouslymatchthe
conditionsontheconstraineddimensionswillbeaccessed.Boththedecision
onwhethertheuseofasubqueryforaparticulardimensioniscost-effective,
and the decision on whether the rewritten query is better than the original,
arebasedontheoptimizer’scostestimates.
28.4.2.2 AccessPathSelection
Oracle has a cost-based optimizer that determines join order, join methods, and
access paths. Each operation that the optimizer considers has an associated cost
28.4 QueryProcessing andOptimization 1175
function, and the optimizer tries to generate the combination of operations that
hasthelowestoverallcost.
In estimating the cost of an operation, the optimizer relies on statistics that
have been computed for schema objects such as tables and indices. The statis-
tics contain information about the size of the object, the cardinality, the data
distributionoftablecolumns,andsoforth.Oraclesupportsheight-balancedand
frequencyhistogramsfordatadistributions.Height-balancedhistogramsarealso
referredto as equi-depth histograms,andaredescribedinSection13.3.1.
To facilitatethe collection of optimizerstatistics,Oracle can monitor modi?-
cation activity on tables and keep track of those tables that have been subject to
enough changes that recalculating the statistics may be appropriate. Oracle also
tracks what columns are used in where clauses of queries, which makes them
potential candidates for histogram creation. With a single command, a user can
tellOracletorefreshthestatisticsforthosetablesthatweremarkedassuf?ciently
changed. Oracle uses sampling to speed up the process of gathering the new
statisticsand automatically chooses the smallestadequatesamplepercentage.It
also determineswhether the distribution of the marked columns merits the cre-
ationofhistograms;if thedistributionisclosetouniform, Oracleusesasimpler
representationofthecolumnstatistics.
In some cases, it may be impossible for the optimizer to accurately estimate
theselectivityofaconditioninthe whereclauseofaqueryjustbasedonsimple
column statistics. For example, the condition may be an expression involving a
column,suchas f(col +3)> 5.Anotherclassofproblematicqueriesisthosethat
havemultiplepredicatesoncolumnsthathavesomeformofcorrelation.Assess-
ing the combined selectivity of those predicates may be hard. Oracle therefore
allows statistics to be created for expressions as well as for groups of columns.
Inaddition,Oraclecanaddresstheseissuesthrough dynamic sampling.Theopti-
mizercanrandomlysamplea smallportionof atableand applyalltherelevant
predicatestothesampletoseethepercentageoftherowsthatmatch.Thisfeature
canalsohandletemporarytableswherethelifespanandvisibilityofthedatamay
preventregularstatisticscollection.
Oracle uses both CPU cost and disk I/Os in the optimizer cost model. To
balance the two components, it stores measures about CPU speed and disk I/O
performance as part of the optimizer statistics. Oracle’s package for gathering
optimizerstatisticscomputesthesemeasures.
Forqueriesinvolvinganontrivialnumberofjoins,thesearchspaceisanissue
foraqueryoptimizer.Oracleaddressesthisissueinseveralways.Theoptimizer
generatesaninitialjoinorderandthendecidesonthebestjoinmethodsandaccess
paths for that join order. It then changes the order of the tables and determines
thebestjoinmethodsandaccesspathsforthenewjoinorderandsoforth,while
keeping the best plan that has been found so far. Oracle cuts the optimization
short if the number of different join orders that have been considered becomes
so large that the time spent in the optimizer may be noticeable compared to the
timeitwouldtaketoexecutethebestplanfoundsofar.Sincethiscutoffdepends
on the cost estimate for the best plan found so far, ?nding a good plan early
is important so that the optimization can be stopped after a smaller number of
1176 Chapter 28 Oracle
joinorders,resultinginbetterresponsetime.Oracleusesseveralinitialordering
heuristics to increase the likelihoodthat the ?rst join orderconsideredis a good
one.
For each join order that is considered, the optimizer may make additional
passes over the tables to decide join methods and access paths. Such additional
passes would target speci?c global side effects of the access path selection. For
instance,aspeci?ccombination ofjoinmethodsandaccesspathsmayeliminate
the need to perform an order by sort. Since such a global side effect may not
be obvious when the costs of the different join methods and access paths are
consideredlocally,aseparatepasstargetingaspeci?csideeffectisusedto?nda
possibleexecutionplanwithabetteroverallcost.
28.4.2.3 PartitionPruning
Forpartitionedtables,theoptimizertriestomatchconditionsinthewhereclause
ofaquerywiththepartitioningcriteriaforthetable,inordertoavoidaccessing
partitionsthatarenotneededfortheresult.Forexample,ifatableispartitioned
by date range and the query is constrained to data between two speci?c dates,
the optimizer determines which partitions contain data between the speci?ed
dates and ensures that only those partitions are accessed. This scenario is very
common,andthespeedupcanbedramaticifonlyasmallsubsetofthepartitions
areneeded.
28.4.2.4 SQLTuningAdvisor
In addition to the regular optimization process, Oracle’s optimizer can be used
in tuning mode as part of the SQL Tuning Advisor in order to generate more
ef?cientexecutionplansthanitnormallywould.Thisfeatureisespeciallyuseful
forpackagedapplicationsthatgeneratethesamesetofSQLstatementsrepeatedly
sothatefforttotunethesestatementsforperformancecanhavefuturebene?ts.
Oracle monitors the database activity and automatically stores information
abouthigh-loadSQLstatementsinaworkloadrepository;seeSection28.8.2.High-
load SQL statements are those that use up the most resources because they are
executeda verylarge number of timesor because each executionis very expen-
sive. Such statements are logical candidates for tuning since their impact on the
systemisthegreatest.TheSQLTuningAdvisorcanbeusedtoimprovetheperfor-
manceofthesestatementsbymakingmakingvariouskindsofrecommendations
thatfallintothefollowingdifferentcategories:
  StatisticsAnalysis.Oraclecheckswhetherstatisticsneededbytheoptimizer
aremissingorstaleandmakesrecommendationsforcollectingthem.
  SQL Pro?ling.Apro?leforanSQL statement is a set of information that is
intendedtohelptheoptimizermakebetterdecisionsthenexttimethestate-
mentisoptimized.Anoptimizercansometimesgenerateinef?cientexecution
plansifitisunabletoaccuratelyestimatecardinalitiesandselectivities,some-
thingthatcanhappenbecauseofdatacorrelationortheuseofcertaintypes
28.4 QueryProcessing andOptimization 1177
ofconstructs.Whenrunningtheoptimizerintuningmodetocreateapro?le,
the optimizer tries to verify that its assumptions are correct using dynamic
sampling and partial evaluation of the SQL statement. If it ?nds that there
arestepsintheoptimizationprocesswheretheoptimizer’sassumptionsare
wrong,itwillgenerateacorrectionfactorforthatstepthatwillbecomepart
ofthepro?le.Optimizingintuningmodecanbeverytime-consuming,butit
canbeworthwhileiftheuseofthepro?lesigni?cantlyimprovestheperfor-
manceofthestatement.Ifapro?leiscreated,itwillbestoredpersistentlyand
usedwheneverthestatementisoptimizedinthefuture.Pro?lescanbeused
totuneSQLstatementswithoutchangingthetextofthestatement,something
that is important since it is often impossible for the database administrator
tomodifystatementsgeneratedbyanapplication.
  Access Path Analysis. Based on analysis by the optimizer, Oracle suggests
thecreationofadditionalindicesthatcouldspeedupthestatement.
  SQL Structure Analysis. Oracle suggests changes in the structure of the
SQLstatementthatwouldallowformoreef?cientexecution.
28.4.2.5 SQLPlanManagement
Packaged applications often generate a large number of SQL statements that are
executed repeatedly. If the application is performing adequately, it is common
that database administrators are averse to changes in database behavior. If the
change resultsinbetterperformance,thereislimitedperceivedupsidesincethe
performance was already good enough. On the other hand, if the change leads
to a performance degradation, it may break an application if a critical query
deterioratestoaresponsetimethatisunacceptable.
An example of a change of behavior is a change of an execution plan for
a query. Such a change may be a perfectly legitimate re?ection of changes to
propertiesofthedata,suchasatablehavinggrownmuchlarger.Butthechange
couldalsobeanunintendedconsequenceofanumberofotheractions,suchasa
change in the routines for collecting optimizer statistics or an upgrade to a new
versionofthe RDBMSwithnewoptimizerbehavior.
Oracle’s SQL Plan Management feature addresses the risk associated with
execution plan changes by maintaining a set of trusted execution plans for a
workload and phasing in plans changed by the query optimizer only after they
have been veri?ed not to cause any performance degradations. The feature has
threemajorcomponents:
1. SQLplanbaselinecapture.Oraclecancaptureexecutionplansforawork-
load and store a plan history for each SQL statement. The plan baseline is
a set of plans for a workload with trusted performance characteristics and
against which future plan changes can be compared. A statement could
havemorethanonebaselineplan.
2. SQLplanbaselineselection.AftertheoptimizergeneratesaplanforanSQL
statement, it checks whether there exists a baseline plan for the statement.
1178 Chapter 28 Oracle
Ifthestatementexistsinthebaselinebutthenewplanisdifferentfromany
existing one, the baseline plan that the optimizer considers to be the best
will be used. The newly generated plan will be added to the plan history
forthestatementandcouldbecomepartofafuturebaseline.
3. SQL plan baseline evolution. Periodically, it may make sense to try to
make newly generated execution plans part of the trusted plans in the
baseline.Oraclesupportsaddingnewplanstothebaselinewithorwithout
veri?cation. If veri?cation is the chosen option, Oracle will execute a newly
generated plan and compare its performance to the baseline in order to
makesureitdoesnotcauseperformanceregressions.
28.4.3 ParallelExecution
OracleallowstheexecutionofasingleSQLstatementtobeparallelizedbydividing
theworkbetweenmultipleprocessesonamultiprocessorcomputer.Thisfeature
isespeciallyusefulforcomputationallyintensiveoperationsthatwouldotherwise
takeanunacceptablylongtimetoperform.Representativeexamplesaredecision
support queriesthat needto process large amounts of data, data loads in a data
warehouse,andindexcreationorrebuild.
Inordertoachievegoodspeedupthroughparallelism,itisimportantthatthe
work involved in executing the statement be divided into granules that can be
processed independently by the different parallel processors. Depending on the
typeofoperation,Oraclehasseveralwaystosplitupthework.
Foroperationsthataccessbaseobjects(tablesandindices),Oraclecandivide
theworkbyhorizontalslicesofthedata.Forsomeoperations,suchasafulltable
scan,eachsuchslicecanbearangeofblocks—eachparallelqueryprocessscans
thetablefromtheblockatthestartoftherangetotheblockattheend.Forsome
operations on a partitioned table, such as an index range scan, the slice would
be a partition.Parallelismbasedon block ranges ismore?exiblesince thesecan
be determineddynamically based on a variety of criteria and are not tied to the
tablede?nition.
Joinscanbeparallelizedinseveraldifferentways.Onewayistodivideoneof
theinputstothejoinbetweenparallelprocessesandleteachprocessjoinitsslice
with the other input to the join; this is the asymmetric fragment-and-replicate
methodofSection18.5.2.2.Forexample,ifalargetableisjoinedtoasmalloneby
ahash join,Oracle dividesthelargetableamong theprocessesandbroadcastsa
copyofthesmalltabletoeachprocess,whichthenjoinsitsslicewiththesmaller
table.Ifbothtablesarelarge,itwouldbeprohibitivelyexpensivetobroadcastone
ofthemtoallprocesses.Inthatcase,Oracleachievesparallelismbypartitioning
the data among processes by hashing on the values of the join columns (the
partitionedhash-joinmethodofSection18.5.2.1).Eachtableisscannedinparallel
by a set of processes and each row in the output is passed on to one of a set of
processesthataretoperformthejoin.Whichoneoftheseprocessesgetstherow
is determined by a hash function on the values of the join column. Hence, each
joinprocessgetsonlyrowsthatcouldpotentiallymatch,andnorowsthatcould
matchcouldendupindifferentprocesses.
28.4 QueryProcessing andOptimization 1179
Oracle parallelizes sort operations by value ranges of the column on which
thesortisperformed(thatis,usingtherange-partitioningsortofSection18.5.1).
Eachprocessparticipatinginthesortissentrowswithvaluesinitsrange,andit
sortstherowsinitsrange.Tomaximizethebene?tsofparallelism,therowsneed
tobedividedasevenlyaspossibleamongtheparallelprocesses,andtheproblem
of determining range boundaries that generates a good distribution then arises.
Oracle solves the problem by dynamically sampling a subset of the rows in the
inputtothesortbeforedecidingontherangeboundaries.
28.4.3.1 ProcessStructure
Theprocessesinvolvedintheparallelexecutionofan SQLstatementconsistofa
coordinator process and a number of parallel server processes. The coordinator
is responsible for assigning work to the parallel servers and for collecting and
returning data to the user process that issued the statement. The degree of par-
allelismis thenumber of parallelserverprocessesthat areassignedtoexecutea
primitive operation as part of the statement. The degree of parallelism is deter-
minedbytheoptimizer,butcanbethrottledbackdynamicallyiftheloadonthe
systemincreases.
The parallel servers operate on a producer/consumer model. When a se-
quenceofoperationsisneededtoprocessastatement,theproducersetofservers
performs the ?rst operation and passes the resulting data to the consumer set.
For example, if a full table scan is followed by a sort and the degree of paral-
lelism is 32, there would be 32 producer servers performing the table scan and
passing the result to 32 consumer servers that perform the sort. If a subsequent
operation is needed, such as another sort, the roles of the two sets of servers
switch. The servers that originally performed the table scan take on the role of
consumers of the output produced by the ?rst sort and use it to perform the
secondsort.Hence,asequenceofoperationsproceedsbypassingdatabackand
forth between two sets of servers that alternate in their roles as producers and
consumers. The servers communicate with each other through memory buffers
on shared-memory hardware and through high-speed network connections on
MPP(sharednothing)con?gurationsandclustered(shareddisk)systems.
Forshared-nothingsystems,thecostofaccessingdataondiskisnotuniform
amongprocesses.Aprocessrunningonanodethathasdirectaccesstoadevice
isabletoprocessdataonthatdevicefasterthanaprocessthathastoretrievethe
dataoveranetwork.Oracleusesknowledgeaboutdevice-to-nodeanddevice-to-
processaf?nity—thatis,theabilitytoaccessdevicesdirectly—whendistributing
workamongparallelexecutionservers.
28.4.4 ResultCaching
Oracle’s result caching feature allows the result of a query or query block (e.g.,
a view referenced in a query) to be cached in memory and reused if the same
query is executedagain. Updatesof the data in the underlying tables invalidate
the cached results, so this feature works best for queries against tables that are
relatively static and where the result sets are relatively small. Consider, as a
1180 Chapter 28 Oracle
usage example, some part of a Web page that is stored in the database and
doesnotchangeveryfrequentlycomparedtohowoftenitaccessed.Forsuchan
application,resultcachingwouldbeamuchmorelightweightalternativetousing
materialized views, which would require explicitly creating and administering
newpersistentdatabaseobjects.
28.5 ConcurrencyControlandRecovery
Oracle supports concurrency control and recovery techniques that provide a
numberofusefulfeatures.
28.5.1 Concurrency Control
Oracle’s multiversion concurrency control mechanism is based on the snapshot
isolation protocol described in Section 15.7. Read-only queries are given a read-
consistent snapshot, which is a view of the database as it existed at a speci?c
point in time, containing all updates that were committed by that point in time,
and not containing any updates that were not committed at that point in time.
Thus, read locks are not used and read-only queries do not interfere with other
databaseactivityintermsoflocking.
Oracle supports both statement- and transaction-level read consistency: at
the beginning of the execution of either a statement or a transaction (depend-
ing on what level of consistency is used), Oracle determines the current system
changenumber(SCN).The SCNessentiallyactsasatimestamp,wherethetimeis
measuredintermsoftransactioncommitsinsteadofwall-clocktime.
If in the course of a query a data block is found that has a higher SCN than
theonebeingassociatedwiththequery,itisevidentthatthedatablockhasbeen
modi?edafterthetimeoftheoriginalquery’sSCNbysomeothertransactionthat
mayormaynothavecommitted.Hence,thedataintheblockcannotbeincluded
in a consistent view of the database as it existed at the time of the query’s SCN.
Instead, an older version of the data in the block must be used; speci?cally, the
onethathasthehighestSCN that does not exceed the SCN of the query. Oracle
retrieves that version of the data from the undo segment (undo segments are
described in Section 28.5.2). Hence, provided that the undo space is suf?ciently
large,Oraclecanreturnaconsistentresultofthequeryevenifthedataitemshave
been modi?edseveraltimessince the query startedexecution. Should the block
with the desired SCN no longer exist in the undo, the query will return an error.
Itwould be an indicationthat the undotable space has not beenproperlysized,
giventheactivityonthesystem.
In the Oracle concurrency model, read operations do not block write opera-
tions and write operations do not block read operations, a property that allows
a high degree of concurrency. In particular, the scheme allows for long-running
queries(forexample,reportingqueries)to runona systemwitha largeamount
of transactional activity. This kind of scenario is often problematic for database
systems where queries use read locks, since the query may either fail to acquire
28.5 ConcurrencyControl and Recovery 1181
them or lock large amounts of data for a long time, thereby preventing transac-
tionalactivityagainstthatdataandreducingconcurrency.(Analternativethatis
usedinsomesystemsistousealowerdegreeofconsistency,suchasdegree-two
consistency,butthatcouldresultininconsistentqueryresults.)
Oracle’sconcurrencymodelisusedasabasisforthe ?ashbackfeature.This
featureallowsausertosetacertainSCNnumberorwall-clocktimeinhissession
and perform operations on the data that existed at that point in time (provided
that the data still exist in the undo). Normally in a database system, once a
change has been committed, there is no way to get back to the previous state of
the data other than performing point-in-time recovery from backups. However,
recovery of a very large database can be very costly, especially if the goal is just
to retrieve some data item that had been inadvertently deleted by a user. The
?ashback feature provides a much simpler mechanism to deal with user errors.
The?ashbackfeatureincludestheabilitytorestoreatableoranentiredatabaseto
an earlierpoint in timewithout recoveringfrom backups, the ability to perform
queriesonthedataastheyexistedatanearlierpointintime,theabilitytotrack
howoneormorerowshavechangedovertime,andtheabilitytoexaminechanges
tothedatabaseatthetransactionlevel.
Itmaybedesirabletobeabletotrackchangestoatablebeyondwhatwould
bepossiblethroughnormalundoretention.(Forinstance,corporategovernance
regulations may require that such changes be trackable for a certain number of
years.)Forthispurpose,atablecanbetrackedbythe ?ashback archivefeature,
which creates an internal, history version of the table. A background process
convertstheundoinformationintoentriesinthehistorytable,whichcanbeused
toprovide?ashbackfunctionalityforarbitrarilylongperiodsoftime.
Oraclesupportstwo ANSI/ISOisolationlevels,read committedand serializ-
able.Thereisnosupportfordirtyreadssinceitisnotneeded.Statement-levelread
consistencycorrespondstothereadcommittedisolationlevel,whiletransaction-
levelreadconsistencycorrespondstotheserializableisolationlevel.Theisolation
level can be set for a session or an individual transaction. Statement-level read
consistency(thatis,readcommitted)isthedefault.
Oracleusesrow-levellocking.Updatestodifferentrowsdonotcon?ict.Iftwo
writersattempttomodifythesamerow,onewaitsuntiltheothereithercommits
or is rolled back, and then it can either return a write-con?ict error or go ahead
andmodifytherow;write-con?icterrorsaredetectedbasedonthe?rst-updater-
wins version of snapshot isolation, described in Section 15.7. (Section 15.7 also
describescertaincasesofnon-serializableexecutionthatcanoccurwithsnapshot
isolation,andoutlinestechniquesforpreventingsuchproblems.)Locksareheld
forthedurationofatransaction.
In addition to row-level locks that prevent inconsistencies due to DML ac-
tivity, Oracle uses table locks that prevent inconsistencies due to DDL activity.
These locks prevent one user from, say, dropping a table while another user
has an uncommitted transaction that isaccessing that table. Oracle does not use
lock escalation to convert row locks to table locks for the purpose of its regular
concurrencycontrol.
1182 Chapter 28 Oracle
Oracledetectsdeadlocksautomaticallyandresolvesthembyrollingbackone
ofthetransactionsinvolvedinthedeadlock.
Oracle supports autonomous transactions, which are independent transac-
tionsgeneratedwithinothertransactions.WhenOracle invokesanautonomous
transaction, it generates a new transaction in a separate context. The new trans-
actioncanbeeithercommittedorrolledbackbeforecontrolreturnstothecalling
transaction. Oracle supports multiple levels of nesting of autonomous transac-
tions.
28.5.2 BasicStructures forRecovery
Oracle’sFlashbacktechnology,describedinSection28.5.1,canbeusedasarecov-
erymechanism,butOraclealsosupportsmediarecoverywhere?lesarebacked
up physically. We describe this more traditional form of backup and recovery
here.
In order to understand how Oracle recovers from a failure, such as a disk
crash, it is important to understand the basic structures that are involved. In
addition to the data ?les that contain tables and indices, there are control ?les,
redologs,archivedredologs,andundosegments.
The control ?le contains various metadata that are needed to operate the
database,includinginformationaboutbackups.
Oraclerecordsanytransactionalmodi?cationofadatabasebufferintheredo
log, which consists of two or more ?les. It logs the modi?cation as part of the
operation that causes it and regardless of whether the transaction eventually
commits.Itlogschangestoindicesandundosegmentsaswellaschangestotable
data. As the redo logs ?ll up, they are archived by one or several background
processes(ifthedatabaseisrunninginarchivelogmode).
Theundosegmentcontainsinformationaboutolderversionsofthedata(that
is, undo information). In addition to its role in Oracle’s consistency model, the
information is used to restore the old version of data items when a transaction
thathasmodi?edthedataitemsisrolledback.
To be able to recover from a storage failure, the data ?les and control ?les
should be backed up regularly. The frequency of the backup determines the
worst-case recovery time, since it takes longer to recover if the backup is old.
Oraclesupportshotbackups—backups performedonanonlinedatabasethatis
subjecttotransactionalactivity.
Duringrecoveryfromabackup,Oracleperformstwostepstoreachaconsis-
tent state of the database as it existed just prior to the failure. First, Oracle rolls
forwardbyapplyingthe(archived)redologstothebackup.Thisactiontakesthe
databasetoastatethatexistedatthetimeofthefailure,butnotnecessarilyacon-
sistent state since the redo logs include uncommitted data. Second, Oracle rolls
backuncommittedtransactionsbyusingtheundosegmentdata.Thedatabaseis
nowinaconsistentstate.
Recovery on a database that has been subject to heavy transactional activity
since the last backup can be time-consuming. Oracle supports parallel recovery
in which several processes are used to apply redo information simultaneously.
28.6 SystemArchitecture 1183
Oracle provides a GUI tool, Recovery Manager, which automates most tasks as-
sociatedwithbackupandrecovery.
28.5.3 OracleData Guard
To ensure high availability, Oracle provides a standby database feature, data
guard.(Thisfeatureisthesameasremotebackups,describedinSection16.9.)A
standbydatabaseisacopyoftheregulardatabasethatisinstalledonaseparate
system. If a catastrophic failure occurs on the primary system, the standby sys-
tem is activated and takes over, thereby minimizing the effect of the failure on
availability. Oracle keeps the standby database up-to-date by constantly apply-
ing archived redo logs that are shipped from the primary database. The backup
database can be brought online in read-only mode and used for reporting and
decisionsupportqueries.
28.6 SystemArchitecture
WheneveradatabaseapplicationexecutesanSQLstatement,thereisanoperating
systemprocessthatexecutescodeinthedatabaseserver.Oraclecanbecon?gured
so that the operating system process is dedicated exclusively to the statement it
is processing or so that the process can be shared among multiple statements.
The latter con?guration, known as the shared server, has somewhat different
propertieswithregardtotheprocessandmemoryarchitecture.Weshalldiscuss
the dedicated server architecture?rst andthemultithreadedserverarchitecture
later.
28.6.1 Dedicated Server:MemoryStructures
The memory used by Oracle falls mainly into three categories: software code
areas, which are the parts of the memory where the Oracle server code resides,
thesystemglobalarea(SGA),andtheprogramglobalarea(PGA).
A PGA is allocated for each process to hold its local data and control infor-
mation. This area contains stack space for various session data and the private
memory for the SQLstatementthatitisexecuting.Italsocontainsmemoryfor
sortingandhashingoperationsthatmayoccurduringtheevaluationofthestate-
ment.Theperformanceofsuchoperationsissensitivetotheamountofmemory
thatisavailable.Forexample,ahashjointhatcanbeperformedinmemorywill
be faster than if it is necessary to spill to disk. Since there can be a large num-
berofsortingandhashingoperationsactivesimultaneously(becauseofmultiple
queries as well as multiple operations within each query), deciding how much
memoryshouldbeallocatedtoeachoperationisnontrivial,especiallyastheload
on the system may ?uctuate. Underallocation of memory can lead to extra disk
I/Os if an operation needlessly spills to disk and overallocation of memory can
leadtothrashing.Oracleletsthedatabaseadministratorspecifya targetparame-
terforthetotalamountofmemorythatshouldbeconsideredavailableforthese
operations.Thesizeofthistargetwouldtypicallybebasedonthetotalamountof
1184 Chapter 28 Oracle
memoryavailableonthesystemandsomecalculationastohowitshouldbedi-
videdbetweenvariousOracleandnon-Oracleactivities.Oraclewilldynamically
decidethebestwaytodividethememoryavailablewithinthetargetbetweenthe
activeoperationsinordertomaximizethroughput.Thememoryallocationalgo-
rithmknowstherelationshipbetweenmemoryandperformanceforthedifferent
operationsandseekstoensurethattheavailablememoryisusedasef?cientlyas
possible.
The SGA is a memory area for structures that are shared among users. It is
madeupofseveralmajorstructures,includingthefollowing.
  Buffer cache. This cache keepsfrequently accessed data blocks (from tables
aswellasindices)inmemorytoreducetheneedtoperformphysicaldiskI/O.
A least recently used replacement policy is used except for blocks accessed
during a full table scan. However,Oracle allows multiplebuffer pools to be
createdthathavedifferentcriteriaforagingoutdata.SomeOracleoperations
bypassthebuffercacheandreaddatadirectlyfromdisk.
  Redologbuffer.Thisbuffercontainsthepartoftheredologthathasnotyet
beenwrittentodisk.
  Sharedpool.Oracleseekstomaximizethenumberofusersthatcanusethe
databaseconcurrentlybyminimizingtheamountofmemorythatisneeded
for each user. One important concept in this context is the ability to share
theinternalrepresentationof SQLstatementsandproceduralcodewrittenin
PL/SQL.WhenmultipleusersexecutethesameSQLstatement,theycanshare
mostdatastructuresthatrepresenttheexecutionplanforthestatement.Only
datathatarelocaltoeachspeci?cinvocationofthestatementneedtobekept
inprivatememory.
The sharable parts of the data structures representing the SQL statement
arestoredinthesharedpool,includingthetextofthestatement.Thecaching
of SQL statements in the shared pool also saves compilation time, since a
new invocation of a statement that is already cached does not have to go
through the complete compilation process. The determination of whether
an SQL statement is the same as one existing in the shared pool is based
on exact text matching and the setting of certain session parameters.Oracle
canautomaticallyreplaceconstantsinan SQLstatementwithbindvariables;
future queries that are the same except for the values of constants will then
matchtheearlierqueryinthesharedpool.
Thesharedpoolalsocontainscachesfordictionaryinformationandvarious
controlstructures.Cachingdictionarymetadataisimportantforspeedingup
thecompilationtimefor SQLstatements.Inaddition,thesharedpoolisused
forOracle’sresultcachefeature.
28.6.2 Dedicated Server:ProcessStructures
There are two types of processes that execute Oracle server code: server pro-
cesses that process SQL statements and background processes that perform var-
28.6 SystemArchitecture 1185
ious administrative and performance-related tasks. Some of these processes are
optional, and in some cases, multiple processes of the same type can be used
forperformancereasons.Oraclecangenerateabouttwodozendifferenttypesof
backgroundprocesses.Someofthemostimportantonesare:
  Databasewriter.Whenabufferisremovedfromthebuffercache,itmustbe
writtenbacktodiskifithasbeenmodi?edsinceitenteredthecache.Thistask
is performed by the database writer processes, which help the performance
ofthesystembyfreeingupspaceinthebuffercache.
  Log writer. The log-writer process writes entries in the redo log buffer to
the redo log ?le on disk. It also writes a commit record to disk whenever a
transactioncommits.
  Checkpoint. The checkpoint process updates the headers of the data ?le
whenacheckpointoccurs.
  System monitor. This process performs crash recovery if needed. It also
performs some space management to reclaim unused space in temporary
segments.
  Processmonitor.Thisprocessperformsprocessrecoveryforserverprocesses
thatfail,releasingresourcesandperformingvariouscleanupoperations.
  Recoverer.Therecovererprocessresolvesfailuresandconductscleanupfor
distributedtransactions.
  Archiver.Thearchivercopiestheonlineredolog?letoanarchivedredolog
everytimetheonlinelog?le?llsup.
28.6.3 SharedServer
Theshared-servercon?gurationincreasesthenumberofusersthatagivennum-
berofserverprocessescansupportbysharingserverprocessesamongstatements.
Itdiffersfromthededicatedserverarchitectureinthesemajoraspects:
  A background dispatch process routes user requests to the next available
serverprocess.Indoing so,it usesarequestqueueand aresponsequeuein
theSGA.Thedispatcherputsanewrequestintherequestqueuewhereitwill
bepickedupbyaserverprocess.Asaserverprocesscompletesarequest,it
puts the result in the response queueto be picked up by the dispatcher and
returnedtotheuser.
  SinceaserverprocessissharedamongmultipleSQLstatements,Oracledoes
notkeepprivatedatainthePGA.Instead,itstoresthesession-speci?cdatain
the SGA.
1186 Chapter 28 Oracle
28.6.4 OracleRealApplicationClusters
OracleRealApplicationClusters(RAC)isafeaturethatallowsmultipleinstances
of Oracle to run against the same database. (Recall that, in Oracle terminology,
aninstanceisthecombinationofbackgroundprocessesandmemoryareas.)This
featureenablesOracletorunonclusteredandMPP(shareddiskandsharednoth-
ing) hardware architectures. The ability to cluster multiplenodes has important
bene?ts for scalability and availability that are useful in both OLTP and data
warehousingenvironments.
The scalability bene?ts of the feature are obvious, since more nodes mean
moreprocessingpower.Onshared-nothingarchitectures,addingnodestoaclus-
ter typically requires redistributing the data between the nodes. Oracle uses a
shared-disk architecture where all the nodes have access to all the data and as
a result, more nodes can be added to a RAC cluster without worrying how the
data should be divided between the nodes. Oracle further optimizes the use of
thehardwarethroughfeaturessuchasaf?nityandpartitionwisejoins.
RAC can also be used to achieve high availability. If one node fails, the re-
maining ones are still available to the application accessing the database. The
remaining instances will automatically roll back uncommitted transactions that
werebeingprocessedonthefailednodeinordertopreventthemfromblocking
activityontheremainingnodes.RACalsoallowsrollingpatchingsothatsoftware
patchescanbeappliedtoonenodeatatimewithoutdatabasedowntime.
Oracle’s shared-disk architecture avoids many of the issues that shared-
nothing architectures have with data on disk either being local to a node or
not. Still, having multiple instances run against the same database gives rise to
sometechnicalissuesthatdonotexistonasingleinstance.Whileitissometimes
possibleto partitionan application among nodes sothat nodes rarelyaccess the
same data, there is always the possibility of overlaps, which affects cache man-
agement. In order to achieve ef?cient cache management over multiple nodes,
Oracle’s cache fusion feature allows data blocks to ?ow directly among caches
ondifferentinstancesusingtheinterconnect,withoutbeingwrittentodisk.
28.6.5 Automatic Storage Manager
The Automatic Storage Manager (ASM) is a volume manager and ?le system
developed by Oracle. While Oracle can be used with other volume managers
and ?le systems as well as raw devices, ASM is speci?cally designed to simplify
storagemanagementfortheOracledatabasewhileoptimizingperformance.
ASM manages collections of disks,known as disk groups,andexposesa?le
system interface to the database. (Recall that an Oracle table space is de?ned in
terms of data ?les.) Examples of what could constitute ASM disks include disks
or partitions of disk arrays, logical volumes, and network attached ?les. ASM
automaticallystripesthedataoverthedisksinadiskgroupandprovidesseveral
optionsfordifferentlevelsofmirroring.
If the disk con?guration changes, e.g., when more disks are added to in-
creasestoragecapacity,a diskgroupmay needtoberebalancedso thatthedata
are spread evenly over all the disks. The rebalancing operation can be done in
28.6 SystemArchitecture 1187
the background while the database remains fully operational and with minimal
impactondatabaseperformance.
28.6.6 OracleExadata
Exadata is a set of Oracle libraries that can run on the storage array CPUson
certain types of storage hardware. While Oracle is fundamentally based on a
shared-disk architecture, Exadata contains a shared-nothing ?avor in that some
operationsthatwouldnormallybeexecutedonthedatabaseserveraremovedto
storagecellsthatcanonlyaccessdatathatarelocaltoeachcell.(Eachstoragecell
consistsofanumberofdisksandseveralmulticore CPUs.)
Thearemajoradvantagestoof?oadingcertaintypesofprocessingtostorage
CPUs:
  It allows a large, but relatively inexpensive, expansion of the amount of
processingpowerthatisavailable.
  The amount of data that needs to be transferred from a storage cell to the
database server can be dramatically reduced, which can be very important
sincethebandwidthbetweenthestoragecellanddatabaseserverisusually
expensiveandoftenabottleneck.
WhenexecutingaqueryagainstExadatastorage,thereductionoftheamount
of data that needs to be retrieved comes from several techniques that can be
pushedtothestoragecellsandexecutedtherelocally:
  Projection. A table may have hundreds of columns, but a given query may
onlyneedtoaccessaverysmallsubsetofthem.Thestoragecellscanproject
out the unneeded columns and only send the relevant ones back to the
databaseserver.
  Table?ltering.Thedatabaseservercansendalistofpredicatesthatarelocal
to a table to the storage cells and only rows matching these predicates get
sentbacktotheserver.
  Join ?ltering. The ?ltering mechanism allows for predicates that are Bloom
?ltersallowingrowstobe?lteredoutbasedonjoinconditionsaswell.
In combination, of?oading these techniques to the storage cells can speed
up query processing by orders of magnitude. It requires that the storage cells,
in addition to sending back regular, unaltered database blocks to the server,
can send back a compacted version where certain columns and rows have been
removed.ThisabilityinturnrequiresthestoragesoftwaretounderstandOracle’s
block format and data types, and to include Oracle’s expression and predicate
evaluationroutines.
Inadditiontoprovidingbene?tsforqueryprocessing,Exadatacanalsospeed
up incremental backups by performing block-level change tracking and only
1188 Chapter 28 Oracle
returning blocks that have changed. Also, the work of formatting extents when
creatinganewtablespaceisof?oadedtoExadatastorage.
ExadatastoragesupportsallregularOraclefeatures,anditispossibletohave
adatabasethatincludesbothExadataandnon-Exadatastorage.
28.7 Replication, Distribution,andExternalData
Oracle provides support for replication and distributed transactions with two-
phasecommit.
28.7.1 Replication
Oraclesupportsseveraltypesofreplication.(SeeSection19.2.1foranintroduction
to replication.) In one form, data in a master site are replicated to other sites in
the form of materialized views. A materialized view does not have to contain all
the master data—it can, for example, exclude certain columns from a table for
securityreasons.Oraclesupportstwotypesofmaterializedviewsforreplication:
read-onlyand updatable.Anupdatablematerializedviewcanbemodi?edandthe
modi?cations propagated to the master table. However, read-only materialized
views allow for a wider range of view de?nitions. For instance, a read-only
materialized view can be de?ned in terms of set operations on tables at the
master site. Changes to the master data are propagated to the replicas through
thematerializedviewrefreshmechanism.
Oraclealsosupportsmultiplemastersitesforthesamedata,whereallmaster
sitesactaspeers.Areplicatedtablecanbeupdatedatanyofthemastersitesand
theupdateispropagatedtotheothersites.Theupdatescanbepropagatedeither
asynchronouslyorsynchronously.
Forasynchronousreplication,theupdateinformationissentinbatchestothe
othermastersitesandapplied.Sincethesamedatacouldbesubjecttocon?icting
modi?cations at different sites, con?ict resolution based on some business rules
mightbeneeded.Oracleprovidesanumberofbuilt-incon?ictresolutionmethods
andallowsuserstowritetheirownifneedbe.
With synchronous replication, an update to one master site is propagated
immediatelytoallothersites.
28.7.2 Distributed Databases
Oraclesupportsqueriesandtransactionsspanningmultipledatabasesondiffer-
entsystems.Withtheuseofgateways,theremotesystemscanincludenon-Oracle
databases.Oraclehasbuilt-incapabilitytooptimizeaquerythatincludestables
atdifferentsites,retrievetherelevantdata,andreturntheresultasifithadbeen
a normal, local query. Oracle also transparently supports transactions spanning
multiplesitesbyabuilt-intwo-phase-commitprotocol.
28.8 DatabaseAdministration Tools 1189
28.7.3 ExternalDataSources
Oracle has several mechanisms for supporting external data sources. The most
commonusageisindatawarehousingwhenlargeamountsofdataareregularly
loadedfromatransactionalsystem.
28.7.3.1 SQL*Loader
Oracle has a direct-load utility, SQL*Loader, that supports fast parallel loads of
large amounts of data from external ?les. It supports a variety of data formats
anditcanperformvarious?lteringoperationsonthedatabeingloaded.
28.7.3.2 ExternalTables
Oracleallowsexternaldatasources,suchas?at?les,tobereferencedinthefrom
clause of a query as if they were regular tables. An external table is de?ned by
metadatathatdescribetheOraclecolumntypesandthemappingoftheexternal
data into those columns. An access driver is also needed to access the external
data.Oracleprovidesadefaultdriverfor?at?les.
Theexternaltablefeatureisprimarilyintendedforextraction,transformation,
and loading (ETL) operations in a data warehousing environment. Data can be
loadedintothedatawarehousefroma?at?leusing
create table tableas
select...from<externaltable>
where...
Byaddingoperationsonthedataineithertheselectlistorwhereclause,trans-
formationsand?lteringcanbedoneaspartofthesameSQLstatement.Sincethese
operationscanbeexpressedeitherinnativeSQLorinfunctionswritteninPL/SQL
or Java, the external table feature provides a very powerful mechanism for ex-
pressingallkindsofdatatransformationand?lteringoperations.Forscalability,
theaccessto theexternaltablecanbeparallelizedbyOracle’sparallelexecution
feature.
28.7.3.3 DataPumpExportandImport
Oracle provides an export utility for unloading data and metadata into dump
?les.These?lesareregular?lesusingaproprietaryformatthatcanbemovedto
anothersystemandloadedintoanotherOracledatabaseusingthecorresponding
importutility.
28.8 DatabaseAdministrationTools
Oracle provides users a number of tools and features for system management
and application development. In recent releases of Oracle, a lot of emphasis
was put on the concept of manageability, that is, reducing the complexity of all
1190 Chapter 28 Oracle
aspects of creating and administering an Oracle database. This effort covered a
wide variety of areas, including database creation, tuning, space management,
storagemanagement,backupandrecovery,memorymanagement,performance
diagnostics,andworkloadmanagement.
28.8.1 OracleEnterpriseManager
Oracle Enterprise Manager (OEM) is Oracle’s main tool for database systems
management. It provides an easy-to-use graphical user interface for most tasks
associated with administering an Oracle database including con?guration, per-
formance monitoring, resource management, security management, and access
to the various advisors. In addition to database management, OEM provides in-
tegratedmanagementofOracle’sapplicationsandmiddlewaresoftwarestack.
28.8.2 Automatic Workload Repository
TheAutomaticWorkloadRepository(AWR)isoneofthecentralpiecesofinfras-
tructure for Oracle’s manageability effort. Oracle monitors the activity on the
database system and records a varietyof information relating to workloads and
resourceconsumptionandrecordsthemin AWR atregularintervals.Bytracking
thecharacteristicsofaworkloadovertime,Oraclecandetectandhelpdiagnose
deviationsfrom normal behaviorsuch as a signi?cant performance degradation
ofaquery,lockcontention,and CPUbottlenecks.
The information recorded in AWR provides a basis for a variety of advisors
that provide analysis of various aspects of the performance of the system and
advice for how it can be improved.Oracle has advisors for SQL tuning, creating
access structures, such as indices and materialized views, and memory sizing.
Oraclealsoprovidesadvisorsforsegmentdefragmentationandundosizing.
28.8.3 Database Resource Management
A database administrator needs to be able to control how the processing power
of the hardware is divided among users or groups of users. Some groups may
execute interactive queries where response time is critical; others may execute
long-running reports that can be run as batch jobs in the background when
the system load is low. It is also important to be able to prevent a user from
inadvertentlysubmitting anextremelyexpensivead hoc querythat will unduly
delayotherusers.
Oracle’sDatabaseResourceManagementfeatureallowsthedatabaseadmin-
istrator to divide users into resource consumer groups with different priorities
and properties. For example, a group of high-priority, interactive users may be
guaranteed at least 60 percent of the CPU. The remainder, plus any part of the
60 percent not used up by the high-priority group, would be allocated among
resourceconsumergroupswithlowerpriority.Areallylow-prioritygroupcould
get assigned 0 percent, which would mean that queries issued by this group
wouldrunonlywhentherearespare CPUcyclesavailable.Limitsforthedegree
of parallelism for parallel execution can be set for each group. The database ad-
Bibliographical Notes 1191
ministrator can also set time limits for how long an SQL statement is allowed
to run for each group. When a user submits a statement, the resource manager
estimates how long it would take to execute it and returns an error if the state-
ment violates the limit. The resource manager can also limit the number of user
sessionsthatcanbeactiveconcurrentlyforeachresourceconsumergroup.Other
resourcesthatcanbecontrolledbytheresourcemanagerincludeundospace.
28.9 DataMining
OracleDataMiningprovidesavarietyofalgorithmsthatembedthedatamining
processinsidethedatabasebothforbuildingamodelonatrainingsetofdataand
for applying the model for scoring the actual production data. The fact the data
never needs to leave the database is a signi?cant advantage compared to using
other data mining engines. Having to extract and insert potentially very large
datasetsintoaseparateengineiscumbersome,costly,andmaypreventnewdata
from being scored instantaneously as they are entered into the database. Oracle
providesalgorithmsforbothsupervisedandunsupervisedlearningincluding:
  Classi?cation—NaiveBayes,generalizedlinearmodels,SupportVectorMa-
chines,andDecisionTrees.
  Regression—Supportvectormachinesandgeneralizedlinearmodels.
  Attributeimportance—Minimumdescriptionlength.
  Anomalydetection—Oneclasssupportvectormachines.
  Clustering—Enhancedk-meansclusteringandorthogonalPartitioningClus-
tering.
  Associationrules—Apriori.
  Featureextraction—Nonnegativematrixfactorization.
In addition, Oracle provides a wide range of statistical functions inside the
database covering areas including linear regression, correlation, cross tabs, hy-
pothesistesting,distribution?tting,andParetoanalysis.
Oracleprovidestwointerfacestothedataminingfunctionality,onebasedon
JavaandonethatisbasedonOracle’sprocedurallanguagePL/SQL.Onceamodel
has been built on an Oracle database, it can be shippedto be deployedon other
Oracledatabases.
BibliographicalNotes
Up-to-date product information, including documentation, on Oracle products
canbefoundattheWebsiteshttp://www.oracle.comand http://technet.oracle.com.
1192 Chapter 28 Oracle
Oracle’s intelligent algorithms for allocating available memory for opera-
tions such as hashing and sorting are discussed in Dageville and Za¨ ?t [2002].
Murthy and Banerjee [2003] discussed XML schemas. Table compression in Ora-
cleisdescribedinP¨ ossandPotapov[2003].AutomaticSQLtuningisdescribedin
Dageville et al. [2004]. The optimizer’s cost-based query transformation frame-
work is described in Ahmed et al. [2006]. The SQL Plan-Management feature is
discussedinZiauddinetal.[2008].Antoshenkov[1995]describesthebyte-aligned
bitmapcompressiontechniqueusedinOracle;seealsoJohnson[1999].
CHAPTER
29
IBM DB2 Universal Database
Sriram Padmanabhan
IBM’s DB2 Universal Database family of products consists of ?agship database
serversandsuitesofrelatedproductsforbusinessintelligence,informationinte-
gration, and content management. The DB2 Universal Database Server is avail-
ableonavarietyofhardwareandoperating-systemplatforms.Thelistofserver
platforms supported includes high-end systems such as mainframes, massively
parallel processors (MPP), and large symmetric multiprocessors (SMP) servers;
medium-scalesystemssuchasfour-wayandeight-way SMPs;workstations;and
evensmallhandhelddevices.OperatingsystemsthataresupportedincludeUnix
variantssuchasLinux,IBMAIX,Solaris,andHP-UX,aswellasMicrosoftWindows,
IBMMVS,IBMVM,IBMOS/400,andanumberofothers.TheDB2Everyplaceedition
supports operating systems such as PalmOS and Windows CE. There is even a
no-charge (free) version of DB2 called DB2 Express-C. Applications can migrate
seamlesslyfromthelow-endplatformstohigh-endserversbecauseoftheporta-
bilityoftheDB2interfacesandservices.Besidesthecoredatabaseengine,theDB2
family consists of several other products that provide tooling, administration,
replication,distributeddataaccess,pervasivedataaccess,OLAP,andmanyother
features.Figure29.1describesthedifferentproductsinthefamily.
29.1 Overview
The origin of DB2 can be traced back to the System R project at IBM’s Almaden
ResearchCenter(thencalledtheIBMSanJoseResearchLaboratory).The?rstDB2
product was released in 1984 on the IBM mainframe platform, and this was fol-
lowedovertimewithversionsfortheotherplatforms.IBMresearchcontributions
havecontinuallyenhancedtheDB2productinareassuchastransactionprocessing
(write-aheadlogging and ARIES recoveryalgorithms), queryprocessing and op-
timization(Starburst),parallelprocessing(DB2ParallelEdition),active-database
support(constraints,triggers),advancedqueryandwarehousingtechniquessuch
as materialized views, multidimensional clustering, “autonomic” features, and
object-relationalsupport(ADTs, UDFs).
1193
1194 Chapter 29 IBMDB2Universal Database
•Database Servers
–DB2 UDB for Linux, Unix, Windows
–DB2 UDB for z/OS
–DB2 UDB for OS/400
–DB2 UDB for VM/VSE
•Business Intelligence
–DB2 Data Warehouse Edition
–DB2 OLAP Server
–DB2 Alphablox
–DB2 CubeViews
–DB2 Intelligent Miner
–DB2 Query Patroller
•Data Integration
–DB2 Information Integrator
–DB2 Replication
–DB2 Connect
–Omni?nd (For Enterprise Search)
•Content Management
–DB2 Content 
–IBM Enterprise Content Manager 
•Application Development
–IBM Rational Application
Developer Studio
–DB2 Forms for z/OS
–QMF
•Database-Management Tools
–DB2 Control Center
–DB2 Admin Tool for z/OS
–DB2 Performance Expert
–DB2 Query Patroller
–DB2 Visual Explain
•Embedded and Mobile Databases
–DB2e (Everyplace)
Manager
Figure 29.1 The DB2 family of products.
Since IBM supports a number of server and operating-system platforms, the
DB2 database engine consists of four code base types: (1) Linux, Unix, and Win-
dows, (2) z/OS (3) VM,and(4)OS/400. All of these support a common subset
of data-de?nition language, SQL, and administration interfaces. However, the
engines have somewhat different features due to their platform origins. In this
chapter, the focus is on the DB2 Universal Database (UDB) engine that supports
Linux,Unix,andWindows.Speci?cfeaturesofinterestinother DB2systemsare
highlightedinappropriatesections.
The latest version of DB2 UDB for Linux, Unix, and Windows as of 2009 is
version 9.7. DB2 version 9.7 includes several new feature such as extension of
nativesupportfor XMLtoshared-nothingenvironments,nativecompressionfor
tables and indexes, automatic storage management, and improved support for
procedurallanguagessuchas SQLPLandOracle’s PL/SQL.
29.2 Database-Design Tools
Most industry database-design and CASE tools can be used to design a DB2
database. In particular, data modeling tools such as ERWin and Rational Rose
allow the designer to generate DB2-speci?c DDL syntax. For instance, Rational
Rose’s UMLDataModelertoolcangenerateDB2-speci?ccreatedistincttypeDDL
statements for user-de?ned types and use them subsequently in column de?ni-
29.3 SQLVariations andExtensions 1195
tions.Mostdesigntoolsalsosupportareverse-engineeringfeaturethatreadsthe
DB2 catalog tables and builds a logical design for additional manipulation. The
toolssupportthegenerationofconstraintsandindices.
DB2providessupportfor many logicaland physical database featuresusing
SQL.Thefeaturesincludeconstraints,triggers,andrecursionusingSQLconstructs.
Likewise,certainphysicaldatabasefeaturessuchastablespaces,bufferpools,and
partitioningarealsosupportedbyusing SQLstatements.TheControlCenterGUI
toolforDB2allowsadesigneroranadministratortoissuetheappropriateDDLfor
thesefeatures.Anothertool,db2look,allowstheadministratortoobtainafullsetof
DDLstatementsforadatabaseincludingtablespaces,tables,indices,constraints,
triggers,andfunctionsthatcanbeusedtocreateanexactreplicaofthedatabase
schemafortestingorreplication.
The DB2 Control Center includes a variety of design- and administration-
related tools. For design, the Control Center provides a tree view of a server,its
databases, tables, views, and all other objects. It also allows users to de?ne new
objects, create ad hoc SQL queries, and view query results. Design tools for ETL,
OLAP,replication,andfederationalsointegrateintotheControlCenter.Theentire
DB2familysupportstheControlCenterfordatabasede?nitionaswellasrelated
tools. DB2alsoprovidesplug-inmodulesforapplicationdevelopmentinthe IBM
RationalApplicationDeveloperproductaswellasintheMicrosoftVisualStudio
product.
29.3 SQL Variations and Extensions
DB2providessupportforarichsetofSQLfeaturesforvariousaspectsofdatabase
processing. Many of the DB2 features and syntax have provided the basis for
standards in SQL-92,orSQL:1999. In this section, we highlight the XML object-
relational and application-integration features in DB2 UDB version 8, along with
somenewfeaturesfromversion9.
29.3.1 XML Features
ArichsetofXML functions have been included in DB2. The following is a list of
severalimportant XMLfunctionsthatcanbeusedin SQL,aspartoftheSQL/XML
extensionto SQL(describedearlierinSection23.6.3):
  xmlelement. Constructs an element tag with given name. For example the
functioncall,xmlelement(book)createsthebookelement.
  xmlattributes.Constructsthesetofattributesforanelement.
  xmlforest.Constructsasequenceof XMLelementsfromarguments.
  xmlconcat. Returns the concatenation of a variable number of XML argu-
ments.
  xmlserialize. Provides a character-oriented serialized version of the argu-
ment.
1196 Chapter 29 IBMDB2Universal Database
select xmlemement(name’PO’,
xmlattributes(poid, orderdate),
(select xmlagg(xmlelement(name’item’,
xmlattributes(itemid, qty, shipdate),
(select xmlelement(name’itemdesc’,
xmlattributes(name, price))
from product
where product.itemid= lineitem.itemid)))
from lineitem
where lineitem.poid= orders.poid))
from orders
where orders.poid=349;
Figure 29.2 DB2SQLXMLquery.
  xmlagg.Returnsaconcatenationofasetof XMLvalues.
  xml2clob. Constructs a character large object (clob) representation of the
XML.Thisclobcanthenberetrievedby SQLapplications.
The XML functions can be incorporated into SQL effectively to provide ex-
tensive XML manipulation capabilities. For instance, suppose that one needs to
construct a purchase-order XML document from relational tables orders, lineitem,
and product for order number 349. In Figure 29.2, we show an SQL query with
XML extensions that can be used to create such a purchase order. The resultant
outputisasshowninFigure29.3.
Version 9 of DB2supportsnativestorageofXMLdataasanxmltype.andnative
supportfortheXQuerylanguage.Specializedstorage,indexing,queryprocessing
andoptimizationtechniqueshavebeenintroducedforef?cientprocessingofXML
data and queries in the XQuery language, and APIs have been extended to deal
with XMLdataand XQuery.
29.3.2 Support for Data Types
DB2providessupportforuser-de?neddatatypes(UDTs).Userscande?nedistinct
or structureddatatypes.Distinctdatatypesarebasedon DB2built-indatatypes.
<PO poid = "349" orderdate = "2004-10-01">
<item itemid="1", qty="10", shipdate="2004-10-03">
<itemdesc name = "IBM ThinkPad T41", Price = "1000.00 USD"/>
</item>
</PO>
Figure 29.3 Purchase order in XML for id=349.
29.3 SQLVariations andExtensions 1197
However, the user can de?ne additional or alternative semantics for these new
types.Forexample,theusercande?neadistinctdatatypecalledus dollar,using:
createdistincttype us dollar as decimal(9,2);
Subsequently,theusercancreatea?eld(e.g., price)inatablewithtypeus dollar.
Queriesmaynowusethetyped?eldinpredicatessuchasthefollowing:
select productfrom us sales
where price> us dollar(1000);
Structureddatatypesarecomplexobjectsthatusuallyconsistoftwoormore
attributes.Forexample,onecanusethefollowingDDLtocreateastructuredtype
called department t:
create type department tas
(deptnamevarchar(32),
deptheadvarchar(32),
faculty countinteger)
modedb2/sql;
createtype point tas
(x coord?oat,
y coord?oat)
modedb2/sql;
Structuredtypescanbeusedtode?ne typed tables:
createtable deptof department t;
One can create a type hierarchy and tables in the hierarchy that can inherit
speci?cmethodsandprivileges.Structuredtypescanalsobeusedtode?nenested
attributes inside a column of a table. Although such a de?nition would violate
normalization rules, it may be suitable for object-oriented applications that rely
onencapsulationandwell-de?nedmethodsonobjects.
29.3.3 User-De?ned Functions and Methods
Another important feature is the ability for users to de?ne their own functions
and methods. These functions can subsequently be included in SQL statements
and queries. Functions can generate scalars (single attribute) or tables (multiat-
tribute row) as their result. Users can register functions (scalar or table) using
thecreatefunctionstatement.Thefunctionscanbewrittenincommonprogram-
minglanguagessuchasCorJavaorscriptssuchasREXX or PERL. User-de?ned
functions (UDFs) can operate in fenced or unfenced modes. In fenced mode, the
functionsareexecutedbyaseparatethreadinitsownaddressspace.Inunfenced
1198 Chapter 29 IBMDB2Universal Database
createfunctiondb2gse.GsegeFilterDist(
operation integer, g1XMindouble, g1XMaxdouble,
g1YMindouble, g1YMax double, distdouble,
g2XMindouble, g2XMaxdouble, g2YMindouble,
g2YMax double)
returnsinteger
speci?cdb2gse.GsegeFilterDist
externalname’db2gsefn!gsegeFilterDist’
languageC
parameterstyledb2sql
deterministic
notfenced
threadsafe
calledonnullinput
nosql
noexternalaction
noscratchpad
no?nalcall
allowparallel
nodbinfo;
Figure 29.4 De?nition of a UDF.
mode, the database-processing agent is allowed to execute the function in the
server’s address space. UDFs can de?ne a scratch pad (work) area where they
can maintain local and static variables across different invocations. Thus, UDFs
canperformpowerfulmanipulationsofintermediaterowsthatareitsinputs.In
Figure29.4,weshowade?nitionofaUDF,db2gse.GsegeFilterDist,inDB2pointing
toaparticularexternalmethodthatperformstheactualfunction.
Methodsareanotherfeaturethatde?nethebehaviorofobjects.UnlikeUDFs,
theyaretightlyencapsulatedwithaparticularstructureddatatype.Methodsare
registeredbyusingthecreatemethodstatement.
DB2alsosupportsproceduralextensionstoSQL,usingtheDB2’sSQLPLexten-
sion,includingprocedures,functions,andcontrol?ow.Proceduralfeaturesofthe
SQLstandardaredescribedinSection5.2).Inaddition,asofversion9.7, DB2also
supportsmuchofOracle’s PL/SQL language,forcompatibilitywithapplications
developedonOracle.
29.3.4 Large Objects
Newdatabaseapplicationsrequiretheabilitytomanipulatetext,images,video,
and other types of data that are typically quite large in size. DB2 supports these
requirementsbyprovidingthreedifferentlargeobject(LOB)types.EachLOBcan
be as large as two gigabytes in size. The large objects in DB2 are (1) binary large
objects(blobs),(2)singlebytecharacterlargeobjects(clobs),and(3)doublebyte
characterlargeobjects(dbclobs).DB2organizestheseLOBsasseparateobjectswith
29.3 SQLVariations andExtensions 1199
createindexextension db2gse.spatial index(
gS1double, gS2double, gS3double)
fromsourcekey(geometry db2gse.ST Geometry)
generatekeyusing
db2gse.GseGridIdxKeyGen(geometry..srid,
geometry..xMin, geometry..xMax,
geometry..yMin, geometry..yMax,
gS1, gS2, gS3)
withtargetkey(srsIdinteger,
lvl integer, gX integer, gY integer, xMindouble,
xMaxdouble, yMindouble, yMaxdouble)
searchmethods<conditions><actions>
Figure 29.5 Spatial index extension in DB2.
each row in the table maintaining pointers to its corresponding LOBs. Users can
register UDFsthatmanipulatethese LOBsaccordingtoapplicationrequirements.
29.3.5 Indexing Extensions and Constraints
Arecentfeatureof DB2enablesuserstocreateindexextensionstogeneratekeys
fromstructureddatatypesbyusingthecreateindexextensionstatement.Forex-
ample,onecancreateanindexonanattributebasedonthedepartment tdatatype
de?nedearlierbygeneratingkeys,usingthedepartmentname. DB2’sspatialex-
tenderusestheindexextensionmethodtocreateindicesasshowninFigure29.5.
Finally,userscantakeadvantageoftherichsetofconstraintcheckingfeatures
available in DB2 for enforcing object semantics such as uniqueness, validity,and
inheritance.
29.3.6 Web Services
DB2 can integrate Web services as producer or consumer. A Web service can be
de?ned to invoke DB2,usingSQL statements. The resultant Web-service call is
processedbyanembeddedWeb-serviceenginein DB2andtheappropriate SOAP
responsegenerated.Forexample,ifthereisaWebservicecalled GetRecentActiv-
ity(cust id)thatinvokesthefollowingSQL,theresultshouldbethelasttransaction
forthiscustomer.
select trn id, amount, date
from transactions
where cust id=<input>
orderby date
fetch ?rst1rowonly;
1200 Chapter 29 IBMDB2Universal Database
The following SQL shows DB2 acting as a consumer of a Web service. In this
example, the GetQuote() user-de?ned function is a Web service. DB2 makes the
Web-service call using an embedded Web-service engine. In this case, GetQuote
returnsanumericquotevalueforeach ticker idintheportfoliotable.
select ticker id, GetQuote(ticker id)
from portfolio;
29.3.7 Other Features
DB2 also supports IBM’s Websphere MQ product by de?ning appropriate UDFs.
UDFs are de?nedfor both readand writeinterfaces. These UDFscanbeincorpo-
ratedin SQLforreadingfromorwritingtomessagequeues.
From version 9, DB2 supports ?ne-grained authorization through the label-
basedaccesscontrolfeature,whichplaysarolesimilartoOracle’sVirtualPrivate
Database(describedearlierinSection9.7.5).
29.4 Storage and Indexing
The storage and indexing architecture in DB2 consists of the ?le-system or disk-
management layer, the servicesto manage the buffer pools, data objects such as
tables,LOBs,indexobjects,andconcurrencyandrecoverymanagers.Weoverview
the general storage architecture in this section. In addition, we describe a new
featurein DB2version8calledmultidimensionalclusteringinthefollowingsec-
tion.
29.4.1 Storage Architecture
DB2 provides storage abstractions for managing logical database tables usefully
inamultinodeandmultidiskenvironment.Nodegroupscanbede?nedtosupport
tablepartitioningacrossaspeci?csetofnodesinamultinodesystem.Thisallows
complete ?exibility in allocating table partitions to different nodes in a system.
For example, large tables may be partitioned across all nodes in a system while
smalltablesmayresideonasinglenode.
Within a node, DB2 uses tablespaces to organize tables. A tablespace consists
of one or more containers, which are references to directories, devices, or ?les.
A tablespace may contain zero or more database objects such as tables, indices,
or LOBs. Figure 29.6 illustrates these concepts. In this ?gure, two tablespaces
have been de?ned for a nodegroup. The humanres tablespace is assigned four
containers, while the sched tablespace has only one container. The employee and
department tables are assigned to the humanres tablespace, while the project table
is in the sched tablespace. Striping is used to allocate fragments (extents) of the
employee and department table to the containers of the humanres tablespace. DB2
permits the administrator to create either system-managed or DBMS-managed
tablespaces.System-managedspaces(SMS)aredirectoriesor?lesystemsthatare
maintained by the underlying operating system. In SMS, DB2 creates ?le objects
29.4 Storage andIndexing 1201
Nodegroup  MyDepts
Tablespace humanres Tablespace 
     sched
Containers
 department employee project
Figure 29.6 Tablespaces and containers in DB2.
in the directories and allocates data to each of the ?les. Data-managed spaces
(DMS) are raw devices or preallocated ?les that are then controlled by DB2.The
sizeofthesecontainerscannevergroworshrink.DB2createsallocationmapsand
managesthe DMStablespaceitself.Inbothcases,anextentofpagesistheunitof
spacemanagement.Theadministratorcanchoosetheextentsizeforatablespace.
DB2 supports striping across the different containers as a default behavior.
Forexample,whendataareinsertedintoanewlycreatedtable,the?rstextentis
assigned to a container. Once the extent is full, the next data items are allocated
to the next container in round-robin fashion. Striping provides two signi?cant
bene?ts:parallel I/Oandloadbalancing.
29.4.2 Buffer Pools
One or more buffer pools may be associated with each tablespace for managing
different objects such as data and indices. The buffer pool is a common shared
data area that maintains memory copies of objects. These objects are typically
organized as pages for management in the buffer pool. DB2 allows buffer pools
to be de?ned by SQL statements. DB2 version 8 has the ability to grow or shrink
bufferpoolsonlineandalsoautomaticallybychoosingthe automaticsettingfor
the buffer pool con?guration parameter. An administrator can add more pages
toabufferpoolordecreaseitssizewithoutquiescingthedatabaseactivity.
createbufferpool<buffer-pool>....
alterbufferpool<buffer-pool>size<n>
1202 Chapter 29 IBMDB2Universal Database
0
2
500
page 0 contains a set 
of internal records (e.g., FSCR)
user Records
every 500th page
contains another FSCR
1
3
more user records RID (Record ID) = Page 3, Slot 2 
 
K
C 3, 2
K
A K RID RID K S RID
Leaf    
Pages 
Logical Table View Logical Index View
RID K
RID K
CR I D
Figure 29.7 Logical view of tables and indices in DB2.
DB2alsosupportsprefetchingandasynchronouswritesusingseparatethreads.
Thedatamanagercomponenttriggersprefetchofdataandindexpagesbasedon
the query access patterns. For instance, a table scan always triggers prefetch of
datapages.Indexscanscantriggerprefetchofindexpagesaswellasdatapages
if they are being accessed in a clusteredfashion. The number of prefetchers and
theprefetchsizearecon?gurableparametersthatneedtobeinitializedaccording
tothenumberofdisksorcontainersinthetablespace.
29.4.3 Tables, Records, and Indices
DB2organizestherelationaldataasrecordsinpages.Figure29.7showsthelogical
viewofatableandanassociatedindex.Thetableconsistsofasetofpages.Each
pageconsistsofasetofrecordsthatareeitheruserdatarecordsorspecialsystem
records. Page zero of the table contains special system records about the table
anditsstatus.DB2usesaspace-maprecordcalledfreespacecontrolrecord(FSCR)
to ?nd free space in the table. The FSCR record usually contains a space map for
500 pages. The FSCR entry is a bit mask that provides a rough indication of the
possibility of free space in a page. The insert or update algorithm must validate
the FSCRentriesbyperformingaphysicalcheckoftheavailablespaceinapage.
Indicesarealsoorganizedaspagescontainingindexrecordsandpointersto
childandsiblingpages. DB2providessupportfortheB
+
-treeindexmechanisms
internally.The B
+
-treeindexcontains internal pagesand leafpages. The indices
havebidirectionalpointersattheleafleveltosupportforwardandreversescans.
Leafpagescontainindexentriesthatpointtorecordsinthetable.Eachrecordin
thetablecanbeuniquelyidenti?edbyusingitspageandslotinformation,which
arecalledtherecord IDor RID.
29.5 Multidimensional Clustering 1203
Embedded 
free space
(usable anullr 
on-line page
reorganization*)
*Exception: Any space 
reserved by an 
uncommited 
delete is not usable
Free space 
(usable 
without page 
reorganization*) 
-1
Record  0 Record  0
Record  2
Page Header Page Header
3800 3400 3800 3700
Page  473 Page  1056
473,2
set on tablespace creation
1056 1
3 bytes 1 byte
page#    slot#
Figure 29.8 Data page and record layout in DB2.
DB2supports “includecolumns”intheindexde?nition,as:
createuniqueindexI1onT1(C1)include(C2);
The included index columns enable DB2 to extend the use of “index-only”
query-processing techniques whenever possible. Additional directives such as
minpctused and pctfree can be used to control the merge and initial space allo-
cationofindexpages.
Figure29.8showsthetypicaldatapageformatinDB2.Eachdatapagecontains
aheaderandaslotdirectory.Theslotdirectoryisanarrayof255entriesthatpoints
to record offsets in the page. The ?gure shows that page number 473 contains
record zero at offset 3800 and record 2 at offset 3400. Page 1056 contains record
1atoffset3700,whichisaforwardpointertotherecord<473,2>.Hence,record
<473,2>isanover?owrecordthatwascreatedasaresultofanupdateoperation
of the original record <1056,1>. DB2 supports different page sizes such as 4, 8,
16,and32kilobytes.However,eachpagemaycontainonly255userrecordsinit.
Largerpagesizesareusefulinapplicationssuchasdatawarehousing,wherethe
table contains many columns. Smallerpage sizes are useful for operational data
withfrequentupdates.
29.5 Multidimensional Clustering
This section provides a brief overview of the main features of MDC.Withthis
feature,aDB2tablemaybecreatedbyspecifyingoneormorekeysasdimensions
1204 Chapter 29 IBMDB2Universal Database
along which to cluster the table’s data. DB2 includes a clause called organize by
dimensions for this purpose. For example, the following DDL describes a sales
tableorganizedby storeId, year(orderDate),and itemIdattributesasdimensions.
createtable sales(storeIdint,
orderDatedate,
shipDatedate,
receiptDate date,
region int,
itemIdint,
price?oat
yearOdintgeneratedalways as year(orderDate))
organizedbydimensions(region, yearOd, itemId);
Each of these dimensions may consist of one or more columns, similar to
indexkeys.Infact,a “dimensionblockindex”(describedbelow)isautomatically
created for each of the dimensions speci?ed and is used to access data quickly
and ef?ciently.Acompositeblock index,containing alldimensionkeycolumns,
is created automatically if necessary, and is used to maintain the clustering of
dataoverinsertandupdateactivity.
Every unique combination of dimension values forms a logical “cell,” that
is physically organized as blocks of pages, where a block is a set of consecutive
pages on disk. The set of blocks that contain pages with data having a certain
key value of one of the dimension block indices is called a “slice.” Every page
of the table is part of exactly one block, and all blocks of the table consist of the
same number of pages, namely, the block size. DB2 has associated the block size
withtheextentsizeofthetablespacesothatblockboundarieslineupwithextent
boundaries.
Figure 29.9 illustrates these concepts. This MDC table is clustered along the
dimensions year(orderDate),
1
region,anditemId.The?gureshowsasimplelogical
cube with only two values for each dimension attribute. In reality, dimension
attributescaneasilyextendtolargenumbersofvalueswithoutrequiringanyad-
ministration.Logicalcellsarerepresentedbythesubcubesinthe?gure.Records
in the table are stored in blocks, which contain an extent’s worth of consecutive
pages on disk. In the diagram, a block is represented by a shaded oval, and is
numbered according to the logical order of allocated extents in the table. We
show only a few blocks of data for the cell identi?ed by the dimension values
<1997,Canada,2>.Acolumnorrowinthegridrepresentsasliceforaparticular
dimension.For example,all recordscontaining the value “Canada” inthe region
dimensionarefoundintheblockscontainedintheslicede?nedbythe “Canada”
column inthe cube. In fact, each block inthis slice only contains records having
“Canada”inthe region?eld.
1
Dimensionscanbecreatedbyusingageneratedfunction.
29.5 Multidimensional Clustering 1205
1997, 
Mexico, 
1
1997, 
Canada, 
2
1997, 
Canada, 
1
1997, 
Mexico,  
2
1998, 
Canada, 
2
1998, 
Mexico, 
2
1997, 
Mexico, 
2
year(orderDate)
itemId
region
31
45
127
1997
1998
1
2
Mexico
Canada
Figure 29.9 Logical view of physical layout of an MDC table.
29.5.1 Block Indices
Inourexample,adimensionblockindexiscreatedoneachofthe year(orderDate),
region,anditemIdattributes.Eachdimensionblockindexisstructuredinthesame
mannerasatraditionalB-treeindexexceptthat,attheleaflevel,thekeyspointto
ablockidenti?er(BID)insteadofarecordidenti?er(RID).Sinceeachblockcontains
potentially many pages of records, these block indices are much smaller than
RID indices and need be updated only when a new block is added to a cell or
existingblocksareemptiedandremovedfromacell.Aslice,orthesetofblocks
containing pages with all records having a particular key value in a dimension,
are represented in the associated dimension block index by a BID list for that
keyvalue.Figure29.10illustratesslicesofblocksforspeci?cvaluesof regionand
itemIddimensions,respectively.
In the example above, to ?nd the slice containing all records with “Canada”
fortheregiondimension,wewouldlookupthiskeyvalueintheregiondimension
blockindexand?ndakeyasshowninFigure29.10a.Thiskeypointstotheexact
setof BIDsfortheparticularvalue.
29.5.2 Block Map
Ablockmapisalsoassociatedwiththetable.Thismaprecordsthestateofeach
blockbelongingtothetable.Ablockmaybeinanumberofstatessuchasinuse,
free, loaded,requiring constraint enforcement.Thestatesoftheblockareused
1206 Chapter 29 IBMDB2Universal Database
Canada 21 31 45 77 127 376 501 719
Key
Key
BID List
BID List
(a) Dimension block index entry for region 'Canada'
(b) Dimension block index entry for itemId = 1
    1       2 7 20 65 101 273 274 476
Figure 29.10 Block index key entries.
bythedata-managementlayerinordertodeterminevariousprocessingoptions.
Figure29.11showsanexampleblockmapforatable.
Element0 in the block map representsblock 0 in the MDC table diagram.Its
availabilitystatusis “U,”indicatingthatitisinuse.However,itisaspecialblock
and does not contain any user records. Blocks 2, 3, 9, 10, 13, 14, and 17 are not
beingusedinthetableandareconsidered “F,”orfree,intheblockmap.Blocks7
and18haverecentlybeenloadedintothetable.Block12waspreviouslyloaded
andrequiresthataconstraintcheckbeperformedonit.
29.5.3 Design Considerations
A crucial aspect of MDC is to choose the right set of dimensions for clustering
a table and the right block size parameter to minimize the space utilization.
If the dimensions and block sizes are chosen appropriately, then the clustering
bene?ts translateintosigni?cant performanceandmaintenance advantages.On
theotherhand,ifchosenincorrectly,theperformancemaydegradeandthespace
utilization could be signi?cantly worse. There are a number of tuning knobs
thatcanbeexploitedtoorganizethetable.Theseincludevaryingthenumberof
dimensions,andvaryingthegranularityofoneormoredimensions,varyingthe
0 1 2 3 4 5 6 7 8 9 19 11 12 13 14 15 16 17 18 19
U UFFU U U L U FFU C F FUU F L ...
Figure 29.11 Block map entries.
29.6 QueryProcessing andOptimization 1207
block size(extentsize)and pagesize ofthe tablespacecontaining the table.One
or more of these techniques can be used jointly to identifythe best organization
ofthetable.
29.5.4 Impact on Existing Techniques
ItisnaturaltoaskwhetherthenewMDCfeaturehasanadverseimpactordisables
some existing features of DB2 for normal tables. All existing features such as
secondary RID indices, constraints, triggers, de?ning materialized views, and
queryprocessingoptions,areavailableforMDCtables.Hence,MDCtablesbehave
justlikenormaltablesexceptfortheirenhancedclusteringandprocessingaspects.
29.6 Query Processing and Optimization
DB2queriesaretransformedintoatreeofoperationsbythequerycompiler.The
queryoperatortreeisusedatexecutiontimeforprocessing. DB2supportsarich
set of query operators that enables it to consider the best processing strategies
andprovidesthe?exibilitytoexecutecomplexquerytasks.
Figures29.12and29.13showaqueryanditsassociatedqueryplaninDB2.The
queryisarepresentativecomplexquery(query5)fromtheTPC-Hbenchmarkand
containsseveraljoinsandaggregations.Thequeryplanchosenforthisparticular
exampleis rather simplesince many indicesand other auxiliary structuressuch
as materialized views were not de?ned for these tables. DB2 provides various
“explain” facilities including a powerful visual explain feature in the Control
Centerthatcanhelpusersunderstandthedetailsofaquery-executionplan.The
queryplanshowninthe?gureisbasedonthevisualexplainforthequery.Visual
––’TPCDLocalSupplierVolumeQuery(Q5)’;
select n name,sum(l extendedprice*(1-l discount))as revenue
from tpcd.customer, tpcd.orders, tpcd.lineitem,
tpcd.supplier, tpcd.nation, tpcd.region
where c custkey= o custkeyand
o orderkey= l orderkeyand
l suppkey= s suppkeyand
c nationkey= s nationkey and
s nationkey= n nationkey and
n regionkey= r regionkeyand
r name=’MIDDLEEAST’and
o orderdate>=date(’1995-01-01’) and
o orderdate<date(’1995-01-01’)+1year
groupby n name
orderby revenuedesc;
Figure 29.12 SQL query.
1208 Chapter 29 IBMDB2Universal Database
Scan
Scan
Scan
NLJOIN
Scan
NLJOIN
Sort
Sort
Merge join
Sort
Index Scan
Sort
Merge join 
Scan
Hash Join 
Sort
Scan 
Group By
Sort 
Scan 
Results
customer
supplier
nation region
orders
lineitem
Figure 29.13 DB2 query plan (graphical explain).
explain allows the user to understand cost and other relevant properties of the
differentoperationsofthequeryplan.
All SQL queries and statements, however complex they may be, are trans-
formedintoaquerytree.Thebaseorleafoperatorsofthequerytreemanipulate
records in database tables. These operations are also called as access methods.In-
termediate operations of the tree include relational-algebra operations such as
join,setoperations,andaggregation.Therootofthetreeproducestheresultsof
thequeryor SQLstatement.
29.6.1 Access Methods
DB2supportsacomprehensivesetofaccessmethodsonrelationaltables.Thelist
ofaccessmethodsincludes:
29.6 QueryProcessing andOptimization 1209
  Tablescan.Thisisthemostbasicmethodandperformsapage-by-pageaccess
ofallrecordsinthetable.
  Index scan. An index is used to select the speci?c records that satisfy the
query. The qualifying records are accessed using the RIDsintheindex.DB2
detects opportunities to prefetch data pages when it observes a sequential-
accesspattern.
  Block index scan. This is a new access method for MDC tables. One of the
blockindicesisusedtoscanaspeci?csetofMDCdatablocks.Thequalifying
blocksareaccessedandprocessedinblocktablescanoperations.
  Indexonly.Inthiscase,theindexcontainsalltheattributesthatarerequired
by the query.Hence, a scan of the indexentriesis suf?cient. The index-only
techniqueisusuallyagoodperformancesolution.
  List prefetch. This access method is chosen for an unclustered index scan
with a signi?cant number of RIDs. DB2 has a sort operation on the RIDsand
performs a fetch of the records in sorted order from the data pages. Sorted
access changes the I/O pattern from random to sequential and also enables
prefetchingopportunities.Listprefetchhasbeenextendedtodealwithblock
indicesaswell.
  Blockandrecordindex ANDing.Thismethodisusedwhen DB2determines
that more than one index can be used to constrain the number of satisfying
records in a base table. The most selective index is processed to generate a
list of BIDsorRIDs. The next selective index is then processed to return the
BIDsorRIDsthatitquali?es.ABIDorRIDquali?esforfurtherprocessingonly
if it is present in the intersection (AND operation) of the index scan results.
TheresultofanindexANDoperationisasmalllistofqualifyingBIDsorRIDs
whichareusedtofetchthecorrespondingrecordsfromthebasetable.
  Block andrecord indexordering.Thisstrategyisusediftwoormoreblock
orrecordindicescanbeusedtosatisfyquerypredicatesthatarecombinedby
using the OR operator. DB2 eliminates duplicate BIDsorRIDsbyperforming
a sort and then fetching the resulting set of records. Index ORing has been
extendedtoconsiderblockand RIDindexcombinations.
Alltheselectionandprojectionpredicatesofaqueryareusuallypusheddownto
theaccessmethods.Inaddition, DB2performscertainoperationssuchassorting
andaggregationin “pusheddown”modeinordertoreduceinstructionpaths.
This MDCfeaturetakesadvantageofthenewsetofaccess-methodimprove-
mentsforblockindexscans,blockindexprefetch,blockindexANDing,andblock
index ORingtoprocessblocksofdata.
29.6.2 Join, Aggregation, and Set Operations
DB2 supports a number of techniques for these operations. For join, DB2 can
choosebetweennested-loop,sort-merge,andhash-jointechniques.Indescribing
1210 Chapter 29 IBMDB2Universal Database
the join and set binary operations, we use the notation of “outer” and “inner”
tables to distinguish the two input streams. The nested-loop technique is useful
if the inner table is very small or can be accessed by using an index on a join
predicate.Sort-merge-joinand hash-join techniques are used for joins involving
largeouterandinnertables.Setoperationsareimplementedbyusingsortingand
merging techniques. The merging technique eliminates duplicates in the case of
unionwhileduplicatesareforwardedinthecaseofintersection.DB2alsosupports
outer-joinoperationsofallkinds.
DB2 processes aggregation operations in early or “push-down” mode when-
everpossible.Forinstance,agroupbyaggregationcanbeperformedbyincorpo-
ratingtheaggregationintothesortphase.Thejoinandaggregationalgorithmscan
take advantage of superscalar processing in modern CPUs using block-oriented
andcache-conscioustechniques.
29.6.3 Support for Complex SQL Processing
One of the most important aspects of DB2 is that it uses the query-processing
infrastructure in an extensible fashion to support complex SQL operations. The
complexSQLoperationsincludesupportfordeeplynestedandcorrelatedqueries
as well as constraints, referential integrity, and triggers. Because most of these
actionsarebuiltintothequeryplan,DB2isabletoscaleandprovidesupportfora
largernumber of theseconstraints and actions. Constraints and integritychecks
arebuiltasquerytreeoperationsoninsert,delete,orupdateSQLstatements.DB2
alsosupportsmaintenanceofmaterializedviewbyusingbuilt-intriggers.
29.6.4 Multiprocessor Query-Processing Features
DB2 extends the base set of query operations with control and data exchange
primitivestosupportSMP(thatis,sharedmemory),MPP(thatis,sharednothing),
and SMP cluster (that is, shared disk) modes of query processing. DB2 uses a
“tablequeue” abstraction for data exchange between threads on different nodes
or on the same node. The tablequeue is used as a buffer that redirects data to
appropriatereceiversusingbroadcast,one-to-one,ordirectedmulticastmethods.
Control operations are used to create threads and coordinate the operation of
differentprocessesandthreads.
In all these modes, DB2 employs a coordinator process to control the query
operations and ?nal result gathering. Coordinator processes can also perform
some global database-processing actions if required. An example is the global
aggregation operation to combine the local aggregation results. Subagents or
slavethreadsperformthebasedatabaseoperationsinoneormorenodes.In SMP
mode, the subagents use shared memory to synchronize between themselves
when sharing data. In an MPP, the tablequeue mechanisms provide buffering
and ?ow control to synchronize across different nodes during execution. DB2
employs extensive techniques to optimize and process queries ef?ciently in an
MPPor SMPenvironment.Figure29.14showsasimplequeryexecutinginafour-
node MPP system. In this example, the sales table is partitioned across the four
nodes P
1
,...,P
4
.Thequeryisexecutedbyspawningagentsthatexecuteateach
29.6 QueryProcessing andOptimization 1211
• Distribute subsection
• Tablequeue (TQ) receive
• Bind out
Subsection
• Table access (sales)
• Predicate (quantity > 10)
• TQ send to coordinator
…
P1 P2 P3 P4
Nodes
SQL query: select * from sales where quantity > 10
Coordinator
Receive
Bind out
Scan sales
Filter quantity > 10
Send to coordinator
Scan sales
Filter quantity > 10
Send to coordinator
Figure 29.14 DB2 MPP query processing using function shipping.
of these nodes to scan and ?lter the rows of the sales table at that node (called
functionshipping),andtheresultingrowsaresenttothecoordinatornode.
29.6.5 Query Optimization
DB2’s query compiler uses an internal representation of the query, called the
query-graph model (QGM), in order to perform transformations and optimiza-
tions. After parsing the SQL statement, DB2 performs semantic transformations
ontheQGMtoenforceconstraints,referentialintegrity,andtriggers.Theresultof
thesetransformations isanenhanced QGM.Next,DB2attemptstoperform query
rewrite transformations that are considered mostly bene?cial. Rewrite rules are
?red if applicable to perform the requiredtransformations. Examples of rewrite
transformationsinclude(1)decorrelationofcorrelatedsubqueries,(2)transform-
ingcertainsubqueriesintojoinsusingearly-outprocessing,(3)pushingthegroup
byoperationbelowjoinsifapplicable,and (4)usingmaterializedviewsforpor-
tionsoftheoriginalquery.
The query optimizer component uses this enhanced and transformed QGM
asitsinputforoptimization.Theoptimizeriscostbasedandusesanextensible,
rule-driven framework. The optimizer can be con?gured to operate at different
levels of complexity. At the highest level, it uses a dynamic-programming al-
gorithm to consider all query-plan options and chooses the optimal cost plan.
At an intermediate level, the optimizer does not consider certain plans, access
methods (e.g., index ORing), or rewrite rules. At the lowest level of complexity,
theoptimizerusesasimplegreedyheuristictochooseagoodbutnotnecessarily
optimalqueryplan.Theoptimizerusesdetailedmodelsofthequery-processing
operations,includingmemorysizesandprefetching,toobtainaccurateestimates
of the I/O and CPU costs. Itrelieson thestatisticsof thedata to estimatethecar-
dinalityandselectivitiesoftheoperations. DB2allowstheusertoobtaindetailed
histogramsofcolumn-leveldistributionsandcombinationsofcolumnsusingthe
runstatsutility.Thedetailedhistogramscontaininformationaboutthemostfre-
quentvalueoccurrencesaswellasquantile-basedfrequencydistributionsofthe
attributes.Theoptimizergeneratesaninternalqueryplanthat isconsideredthe
1212 Chapter 29 IBMDB2Universal Database
createtable emp dept(dept idinteger, emp id integer,
emp namevarchar(100), mgr idinteger)as
select dept id, emp id, emp name, mgr id
from employee, department
datainitiallydeferred
refreshimmediate––(ordeferred)
maintainedbyuser––(orsystem)
Figure 29.15 DB2 materialized query tables.
bestqueryplanfortheparticularoptimizationlevel.Thisqueryplanisconverted
into threads of query operators and associated data structures for execution by
thequery-processingengine.
29.7 Materialized Query Tables
MaterializedviewsaresupportedinDB2inLinux,Unix,andWindowsaswellas
on the z/OS platforms. A materialized view can be any general view de?nition
on one or more tables or views.A materializedview is useful since it maintains
a persistent copy of the view data to enable faster query processing. In DB2 a
materializedviewiscalledamaterializedquerytable(MQT).MQTsarespeci?ed
byusingthecreate tablestatementasshownbytheexampleinFigure29.15.
In DB2, MQTs can referenceother MQTsto createa treeor forestof dependent
views. These MQTs are highly scalable as they can be partitioned in an MPP
environment and can have MDC clustering keys. MQTs are most valuable if the
database engine can route queries to them seamlessly and also if the database
engine can maintain them ef?ciently whenever possible. DB2 provides both of
thesefeatures.
29.7.1 Query Routing to MQTs
The query-compiler infrastructure in DB2 is ideally suited to leverage the full
powerof MQTs.Theinternal QGMmodelallowsthecompilertomatchtheinput
query against the available MQT de?nitions and choose appropriate MQTsfor
consideration. After matching, the compiler considers several options for opti-
mization. They include the base query as well as suitable MQT reroute versions.
The optimizer loops through these options before choosing the optimal version
for execution. The entire ?ow of the reroute and optimization is shown in Fig-
ure29.16.
29.7.2 Maintenance of MQTs
MQTs are useful only if the database engine provides ef?cient techniques for
maintenance. There are two dimensions to maintenance: time and cost. In the
timedimension,thetwochoicesareimmediateordeferred.DB2supportsboththese
choices.Ifoneselectsimmediate,theninternaltriggersarecreatedandcompiled
29.7 Materialized QueryTables 1213
SQL query
Query  semantics
(validate reroute possibility)
MQT candidate match phase
MQT de?nitions
Query candidates
Optimization phase
Select best plan
Figure 29.16 MQT matching and optimization in DB2.
into the insert, update,ordelete statements of the source objects to process
the updates to the dependent MQTs. In the case of deferred maintenance, the
updatedtablesaremovedintoanintegritymodeandanexplicitrefreshstatement
must be issued to perform the maintenance. In the size dimension, the choices
are incremental or full. Incremental maintenance implies that only the recently
updatedrowsshouldbeusedformaintenance.Fullmaintenanceimpliesthatthe
entire MQT be refreshed from its sources. The matrix in Figure 29.17 shows the
twodimensionsandtheoptionsthataremostusefulalongthesedimensions.For
instance, immediateandfull maintenance arenot compatibleunlessthe sources
are extremely small. DB2 also allows for the MQTs to be maintained by user.In
this case, the refresh of the MQTs is determined by users performing explicit
processingusing SQLorutilities.
Thefollowingcommandsprovideonesimpleexampleofperformingdeferred
maintenance for the emp dept materialized view after a load operation to one of
itssources.
Yes
Usually no
Yes,
Anullr insert/update/delete
Yes,
Anullr load
Immediate
Deferred
Full Incremental Choices
Figure 29.17 Options for MQT maintenance in DB2.
1214 Chapter 29 IBMDB2Universal Database
loadfromnewdata.txtoftypedel
insertinto employee;
refreshtable emp dept
29.8 Autonomic Features in DB2
DB2 UDB provides features for simplifying the design and manageability of
databases. Autonomic computing encompasses a set of techniques that allow
thecomputingenvironmenttomanageitselfandreducetheexternaldependen-
ciesinthefaceofexternalandinternalchangesinsecurity,systemload,orother
factors.Con?guration,optimization,protection,andmonitoringareexamplesof
subjectareasthatbene?tfromautonomic-computingenhancements.Thefollow-
ingsectionsbrie?ydescribethecon?gurationandoptimizationareas.
29.8.1 Con?guration
DB2 is providing support for automatic tuning of various memory and system
con?gurationparameters.Forinstance,parameterssuchasbufferpoolsizesand
sort heap sizes can be speci?ed as automatic. In this case, DB2 monitors the
systemandslowlygrowsorshrinkstheseheapmemorysizes,dependingonthe
workloadcharacteristics.
29.8.2 Optimization
Auxiliary data structures (indices, MQTs) and data organization features (par-
titioning, clustering) are important aspects of improving the performance of
database processing in DB2. In the past, the database administrator (DBA) had
to use experience and known guidelines to choose meaningful indices, MQTs,
partitionkeys,and clusteringkeys.Giventhepotentialnumber of choices, even
the best experts are not capable of ?nding the right mix of these features for
a given workload in a short time. DB2 includes a Design Advisor that provides
workload-based advice for all of these features. The Design Advisor tool auto-
maticallyanalyzesaworkload,usingoptimizationtechniquestopresentasetof
recommendations.TheDesignAdvisorcommandsyntaxis:
db2advis -d <DB name> -i <workload?le> -m MICP
The“-m”parameterallowstheusertospecifythefollowingoptions:
  M—Materializedquerytables.
  I—Indices.
  C—Clustering,namely, MDC.
  P—Partitioningkeyselection.
29.9 Tools andUtilities 1215
Theadvisorusesthefullpowerofthe DB2query-optimizationframeworkin
these recommendations. It uses an input workload and constraints on size and
time of advise as its parameters. Given that it leverages the DB2 optimization
framework, it has full knowledge of the schema and statistics of the underlying
data.Theadvisorusesseveralcombinatorialtechniquestoidentifyindices,MQTs,
MDCs,andpartitioningkeystoimprovetheperformanceofthegivenworkload.
Anotheraspectofoptimizationisbalancingtheprocessingloadonthesystem.
Inparticular,utilitiestendtoincreasetheloadonasystemandcausesigni?cant
reductioninuserworkloadperformance.Giventhetrendtowardonlineutilities,
thereisaneedtobalancetheloadconsumptionofutilities. DB2includesautility
load-throttlingmechanism.Thethrottlingtechniqueisbasedonfeedbackcontrol
theory.Itcontinuallyadjustsandthrottlestheperformanceofthebackuputility,
usingspeci?ccontrolparameters.
29.9 Tools and Utilities
DB2 providesa number of tools for ease of use and administration. This core set
oftoolsisaugmentedandenhancedbyalargenumberoftoolsfromvendors.
The DB2ControlCenteristheprimarytoolforuseandadministrationof DB2
databases. The Control Center runs on many workstation platforms. It is orga-
nizedfromdataobjectssuchasservers,databases,tables,andindices.Itcontains
task-orientedinterfacestoperformcommands and allows userstogenerate SQL
scripts.Figure29.18showsascreenshotofthemainpaneloftheControlCenter.
ThisscreenshotshowsalistoftablesintheSampledatabaseintheDB2instanceon
node Crankarm.Theadministratorcanusethemenutoinvokeasuiteofcompo-
nenttools.ThemaincomponentsoftheControlCenterincludecommandcenter,
scriptcenter,journal,licensemanagement,alertcenter,performancemonitor,vi-
sual explain, remote database management, storage management, and support
for replication. The command center allows users and administrators to issue
database commands and SQL. The script center allows users to run SQL scripts
constructedinteractivelyorfroma?le.Theperformancemonitorallowsusersto
monitor various events in the database system and obtain snapshots of perfor-
mance. “SmartGuides” provide help on con?guring parameters and setting up
the DB2system.Astored-procedurebuilderhelpstheusertodevelopandinstall
storedprocedures.Visualexplainallowstheusertoobtaingraphicalviewsofthe
query-execution plan. An index wizard helps the administrator by suggesting
indicesforperformance.
While the Control Center is an integrated interface for many of the tasks,
DB2alsoprovidesdirectaccesstomosttools.Forusers,toolssuchastheexplain
facility,explaintables,andgraphicalexplainprovideadetailedbreakdownofthe
queryplans.Usersarealsoallowedtomodifystatistics(ifpermitted)inorderto
generatethebestqueryplans.
Administratorsaresupportedbyanumberoftools.DB2providescomprehen-
sive support for load, import, export, reorg, redistribute, and other data-related
utilities.Mostofthesesupportincrementalandonlineprocessingcapability.For
1216 Chapter 29 IBMDB2Universal Database
Figure 29.18 DB2 Control Center.
instance, one can issue a load command in online mode to allow applications
to access the original contents of a table concurrently. DB2’s utilities are all fully
enabledtoruninparallelmode.
Additionally, DB2supportsanumberoftoolssuchas:
  Auditfacilityformaintainingtheaudittraceofdatabaseactions.
  Governorfacilityforcontrollingthepriorityandexecutiontimesofdifferent
applications.
  Querypatrollerfacilityformanagingthequeryjobsinthesystem.
  Traceanddiagnosticfacilitiesfordebugging.
  Eventmonitoringfacilitiesfortrackingtheresourcesandeventsduringsys-
temexecution.
DB2 for OS/390 has a very rich set of tools. QMF is a widely used tool for
generatingadhocqueriesandintegratingitintoapplications.
29.10 ConcurrencyControl and Recovery 1217
29.10 Concurrency Control and Recovery
DB2supportsacomprehensivesetofconcurrency-control,isolation,andrecovery
techniques.
29.10.1 Concurrency and Isolation
Forisolation,DB2supportstherepeatableread(RR),readstability(RS),cursorstability
(CS),and uncommitted read(UR)modes.RR,CS,andURmodesneednofurtherex-
planation.TheRSisolationmodelocksonlytherowsthatanapplicationretrieves
in a unit of work. On a subsequent scan, the application is guaranteed to see
all these rows (like RR) but might also see new rows that qualify. However, this
might be an acceptable trade-off for some applications with respect to strict RR
isolation.Typically,thedefaultisolationlevelisCS.Applicationscanchoosetheir
level of isolation at the binding stage. Most commercially available applications
areboundusingmostisolationlevels,enablinguserstochoosetherightversion
oftheapplicationfortheirrequirement.
The various isolation modes are implemented by using locks. DB2 supports
record-level and table-level locks. A separate lock-table data structure is main-
tained with the lock information. DB2 escalates from record-level to table-level
locksifthespaceinthelocktablebecomestight.DB2implementsstricttwo-phase
locking for all update transactions. Write locks or update locks are held until
commit or rollback time. Figure 29.19 shows the different lock modes and their
descriptions. The set of lock modes supported includes intent locks at the table
Lock Mode
IN (intent none)
IS (intent share)
NS (next key share)
S (share)
IX (intent exclusive)
SIX (share with 
         intent exclusive)
U (update)
NX (next-key exclusive)
X (exclusive)
Z (superexclusive)
Objects Interpretation
Tablespaces, tables
Tablespaces, tables
Rows
Rows, tables
Tablespaces, tables
Tables
Rows, tables
Rows
Rows, tables
Tablespaces, tables
Read with no row locks
Read with row locks
Read locks for RS or CS 
isolation levels
Read lock
Intend to update rows
No read locks on rows but
X locks on updated rows
Update lock but allows others 
to read
Next key lock for inserts/deletes 
to prevent phantom reads 
during RR index scans
Only uncomminulld readers 
allowed
Complete exclusive access
Figure 29.19 DB2 lock modes.
1218 Chapter 29 IBMDB2Universal Database
level in order to maximize concurrency. Also, DB2 implements next-key locking
andvariantschemesforupdatesaffectingindexscanstoeliminatetheHalloween
andphantom-readproblems.
The transaction can set the lock granularity to table level by using the lock
table statement. This is useful for applications that know their desired level of
isolation is at the table level. Also, DB2 chooses the appropriate locking granu-
larities for utilities such as reorg and load. The of?ine versions of these utilities
usuallylockthetableinexclusivemode.Theonlineversionsoftheutilitiesallow
othertransactionstoproceedconcurrentlybyacquiringrowlocks.
A deadlock detection agent is activated for each database and periodically
checksfordeadlocksbetweentransactions.Theintervalfordeadlockdetectionis
a con?gurable parameter. In case of a deadlock, the agent chooses a victim and
abortsitwithadeadlock SQLerrorcode.
29.10.2 Commit and Rollback
Applicationscancommitorrollbackbyusingexplicitcommitorrollbackstate-
ments. Applications can also issue begin transaction and end transaction state-
mentstocontrolthescopeoftransactions.Nestedtransactionsarenotsupported.
Normally, DB2releasesalllocksthatitholdsonbehalfofatransactionatcommit
or rollback.However,ifacursorstatementhasbeendeclaredbyusingthe with
holdclause,thensomelocksaremaintainedacrosscommits.
29.10.3 Logging and Recovery
DB2implementsstrict ARIESloggingandrecoveryschemes.Write-aheadlogging
is employed to ?ush log records to the persistent log ?le before data pages are
written or at commit time. DB2 supports two types of log modes: circular log-
ging and archive logging. In circular logging, a prede?ned set of primary and
secondarylog?lesisused.Circularloggingisusefulforcrashrecoveryorappli-
cationfailurerecovery.Inarchival logging, DB2createsnewlog?lesandthe old
log ?les must be archived in order to free up space in the ?le system. Archival
logging is required to perform roll-forward recovery. In both cases, DB2 allows
theusertocon?gurethenumberoflog?lesandthesizesofthelog?les.
In update-intensive environments, DB2 can be con?gured to look for group
commitsinordertobunchlogwrites.
DB2supportstransactionrollbackandcrashrecoveryaswellaspoint-in-time
orroll-forwardrecovery.Inthecaseofcrashrecovery,DB2performsthestandard
phasesofundoprocessingand redoprocessinguptoandfromthelastcheckpoint
inordertorecoverthepropercommittedstateofthedatabase.Forpoint-in-time
recovery,the database can be restoredfrom a backup and can be rolledforward
to a speci?c point in time, using the archived logs. The roll-forward recovery
commandsupportsbothdatabaseandtablespacelevels.Itcanalsobeissuedon
speci?c nodes on a multinode system. A parallel recovery scheme improvesthe
performance in SMP systems by utilizing many CPUs. DB2 performs coordinated
recoveryacross MPPnodesbyimplementingaglobalcheckpointingscheme.
29.11 System Architecture 1219
29.11 System Architecture
Figure29.20showssomeofthedifferentprocessesorthreadsina DB2server.Re-
moteclientapplicationsconnecttothedatabaseserverbyusingcommunication
agentssuchasdb2tcpcm.Eachapplicationisassignedanagent(coordinatoragent
in MPP or SMP environments) called the db2agent thread. This agent and its sub-
ordinateagentsperformtheapplication-relatedtasks.Eachdatabasehasasetof
processesorthreadsthatperformstaskssuchasprefetching,pagecleaningfrom
bufferpool,logging,anddeadlockdetection.Finally,thereisasetofagentsatthe
leveloftheservertoperformtaskssuchascrashdetection,licenseserver,process
creation,andcontrolofsystemresources.DB2providescon?gurationparameters
tocontrolthenumberofthreadsandprocessesinaserver.Almostallthedifferent
typesofagentscanbecontrolledbyusingthecon?gurationparameters.
Figure 29.21 shows the different types of memory segments in DB2.Private
memoryinagentsorthreadsismainlyusedforlocalvariablesanddatastructures
that are relevant only for the current activity. For example, a private sort could
allocate memory from the agent’s private heap. Shared memory is partitioned
into server shared memory, database shared memory,andapplication shared memory.
Processing Model: Single Partition
Remote
Client
Machine
Server
Machine
db2agntp
db2agent
db2agntp
db2pclnr
db2wdog db2gds
db2resyn db2dart
db2sysc db2cart
db2pfchr
db2loggi db2dlock
db2dari db2udfp
db2agntp
db2agent
db2agntp
db2agntp
db2agent
db2agent
db2agntp
App B 
"SQL 
Connect 
to Test"
App C 
"SQL 
Connect 
to Prod"
UserDB2
Processes
Per-instance EDUs
Fenced
UDFs
Processes
Fenced Stored
Procedure Processes
Active subagent
Coordinator
agent
TCPIP
Idle
subagentsg
Unassociated Idle
agents
Per-request
EDUs
Per-connection 
EDUs
Per-active
database
App A 
App B 
Database "TEST"
db2pclnr
db2pfchr
db2loggi
db2bm, db2med, ...
db2dlock
Database "PROD" App C
db2ipccm
db2tcpcm
App A 
"SQL 
Connect 
to Test"
Shared 
memory
and 
sema-
phores
Processes
(threads)
Figure 29.20 Process model in DB2.
1220 Chapter 29 IBMDB2Universal Database
Database Shared Memory Database Shared Memory
Instance Shared Memory
Application Shared Memory
Agent Private Memory
internal structures (appl_ctl_heap_sz)
private sorts (sortheap, sheapthresh)
application heap (applheapsz)
agent stack (agent_stack_sz)
query heap (query_heap_sz)
statement heap (stmtheap)
statistics heap(stat_heap_sz)
bu?er pools (bu?page or ALTERBUF..)
lock list (locklist)
package cache(pckcachesz)
shared sorts(sortheap, sheapthresh)
database heap(dbheap)
   log bu?er(logbufsz)
   catalog cache(catalogcache_sz)
utility heap(util_heap_sz)
includes FCM (fast
1...maxappls
1...maxagents
1...numdb
communication manager) 
Figure 29.21 DB2 memory model.
The database-level shared memory contains useful data structures such as the
buffer pool, lock lists, application package caches, and shared sort areas. The
serverandapplicationsharedmemoryareasareprimarilyusedforcommondata
structuresandcommunicationbuffers.
DB2supportsmultiplebufferpoolsforadatabase.Bufferpoolscanbecreated
byusingthecreatebufferpoolstatementandcanbeassociatedwithtablespaces.
Multiple buffer pools are useful for a variety of reasons but they should be de-
?nedafteracarefulanalysisoftheworkloadrequirements. DB2supportsacom-
prehensive list of memory con?guration and tuning parameters. This includes
parameters for all the large data structure heap areas such as the default buffer
pool, the sort heap, package cache, application-control heaps, and the lock-list
area.
29.12 Replication, Distribution, and External Data
DB2ReplicationisaproductintheDB2familythatprovidesreplicationcapabilities
among other DB2 relational data sources such as Oracle, Microsoft SQL Server,
Sybase Adaptive Server Enterprise, and Informix, as well as nonrelational data
sources such as IBM’s IMS. It consists of capture and apply components, which
arecontrolledbyadministrationinterfaces.Thechange-capturemechanismsare
either “log-based” for DB2 tables or “trigger-based” in the case of other data
sources.Thecapturedchangesarestoredintemporarystagingtableareasunder
thecontrolofDB2Replication.Thesestagedintermediatetableswithchangesare
29.13 BusinessIntelligence Features 1221
thenappliedtodestinationtablesusingregular SQLstatements:inserts,updates,
and deletes. SQL-based transformations can be performed on the intermediate
stagingtablesbyusing?lteringconditionsaswellasaggregations.Theresulting
rowscanbeappliedtooneormoretargettables.Alloftheseactionsarecontrolled
bytheadministrationfacility.
DB2supportsafeaturecalledqueuereplication.Queue(Q)replicationcreatesa
queuetransportmechanismusingIBM’smessage-queueproducttoshipcaptured
log records as messages. These messages are extracted from the queues at the
receivingendandappliedagainsttargets.Theapplyprocesscanbeparallelized
andallowsforuser-speci?edcon?ictresolutionrules.
AnothermemberoftheDB2familyistheDB2information-integratorproduct,
which provides federation, replication (using the replication engine described
above),andsearchcapabilities.Thefederatededitionintegratestablesinremote
DB2 or other relational databases into a single distributed database. Users and
developerscanaccessvariousnonrelationaldatasourcesintabularformat,using
wrapper technology. The federation engine provides a cost-based method for
queryoptimizationacrossthedifferentdatasites.
DB2supportsuser-de?nedtablefunctionsthatenableaccesstononrelational
and external data sources. User-de?ned table functions are created by using the
createfunctionstatementwiththeclausereturnstable.Usingthesefeatures,DB2
isabletoparticipateinthe OLEDBprotocols.
Finally, DB2 provides full support for distributed transaction processing us-
ing the two-phase commit protocol. DB2 can act as the coordinator or agent for
distributed XAsupport. As a coordinator, DB2 can perform all stages of the two-
phase commit protocol. As a participant, DB2 can interact with any commercial
distributedtransactionmanager.
29.13 Business Intelligence Features
DB2DataWarehouseEditionisanofferingintheDB2familythatincorporatesbusi-
ness intelligence features. Data Warehouse Editionhas at its foundation the DB2
engine,andenhancesitwithfeaturesforETL,OLAP,mining,andonlinereporting.
The DB2engineprovidesscalabilityusingits MPPfeatures.Inthe MPPmode, DB2
can support con?gurations that can scale to severalhundredsof nodes for large
database sizes (terabytes). Additionally, features such as MDC and MQT provide
supportforthecomplexquery-processingrequirementsofbusinessintelligence.
Another aspect of business intelligence is online analytical processing or
OLAP.TheDB2 family includes a feature called cube views that provides a mech-
anism to construct appropriate data structures and MQTsinsideDB2 that can be
used for relational OLAP processing. Cube views provide modeling support for
multidimensionalcubesandprovidesamappingmechanismtoarelationalstar
schema. This model is then used to recommend appropriate MQTs, indices, and
MDCde?nitionstoimprovetheperformanceofOLAPqueriesagainstthedatabase.
Inaddition,cube viewscantake advantageof DB2’s nativesupport for the cube
by and rollup operations for generating aggregated cubes. Cube views is a tool
1222 Chapter 29 IBMDB2Universal Database
that can be used to integrate DB2 tightly with OLAP vendors such as Business
Objects,Microstrategy,andCognos.
Inaddition,DB2alsoprovidesmultidimensionalOLAPsupportusingthe DB2
OLAPserver.TheDB2OLAPservercancreateamultidimensionaldatamartfrom
an underlying DB2 database for analysis by OLAP techniques. The OLAP engine
fromtheEssbaseproductisusedintheDB2 OLAPserver.
DB2 Alphablox is a new feature that provides online, interactive, reporting,
andanalysiscapabilities.AveryattractivefeatureoftheAlphabloxfeatureisthe
abilitytoconstructnewWeb-basedanalysisformsrapidly,usingabuildingblock
approachcalled blox.
For deep analytics, DB2 Intelligent Miner provides various components for
modeling,scoring,andvisualizingdata.Miningenablesuserstoperformclassi-
?cation, prediction, clustering, segmentation, and association against large data
sets.
Bibliographical Notes
The origin of DB2 can be traced back to the System R project (Chamberlin et al.
[1981]). IBM Research contributions include areas such as transaction processing
(write-aheadloggingandARIESrecoveryalgorithms)(Mohanetal.[1992]),query
processing and optimization (Starburst) (Haas et al. [1990]), parallel processing
(DB2 Parallel Edition) (Baru et al. [1995]), active database support (constraints,
triggers) (Cochrane et al. [1996]), advanced query and warehousing techniques
suchasmaterializedviews(Zaharioudakisetal.[2000],Lehneretal.[2000]),mul-
tidimensionalclustering(Padmanabhanetal.[2003],Bhattacharjeeetal.[2003]),
autonomicfeatures(Zilioetal.[2004]),andobject-relationalsupport(ADTs,UDFs)
(Careyetal.[1999]).Multiprocessorquery-processingdetailscanbefoundinBaru
etal.[1995].DonChamberlin’sbooksprovideagoodreviewofthe SQLandpro-
gramming features of earlier versions of DB2 (Chamberlin [1996], Chamberlin
[1998]). Earlier books by C. J. Date and others provide a good review of the
featuresof DB2UniversalDatabasefor OS/390(Date[1989],Martinetal.[1989]).
The DB2manualsprovidethede?nitiveviewofeachversionof DB2.Mostof
thesemanualsareavailableonline(http://www.software.ibm.com/db2).Books
on DB2 for developers and administrators include Gunning [2008], Zikopoulos
etal.[2004],Zikopoulosetal.[2007]andZikopoulosetal.[2009].
CHAPTER
30
Microsoft SQL Server
Sameet Agarwal, Jos´ e A. Blakeley, Thierry D’Hers,
Ketan Duvedi, C´ esar A. Galindo-Legaria, Gerald
Hinson, Dirk Myers, Vaqar Pirzada, Bill Ramos,
Balaji Rathakrishnan, Jack Richins, Michael Rys,
Florian Waas, Michael Zwilling
MicrosoftSQLServerisarelationaldatabase-managementsystemthatscalesfrom
laptopsand desktopsto enterpriseservers,with a compatible version, based on
the Windows Mobile operating system, available for handheld devices such as
PocketPCs,SmartPhones,andPortableMediaCenters.SQLServerwasoriginally
developedinthe1980satSybasefor UNIXsystemsandlaterportedtoWindows
NT systems by Microsoft. Since 1994, Microsoft has shipped SQL Server releases
developedindependentlyofSybase,whichstoppedusingtheSQLServernamein
thelate1990s.Thelatestrelease,SQLServer2008,isavailableinexpress,standard,
and enterprise editions and localized for many languages around the world. In
thischapter,theterm SQLServerreferstoalloftheseeditionsof SQLServer2008.
SQLServerprovidesreplicationservicesamongmultiplecopiesofSQLServer
and with other database systems. Its Analysis Services, an integral part of the
system, includes online analytical processing (OLAP) and data-mining facilities.
SQLServerprovidesalargecollectionofgraphicaltoolsand “wizards”thatguide
databaseadministratorsthroughtaskssuchassettingupregularbackups,repli-
catingdataamongservers,andtuningadatabaseforperformance.Manydevel-
opment environments support SQL Server, including Microsoft’s Visual Studio
andrelatedproducts,inparticularthe .NETproductsandservices.
30.1 Management, Design, and Querying Tools
SQL Server provides a suite of tools for managing all aspects of SQL Server de-
velopment, querying, tuning, testing, and administration. Most of these tools
center around the SQL Server Management Studio. SQL ServerManagement Stu-
1223
1224 Chapter 30 MicrosoftSQL Server
dio provides a common shell for administering all services associated with SQL
Server, which includes Database Engine, Analysis Services, Reporting Services,
SQLServerMobile,andIntegrationServices.
30.1.1 Database Development and Visual Database Tools
While designing adatabase, the database administrator createsdatabase objects
such as tables, columns, keys, indices, relationships, constraints, and views. To
help create these objects, the SQL Server Management Studio provides access to
visual database tools. These tools provide three mechanisms to aid in database
design:theDatabaseDesigner,theTableDesigner,andtheViewDesigner.
The Database Designer is a visual tool that allows the database owner or
theowner’s delegatestocreatetables,columns,keys,indices,relationships,and
constraints. Within this tool, a user can interact with database objects through
database diagrams, which graphically show the structure of the database. The
ViewDesignerprovidesavisualquerytoolthatallowstheusertocreateormodify
SQL views through the use of Windows drag-and-drop capabilities. Figure 30.1
showsaviewopenedfromtheManagementStudio.
30.1.2 Database Query and Tuning Tools
SQL ServerManagement Studioprovidesseveraltools to aid the application de-
velopmentprocess.Queriesandstoredprocedurescan bedevelopedand tested
usingtheintegratedQueryEditor .TheQueryEditorsupportscreatingand
editing scripts for a variety of environments, including Transact-SQL,theSQL
Server scripting language SQLCMD, the multidimensional expression language
MDXwhichisusedfordataanalysis,the SQL Serverdata-mininglanguage DMX,
the XML-analysis language XMLA,andSQL Server Mobile. Further analysis can
bedoneusingthe SQLServerPro?ler.Databasetuningrecommendationsarepro-
videdbytheDatabaseTuningAdvisor.
30.1.2.1 QueryEditor
The integrated Query Editor providesa simple graphical user interface for run-
ning SQL queries and viewing the results. The Query Editor also provides a
graphical representation of showplan, the steps chosen by the optimizer for
queryexecution.TheQueryEditorisintegratedwithManagementStudio’sOb-
jectExplorer,which letsauserdraganddropobjectortablenamesintoaquery
windowandhelpsbuildselect,insert,update,ordeletestatementsforanytable.
AdatabaseadministratorordevelopercanuseQueryEditorto:
  Analyze queries: Query Editor can show a graphical or textual execution
plan for any query, as well as displaying statistics regarding the time and
resourcesrequiredtoexecuteanyquery.
  Format SQL queries:Includingindentingandcolorsyntaxcoding.
30.1 Management, Design, and Querying Tools 1225
 
Figure 30.1 The View Designer opened for the HumanResources.vEmployee view.
  Use templates for stored procedures, functions, and basic SQL statements:
The Management Studio comes with dozens of prede?ned templates for
building DDLcommands,oruserscande?netheirown.
Figure 30.2 shows the Management Studio with the Query Editor display-
ing the graphical execution plan for a query involving a four-table join and an
aggregation.
30.1.2.2 SQL Pro?ler
SQL Pro?ler is a graphical utility that allows database administrators to monitor
andrecorddatabaseactivityoftheSQLServerDatabaseEngineandAnalysisSer-
vices.SQLPro?lercandisplayallserveractivityinrealtime,oritcancreate?lters
1226 Chapter 30 MicrosoftSQL Server
 
Figure 30.2 A showplan for a four-table join with group by aggregation.
thatfocusontheactionsofparticularusers,applications,ortypesofcommands.
SQL Pro?ler can display any SQL statement or stored procedure sent to any in-
stanceofSQLServer(ifthesecurityprivilegesallowit)inadditiontoperformance
data indicating how long the query took to run, how much CPU and I/O was
needed,andtheexecutionplanthatthequeryused.
SQL Pro?ler allows drilling down even deeper into SQL Server to monitor
everystatementexecutedaspartofastoredprocedure,everydatamodi?cation
operation,everylockacquiredorreleased,oreveryoccurrenceofadatabase?le
growingautomatically.Dozensofdifferenteventscanbecaptured,anddozensof
dataitemscanbecapturedforeachevent.SQLServeractuallydividesthetracing
functionality into two separate but connected components. The SQL Pro?ler is
the client-side trace facility. Using SQL Pro?ler, a user can choose to save the
captured data to a ?le or a table, in additionto displayingit in the Pro?ler User
Interface (UI). The Pro?ler tool displays every event that meets the ?lter criteria
as it occurs. Once trace data are saved, SQL Pro?ler can read the saved data for
displayoranalysispurposes.
30.1 Management, Design, and Querying Tools 1227
On the server side is the SQL trace facility, which manages queues of events
generatedbyeventproducers.Aconsumerthreadreadseventsfromthequeues
and?ltersthembeforesendingthemtotheprocessthatrequestedthem.Events
are the main unit of activity as far as tracing is concerned, and an event can be
anythingthathappensinside SQLServer,orbetween SQLServerandaclient.For
example,creatingordroppinganobjectisanevent,executingastoredprocedure
isanevent,acquiringorreleasingalockisanevent,andsendingaTransact-SQL
batch from a client to the SQL Server is an event. There is a set of stored system
procedurestode?newhicheventsshouldbetraced,whatdataforeacheventare
interesting, and where to save the information collected from the events. Filters
appliedtotheeventscanreducetheamountofinformationcollectedandstored.
SQL Server guarantees that certain critical information will always be gath-
ered,anditcanbeusedasausefulauditingmechanism.SQLServeriscerti?edfor
U.S.governmentC2-levelsecurity,andmanyofthetraceableeventsareavailable
solelytosupportC2-certi?cationrequirements.
30.1.2.3 The Database TuningAdvisor
Queriesandupdatescanoftenexecutemuchfasterifanappropriatesetofindices
isavailable.Designingthebestpossibleindicesforthetablesinalargedatabase
is a complex task; it not only requires a thorough knowledge of how SQL Server
usesindicesandhowthequeryoptimizermakesitsdecisions,buthowthedata
will actually be used by applications and interactive queries. The SQL Server
DatabaseTuningAdvisor(DTA)isapowerfultoolfordesigningthebestpossible
indices and indexed (materialized) views based on observed query and update
workloads.
DTAcantuneacrossmultipledatabasesanditbasesitsrecommendationson
aworkloadthatcanbea?leofcapturedtraceevents,a?leof SQLstatements,or
anXMLinput?le.SQLPro?lercanbeusedtocaptureallSQLstatementssubmitted
byallusersoveraperiodoftime.DTAcanthenlookatthedataaccesspatternsfor
allusers,forallapplications,foralltables,andmakebalancedrecommendations.
30.1.3 SQL Server Management Studio
Inadditiontoprovidingaccesstothedatabasedesignandvisualdatabasetools,
the easy-to-use SQL Server Management Studio supports centralized manage-
ment of all aspects of multiple installations of the SQL Server Database Engine,
Analysis Services, Reporting Services, Integration Services, and SQL Server Mo-
bile, including security,events, alerts, scheduling, backup, servercon?guration,
tuning,full-textsearch,andreplication. SQLServerManagementStudioallowsa
databaseadministratortocreate,modify,andcopy SQLServerdatabaseschemas
and objects such as tables, views, and triggers. Because multiple installations
of SQL Server can be organized into groups and treated as a unit, SQL Server
ManagementStudiocanmanagehundredsofserverssimultaneously.
AlthoughitcanrunonthesamecomputerastheSQLServerengine,SQLServer
Management Studio offers the same management capabilities while running on
1228 Chapter 30 MicrosoftSQL Server
 
Figure 30.3 The SQL Server Management Studio interface.
any Windows 2000 (or later) machine. In addition, the ef?cient client–server
architecture of SQL Server makes it practical to use the remote-access (dial-up
networking)capabilitiesofWindowsforadministrationandmanagement.
SQLServerManagementStudiorelievesthedatabaseadministratorfromhav-
ing to know the speci?c steps and syntax to complete a job. It provideswizards
toguidethedatabaseadministratorthroughtheprocessofsettingupandmain-
taining an installation of SQL Server. Management Studio’s interface is shown
in Figure 30.3 and illustrates how a script for database backup can be created
directlyfromitsdialogs.
30.2 SQL Variations and Extensions
SQLServerallowsapplicationdeveloperstowriteserver-sidebusinesslogicusing
Transact-SQL or a .NET programming language such as C#, Visual Basic, COBOL,
orJ++.Transact-SQLisacompletedatabaseprogramminglanguagethatincludes
data-de?nitionanddata-manipulationstatements,iterativeandconditionalstate-
ments, variables, procedures, and functions. Transact-SQL supports most of the
mandatory DDL query and data modi?cation statements and constructs in the
30.2 SQL Variations and Extensions 1229
SQL:2003standard.SeeSection30.2.1forthelistof SQL:2003datatypessupported.
Inadditiontothemandatoryfeatures,Transact-SQLalsosupportsmanyoptional
features in the SQL:2003 standard such as recursive queries, common table ex-
pressions,user-de?nedfunctions, andrelationaloperatorssuchas intersect and
exceptamongothers.
30.2.1 Data Types
SQL Server 2008 supports all the mandatory scalar data types in the SQL:2003
standard. SQL Server also supports the ability to alias system types using user-
supplied names; the aliasing is similar in functionality to the SQL:2003 distinct
types,butnotfullycompliantwiththem.
Someprimitivetypesuniqueto SQLServerinclude:
  Large character and binary string types of variable size up to 2
31
?1bytes,
usingthevarchar/nvarchar/varbinary(max)datatype,whichhasaprogram-
ming modelthat issimilartothesmall-character and byte-stringtypes.Ad-
ditionally,theysupportastorageattributecalled FILESTREAMtospecifythat
data for each individual column value is stored as a separate ?le in the
?lesystem. FILESTREAMstorageallowshigherperformancestreamingaccess
toapplicationsusingthenative?lesystem API.
  An XML type, described in Section 30.11, which is used to store XML data
inside a table column. The XML type can optionally have an associated XML
schema collectionspecifyingaconstraint thattheinstancesofthetypeshould
adheretooneofthe XMLtypesde?nedintheschemacollection.
  sql variantisascalardatatypethatcancontainvaluesofanySQLscalartype
(except large character and binary types and sql variant). This type is used
by applications that need to store data whose type cannot be anticipated at
data-de?nitiontime.sql variantisalsothetypeofacolumnformedfromthe
executionofanunpivotrelationaloperator(seeSection30.2.2).Internally,the
systemkeepstrackoftheoriginaltypeofthedata.Itispossibleto?lter,join,
and sort on sql variant columns. The system function sql variant property
returns details on the actual data stored in a column of type sql variant,
includingthebasetypeandsizeinformation.
  ThehierarchyIddatatypemakesiteasiertostoreandqueryhierarchicaldata.
Hierarchicaldataarede?nedasasetofdataitemsrelatedtooneanotherby
hierarchical relationships where one item of data is the parent of another
item.Commonexamplesinclude:anorganizationalstructure,ahierarchical
?lesystem,asetoftasksinaproject,ataxonomyoflanguageterms,asingle-
inheritance type hierarchy, part-subpart relationships, and a graph of links
amongWebpages.
  SQLServersupportsstoringandqueryingofgeospatialdata,thatis,location
data referenced to the earth. Common models of these data are the planar
and geodetic coordinate systems. The main distinction between these two
systems is that the latter takes into account the curvature of the earth. SQL
1230 Chapter 30 MicrosoftSQL Server
Server supports geometry and geography, which correspond to the planar
andgeodeticmodels.
In addition, SQL Server supports a table type and a cursor type that cannot
be used as columns in a table, but can be used in the Transact-SQL language as
variables:
  Atabletypeenablesavariabletoholdasetofrows.Aninstanceofthistype
is used primarily to hold temporary results in a stored procedure or as the
returnvalueofatable-valuedfunction. Atablevariablebehaveslikealocal
variable.Ithasawell-de?nedscope,whichisthefunction,storedprocedure,
orbatchinwhichitisdeclared.Withinitsscope,atablevariablemaybeused
likearegulartable.Itmaybeappliedanywhereatableortableexpressionis
usedinselect, insert, update,and deletestatements.
  A cursortypethat enablesreferencestoacursor object.Thecursor typecan
beusedtodeclarevariables,orroutineinput/outputargumentstoreference
cursorsacrossroutinecalls.
30.2.2 Query Language Enhancements
Inadditiontothe SQLrelationaloperatorssuchasinnerjoinandouterjoin, SQL
Serversupportstherelationaloperatorspivot,unpivot,andapply.
  pivotisanoperatorthattransformstheshapeofitsinputresultsetfromtwo
columnsthatrepresentname-valuepairsintomultiplecolumns,oneforeach
namefromtheinput.Thenamecolumnfromtheinputiscalledthepivotcol-
umn.Theuserneedstoindicatewhichnamestotransposefromtheinputinto
individualcolumnsintheoutput.ConsiderthetableMonthlySales(ProductID,
Month, SalesQty). The following query, using the pivot operator, returns the
SalesQtyforeachofthemonthsJan,Feb,andMarasseparatecolumns.Note
thatthepivotoperatoralsoperformsanimplicitaggregationonalltheother
columnsinthetableandanexplicitaggregationonthepivotcolumn.
select*
from MonthlySales pivot(sum(SalesQty)for Month in(’Jan’,’Feb’,’Mar’)) T;
Theinverseoperationof pivotisunpivot.
  The apply operator is similar to join, except its right input is an expression
thatmaycontainreferencestocolumnsintheleftinput,forexampleatable-
valuedfunctioninvocationthattakesasargumentsoneormorecolumnsfrom
the left input. The set of columns produced by the operator is the union of
thecolumnsfromitstwoinputs.Theapplyoperatorcanbeusedtoevaluate
its right input for each row of its left input and perform a union all of the
rowsacrossalltheseevaluations.Therearetwo?avorsoftheapplyoperator
similar to join,namely ,cross and outer. The two ?avors differ in terms of
30.2 SQL Variations and Extensions 1231
howtheyhandlethecaseoftherightinputproducinganemptyresult-set.In
thecaseofcrossapply,thiscausesthecorrespondingrowfromtheleftinput
tonot appearinthe result.Inthe caseof outer apply, the row appears from
theleftinputwithNULLvaluesforthecolumnsintherightinput.Consider
atable-valuedfunctioncalledFindReportsthattakesasinputtheIDofagiven
employeeandreturnsthesetofemployeesreportingdirectlyorindirectlyto
thatemployeeinanorganization.Thefollowingquerycallsthisfunctionfor
themanagerofeachdepartmentfromthe Departmentstable:
select*
from Departments D cross apply FindReports(D.ManagerID)
30.2.3 Routines
Users can write routines that run inside the server process as scalar or table
functions,storedprocedures,andtriggersusingTransact-SQLora.NETlanguage.
Alltheseroutinesarede?nedtothedatabasebyusingthecorresponding create
[function,procedure,trigger]DDLstatement.Scalarfunctionscanbeusedinany
scalar expression inside an SQL DML or DDL statement. Table-valued functions
canbeusedanywhereatableisallowedinaselectstatement.Transact-SQLtable-
valued functions whose body contains a single SQL select statement are treated
asaview(expandedinline)inthequerythatreferencesthefunction.Sincetable-
valued functions allow input arguments, inline table-valued functions can be
consideredparameterizedviews.
30.2.3.1 IndexedViews
In addition to traditional views as de?ned in ANSI SQL, SQL Server supports
indexed (materialized)views. Indexedviews can substantially enhance the per-
formance of complex decision support queries that retrieve large numbers of
base table rows and aggregate large amounts of information into concise sums,
counts, and averages. SQL Server supports creating a clustered index on a view
and subsequently any number of nonclustered indices. Once a view is indexed,
the optimizer can use its indices in queries that reference the view or its base
tables.Thereisnoneedforqueriestorefertotheviewexplicitlyfortheindexed
view to be used in the query plan, as the matching is done automatically from
the view de?nition. This way, existing queries can bene?t from the improved
ef?ciency of retrieving data directly from the indexed view without having to
be rewritten. The indexedview is maintained consistent with the base tables by
automaticallypropagatingallupdates.
30.2.4 Filtered Indexes
A ?ltered index is an optimized nonclustered index, especially suited to cover
queriesthatselectfromawell-de?nedsubsetofdata.Itusesa?lterpredicateto
indexaportionofrowsinthetable.Awell-designed?lteredindexcanimprove
1232 Chapter 30 MicrosoftSQL Server
query performance, reduce index-maintenance costs, and reduce index-storage
costscomparedwithfull-tableindices.Filteredindicescanprovidethefollowing
advantagesoverfull-tableindices:
  Improved query performance and plan quality. A well-designed ?ltered
index improves query performance and execution plan quality because it
issmallerthanafull-tablenonclusteredindexandhas?lteredstatistics.The
?lteredstatisticsaremoreaccuratethanfull-tablestatisticsbecausetheycover
onlytherowsinthe?lteredindex.
  Reduced index maintenance costs. Anindexismaintainedonlywhendata
manipulation language (DML) statements affect the data in the index. A ?l-
tered index reduces index maintenance costs compared to a full-table non-
clusteredindexbecauseitissmallerandisonlymaintainedwhenthedatain
theindexareaffected.Itispossibletohavealargenumberof?lteredindices,
especiallywhentheycontaindatathatareaffectedinfrequently.Similarly,if
a?lteredindexcontainsonlythefrequentlyaffecteddata,thesmallersizeof
theindexreducesthecostofupdatingthestatistics.
  Reducedindexstoragecosts.Creatinga?lteredindexcanreducediskstorage
for nonclustered indices when a full-table index is not necessary. You can
replaceafull-tablenonclusteredindexwithmultiple?lteredindiceswithout
signi?cantlyincreasingthestoragerequirements.
Filtered statistics can also be created explicitly, independently from ?ltered
indices.
30.2.4.1 UpdatableViews and Triggers
Generally,viewscanbethetargetofupdate,delete,orinsertstatementsifthedata
modi?cationappliestoonlyoneoftheview’sbasetables.Updatestopartitioned
views can be propagated to multiple base tables. For example, the following
updatewillincreasethepricesforpublisher “0736”by10percent:
update titleview
set price= price*1.10
where pub id=’0736’;
For data modi?cations that affect more than one base table, the view can be
updated if there is an instead trigger de?ned for the operation; instead triggers
for insert, update,ordelete operations can be de?ned on a view, to specify the
updates that must be performed on the base tables to re?ect the corresponding
modi?cationsontheview.
TriggersareTransact-SQLor.NETproceduresthatareautomaticallyexecuted
wheneitheraDML(update,insert,ordelete)orDDLstatementisissuedagainsta
basetableorview.Triggersaremechanismsthatenableenforcementofbusiness
logic automatically when data are modi?ed or when DDL statements are exe-
cuted.Triggerscanextendtheintegritycheckinglogicofdeclarativeconstraints,
30.3 Storage and Indexing 1233
defaults, and rules, although declarative constraints should be used preferably
whenevertheysuf?ce,astheycanbeusedbythequeryoptimizertoreasonabout
thedatacontents.
Triggerscan be classi?ed into DML and DDL triggers depending on the kind
ofeventthat?resthetrigger.DMLtriggersarede?nedagainstatableorviewthat
isbeingmodi?ed. DDLtriggersarede?nedagainstanentiredatabaseforoneor
more DDLstatementssuchas create table, dropprocedure,etc.
Triggers can be classi?ed into after and instead triggers according to when
thetriggergetsinvokedrelativetotheactionthat?resthetrigger. Aftertriggers
execute after the triggering statement and subsequent declarative constraints
are enforced. Instead triggers execute instead of the triggering action. Instead
triggerscanbethoughtofassimilarto beforetriggers,buttheyactuallyreplace
thetriggeringaction.InSQLServer,DMLaftertriggerscanbede?nedonlyonbase
tables,whileDMLinsteadtriggerscanbede?nedonbasetablesorviews.Instead
triggersallowpracticallyanyviewtobemadeupdatableviauser-providedlogic.
DDLinsteadtriggerscanbede?nedonany DDLstatement.
30.3 Storage and Indexing
In SQL Server, a database refers to a collection of ?les that contain data and
are supported by a single transaction log. The database is the primary unit of
administrationinSQLServerandalsoprovidesacontainerforphysicalstructures
suchastablesandindicesandforlogicalstructuressuchasconstraintsandviews.
30.3.1 Filegroups
In order to manage space effectively in a database, the set of data ?les in a
databaseisdividedintogroupscalled?legroups.Each?legroupcontainsoneor
moreoperating-system?les.
Every database has at least one ?legroup known as the primary ?legroup.
This ?legroup contains all the metadata for the database in system tables. The
primary?legroupmayalsostoreuserdata.
Ifadditional,user-de?ned?legroupshavebeencreated,ausercanexplicitly
control the placement of individual tables, indices, or the large-object columns
of a table by placing them in a particular ?legroup. For example, the user may
choose to store performance critical indices on a ?legroup located on solid state
disks. Likewise they may choose to place varbinary(max) columns containing
videodataonanI/Osubsystemoptimizedforstreaming.
30.3.2 Space Management within Filegroups
One of the main purposes for ?legroups is to allow for effective space manage-
ment.Alldata?lesaredividedinto?xed-size8-kilobyteunitscalled pages.The
allocation system is responsible for allocating these pages to tables and indices.
1234 Chapter 30 MicrosoftSQL Server
The goal of the allocation system is to minimize the amount of space wasted
while,atthesametime,keepingtheamountoffragmentationinthedatabaseto
a minimum to ensure good scan performance. In order to achieve this goal, the
allocationmanagerusuallyallocatesanddeallocatesallthepagesinunitsofeight
contiguouspagescalledextents.
Theallocationsystemmanagestheseextentsthroughvariousbitmaps.These
bitmaps allow the allocation system to ?nd a page or an extent for allocation
quickly.Thesebitmapsarealsousedwhenafulltableorindexscanisexecuted.
The advantage of using allocation-based bitmaps for scanning is that it allows
disk-order traversals of all the extents belonging to a table or index-leaf level,
whichsigni?cantlyimprovesthescanperformance.
If there is more than one ?le in a ?legroup, the allocation system allocates
extents for any object on that ?legroup by using a “proportional ?ll” algorithm.
Each ?le is ?lled up in the proportion of the amount of free space in that ?le
compared to other ?les. This ?lls all the ?les in a ?legroup at roughly the same
rateandallowsthesystemtoutilizeallthe?lesinthe?legroupevenly.Filescan
alsobecon?guredtogrowautomaticallyifthe?legroupisrunningoutofspace.
SQL Serverallows?lestoshrink.Inordertoshrinkadata?le, SQL Servermoves
allthedatafromthephysical endofthe?letoapointclosertothebeginning of
the ?le and then actually shrinks the ?le, releasing space back to the operating
system.
30.3.3 Tables
SQL Server supports heap and clustered organizations for tables. In a heap-
organized table, the location of every row of the table is determined entirely
by the system and is not speci?ed in any way by the user. The rows of a heap
havea?xedidenti?erknownastherow(RID),andthisvalueneverchangesun-
lessthe?leisshrunkandtherowismoved.Iftherowbecomeslargeenoughthat
it cannot ?t in the page in which it was originally inserted, the record is moved
to a different place but a forwarding stub is left in the original place so that the
recordcanstillbefoundbyusingitsoriginal RID.
Inaclustered-indexorganization for atable,the rows ofthe table arestored
in a B
+
-tree sorted by the clustering key of the index. The clustered-index key
also serves as the unique identi?er for each row. The key for a clustered index
can be de?ned to be nonunique, in which case SQL Server adds an additional
hidden column to make the key unique. The clustered index also serves as a
searchstructuretoidentifyarowofthetablewithaparticularkeyorscanasetof
rowsofthetablewithkeyswithinacertainrange.Aclusteredindexisthemost
commontypeoftableorganization.
30.3.4 Indices
SQL Server also supports secondary (nonclustered) B
+
-tree indices. Queries that
referonlytocolumnsthatareavailablethroughsecondaryindicesareprocessed
by retrieving pages from the leaf level of the indices without having to retrieve
data from the clustered index or heap. Nonclustered indices over a table with a
30.3 Storage and Indexing 1235
clusteredindexcontainthekeycolumnsoftheclusteredindex.Thus,theclustered
indexrowscanmovetoadifferentpage(viasplits,defragmentation,orevenindex
rebuilds)withoutrequiringchangestothenonclusteredindices.
SQLServersupportstheadditionofcomputedcolumnstoatable.Acomputed
columnisacolumnwhosevalueisanexpression,usuallybasedonthevalueof
othercolumnsinthatrow. SQLServerallowstheusertobuildsecondaryindices
oncomputedcolumns.
30.3.5 Partitions
SQL Server supports range partitioning on tables and nonclustered indices. A
partitionedindexismadeupofmultipleB
+
-trees,oneperpartition.Apartitioned
tablewithout an index(aheap)ismadeupofmultipleheaps,one perpartition.
For brevity, we refer only to partitioned indices (clustered or nonclustered) and
ignoreheapsfortherestofthisdiscussion.
Partitioningalargeindexallowsanadministratormore?exibilityinmanag-
ingthestoragefortheindexandcan improvesomequeryperformancebecause
thepartitionsactasacoarse-grainedindex.
The partitioning for an index is speci?ed by providing both a partitioning
functionandapartitioningscheme.Apartitioningfunctionmapsthedomainof
apartitioningcolumn(anycolumnintheindex)topartitionsnumbered1to N.A
partitioningschememapspartitionnumbersproducedbyapartitioningfunction
tospeci?c?legroupswherethepartitionsarestored.
30.3.6 Online Index Build
Buildingnewindicesandrebuildingexistingindicesonatablecanbeperformed
online, i.e., while select, insert, delete,andupdate operations are being per-
formed on the table. The creation of a new index happens in three phases. The
?rstphaseissimplycreatinganemptyB
+
-treeforthenewindexwiththecatalog
showingthenewindexisavailableformaintenanceoperations.Thatis,thenew
index must be maintained by all subsequent insert, delete,andupdate opera-
tions,butitisnotavailableforqueries.Thesecondphaseconsistsofscanningthe
table to retrieve the index columns for each row, sorting the rows and inserting
themintothenewB
+
-tree.Theseinsertsmustbecarefultointeractwiththeother
rows in the new B
+
-treeplacedtherebyindexmaintenanceoperationsfromup-
datesonthebasetable.Thescanisasnapshotscanthat,withoutlocking,ensures
thescanseestheentiretablewithonlytheresultsofcommittedtransactionsasof
the start of the scan. This is achieved by using the snapshot isolation technique
describedinSection30.5.1.The?nalphaseoftheindexbuildinvolvesupdating
the catalog to indicate the index build is complete and the index is available for
queries.
30.3.7 Scans and Read-ahead
ExecutionofqueriesinSQLServercaninvolveavarietyofdifferentscanmodeson
theunderlyingtablesandindices.Theseincludeorderedversusunorderedscans,
1236 Chapter 30 MicrosoftSQL Server
serial versus parallel scans, unidirectional versus bidirectional scans, forward
versus backward scans, and entire table or index scans versus range or ?ltered
scans.
Each of the scan modes has a read-ahead mechanism that tries to keep the
scanaheadoftheneedsofthequeryexecution,inordertoreduceseekandlatency
overheads and utilize disk idle time. The SQL Server read-ahead algorithm uses
the knowledge from the query-execution plan in order to drive the read-ahead
and make sure that only data that are actually needed by the query are read.
Also, the amount of read-ahead is automatically scaled according to the size of
thebufferpool,theamountofI/Othedisksubsystemcansustain,andtherateat
whichthedataarebeingconsumedbyqueryexecution.
30.3.8 Compression
SQL Server supports both row and page compression for tables and indices. Row
compression uses a variable-length format for data types such as integers that
are traditionally considered ?xed-length. Page compression removes common
pre?xesoncolumnsandbuildsaper-pagedictionaryforcommonvalues.
30.4 Query Processing and Optimization
The query processor of SQL Server is based on an extensible framework that
allows rapid incorporation of new execution and optimization techniques. Any
SQLquerycanbeexpressedasatreeofoperatorsinanextendedrelationalalgebra.
Abstractingoperatorsofthisalgebraintoiterators,queryexecutionencapsulates
data-processingalgorithmsaslogicalunitsthatcommunicatewitheachotherby
usingaGetNextRow()interface.Startingoutwithaninitialquerytree,thequery
optimizergeneratesalternativesbyusingtreetransformationsandestimatestheir
execution cost by taking into account iterator behavior and statistical models to
estimatethenumberofrowstoprocess.
30.4.1 Overview of Compilation Process
Complex queries present signi?cant optimization opportunities that require re-
ordering operators across query block boundaries and selecting plans solely on
thebasisofestimatedcosts.Togoaftertheseopportunities,thequeryoptimizer
deviatesfromtraditionalquery-optimizationapproachesusedinothercommer-
cialsystemsinfavorofamoregeneral,purelyalgebraicframeworkthatisbased
on the Cascades optimizer prototype. Query optimization is part of the query-
compilationprocess,whichconsistsoffoursteps:
  Parsing/binding.Afterparsing,thebinderresolvestableandcolumnnames
by using the catalogs. SQL Server utilizes a plan cache to avoid repeated
optimizationofidenticalor structurallysimilarqueries.Ifnocached planis
available, an initial operator tree is generated. The operator tree is simply a
30.4 Query Processing and Optimization 1237
combination ofrelationaloperatorsandisnot constrainedbyconcepts such
asqueryblocksorderivedtables,whichtypicallyobstructoptimization.
  Simpli?cation/normalization.Theoptimizerappliessimpli?cationruleson
theoperatortreetoobtainanormal,simpli?edform.Duringsimpli?cation,
theoptimizerdeterminesandloadsstatisticsrequiredforcardinalityestima-
tion.
  Cost-basedoptimization.Theoptimizerappliesexplorationandimplemen-
tation rules to generate alternatives, estimates execution cost, and chooses
theplanwiththecheapestanticipatedcost.Explorationrulesimplementre-
ordering for an extensive set of operators, including join and aggregation
reordering. Implementation rules introduce execution alternatives such as
mergejoinandhashjoin.
  Plan preparation. The optimizer creates query-execution structures for the
selectedplan.
Toachievebestresults,cost-basedoptimizationisnotdividedintophasesthat
optimizedifferentaspectsofthequeryindependently;also,itisnotrestrictedtoa
singledimensionsuchasjoinenumeration.Instead,acollectionoftransformation
rulesde?nesthespaceofinterest,andcostestimationisuseduniformlytoselect
anef?cientplan.
30.4.2 Query Simpli?cation
During simpli?cation,only transformations that areguaranteedtogenerateless
costly substitutes are applied. The optimizer pushes selects down the operator
treeasfaraspossible;itcheckspredicatesforcontradictions,takingintoaccount
declaredconstraints.Itusescontradictionstoidentifysubexpressionsthatcanbe
removedfromthetree.Acommonscenarioistheeliminationof unionbranches
thatretrievedatafromtableswithdifferentconstraints.
Anumberofsimpli?cationrulesarecontextdependent;thatis,thesubstitution
is valid only in the context of utilization of the subexpression. For example,
an outer join can be simpli?ed into an inner join if a later ?lter operation will
discard nonmatching rows that were paddedwith null.Another exampleis the
eliminationofjoinsonforeignkeys,whentherearenolaterusesofcolumnsfrom
the referenced table. A third example is the context of duplicate insensitivity,
whichspeci?esthatdeliveringoneormultiplecopiesofarowdoesnotaffectthe
query result. Subexpressions under semijoins and under distinct are duplicate
insensitive,whichallowsturning unionintounion all,forexample.
For grouping and aggregation, the GbAgg operator is used, which creates
groups and optionally applies an aggregate function on each group. Duplicate
removal, expressed in SQL by the distinct keyword, is simply a GbAgg with no
aggregate functions to compute. During simpli?cation, information about keys
andfunctionaldependenciesisusedtoreducegroupingcolumns.
Subqueriesare normalized by removing correlated query speci?cations and
usingsomejoinvariantinstead.Removingcorrelationsisnota “subqueryexecu-
1238 Chapter 30 MicrosoftSQL Server
tionstrategy,”butsimplyanormalizationstep.Avarietyofexecutionstrategies
isthenconsideredduringcost-basedoptimization.
30.4.3 Reordering and Cost-Based Optimization
In SQLServer,transformationsarefullyintegratedintothecost-basedgeneration
and selection of execution plans. The query optimizer includes about 350 logi-
cal and physical transformation rules. In addition to inner-join reordering, the
queryoptimizeremploysreorderingtransformationsfortheoperatorsouterjoin,
semijoin,andantisemijoin,fromthestandardrelationalalgebra(withduplicates,
for SQL). GbAgg is reordered as well, by moving it below or above joins when
possible.Partialaggregation,thatis,introducinganew GbAggwithgroupingon
asupersetofthecolumnsofasubsequentGbAgg,is consideredbelowjoins and
unionall,andalsoinparallelplans.Seethereferencesgiveninthebibliographical
notesfordetails.
Correlatedexecutionisconsideredduringplanexploration,thesimplestcase
being index-lookup join. SQL Server models correlated execution as a single al-
gebraicoperator,called apply,whichoperatesonatable T andaparameterized
relational expression E(t). Apply executes E for each row of T,whichprovides
parametervalues.Correlatedexecutionisconsideredasanexecutionalternative,
regardless of the use of subqueries in the original SQL formulation. It is a very
ef?cient strategy when table T is small and indices support ef?cient parameter-
ized execution of E(t). Furthermore, we consider reduction on the number of
executions of E(t) when there are duplicate parameter values, by means of two
techniques: Sort T on parameter values so that a single result of E(t)isreused
while the parametervalue remains the same, or else use a hash table that keeps
trackoftheresultof E(t)for(somesubsetof)earlierparametervalues.
Someapplicationsselectrowsonthebasisofsomeaggregateresultfortheir
group.Forexample, “Findcustomerswhosebalanceismorethantwicetheaver-
age for their market segment.” The SQL formulation requires a self-join. During
exploration,thispatternisdetectedandper-segmentexecutionoverasinglescan
isconsideredasanalternativetoself-join.
Materialized-viewutilizationisalsoconsideredduringcost-basedoptimiza-
tion. View matching interacts with operator reordering in that utilization may
not be apparent until some other reordering has taken place. When a view is
found to match some subexpression, the table that contains the view result is
added as an alternative for the corresponding expression. Depending on data
distribution and indices available, it may or may not be better than the original
expression—selectionwillbebasedoncostestimation.
To estimate the execution cost of a plan, the model takes into account the
number of times a subexpression is executed, as well as the row goal,whichis
the number of rows expected to be consumed by the parent operator. The row
goal can be less than the cardinality estimate in the case of top-n queries, and
for Apply/semijoin.Forexample, Apply/semijoinoutputsrow tfrom T assoonasa
single row is produced by E(t)(thatis,ittests exists E(t)).Thus,therowgoalof
30.4 Query Processing and Optimization 1239
theoutputof E(t)is1,andtherowgoalsofsubtreesof E(t)arecomputedforthis
rowgoalfor E(t)andusedforcostestimation.
30.4.4 Update Plans
Updateplansoptimizemaintenance ofindices,verifyconstraints,applycascad-
ingactions,andmaintainmaterializedviews.Forindexmaintenance,insteadof
takingeachrowandmaintainingallindicesforit,updateplansmayapplymodi-
?cationsperindex,sortingrowsandapplyingtheupdateoperationinkeyorder.
This minimizes random I/O, especially when the number of rows to update is
large.Constraints arehandledby an assert operator,which executesapredicate
andraisesanerroriftheresultis false.Referentialconstraintsarede?nedby ex-
istspredicates,whichinturnbecomesemijoinsandareoptimizedbyconsidering
allexecutionalgorithms.
The Halloween problem (described earlier in Section 13.6) refers to the fol-
lowinganomaly:Supposeasalaryindexisreadinascendingorder,andsalaries
arebeingraisedby10percent.Asaresultoftheupdate,rowswillmoveforward
intheindexandwillbefoundandupdatedagain,leadingtoanin?niteloop.One
waytoaddressthisproblemistoseparateprocessingintotwophases:Firstread
allrowsthatwillbeupdatedandmakeacopyoftheminsometemporaryplace,
then read from this place and apply all updates. Another alternative is to read
fromadifferentindexwhererowswillnotmoveasaresultoftheupdate.Some
execution plans provide phase separation automatically, if they sort or build a
hashtableontherowstobeupdated.Halloweenprotectionismodeledasaprop-
erty of plans. Multiple plans that provide the required property are considered,
andoneisselectedonthebasisofestimatedexecutioncost.
30.4.5 Data Analysis at Optimization Time
SQLpioneeredtechniquestoperformgatheringofstatisticsaspartofanongoing
optimization. The computation of result size estimates is based on statistics for
columnsusedinagivenexpression.Thesestatisticsconsistofmax-diffhistograms
on the column values and a number of counters that capture densities and row
sizes, among others. Database administrators may create statistics explicitly by
usingextended SQLsyntax.
If no statistics are available for a given column, however, SQL Server’s opti-
mizerputstheongoingoptimizationonholdandgathersstatisticsasneeded.As
soon as the statistics are computed, the original optimization is resumed, lever-
aging the newly created statistics. Optimization of subsequent queries reuses
previously generated statistics. Typically, after a short period of time, statistics
for frequentlyused columns have been created and interruptionsto gather new
statistics become infrequent. By keeping track of the number of rows modi?ed
in a table, a measure of staleness is maintained for all affected statistics. Once
thestalenessexceedsacertainthresholdthestatisticsarerecomputedandcached
plansarerecompiledtotakechangeddatadistributionsintoaccount.
Statistics can be recomputed asynchronously, which avoids potentially long
compile times caused by synchronous computation. The optimization that trig-
1240 Chapter 30 MicrosoftSQL Server
gersthecomputationofstatisticsusespotentiallystalestatistics.However,subse-
quentqueriesareabletoleveragetherecomputedstatistics.Thisallowsstriking
anacceptablebalancebetweentimespentinoptimizationandthequalityofthe
resultingqueryplan.
30.4.6 Partial Search and Heuristics
Cost-basedqueryoptimizersfacetheissueofsearch-spaceexplosionbecauseap-
plicationsdoissuequeriesinvolvingdozensoftables.Toaddressthis,SQLServer
uses multiple optimization stages, each of which uses query transformations to
exploresuccessivelylargerregionsofthesearchspace.
There are simple and complete transformations geared toward exhaustive
optimization,aswellassmarttransformationsthatimplementvariousheuristics.
Smarttransformationsgenerateplansthatareveryfarapartinthesearchspace,
whilesimpletransformationsexploreneighborhoods.Optimizationstagesapply
amixofbothkindsoftransformations,?rstemphasizingsmarttransformations,
and later transitioning to simple transformations. Optimum results on subtrees
arepreserved,sothatlaterstagescantakeadvantageofresultsgeneratedearlier.
Eachstageneedstobalanceopposingplangenerationtechniques:
  Exhaustive generation of alternatives: To generate the complete space, the
optimizer uses complete, local, nonredundant transformations—a transfor-
mationrulethatisequivalenttoasequenceofmoreprimitivetransformations
wouldonlyintroduceadditionaloverhead.
  Heuristicgenerationofcandidates:Ahandfulofinterestingcandidates(se-
lected on the basis of estimated cost) are likely to be far apart in terms of
primitive transformation rules. Here, desirable transformations are incom-
plete,global,andredundant.
Optimization can be terminated at any point after a ?rst plan has been gen-
erated. Such termination is based on the estimated cost of the best plan found
and the time spent already in optimization. For example, if a query requires
only looking up a few rows in some indices, a very cheap plan will likely be
produced quickly in the early stages, terminating optimization. This approach
enabled adding new heuristics easily over time, without compromising either
cost-basedselectionofplans,orexhaustiveexplorationofthesearchspace,when
appropriate.
30.4.7 Query Execution
Execution algorithms support both sort-based and hash-based processing, and
their data structures are designed to optimize use of processor cache. Hash op-
erations support basic aggregation and join, with a number of optimizations,
extensions, and dynamic tuning for data skew. The ?ow-distinct operation is a
variant of hash-distinct, where rows are output early, as soon as a new distinct
valueisfound,insteadofwaitingtoprocessthecompleteinput.Thisoperatoris
30.5 Concurrency and Recovery 1241
effectiveforqueriesthatuse distinctandrequestonlyafewrows,sayusingthe
top n construct. Correlated plans specify executing E(t), often including some
indexlookupbasedontheparameter,foreachrow t ofatable T. Asynchronous
prefetchingallowsissuingmultipleindex-lookuprequeststothestorageengine.
It is implemented this way: A nonblocking index-lookup request is made for a
row t of T,thent isplacedinaprefetchqueue.Rowsaretakenoutofthequeue
and used by apply to execute E(t). Execution of E(t) does not require that data
be already in the buffer pool, but having outstanding prefetch operations maxi-
mizes hardware utilization and increases performance. The size of the queue is
determined dynamically as a function of cache hits. If no ordering is required
ontheoutputrowsof apply,rowsfrom thequeuemaybetakenout oforder,to
minimizewaitingon I/O.
Parallelexecutionisimplementedbytheexchangeoperator,whichmanages
multiple threads, partitions or broadcasts data, and feeds the data to multiple
processes. The query optimizer decides exchange placement on the basis of es-
timated cost. The degree of parallelism is determined dynamically at runtime,
accordingtothecurrentsystemutilization.
Index plans are made up of the pieces described earlier. For example, we
considertheuseofanindexjointoresolvepredicateconjunctions(orindexunion,
for disjunctions), in a cost-based way. Such a join can be done in parallel, using
any of SQL Server’sjoinalgorithms.Wealsoconsiderjoiningindicesforthesole
purposeofassemblingarowwiththesetofcolumnsneededonaquery,whichis
sometimesfasterthanscanningabasetable.Takingrecord IDsfromasecondary
indexandlocatingthecorrespondingrowinabasetableiseffectivelyequivalent
toperformingindex-lookupjoin.Forthis,weuseourgenericcorrelatedexecution
techniquessuchasasynchronousprefetch.
Communication with the storage engine is done through OLE-DB,whichal-
lows accessing other dataprovidersthat implement this interface. OLE-DB is the
mechanism used for distributed and remote queries, which are driven directly
by the query processor. Data providers are categorized according to the range
of functionality they provide, ranging from simple rowset providers with no
indexingcapabilitiestoproviderswithfull SQLsupport.
30.5 Concurrency and Recovery
SQL Server’s transaction, logging, locking, and recovery subsystems realize the
ACIDpropertiesexpectedofadatabasesystem.
30.5.1 Transactions
In SQL Server all statements are atomic and applications can specify various
levelsofisolationforeachstatement.Asingletransactioncanincludestatements
that not only select, insert, delete, or update records, but also create or drop
tables, build indices, and bulk-import data. Transactions can span databases on
remote servers. When transactions are spread across servers, SQL Server uses a
1242 Chapter 30 MicrosoftSQL Server
Windows operating-systemservice called the Microsoft Distributed Transaction
Coordinator(MSDTC)toperformtwo-phasecommitprocessing.MSDTCsupports
the XAtransactionprotocoland,alongwith OLE-DB,providesthefoundationfor
ACIDtransactionsamongheterogeneoussystems.
ConcurrencycontrolbasedonlockingisthedefaultforSQLServer.SQLServer
also offers optimistic concurrency control for cursors. Optimistic concurrency
controlisbasedontheassumptionthatresourcecon?ictsbetweenmultipleusers
are unlikely (but not impossible), and allows transactions to execute without
locking any resources. Only when attempting to change data does SQL Server
check resources to determine if any con?icts have occurred. If a con?ict occurs,
the application must read the data and attempt the change again. Applications
canchoosetodetectchangeseitherbycomparingvaluesorbycheckingaspecial
rowversioncolumnonarow.
SQLServersupportsthe SQLisolationlevelsofreaduncommitted,readcom-
mitted, repeatable read, and serializable. Read committed is the default level.
In addition, SQL Server supports two snapshot-based isolation levels (snapshot
isolationisdescribedearlierinSection15.7).
  Snapshot: Speci?es that data read by any statement in a transaction will be
thetransactionallyconsistentversionofthedatathatexistedatthestartofthe
transaction. The effect is as if the statements in a transaction see a snapshot
of the committed data as it existed at the start of the transaction. Writes are
validatedusingthevalidationstepsdescribedinSection15.7,andpermitted
tocompleteonlyifthevalidationissuccessful.
  Read committed snapshot: Speci?es that each statement executed within a
transactionseesatransactionallyconsistentsnapshotofthedataasitexisted
at the start of the statement. This contrasts with read committed isolation
wherethestatementmayseecommittedupdatesoftransactionsthatcommit
whilethestatementisexecuting.
30.5.2 Locking
Lockingistheprimarymechanismusedtoenforcethesemanticsoftheisolation
levels.Allupdatesacquiresuf?cient exclusivelocksheld forthe duration ofthe
transaction topreventcon?ictingupdatesfromoccurring. Sharedlocksareheld
forvariousdurationstoprovidethedifferent SQLisolationlevelsforqueries.
SQL Server provides multigranularity locking that allows different types of
resources to be locked by a transaction (see Figure 30.4, where the resources
are listed in order of increasing granularity). To minimize the cost of locking,
SQLServerlocksresourcesautomaticallyatagranularityappropriatetothetask.
Locking at a smaller granularity, such as rows, increases concurrency, but has a
higheroverheadbecausemorelocksmustbeheldifmanyrowsarelocked.
The fundamental SQL Server lock modes are shared (S), update (U), and
exclusive(X);intentlocksarealsosupportedformultigranularitylocking.Update
locksareusedtopreventacommonformofdeadlockthatoccurswhenmultiple
sessionsarereading,locking,andpotentiallyupdatingresourceslater.Additional
30.5 Concurrency and Recovery 1243
RID
Key
Page
Extent
Table
DB
Row identi?er; used to lock a single row within a table
Row lock within an index; protects key ranges in serializable transactions
8-kilobyte table or index page
Contiguous group of eight data pages or index pages
Entire table, including all data and indices
Database
Resource Description
Figure 30.4 Lockable resources.
lockmodes—calledkey-rangelocks—aretakenonlyinserializableisolationlevel
forlockingtherangebetweentworowsinanindex.
30.5.2.1 Dynamic Locking
Fine-granularitylockingcanimproveconcurrencyatthecostofextra CPUcycles
andmemorytoacquireandholdmanylocks.Formanyqueries,acoarserlocking
granularityprovidesbetterperformancewithno(orminimal)lossofconcurrency.
Database systems have traditionally required query hints and table options for
applications to specify locking granularity. In addition, there are con?guration
parameters(oftenstatic)forhowmuchmemorytodedicatetothelockmanager.
In SQL Server, locking granularity is optimized automatically for optimal
performance and concurrency for each index used in a query. In addition, the
memory dedicated to the lock manager is adjusted dynamically on the basis
of feedback from other parts of the system, including other applications on the
machine.
Lockgranularityisoptimizedbeforequeryexecutionforeachtableandindex
used in the query. The lock optimization process takes into account isolation
level (that is, how long locks are held), scan type (range, probe, or entire table),
estimatednumber of rows to be scanned, selectivity(percentage of visitedrows
that qualify for the query), row density (number of rows per page), operation
type(scan,update),userlimitsonthegranularity,andavailablesystemmemory.
Once a query is executing, the lock granularity is escalated automatically
to table level if the system acquires signi?cantly more locks than the optimizer
expected or if the amount of available memory drops and cannot support the
numberoflocksrequired.
30.5.2.2 Deadlock Detection
SQL Server automatically detects deadlocks involving both locks and other re-
sources.For example,iftransaction Ais holding alock onTable1and is waiting
for memory to become available and transaction B has some memory it can’t
releaseuntilitacquiresalockonTable1,thetransactionswilldeadlock.Threads
andcommunicationbufferscanalsobeinvolvedindeadlocks.When SQLServer
detectsadeadlock,itchoosesasthedeadlockvictimthetransactionthatwouldbe
1244 Chapter 30 MicrosoftSQL Server
the least expensiveto roll back, considering the amount of work the transaction
hasalreadydone.
Frequent deadlock detection can hurt system performance. SQL Server au-
tomatically adjusts the frequency of deadlock detection to how often deadlocks
are occurring. If deadlocks are infrequent, the detection algorithm runs every 5
seconds.Iftheyarefrequentitwillbegincheckingeverytimeatransactionwaits
foralock.
30.5.2.3 Row VersioningforSnapshotIsolation
The two snapshot-based isolation levelsuse row versioning to achieve isolation
forquerieswhilenotblockingthequeriesbehindupdatesandviceversa.Under
snapshotisolation,updateanddeleteoperationsgenerateversionsoftheaffected
rowsandstoretheminatemporarydatabase.Theversionsaregarbage-collected
whentherearenoactivetransactionsthatcouldrequirethem.Therefore,aquery
runundersnapshotisolationdoesnotneedtoacquirelocksandinsteadcanread
theolderversionsofanyrecordthatgetsupdated/deletedbyanothertransaction.
Rowversioningisalsousedtoprovideasnapshotofatableforonlineindexbuild
operations.
30.5.3 Recovery and Availability
SQLServerisdesignedtorecoverfromsystemandmediafailures,andtherecovery
system can scale to machines with very large buffer pools (100 gigabytes) and
thousandsofdiskdrives.
30.5.3.1 Crash Recovery
Logically, the log is a potentially in?nite stream of log records identi?ed by log
sequencenumbers(LSNs).Physically,aportionofthestreamisstoredinlog?les.
Logrecordsaresavedinthelog?lesuntiltheyhavebeenbackedupandareno
longerneededbythesystemforrollbackorreplication.Log?lesgrowandshrink
insizetoaccommodatetherecordsthatneedtobestored.Additionallog?lescan
beaddedtoadatabase(onnewdisks,for example)whilethesystemisrunning
and without blocking any current operations, and all logs are treated as if they
wereonecontinuous?le.
SQL Server’s recovery system has many aspects in common with the ARIES
recovery algorithm (see Section 16.8), and some of the key differences are high-
lightedinthissection.
SQLServerhasacon?gurationoptioncalledrecovery interval,whichallows
an administrator to limit the length of time SQL Server should take to recover
afteracrash.Theserverdynamicallyadjuststhecheckpointfrequencytoreduce
recovery time to within the recovery interval. Checkpoints ?ush all dirty pages
fromthebufferpoolandadjusttothecapabilitiesoftheI/Osystemanditscurrent
workloadtoeffectivelyeliminateanyimpactonrunningtransactions.
Uponstart-upafteracrash,thesystemstartsmultiplethreads(automatically
scaled to the number of CPUs) to start recoveringmultipledatabases in parallel.
30.5 Concurrency and Recovery 1245
The?rstphaseofrecoveryisananalysispassonthelog,whichbuildsadirtypage
table and active transaction list. The next phase is a redo pass starting from the
lastcheckpointandredoingalloperations.Duringtheredophase,thedirtypage
tableisusedtodriveread-aheadofdatapages.The?nalphaseisanundophase
whereincompletetransactionsarerolledback.Theundophaseisactuallydivided
intotwopartsasSQLServerusesatwo-levelrecoveryscheme.Transactionsatthe
?rstlevel(thoseinvolvinginternaloperationssuchasspaceallocationandpage
splits) are rolled back ?rst, followed by user transactions. Once the transactions
atthe?rstlevelarerolledback,thedatabaseisbroughtonlineandisavailablefor
newuser transactions tostart while the ?nal rollback operations areperformed.
This is achieved by having the redo pass reacquire locks for all incomplete user
transactionsthatwillberolledbackintheundophase.
30.5.3.2 Media Recovery
SQL Server’s backup and restore capabilitiesallow recoveryfrom many failures,
including loss or corruption of disk media, user errors, and permanent loss of
a server. Additionally, backing up and restoring databases is useful for other
purposes,suchascopyingadatabasefromoneservertoanotherandmaintaining
standbysystems.
SQLServerhasthreedifferentrecoverymodelsthatuserscanchoosefromfor
eachdatabase.Byspecifyingarecoverymodel,anadministratordeclaresthetype
ofrecoverycapabilitiesrequired(suchaspoint-in-timerestoreandlogshipping)
and the required backups to achieve them. Backups can be taken on databases,
?les, ?le-groups, and the transaction log. All backups are fuzzy and completely
online;thatis,theydonotblockany DMLor DDLoperationswhiletheyexecute.
Restorescanalsobedoneonlinesuchthatonlytheportionofthedatabasebeing
restored(e.g.,acorruptdiskblock)istakenof?ine.Backupandrestoreoperations
arehighlyoptimizedandlimitedonlybythespeedofthemediaontowhichthe
backupistargeted.SQLServercanbackuptobothdiskandtapedevices(upto64
inparallel)andhashigh-performancebackupAPIsforusebythird-partybackup
products.
30.5.3.3 Database Mirroring
Databasemirroringinvolvesimmediatelyreproducingeveryupdatetoadatabase
(theprincipaldatabase)ontoaseparate,completecopyofthedatabase(themirror
database)generallylocatedonanothermachine.Intheeventofadisasteronthe
primaryserverorevenjustmaintenance,thesystemcanautomaticallyfailoverto
themirrorinamatterofseconds.Thecommunicationlibraryusedbyapplications
isawareofthemirroringandwillautomaticallyreconnecttothemirrormachine
in the event of a failover. A tight coupling between the primary database and
the mirror is achieved by sending blocks of transaction log to the mirror as it is
generatedontheprimaryandredoingthelogrecordsonthemirror.Infull-safety
mode,atransactioncannotcommituntilthelogrecordsforthetransactionhave
made it to disk on the mirror. Besides supporting failover, a mirror can also be
1246 Chapter 30 MicrosoftSQL Server
used to automatically restore a page by copying it from the mirror in the event
thatthepageisfoundtobecorruptduringanattempttoreadit.
30.6 System Architecture
An SQL Serverinstanceisasingleoperating-systemprocessthatisalsoanamed
endpointforrequestsforSQLexecution.ApplicationsinteractwithSQLServervia
variousclient-sidelibraries(likeODBC,OLE-DB,andADO.NET)inordertoexecute
SQL.
30.6.1 Thread Pooling on the Server
In order to minimize the context switching on the server and to control the
degreeofmultiprogramming,the SQLServerprocessmaintainsapoolofthreads
thatexecuteclientrequests.Asrequestsarrivefromtheclient,theyareassigned
a thread on which to execute. The thread executesthe SQL statements issued by
the client and sends the results back to it. Once the user request completes, the
threadisreturnedbacktothethreadpool.Inadditiontouserrequests,thethread
poolisusedtoassignthreadsforinternalbackgroundtaskssuchas:
  Lazywriter:Thisthreadisdedicatedtomakingsureacertainamountofthe
buffer pool is free and available at all times for allocation by the system.
Thethreadalsointeractswiththeoperatingsystemtodeterminetheoptimal
amountofmemorythatshouldbeconsumedbythe SQLServerprocess.
  Checkpoint: This thread periodically checkpointsalldatabasesinorderto
maintainafastrecoveryintervalforthedatabasesonserverrestart.
  Deadlock monitor: This thread monitors the other threads, looking for a
deadlock in the system. It is responsible for the detection of deadlocks and
alsopickingavictiminordertoallowthesystemtomakeprogress.
When the query processor chooses a parallel plan to execute a particular
query,it can allocate multiplethreads that work on behalf of the main thread to
execute the query. Since the Windows NT family of operating systems provides
native thread support, SQL Server uses NT threads for its execution. However,
SQLServercanbecon?guredtorunwithuser-modethreadsinadditiontokernel
threadsinveryhigh-endsystemstoavoidthecost ofakernelcontext switchon
athreadswitch.
30.6.2 Memory Management
Therearemanydifferentusesofmemorywithinthe SQLServerprocess:
  Buffer pool. The biggest consumer of memory in the system is the buffer
pool. The buffer pool maintains a cache of the most recently used database
pages. It uses a clock replacement algorithm with a steal, no-force policy;
that is, buffer pages with uncommitted updates may be replaced (“stolen”),
30.6 System Architecture 1247
and buffer pages are not forced to disk on transaction commit. The buffers
alsoobeythewrite-aheadloggingprotocoltoensurecorrectnessofcrashand
mediarecovery.
  Dynamic memory allocation. This is the memory that is allocated dynami-
callytoexecuterequestssubmittedbytheuser.
  Plan and execution cache. This cache stores the compiled plans for various
queries that have been previously executed by users in the system. This
allowsvarioususerstosharethesameplan(savingmemory)andalsosaves
onquerycompilationtimeforsimilarqueries.
  Large memory grants. These are for query operators that consume large
amountsofmemory,suchashashjoinandsort.
SQL Server uses an elaborate scheme of memory management to divide its
memory among the various uses described above. A single memory manager
centrally manages all the memory used by SQL Server. The memory manager is
responsiblefordynamicallypartitioningandredistributingthememorybetween
thevariousconsumersofmemoryinthesystem.Itdistributesthismemoryinac-
cordancewithananalysisoftherelativecostbene?tofmemoryforanyparticular
use.Ageneralized LRUinfrastructuremechanismisavailabletoallcomponents.
This caching infrastructure tracks not only the lifetime of cached data but also
therelativeCPUand I/Ocostsincurredtocreateandcacheit.Thisinformationis
usedtodeterminetherelativecostsofvariouscacheddata.Thememorymanager
focusesonthrowingoutthecacheddatathathavenotbeentouchedrecentlyand
werecheaptocache.Asanexample,complexqueryplansthatrequiresecondsof
CPU time to compile are more likely to stay in memory than trivial plans, given
equivalentaccessfrequencies.
Thememorymanagerinteractswiththeoperatingsystemtodecidedynam-
icallyhowmuchmemoryitshouldconsumeoutofthetotalamountofmemory
inthesystem.Thisallows SQLServertobequiteaggressiveinusingthememory
on the system but still return memory back to the system when other programs
needitwithoutcausingexcessivepagefaults.
InadditionthememorymanagerisawareoftheCPUandmemorytopologyof
thesystem.Speci?cally,itleveragestheNUMA(nonuniformmemoryaccess)that
manymachinesemployandattemptstomaintainlocalitybetweentheprocessor
thatathreadisexecutingonandthememoryitaccesses.
30.6.3 Security
SQL Server provides comprehensive security mechanisms and policies for au-
thentication, authorization, audit, and encryption. Authentication can be either
through a username–password pair managed by SQL Server, or through a Win-
dows OSaccount.Authorizationismanagedbypermissiongrantstoschemaob-
jectsorcoveringpermissionsoncontainerobjectssuchasthedatabaseorserver
instance. Atauthorization-check time,permissionsarerolledup and calculated,
accountingforcoveringpermissionsandrolemembershipsoftheprincipal.Au-
1248 Chapter 30 MicrosoftSQL Server
dits are de?ned in the same way as permissions — they are de?ned on schema
objectsforagivenprincipalorcontainingobjectsandatthetimeoftheoperation
they aredynamically calculated basedon audit de?nitionson the object and ac-
counting for any coveringauditsor rolemembershipsofthe principal.Multiple
auditsmaybede?nedsothatauditsfordifferentpurposes,suchasforSarbanes-
OxleyandHIPAA,
1
maybemanagedindependentlywithoutriskofbreakingeach
other.Auditsrecordsarewritteneitherina?leortotheWindowsSecurityLog.
SQL Server provides both manual encryption of data and Transparent Data
Encryption. Transparent Data Encryption encrypts all data pages and log pages
when written to disk and decrypts when read from the disk so that the data
are encrypted at rest on the disk but is plaintext to SQL Server users without
applicationmodi?cation.TransparentDataEncryptioncanbemore CPUef?cient
thanmanual encryptionasdataisonlyencryptedwhenwrittentodiskanditis
doneinlargerunits,pages,ratherthanindividualcellsofdata.
Two things are even more critical to users’ security: (1) the quality of the
entire code base itself and (2) the ability for users to determine if they have
secured the system properly. The quality of the code base is enhanced by using
the Security Development Lifecycle. All developers and testers of the product
go through security training. All features are threat modeled to assure assets are
appropriately protected. Wherever possible, SQL Server utilizes the underlying
securityfeaturesoftheoperatingsystemratherthanimplementingitsown,such
asWindowsOSAuthorizationandtheWindowsSecurityLogforanauditrecord.
Furthermore,numerousinternaltoolsareutilizedtoanalyzethecodebaselooking
forpotentialsecurity?aws.Securityisveri?edusingfuzztesting
2
andtestingof
thethreatmodel.Beforerelease,thereisa?nalsecurityreviewoftheproductand
aresponseplanisinplacefordealingwithsecurityissuesfoundafterrelease —
whichisthenexecutedasissuesarediscovered.
A number of features are provided to help users secure the system prop-
erly.Onesuchfeatureisafundamentalpolicycalledoff-by-default,wheremany
less commonly used components or those requiring extra care for security, are
completelydisabledbydefault.Anotherfeatureisabest-practicesanalyzer that
warnsusersaboutcon?gurationsofsystemsettingsthatcouldleadtoasecurity
vulnerability. Policy-based management further allows users to de?ne what the
settings should be and either warns of or prevents changes that would con?ict
withtheapprovedsettings.
30.7 Data Access
SQLServersupportsthefollowingapplicationprogramminginterfaces(APIs)for
buildingdata-intensiveapplications:
1
The Sarbanes-Oxley Act is a U.S. government ?nancial regulation law.HIPAA is a U.S. government health-care law
thatincludesregulationofhealth-care-relatedinformation.
2
FuzztestingArandomization-basedtechniquefortestingforunexpected,possiblyinvalid,input.
30.7 Data Access 1249
  ODBC. This is Microsoft’s implementation of the standard SQL:1999 call-
levelinterface(CLI).Itincludesobjectmodels—RemoteDataObjects(RDOs)
and Data Access Objects (DAOs)—that make it easier to program multitier
databaseapplicationsfromprogramminglanguageslikeVisualBasic.
  OLE-DB.Thisisalow-level,systems-orientedAPIdesignedforprogrammers
building database components. The interface is architected according to the
MicrosoftComponentObjectModel(COM),anditenablestheencapsulation
oflow-leveldatabaseservicessuchasrowsetproviders,ISAMproviders,and
query engines. OLE-DB is used inside SQL Server to integrate the relational
query processor and the storage engine and to enable replication and dis-
tributed access to SQL and other external data sources. Like ODBC, OLE-DB
includes a higher-level object model called ActiveX Data Objects (ADO)to
makeiteasiertoprogramdatabaseapplicationsfromVisualBasic.
  ADO.NET.Thisisan APIdesignedforapplicationswrittenin .NETlanguages
suchasC#andVisualBasic.NET.Thisinterfacesimpli?essomecommondata
access patterns supported by ODBC and OLE-DB. In addition, it provides a
newdata setmodeltoenablestateless,disconnecteddataaccessapplications.
ADO.NET includes the ADO.NET Entity Framework, which is a platform for
programmingagainstdatathatraisesthelevelofabstractionfromthelogical
(relational) level to the conceptual (entity) level, and thereby signi?cantly
reduces the impedance mismatch for applications and data services such
as reporting, analysis, and replication. The conceptual data model is imple-
mented using an extended relational model, the Entity Data Model (EDM)
that embraces entities and relationships as ?rst-class concepts. It includes a
query language for the EDM called Entity SQL, a comprehensive mapping
enginethattranslatesfromtheconceptualtothelogical(relational)level,and
a set of model-driven tools that help developers de?ne mappings between
objectsandentitiestotables.
  LINQ.Language-integratedquery,orLINQforshort,allowsdeclarative,set-
oriented constructs to be used directly in programming languages such as
C#and Visual Basic.The queryexpressionsarenot processedby an external
tool or language preprocessor but instead are ?rst-class expressions of the
languagesthemselves.LINQallowsqueryexpressionstobene?tfromtherich
metadata, compile-time syntax checking, static typing and auto-completion
that was previouslyavailable only to imperativecode. LINQ de?nes a set of
general-purpose standard query operators that allow traversal, ?lter, join,
projection, sorting, and grouping operations to be expressed in a direct yet
declarative way in any .NET-based programming language. C# and Visual
Basic also support query comprehensions, i.e., language syntax extensions
thatleveragethestandardqueryoperators.
  DB-Lib.TheDB-LibraryforC APIthatwasdevelopedspeci?callytobeused
withearlierversionsof SQLServerthatpredatethe SQL-92standard.
  HTTP/SOAP. Applications can use HTTP/SOAP requeststoinvoke SQL Server
queries and procedures. Applications can use URLs that specify Internet In-
1250 Chapter 30 MicrosoftSQL Server
formation Server (IIS) virtual roots that reference an instance of SQL Server.
The URL can contain an XPath query, a Transact-SQL statement, or an XML
template.
30.8 Distributed Heterogeneous Query Processing
SQLServerdistributedheterogeneousquerycapabilityallowstransactionalqueries
and updates against a variety of relational and nonrelational sources via OLE-
DB data providers running in one or more computers. SQL Server supports two
methodsforreferencingheterogeneousOLE-DBdatasourcesinTransact-SQLstate-
ments.Thelinked-server-namesmethodusessystem-storedprocedurestoasso-
ciate a server name with an OLE-DB data source. Objects in these linked servers
canbereferencedinTransact-SQLstatementsusingthefour-partnameconvention
describedbelow.Forexample,ifalinkedservernameof DeptSQLSrvrisde?ned
againstanothercopyof SQLServer,thefollowingstatementreferencesatableon
thatserver:
select*
from DeptSQLSrvr.Northwind.dbo.Employees;
An OLE-DB data source is registeredin SQL Server as a linked server. Once a
linkedserverisde?ned,itsdatacanbeaccessedusingthefour-partname:
<linked-server>.<catalog>.<schema>.<object>
The following example establishes a linked server to an Oracle server via an
OLE-DBproviderforOracle:
execsp addlinkedserverOraSvr,’Oracle7.3’,’MSDAORA’,’OracleServer’
Aqueryagainstthislinkedserverisexpressedas:
select*
from OraSvr.CORP.ADMIN.SALES;
In addition, SQL Server supports built-in, parameterized table-valued func-
tions called openrowset and openquery, which allow sending uninterpreted
queriestoaproviderorlinkedserver,respectively,inthedialectsupportedbythe
provider. The following query combines information stored in an Oracle server
and a Microsoft Index Server. It lists all documents and their author containing
thewordsDataandAccessorderedbytheauthor’sdepartmentandname.
30.9 Replication 1251
select e.dept, f.DocAuthor, f.FileName
from OraSvr.Corp.Admin.Employee e,
openquery(EmpFiles,
’selectDocAuthor,FileName
fromscope(“c:\EmpDocs”)
wherecontains(’’“Data”near()“Access”’’)>0’) as f
where e.name= f.DocAuthor
orderby e.dept, f.DocAuthor;
TherelationalengineusestheOLE-DBinterfacestoopentherowsetsonlinked
servers,tofetchtherows,andtomanagetransactions.ForeachOLE-DBdatasource
accessed as a linked server, an OLE-DB provider must be present on the server
running SQL Server. The set of Transact-SQL operations that can be used against
aspeci?cOLE-DBdatasourcedependsonthecapabilitiesofthe OLE-DBprovider.
Whenever it is cost-effective, SQL Server pushes relational operations such as
joins,restrictions,projections,sorts,andgroupbyoperationstothe OLE-DBdata
source. SQL Server uses Microsoft Distributed Transaction Coordinator and the
OLE-DB transactioninterfaces oftheprovidertoensureatomicity oftransactions
spanningmultipledatasources.
30.9 Replication
SQL Server replication is a set of technologies for copying and distributing data
and database objects from one database to another, tracking changes, and syn-
chronizingbetweendatabasestomaintainconsistency.SQLServerreplicationalso
provides inline replication of most database schema changes without requiring
anyinterruptionsorrecon?guration.
Data are typically replicated to increase availability of data. Replication can
rollupcorporatedatafromgeographicallydispersedsitesforreportingpurposes
anddisseminatedatatoremoteusersonalocal-areanetworkormobileuserson
dial-upconnectionsortheInternet.MicrosoftSQLServerreplicationalsoenhances
application performance by scaling out for improved total read performance
amongreplicas,asiscommoninprovidingmidtierdata-cachingservicesforWeb
sites.
30.9.1 Replication Model
SQL Server introduced the Publish–Subscribe metaphor to database replication
andextendsthispublishing-industrymetaphorthroughoutitsreplicationadmin-
istrationandmonitoringtools.
The publisher is a server that makes data available for replication to other
servers. The publisher can have one or more publications, each representing a
logically related set of data and database objects. The discrete objects within a
publication, including tables, stored procedures, user-de?ned functions, views,
1252 Chapter 30 MicrosoftSQL Server
materialized views, and more, are called articles. The addition of an article to a
publication allows for extensive customizing of the way the object is replicated,
e.g., restrictions on which users can subscribe to receive its data and how the
datasetshouldbe?lteredonthebasisofaprojectionorselectionofatable,bya
“horizontal”ora “vertical”?lter,respectively.
Subscribers are servers that receive replicated data from a publisher. Sub-
scribers can conveniently subscribe to only the publications they require from
one or more publishers regardless of the number or type of replication options
eachimplements.Dependingonthetypeofreplicationoptionsselected,thesub-
scriber either can be used as a read-only replica or can make data changes that
areautomaticallypropagatedbacktothepublisherandsubsequentlytoallother
replicas.Subscriberscanalsorepublishthedatatheysubscribeto,supportingas
?exibleareplicationtopologyastheenterpriserequires.
The distributorisaserverthatplaysdifferentroles,dependingontherepli-
cation options chosen. At a minimum it is used as a repository for history and
error state information. In other cases, it is used additionallyas an intermediate
store-and-forwardqueuetoscaleupthedeliveryofthereplicatedpayloadtoall
thesubscribers.
30.9.2 Replication Options
MicrosoftSQLServerreplicationoffersawidespectrumofreplicationoptions.To
decide on the appropriate replication options to use, a database designer must
determine the application’s needs with respect to autonomous operation of the
sitesinvolvedandthedegreeoftransactionalconsistencyrequired.
Snapshot replication copies and distributes data and database objects ex-
actly as they appear at a moment in time. Snapshot replication does not require
continuous change tracking because changes are not propagated incrementally
to subscribers. Subscribers are updated with a complete refresh of the data set
de?ned by the publication on a periodic basis. Options available with snap-
shot replication can ?lter published data and can enable subscribers to modify
replicated data and propagate those changes back to the publisher. This type
ofreplicationisbestsuitedforsmallersizesofdataand whenupdatestypically
affectenoughofthedatathatreplicatingacompleterefreshofthedataisef?cient.
Withtransactionalreplication,thepublisherpropagatesaninitialsnapshotof
datatosubscribers,thenforwardsincrementaldatamodi?cationstosubscribers
as discrete transactions and commands. Incremental change tracking occurs in-
sidethecoreengineofSQL Server,whichmarkstransactionsaffectingreplicated
objects in the publishing database’s transaction log. A replicationprocess called
the log reader agent reads these transactions from the database transaction log,
appliesanoptional?lter,andstorestheminthedistributiondatabase,whichacts
as the reliable queue supporting the store-and-forward mechanism of transac-
tionalreplication.(Reliablequeuesarethesameasdurablequeues,describedin
Section 26.1.1.) Another replication process, called the distribution agent,then
forwardsthechangestoeachsubscriber.Likesnapshotreplication,transactional
replicationofferssubscriberstheoptiontomakeupdatesthateitherusetwo-phase
30.10 Server Programming in .NET 1253
commit to re?ect those changes consistently at the publisher and subscriber or
queue the changes at the subscriber for asynchronous retrieval by a replication
process that later propagates the change to the publisher. This type of replica-
tion is suitable when intermediate states between multiple updates need to be
preserved.
Merge replication allows each replica in the enterprise to work with total
autonomywhetheronlineorof?ine.Thesystemtracksmetadataonthechanges
to published objects at publishers and subscribers in each replicated database,
and the replication agent merges those data modi?cations together during syn-
chronization between replicated pairs and ensures data convergence through
automaticcon?ictdetectionandresolution.Numerouscon?ictresolutionpolicy
options are built into the replication agent used in the synchronization process,
and custom con?ict resolution can be written by using stored procedures or by
usinganextensiblecomponentobjectmodel(COM)interface.Thistypeofrepli-
cation does not replicate all intermediate states but only the current state of the
dataatthetimeofsynchronization.Itissuitablewhenreplicasrequiretheability
tomakeautonomousupdateswhilenotconnectedtoanynetwork.
30.10 Server Programming in .NET
SQL Server supports the hosting of the .NET Common Language Runtime (CLR)
insidethe SQL Serverprocesstoenable databaseprogrammerstowrite business
logic as functions, stored procedures, triggers, data types, and aggregates. The
ability to run application code inside the database adds ?exibility to the design
ofapplicationarchitecturesthatrequirebusinesslogictoexecuteclosetothedata
and cannot afford the cost of shipping data to a middle-tier process to perform
computationoutsidethedatabase.
The .NET Common Language Runtime (CLR) is a runtime environment with
astronglytypedintermediatelanguagethatexecutesmultiplemodernprogram-
minglanguagessuchasC#,VisualBasic,C++,COBOL,andJ++,amongothers,and
hasgarbage-collectedmemory,preemptivethreading,metadataservices(typere-
?ection),codeveri?ability,andcodeaccesssecurity.Theruntimeusesmetadatato
locateandloadclasses,layoutinstancesinmemory,resolvemethodinvocations,
generatenativecode,enforcesecurity,andsetruntimecontextboundaries.
Applicationcodeisdeployedinsidethedatabasebyusingassemblies,which
are the units of packaging, deployment, and versioning of application code in
.NET. Deployment of application code inside the database provides a uniform
waytoadminister,backup,andrestorecompletedatabaseapplications(codeand
data).Onceanassemblyisregisteredinsidethedatabase,userscanexposeentry
points within the assembly via SQL DDL statements, which can act as scalar or
tablefunctions,procedures,triggers,types,andaggregates,byusingwell-de?ned
extensibility contracts enforced during the execution of these DDL statements.
Stored procedures, triggers, and functions usually need to execute SQL queries
andupdates.ThisisachievedthroughacomponentthatimplementstheADO.NET
data-access APIforuseinsidethedatabaseprocess.
1254 Chapter 30 MicrosoftSQL Server
30.10.1 Basic .NET Concepts
In the .NET framework, a programmer writes program code in a high-level pro-
gramminglanguagethatimplementsaclassde?ningitsstructure(e.g.,the?elds
or properties of the class) and methods. Some of these methods can be static
functions. The compilation of the program produces a ?le, called an assembly,
containing the compiled code in the Microsoft Intermediate Language (MSIL),
anda manifestcontainingallreferencestodependentassemblies.Themanifestis
anintegralpartofeveryassemblythatrenderstheassemblyself-describing.The
assembly manifest contains the assembly’s metadata, which describes all struc-
tures,?elds,properties,classes,inheritancerelationships,functions,andmethods
de?nedintheprogram.Themanifestestablishestheassemblyidentity,speci?es
the ?les that make up the assembly implementation, speci?es the types and re-
sources that make up the assembly, itemizes the compile-time dependencies on
other assemblies, and speci?es the set of permissions required for the assembly
torunproperly.Thisinformationisusedatruntimetoresolvereferences,enforce
version-bindingpolicy,andvalidatetheintegrityofloadedassemblies.The.NET
frameworksupportsanout-of-bandmechanismcalledcustomattributesforanno-
tatingclasses,properties,functionsandmethodswithadditionalinformationor
facets the applicationmay want tocaptureinmetadata.All.NET compilerscon-
sume theseannotations without interpretationand storethem inthe assembly’s
metadata. All these annotations can be examined in the same way as any other
metadatabyusingacommonsetofre?ection APIs.Managedcoderefersto MSIL
executedinthe CLRratherthandirectlybytheoperatingsystem.Managed-code
applicationsgaincommon-languageruntimeservicessuchasautomaticgarbage
collection,runtimetypechecking,andsecuritysupport.Theseserviceshelppro-
vide uniform platform- and language-independent behavior of managed-code
applications. Atexecutiontime,a just-in-time(JIT)compilertranslates the MSIL
into native code (e.g., Intel X86 code). During this translation, code must pass a
veri?cationprocessthatexaminestheMSILandmetadatato?ndoutwhetherthe
codecanbedeterminedtobetypesafe.
30.10.2 SQL CLR Hosting
SQLServerandthe CLRaretwodifferentruntimeswithdifferentinternalmodels
forthreading,schedulingandmemorymanagement.SQLServersupportsacoop-
erativenon-preemptivethreadingmodelinwhich the DBMS threadsvoluntarily
yield execution periodically or when they are waiting on locks or I/O,whereas
the CLRsupportsapreemptivethreadingmodel.Ifusercoderunninginsidethe
DBMS can directly call the operating-system (OS) threading primitives, then it
does not integrate well with the SQL Server task scheduler and can degrade the
scalability of the system. CLR doesnot distinguishbetweenvirtualand physical
memory,while SQL Serverdirectlymanagesphysicalmemoryandisrequiredto
usephysicalmemorywithinacon?gurablelimit.
The different models for threading, scheduling, and memory management
present an integration challenge for a DBMS that scales to support thousands
of concurrent user sessions. SQL Server solves this challenge by becoming the
30.10 Server Programming in .NET 1255
SQL
Server
Process
SQL Server
Engine
CLR
Windows
SQLCLR
Hosting
SQL OS Layer
(memory, threads, synchronization)
Figure 30.5 Integration of CLR with SQL Server operating-system services.
operating system for the CLR when it is hosted inside the SQL Server process.
The CLR calls low-level primitives implemented by SQL Server for threading,
scheduling, synchronization, and memory management (see Figure 30.5). This
approachprovidesthefollowingscalabilityandreliabilitybene?ts:
Common threading, scheduling, and synchronization. CLRcalls SQLServer
APIsforcreatingthreadsbothforrunningusercodeandforitsowninternaluse
suchasthegarbagecollectorandtheclass?nalizerthread.Inordertosynchronize
betweenmultiplethreads,the CLRcalls SQL Serversynchronization objects.This
allowsSQLServerschedulertoscheduleothertaskswhenathreadiswaitingona
synchronizationobject.Forinstance,whentheCLRinitiatesgarbagecollection,all
ofitsthreadswait forgarbage collectionto?nish. Sincethe CLR threadsand the
synchronizationobjectstheyarewaitingonareknowntotheSQLServerscheduler,
it can schedule threads that are running other database tasks not involving the
CLR.Further,thisenables SQLServertodetectdeadlocksthatinvolvelockstaken
by CLR synchronization objects and employ traditional techniques for deadlock
removal.The SQLServerschedulerhastheabilitytodetectandstopthreadsthat
havenotyieldedforasigni?cantamountoftime.TheabilitytohookCLRthreads
to SQLServerthreadsimpliesthatthe SQLServerschedulercanidentifyrunaway
threadsrunningintheCLRandmanagetheirpriority,sothattheydonotconsume
signi?cant CPU resources, thereby affecting the throughput of the system. Such
runawaythreadsaresuspendedandputbackinthequeue.Repeatoffendersare
not allowed timeslicesthat are unfair to other executing workers. If an offender
took 50 times the allowed quantum, it is punished for 50 “rounds” before being
allowed to run again because the scheduler cannot tell when a computation is
longandrunawayversuslongandlegitimate.
Common memory management. The CLR calls SQL Server primitives for al-
locating and deallocating its memory. Since the memory used by the CLR is
accountedforinthetotalmemoryusageofthesystem,SQLServercanstaywithin
itscon?guredmemorylimitsandensurethe CLRand SQLServerarenotcompet-
ingwitheachotherformemory.Also,SQLServercanrejectCLRmemoryrequests
1256 Chapter 30 MicrosoftSQL Server
whenthesystemisconstrainedandaskCLRtoreduceitsmemoryusewhenother
tasksneedmemory.
30.10.3 Extensibility Contracts
Alluser-managedcoderunningwithintheSQLServerprocessinteractswithDBMS
components as an extension. Current extensions include scalar functions, table
functions, procedures, triggers, scalar types, and scalar aggregates. For each ex-
tension there is a mutual contract de?ning the properties or services user code
must implement to act as one of these extensions as well as the services the ex-
tension can expect from the DBMS when the managed code is called. SQL CLR
leverages the class and custom attributes information stored in assembly meta-
data to enforce that user code implements these extensibility contracts. All user
assemblies are stored inside the database. All relational and assembly metadata
areprocessedinsidetheSQLenginethroughauniformsetofinterfacesanddata
structures.Whendata-de?nitionlanguage(DDL)statementsregisteringapartic-
ularextensionfunction,type,oraggregateareprocessed,thesystemensuresthe
user code implements the appropriate contract by analyzing its assembly meta-
data.Ifthecontractisimplemented,thentheDDLstatementsucceeds,otherwiseit
fails.Thenextsubsectionsdescribekeyaspectsofthespeci?ccontractscurrently
enforcedby SQLServer.
30.10.3.1 Routines
We classify scalar functions, procedures, and triggers generically as routines.
Routines,implementedasstaticclassmethods,canspecifythefollowingproper-
tiesthroughcustomattributes.
  IsPrecise.IfthisBooleanpropertyisfalse,thenitindicatestheroutinebody
involves imprecise computations such as ?oating-point operations. Expres-
sionsinvolvingimprecisefunctionscannotbeindexed.
  UserDataAccess. Ifthevalueofthispropertyis read,thentheroutinereads
user-data tables. Otherwise, the value of the property is None indicating
the routine does not access data. Queries that do not access any user tables
(directly or indirectly through views and functions) are not considered to
haveuser-dataaccess.
  SystemDataAccess. If the value of this property is read, then the routine
readssystemcatalogsorvirtualsystemtables.
  IsDeterministic. Ifthispropertyis true,thenthe routine is assumedtopro-
duce the same output value given the same input values, state of the local
database,andexecutioncontext.
  IsSystemVeri?ed. This indicates whether the determinism and precision
propertiescanbeascertainedorenforcedbySQLServer(e.g.,built-ins,Transact-
SQLfunctions)oritisasspeci?edbytheuser(e.g., CLRfunctions).
30.10 Server Programming in .NET 1257
  HasExternalAccess. If the value of this property is true, then the routine
accessesresourcesoutsideSQLServersuchas?les,network,Webaccess,and
registry.
30.10.3.2 Table Functions
Aclassimplementingatable-valuedfunctionmustimplementaninterfaceIEnu-
merable to enable iteration over the rows returned by the function, a method
to describe the schema of the table returned (i.e., columns, types), a method to
describewhatcolumnscanbeuniquekeys,andamethodtoinsertrowsintothe
table.
30.10.3.3 Types
Classesimplementinguser-de?nedtypesareannotatedwithanSqlUserDe?ned-
Type() attributethatspeci?esthefollowingproperties:
  Format. SQLServersupportsthreestorageformats:native,user-de?ned,and
.NETserialization.
  MaxByteSize.Thisisthemaximumsizeoftheserializedbinaryrepresenta-
tionoftypeinstancesinbytes. UDTscanbeupto2GBinlength.
  IsFixedLength.This is a Boolean propertyspecifyingwhether the instances
ofthetypehave?xedorvariablelength.
  IsByteOrdered.ThisisaBooleanpropertyindicatingwhethertheserialized
binary representation of the type instances is binary ordered. When this
property is true, the system can perform comparisons directly against this
representationwithouttheneedtoinstantiatetypeinstancesasobjects.
  Nullability.AllUDTsinoursystemmustbecapableofholdingthenullvalue
bysupportingtheINullableinterfacecontainingtheBooleanIsNullmethod.
  Type conversions.All UDTsmust implementconversionstoand fromchar-
acterstringsviathe ToString andParsemethods.
30.10.3.4 Aggregates
In addition to supporting the contract for types, user-de?ned aggregates must
implementfourmethodsrequiredbythequery-executionenginetoinitializethe
computationofanaggregateinstance,toaccumulate inputvaluesintothefunc-
tion providedby the aggregate,tomergepartial computations of the aggregate,
andtoretrievethe?nalaggregateresult.Aggregatescandeclareadditionalprop-
erties,viacustomattributes,intheirclassde?nition;thesepropertiesareusedby
thequeryoptimizertoderivealternativeplansfortheaggregatecomputation.
  IsInvariantToDuplicates. If this property is true, then the computation de-
livering the data to the aggregate can be modi?ed by either discarding or
introducingnewduplication-removaloperations.
1258 Chapter 30 MicrosoftSQL Server
  IsInvariantToNulls.Ifthispropertyistrue,thennullrowscanbediscarded
from the input. However, care must be taken in the context of group by
operationsnottodiscardentiregroups.
  IsInvariantToOrder. If this property is true, then the query processor can
ignoreorderbyclausesandexploreplansthatavoidhavingtosortthedata.
30.11 XML Support
RelationaldatabasesystemshaveembracedXMLinmanydifferentwaysinrecent
years. First-generation XML support in relational database systems was mainly
concerned with exporting relational data as XML (“publish XML”), and to im-
port relational data in XML markup form back into a relational representation
(“shred XML”). Themainusage scenariosupportedby thesesystemsis informa-
tionexchangeincontextswhere XMLisusedasthe “wireformat”andwherethe
relationaland XMLschemasareoftenprede?nedindependentlyofeachother.In
ordertocoverthisscenario,MicrosoftSQLServerprovidesextensivefunctionality
suchastheforxmlpublishingrowsetaggregator,the OpenXMLrowsetprovider,
andthe XMLviewtechnologybasedonannotatedschemas.
Shredding of XML data into a relational schema can be quite dif?cult or
inef?cient for storing semistructured data whose structure may vary over time,
andforstoringdocuments.Tosupportsuchapplications SQLServerimplements
native XML based on the SQL:2003 xml data type. Figure 30.6 provides a high-
XML parser
XML
Validation
and typing
XML datatype
(binary XML)
XML
schema
collection
OpenXML/nodes()
FOR XML with
TYPE directive
Relational
rowsets
modify()
query()
XML schemas
XQuery and update execution
SQL value
value()
PRIMARY
XML INDEX
Node
table
PATH
index
PROP
index
VALUE
index
SQL Server metadata
SQL Server indices
Figure 30.6 Architectural overview of the native XML support in SQL Server.
30.11 XML Support 1259
levelarchitecturaldiagramof SQL Server’snativeXMLsupportinthedatabase.It
consistsoftheabilitytostore XMLnatively,toconstrainandtypethestored XML
datawithcollectionsof XMLschemas,andtoqueryandupdatethe XMLdata.In
ordertoprovideef?cientqueryexecutions,severaltypesof XML-speci?cindices
areprovided.Finally,thenativeXMLsupportalsointegrateswiththe “shredding”
and “publishing”toandfromrelationaldata.
30.11.1 Natively Storing and Organizing XML
ThexmldatatypecanstoreXMLdocumentsandcontentfragments(multipletext
orelementnodesatthetop)andisde?nedonthebasisofthe XQuery1.0/XPath
2.0 data model. The data type can be used for parameters of stored procedures,
forvariables,andasacolumntype.
SQLServerstoresdataoftype xmlinaninternalbinaryformatasa bloband
providesindexingmechanismsforexecutingqueries.Theinternalbinaryformat
provides ef?cient retrieval and reconstruction of the original XML document, in
addition to some space savings (on average, 20 percent). The indices support
an ef?cient query mechanism that can utilize the relational query engine and
optimizer;moredetailsareprovidedlater,inSection30.11.3.
SQLServerprovidesadatabase-metadataconceptcalledan XML schemacol-
lection that associates an SQL identi?erwith a collectionof schema components
ofoneormultipletargetnamespaces.
30.11.2 Querying and Updating the XML Data Type
SQLServerprovidesseveralXQuery-basedqueryandmodi?cationcapabilitieson
the XML data type. These query and modi?cation capabilities are supported by
usingmethodsde?nedonthexmldatatype.Someofthesemethodsaredescribed
intherestofthissection.
Each method takes a string literal as the query string and potentially other
arguments. The XML data type (on which the method is applied) provides the
context item for the path expressions and populates the in-scope schema def-
initions with all the type information provided by the associated XML schema
collection (if no collection is provided,the XML data is assumed to be untyped).
The SQL Server XQuery implementation is statically typed, thereby supporting
early detection of path expression typing mistakes, type errors, and cardinality
mismatch,aswellassomeadditionaloptimizations.
The query method takes an XQuery expression and returns an untyped XML
datatype instance (that can thenbe cast toa target schema collectionif the data
needtobetyped).InXQueryspeci?cationterminology,wehavesettheconstruc-
tion mode to “strip.” The following example shows a simple XQuery expression
thatsummarizesacomplexCustomerelementinatripreportdocumentthatcon-
tainsamongotherinformationaname,anIDattribute,andsales-leadinformation
thatarecontainedinthemarked-upactualtripreportnotes.Thesummaryshows
thenameandsalesleadsforCustomerelementsthathavesalesleads.
1260 Chapter 30 MicrosoftSQL Server
select Report.query(’
declarenamespacec="urn:example/customer";
for$custin/c:doc/c:customer
where$cust/c:notes//c:saleslead
return
<customer id="$cust/@id">{
$cust/c:name,
$cust/c:notes//c:saleslead
}</customer>’)
from TripReports;
TheaboveXQueryquerygetsexecutedontheXMLvaluestoredinthedocattribute
ofeachrowofthetableTripReports.EachrowintheresultoftheSQLquerycontains
theresultofexecutingthe XQueryqueryonthedatainoneinputrow.
ThevaluemethodtakesanXQueryexpressionandanSQLtypename,extracts
asingleatomicvaluefromtheresultoftheXQueryexpression,andcastsitslexical
form into the speci?ed SQL type. If the XQuery expression results in a node, the
typedvalueofthenodewillimplicitlybeextractedastheatomicvaluetobecast
intothe SQLtype(in XQueryterminologythenodewillbe “atomized”;theresult
is cast to SQL). Note that the value method performs a static type check that at
mostonevalueisbeingreturned.
The existmethodtakesan XQueryexpressionandreturns1iftheexpression
producesanonemptyresultand0otherwise.
Finally, the modify method provides a mechanism to change an XML value
at the subtree level, inserting new subtrees at speci?c locations inside a tree,
changingthevalueofanelementorattribute,anddeletingsubtrees.Thefollowing
example deletesall customer saleslead elements of years previous to the year
givenbyan SQLvariableorparameterwiththename@year:
update TripReports
set Report.modify(
’declarenamespacec="urn:example/customer";
delete/c:doc/c:customer//c:saleslead[@year<sql:variable("@year")]’);
30.11.3 Execution of XQuery Expressions
Asmentionedearlier,theXMLdataarestoredinaninternalbinaryrepresentation.
However,inordertoexecutethe XQueryexpressions,the XMLdatatypeisinter-
nally transformed into a so-called node table. The internal node table basically
uses a row to represent a node. Each node receives an OrdPath identi?er as its
nodeID(anOrdPathidenti?erisamodi?edDeweydecimalnumberingscheme;
see the bibliographical notes for references to more information on OrdPath).
Eachnodealsocontainskeyinformationtopointbacktotheoriginal SQLrowto
which the node belongs, information about the name and type (in a tokenized
form),values,andmore.SincetheOrdPathencodesboththedocumentorderand
thehierarchyinformation,thenodetablethenisclusteredonthebasisofthekey
30.12 SQL Server Service Broker 1261
informationandOrdPath,sothatapathexpressionorrecompositionofasubtree
canbeachievedwithasimpletablescan.
All XQueryand updateexpressionsarethen translatedinto an algebraicop-
erator tree against this internal node table; the tree uses the common relational
operatorsandsomeoperatorsspeci?callydesignedfortheXQueryalgebraization.
Theresultingtreeisthengraftedintothealgebratreeoftherelationalexpression
sothatintheend,thequery-executionenginereceivesasingleexecutiontreethat
it can optimize and execute. In order to avoid costly runtime transformations, a
usercanprematerializethenodetablebyusingtheprimaryXMLindex.SQLServer
inadditionprovidesthreesecondaryXMLindicessothatthequeryexecutioncan
takefurtheradvantageofindexstructures:
  The pathindexprovidessupportforsimpletypesofpathexpressions.
  The properties indexprovidessupportfor the common scenarioofproperty-
valuecomparisons.
  The valueindexiswellsuitedifthequeryuseswild-cardsincomparisons.
SeethebibliographicalnotesforreferencestomoreinformationonXMLindexing
andqueryprocessingin SQLServer.
30.12 SQL Server Service Broker
Service Broker helps developers create loosely coupled distributed applications
byprovidingsupportforqueued,reliablemessaginginSQLServer.Manydatabase
applications use asynchronous processing to improve scalability and response
timesforinteractivesessions.Onecommonapproachtoasynchronousprocessing
istouseworktables.Insteadofperformingalloftheworkforabusinessprocess
in a single database transaction, an application makes a change indicating that
outstandingworkispresentandtheninsertsarecordoftheworktobeperformed
into a work table. As resources permit, the application processes the work table
andcompletesthebusinessprocess.ServiceBrokerisapartofthedatabaseserver
that directly supports this approach for application development.The Transact-
SQL language includes DDL and DML statements for Service Broker.In addition,
SQL Server Management Objects (SMO) for Service Broker are provided in SQL
Server.TheseallowprogrammaticaccesstoServiceBrokerobjectsfrommanaged
code.
Previous message-queuing technologies concentrated on individual mes-
sages.WithServiceBroker,thebasicunitofcommunicationistheconversation—
apersistent,reliable,full-duplexstreamofmessages. SQL Serverguaranteesthat
themessageswithinaconversationaredeliveredtoanapplicationexactlyonce,
inorder.Itisalsopossibletoassignapriorityfrom1to10toaconversation.Mes-
sages from conversations with higher priority are sent and received faster than
messagesfromconversationswithalowerpriority.Conversationsoccurbetween
twoservices.Aserviceisanamedendpointforaconversation.Eachconversation
1262 Chapter 30 MicrosoftSQL Server
is part of a conversation group. Related conversations can be associated with the
sameconversationgroup.
Messagesarestronglytyped,i.e.,eachmessagehasaspeci?ctype.SQLServer
can optionally validate that messages are well-formed XML, that messages are
empty, or that messages conform to an XML schema. A contract de?nes the mes-
sage types that are allowable for a conversation, and which participant in the
conversationmaysendmessagesofthattype. SQLServerprovidesadefaultcon-
tractandmessagetypeforapplicationsthatonlyneedareliablestream.
SQL Server stores messages in internal tables. These tables are not directly
accessible; instead, SQL Server exposes queues as views of those internal tables.
Applicationsreceivemessagesfromaqueue.Areceiveoperationreturnsoneor
moremessagesfromthesameconversationgroup.Bycontrollingaccesstotheun-
derlyingtable,SQLServercanef?cientlyenforcemessageordering,correlationof
relatedmessages,andlocking.Becausequeuesareinternaltables,queuesrequire
no special treatment for backup, restore, failover, or database mirroring. Both
applicationtablesandtheassociated,queuedmessagesarebackedup,restored,
and failed-over with the database. Broker conversations that exist in mirrored
databasescontinuewheretheyleftoffwhenthemirroredfailoveriscomplete—
eveniftheconversationwasbetweentwoservicesthatliveinseparatedatabases.
The locking granularity for Service Broker operations is the conversation
group rather than a speci?c conversation or individual messages. By enforcing
locking on the conversation group, Service Broker automatically helps applica-
tions avoid concurrency issues while processing messages. When a queue con-
tains multiple conversations, SQL Server guarantees that only one queue reader
at a time can process messages that belong to a given conversation group. This
eliminatestheneedfortheapplicationitselftoincludedeadlock-avoidancelogic
— a common source of errors in many messaging applications. Another nice
side effect of this locking semantic is that applications may choose to use the
conversation group as a key for storing and retrieving application state. These
programming-modelbene?tsarejusttwoexamplesoftheadvantagesthatderive
from thedecisiontoformalizethe conversationas thecommunication primitive
versus the atomic message primitive found in traditional message-queuing sys-
tems.
SQL Server can automatically activate stored procedures when a queue con-
tainsmessagestobeprocessed.Toscalethenumberofrunningstoredprocedures
to the incoming traf?c, the activation logic monitors the queue to see if there is
usefulworkforanotherqueuereader.SQLServerconsidersboththerateatwhich
existingreadersreceivemessagesandthenumberofconversationgroupsavail-
able to decide when to start another queue reader. The stored procedure to be
activated,thesecuritycontextofthestoredprocedure,andthemaximumnumber
ofinstancestobestartedarecon?guredforanindividualqueue. SQLServeralso
providesanExternalActivator.Thisfeatureallowsanapplicationoutsideof SQL
Servertobeactivatedwhennewmessagesareinsertedintoaqueue.Theapplica-
tioncanthenreceiveandprocessthemessages.Bydoingthis,CPU-intensivework
canbeof?oadedoutof SQLServertoanapplication,possiblyinadifferentcom-
puter. Also, long-duration tasks, e.g., invoking a Web service, can be executed
30.13 Business Intelligence 1263
without tying up database resources. The External Activator follows the same
logic as internal activation, and can be con?gured to activate multiple instances
ofanapplicationwhenmessagesaccumulateinaqueue.
Asalogicalextensiontoasynchronousmessagingwithintheinstance,Service
BrokeralsoprovidesreliablemessagingbetweenSQLServerinstancestoallowde-
veloperstoeasilybuilddistributedapplications.Conversationscanoccurwithin
asingleinstanceof SQLServerorbetweentwoinstancesof SQLServer.Localand
remoteconversationsusethesameprogrammingmodel.
Securityandroutingarecon?gureddeclaratively,withoutrequiringchanges
tothequeuereaders.SQLServerusesroutestomapaservicenametothenetwork
addressoftheotherparticipantintheconversation. SQLServercanalsoperform
message forwarding and simple load balancing for conversations. SQL Server
provides reliable, exactly once in-order delivery regardless of the number of
instancesthatamessagetravelsthrough.Aconversationthatspansinstancesof
SQLServercanbesecuredbothatthenetworkinglevel(pointtopoint)andatthe
conversation level (end to end). When end-to-end security is used, the contents
ofthemessageremainencrypteduntilthemessagereachesthe?naldestination,
whiletheheadersareavailabletoeachSQLServerinstancethatthemessagetravels
through. Standard SQL Serverpermissionsapplywithin an instance. Encryption
occurswhenmessagesleaveaninstance.
SQL Server uses a binary protocol for sending messages between instances.
The protocol fragmentslarge messagesand permitsinterleavedfragments from
multiplemessages.Fragmentationallows SQLServertoquicklytransmitsmaller
messages even in cases where a large message is in the process of being trans-
mitted. The binary protocol does not use distributed transactions or two-phase
commit. Instead, the protocol requires that a recipient acknowledge message
fragments. SQL Server simply retries message fragments periodically until the
fragment is acknowledged by the recipient. Acknowledgments are most often
included as part of the headers of a return message, although dedicated return
messagesareusedifnoreturnmessageisavailable.
SQL Server includes a command line diagnostics tool (ssbdiagnose)tohelp
analyzeaServiceBrokerdeploymentandinvestigateproblems.Thetoolcanrun
in either con?guration or runtime mode. In con?guration mode, the tool checks
whetherapairofservicescanexchangemessagesandreturnsanycon?guration
errors. Examples of these errors are disabled queues and missing return routes.
In the second mode, the tool connects to two or more SQL Server instances and
monitorsSQLPro?lereventstodiscoverServiceBrokerproblemsatruntime.The
tooloutputcanbesentintoa?leforautomatedprocessing.
30.13 Business Intelligence
ThebusinessintelligencecomponentofSQLServercontainsthreesubcomponents:
  SQLServerIntegrationServices(SSIS),whichprovidesthemeanstointegrate
datafrommultiplesources,performstransformationsrelatedtocleaningthe
dataandbringingittoacommonform,andloadingthedataintoadatabase
system.
1264 Chapter 30 MicrosoftSQL Server
  SQL Server Analysis Services (SSAS), which provides OLAP and data-mining
capabilities.
  SQLServerReportingServices(SSRS).
Integration Services, Analysis Services, and Reporting Services are each im-
plemented in separate servers and can be installed independently from one an-
other on the same or different machines. They can connect to a variety of data
sources,suchas?at?les,spreadsheets,oravarietyofrelationaldatabasesystems,
throughnativeconnectors, OLE-DB,orODBCdrivers.
Together they provide an end-to-end solution for extracting, transforming,
and loading data, then modeling and adding analytical capability to the data,
and ?nally building and distributingreportson the data.The differentBusiness
Intelligence components of SQL Server can integrate and leverage each others’
capability.Hereareafewcommonscenariosthatwillleverageacombinationof
components:
  Build an SSIS package that cleanses data, using patterns generated by SSAS
datamining.
  Use SSIStoloaddatatoan SSAScube,processit,andexecutereportsagainst
the SSAScube.
  Build an SSRS report to publish the ?ndings of a mining model or the data
containedinan SSAS OLAPcomponent.
The following sections give an overview of the capabilities and architecture
ofeachoftheseservercomponents.
30.13.1 SQL Server Integration Services
Microsoft SQL Server Integration Services (SSIS) is an enterprise data transfor-
mation and data integration solution that you can use to extract, transform, ag-
gregate, and consolidate data from disparate sources and move it to single or
multipledestinations.Youcanuse SSIStoperformthefollowingtasks:
  Mergedatafromheterogeneousdatastores.
  Refreshdataindatawarehousesanddatamarts.
  Cleansedatabeforeloadingitintodestinations.
  Bulk-loaddataintoonlinetransactionprocessing(OLTP)andonlineanalytical
processing(OLAP)databases.
  Sendnoti?cations.
  Buildbusinessintelligenceintoadatatransformationprocess.
  Automateadministrativefunctions.
30.13 Business Intelligence 1265
SSIS provides a complete set of services, graphical tools, programmable objects,
and APIs for the above tasks. These provide the ability to build large, robust,
and complex data transformation solutions without any custom programming.
However,anAPIandprogrammableobjectsareavailablewhentheyareneededto
create custom elements or integrate data transformation capabilities into custom
applications.
The SSIS data-?ow engine provides the in-memory buffers that move data
from source to destination and calls the source adapters that extract data from
?les and relational databases. The engine also provides the transformations that
modifydataandthedestinationadaptersthatloaddataintodatastores.Duplicate
elimination based on fuzzy (approximate) match is an example of a transforma-
tion provided by SSIS. Users can program their own transformations if required.
Figure 30.7 shows an example of how various transformations can be combined
to cleanse and load book sales information; the book titles from the sales data
Figure 30.7 Loading of data by using fuzzy lookup.
1266 Chapter 30 MicrosoftSQL Server
arematchedagainstapublicationsdatabase,andincasethereisnomatch,fuzzy
lookup is performed to handle titleswith minor errors (such as spellingerrors).
Informationaboutcon?denceanddatalineageisstoredwiththecleanseddata.
30.13.2 SQL Server Analysis Services
The Analysis Services component delivers online analytical processing (OLAP)
and data-mining functionality for business intelligence applications. Analysis
Servicessupportsathinclientarchitecture.Thecalculationengineisontheserver,
soqueriesareresolvedontheserver,avoidingtheneedtotransferlargeamounts
ofdatabetweentheclientandtheserver.
30.13.2.1 SQL ServerAnalysis Services: OLAP
Analysis Services utilizes a Uni?ed Dimensional Model (UDM), which bridges
the gap between traditional relational reporting and OLAP ad hoc analysis. The
role of a Uni?ed Dimensional Model (UDM) is to provide a bridge between the
userandthedatasources.A UDMisconstructedoveroneormorephysicaldata
sources, and then the end user issues queries against the UDM,usingoneofa
varietyofclienttools,suchasMicrosoftExcel.
MorethansimplyadimensionmodelinglayeroftheDataSourceschemas,the
UDMprovidesarichenvironmentforde?ningpowerfulyetexhaustivebusiness
logic, rules, and semantic de?nition. Users can browse and generate reports on
theUDMdataintheirnativelanguage(forexample,FrenchorHindi)byde?ning
local language translation of the metadata catalog as well as the dimensional
data.AnalysisServerde?nescomplextimedimensions(?scal,reporting,manu-
facturing,etc.),andenablesthede?nitionofpowerfulmultidimensionalbusiness
logic (year-to-year growth, year-to-date)using the multidimensional expression
(MDX)language.TheUDMallowsuserstode?nebusiness-orientedperspectives,
each one presenting only a speci?c subset of the model (measures, dimensions,
attributes, business rules, and so forth) that is relevant to a particular group of
users. Businessesoften de?ne key performance indicators (KPIs) that are impor-
tant metrics used to measure the health of the business. Examples of such KPIs
include sales, revenue per employee, and customer retention rate. The UDM al-
lows such KPIs to be de?ned, enabling a much more understandable grouping
andpresentationofdata.
30.13.2.2 SQL ServerAnalysis Services: Data Mining
SQLServerprovidesavarietyofminingtechniques,witharichgraphicalinterface
toviewminingresults.Miningalgorithmssupportedinclude:
  Associationrules(usefulforcross-salesapplications).
  Classi?cation and prediction techniques such as decision trees, regression
trees,neuralnetworks,andnaiveBayes.
  Timeseriesforecastingtechniquesincluding ARIMAand ARTXP.
Bibliographical Notes 1267
  ClusteringtechniquessuchasexpectationmaximizationandK-means(cou-
pledwithtechniquesforsequenceclustering).
In addition, SQL Server provides an extensible architecture for plugging in
third-partydataminingalgorithmsandvisualizers.
SQL Server also supports the Data-Mining Extensions (DMX) extensions for
SQL. DMX is the language used to interact with data-mining models just as SQL
is used to interact with tables and views. With DMX, models can be created and
trained and then stored in an Analysis Services database. The model can then
be browsed to look at patterns or, by using a special prediction join syntax,
applied against new data to perform predictions. The DMX language supports
functions and constructs to easily determine a predicted class along with its
con?dence, predict a list of associated items as in a recommendation engine, or
even return information and supporting facts about a prediction. Data mining
in SQL Server can be used against data stored in relational or multidimensional
datasources.Otherdatasourcesaresupportedaswellthroughspecializedtasks
andtransforms,allowingdataminingdirectlyintheoperationaldatapipelineof
Integration Services. Data-mining results can be exposed in graphical controls,
specialdata-miningdimensionsfor OLAPcubes,orsimplyinReportingServices
reports.
30.13.3 SQL Server Reporting Services
ReportingServicesisaserver-basedreportingplatformthatcanbeusedtocreate
and manage tabular, matrix, graphical, and free-form reports that contain data
from relational and multidimensional data sources. The reports that you create
can be viewed and managed over a Web-based connection. Matrix reports can
summarize data for high-level reviews, while providing supporting detail in
drilldownreports.Parameterizedreportscanbeusedto?lterdataonthebasisof
valuesthatareprovidedatruntime.Userscanchoosefromavarietyofviewing
formats to render reports on the ?y in preferred formats for data manipulation
or printing. An API is also available to extend or integrate report capabilities
intocustomsolutions.Server-basedreportingprovidesawaytocentralizereport
storage and management, set policies and secure access to reports and folders,
controlhowreportsareprocessedanddistributed,andstandardizehowreports
areusedinyourbusiness.
Bibliographical Notes
DetailedinformationaboutusingaC2certi?edsystemwithSQLServerisavailable
at www.microsoft.com/Downloads/Release.asp?ReleaseID=25503.
SQL Server’s optimization framework is based on the Cascades optimizer
prototype, which Graefe [1995] proposed. Simmen et al. [1996] discusses the
scheme for reducing grouping columns. Galindo-Legaria and Joshi [2001] and
Elhemali et al. [2007] present the variety of execution strategies that SQL Server
1268 Chapter 30 MicrosoftSQL Server
considers for cost-based optimization of subqueries. Additional information on
the self-tuning aspects of SQL Server are discussed by Chaudhuri et al. [1999].
Chaudhuri and Shim [1994] and Yan and Larson [1995] discuss reordering of
aggregationoperations.
Chatziantoniou and Ross [1997] and Galindo-Legaria and Joshi [2001] pro-
posed the alternative used by SQL Server for SQL queries requiring a self-join.
Underthisscheme,theoptimizerdetectsthepatternandconsidersper-segment
execution.Pellenkoftetal.[1997]discussestheoptimizationschemeforgenerat-
ing the complete search space using a set of transformations that are complete,
local and nonredundant. Graefe et al. [1998] offers discussion concerning hash
operations that support basic aggregation and join, with a number of optimiza-
tions,extensions,anddynamictuningfordataskew.Graefeetal.[1998]presents
theideaofjoiningindicesforthesolepurposeofassemblingarowwiththesetof
columnsneededonaquery.Itarguesthatthissometimesisfasterthanscanning
abasetable.
Blakeley [1996] and Blakeley and Pizzo [2001] offer discussions concerning
communicationwiththestorageenginethroughOLE-DB.Blakeleyetal.[2005]de-
tailstheimplementationofthedistributedandheterogeneousquerycapabilities
ofSQLServer.Achesonetal.[2004]providesdetailsontheintegrationofthe.NET
CLRinsidethe SQLServerprocess.
Blakeley et al. [2008] describes the contracts for UDTs, UDAggs, and UDFs
in more detail. Blakeley et al. [2006] describes the ADO.NET Entity Framework.
Melnik et al. [2007] describes the mapping technology behind the ADO.NET En-
tity Framework. Adya et al. [2007] provides an overview of the ADO.NET Entity
Framework architecture. The SQL:2003 standard is de?ned in SQL/XML [2004].
Rys [2001] provides more details on the SQL Server 2000 XML functionality. Rys
[2004] provides an overview of the extensions to the for xml aggregation. For
information on XML capabilities that can be used on the client side or inside
CLR,refertothecollectionofwhitepapersathttp://msdn.microsoft.com/XML/Building-
XML/XMLandDatabase/default.aspx.TheXQuery1.0/XPath2.0datamodelisde?ned
in Walsh et al. [2007]. Rys [2003] provides an overview of implementation tech-
niquesforXQueryinthecontextofrelationaldatabases.TheOrdPathnumbering
schemeisdescribedinO’Neiletal.[2004];Paletal.[2004]andBarasetal.[2005]
providemoreinformationon XMLindexingand XQueryalgebraizationandopti-
mizationin SQLServer2005.
PART
10
APPENDICES
AppendixApresentsthefulldetailsoftheuniversitydatabasethatwehaveused
asourrunningexample,includinganE-Rdiagram,SQLDDL,andsampledatathat
wehaveusedthroughoutthebook.(The DDLandsampledataarealsoavailable
ontheWebsiteofthebook, db-book.com,foruseinlaboratoryexercises.)
The remaining appendices are not part of the printed book, but are available online
on the Web site of the book, db-book.com.Theseinclude:
  AppendixB (AdvancedRelational Database Design), ?rst covers the theory
of multivalued dependencies; recall that multivalued dependencies were
introduced in Chapter 8. The project-join normal form, which is based on a
typeofconstraintcalledjoindependencyispresentednext;joindependencies
are a generalization of multivalued dependencies. The chapter concludes
withanothernormalformcalledthedomain-keynormalform.
  AppendixC(OtherRelationalQueryLanguages)?rstpresentstherelational
querylanguageQuery-by-Example(QBE),whichwasdesignedtobeusedby
non-programmers.In QBE,querieslooklikeacollectionoftablescontaining
anexampleofdatatoberetrieved.ThegraphicalquerylanguageofMicrosoft
Access, which is based on QBE, is presented next, followed by the Datalog
language,whichhasasyntaxmodeledafterthelogic-programminglanguage
Prolog.
  AppendixD(NetworkModel),andAppendixE(HierarchicalModel),cover
the network and hierarchical data models. Both these data models predate
therelationalmodel,andprovidealevelofabstractionthatislowerthanthe
relational model. They abstract away some, but not all, details of the actual
datastructuresusedtostoredata ondisks.Thesemodelsareonly usedina
fewlegacyapplications.
For appendices B through E, we illsutrate our concepts using a bank enterprise
withtheschemashowninFigure 2.15.
1269
This page intentionally left blank 
APPENDIX
A
Detailed University Schema
In this appendix, we present the full details of our running-example university
database.InSectionA.1wepresentthefullschemaasusedinthetextandthe E-R
diagram that corresponds to that schema. In Section A.2 we present a relatively
complete SQL datade?nitionfor our running universityexample.Besideslisting
a datatype for each attribute, we include a substantial number of constraints.
Finally, in Section A.3 we present sample data that correspond to our schema.
SQL scripts to create all the relations in the schema, and to populate them with
sampledata,areavailableon theWebsiteofthe book, db-book.com.
A.1 Full Schema
The full schema of the University database as used in the text is shown in Fig-
ure A.1. The E-R diagram that corresponds to that schema, and used throughout
thetext,isshown inFigureA.2.
classroom(building, room number, capacity)
department(dept name, building, budget)
course(course id, title, dept name, credits)
instructor(ID, name, dept name, salary)
section(course id, sec id, semester, year, building, room number, time slot id)
teaches(ID, course id, sec id, semester, year)
student(ID, name, dept name, tot cred)
takes(ID, course id, sec id, semester, year, grade)
advisor(s ID, i ID)
time slot(time slot id, day, start time, end time)
prereq(course id, prereq id)
FigureA.1 Schema of the University database.
1271
1272 AppendixA DetailedUniversitySchema
time_slot course
student
ID
name
salary
ID
name
tot_cred
course_id
title
credits
time_slot_id
{ day
start_time
end_time
}
course_id prereq_id
advisor
teaches
takes
sec_course
sec_time_slot
grade
prereq
inst_dept stud_dept
instructor
department
dept_name
building
budget
section
sec_id
semester
year
course_dept
sec_class
classroom
building
room_number
capacity
FigureA.2 E-R diagram for a university enterprise.
A.2 DDL
Inthissection,wepresentarelativelycompleteSQLdatade?nitionforourexam-
ple.Besideslistingadatatypeforeachattribute,weincludeasubstantialnumber
ofconstraints.
createtable classroom
(building varchar(15),
room number varchar(7),
capacity numeric(4,0),
primarykey(building, room number));
A.2 DDL 1273
createtable department
(dept name varchar(20),
building varchar(15),
budget numeric(12,2)check(budget>0),
primarykey(dept name));
createtable course
(course id varchar(8),
title varchar(50),
dept name varchar(20),
credits numeric(2,0)check(credits>0),
primarykey(course id),
foreignkey(dept name)references department
ondeletesetnull);
createtable instructor
(ID varchar(5),
name varchar(20)notnull,
dept name varchar(20),
salary numeric(8,2)check(salary >29000),
primarykey(ID),
foreignkey(dept name)references department
ondeletesetnull);
createtable section
(course id varchar(8),
sec id varchar(8),
semester varchar(6)check (semesterin
(’Fall’, ’Winter’, ’Spring’, ’Summer’)),
year numeric(4,0)check(year >1701 and year <2100),
building varchar(15),
room number varchar(7),
time slot id varchar(4),
primarykey(course id, sec id, semester, year),
foreignkey(course id)references course
ondeletecascade,
foreignkey(building, room number)references classroom
ondeletesetnull);
In the above DDL we add the on delete cascade speci?cation to a foreign
key constraint if the existence of the tuple depends on the referenced tuple. For
exampleweaddtheondeletecascadespeci?cationtotheforeignkeyconstraint
fromsection(whichwasgeneratedfromweakentitysection),tocourse(whichwas
1274 AppendixA DetailedUniversitySchema
its identifying relationship). In other foreign key constraints we either specify
on delete set null, which allows deletion of a referenced tuple by setting the
referencing value to null, or do not add any speci?cation, which prevents the
deletion of any referenced tuple. For example, if a department is deleted, we
would not wish to delete associated instructors; the foreign key constraint from
instructor to department instead sets the dept name attribute to null. On the other
hand, the foreign key constraint for the prereq relation, shown later, prevents the
deletion of a course that is required as a prerequisite for another course. For the
advisor relation, shown later, we allow i ID to be set to null if an instructor is
deleted,butdeletean advisor tupleifthereferencedstudentisdeleted.
createtable teaches
(ID varchar (5),
course id varchar (8),
sec id varchar (8),
semester varchar (6),
year numeric(4,0),
primarykey (ID, course id, sec id, semester, year),
foreignkey (course id, sec id, semester, year)references section
ondeletecascade,
foreignkey (ID)references instructor
ondeletecascade);
createtable student
(ID varchar (5),
name varchar (20)notnull,
dept name varchar (20),
tot cred numeric(3,0)check (tot cred>=0),
primarykey (ID),
foreignkey (dept name)references department
ondeletesetnull);
createtable takes
(ID varchar (5),
course id varchar (8),
sec id varchar (8),
semester varchar (6),
year numeric(4,0),
grade varchar (2),
primarykey (ID, course id, sec id, semester, year),
foreignkey (course id, sec id, semester, year)references section
ondeletecascade,
foreignkey (ID)references student
ondeletecascade);
A.2 DDL 1275
createtable advisor
(s ID varchar(5),
i ID varchar(5),
primarykey(s ID),
foreignkey(i ID)references instructor (ID)
ondeletesetnull,
foreignkey(s ID)references student(ID)
ondeletecascade);
createtable prereq
(course id varchar(8),
prereq id varchar(8),
primarykey(course id, prereq id),
foreignkey(course id)references course
ondeletecascade,
foreignkey(prereq id)references course);
Thefollowingcreatetablestatementforthetabletime slotcanberunonmost
database systems, but does not work on Oracle (at least as of Oracle version 11),
since Oracledoesnot supportthe SQL standardtypetime.
createtable timeslot
(time slot id varchar(4),
day varchar(1)check (dayin (’M’, ’T’, ’W’, ’R’, ’F’, ’S’,’U’)),
start time time,
end time time,
primarykey(time slot id, day, start time));
Thesyntaxforspecifyingtimein SQLisillustratedbytheseexamples:’08:30’,
’13:55’,and’5:30PM’.SinceOracledoesnotsupportthetimetype,forOraclewe
usethe following schemainstead:
createtable timeslot
(time slot id varchar(4),
day varchar(1),
start hr numeric(2) check (start hr >=0and end hr< 24),
start min numeric(2) check (start min >=0and start min <60),
end hr numeric(2) check (end hr>=0and end hr<24),
end min numeric(2) check (end min>=0and end min<60),
primarykey(time slot id, day, start hr, start min));
The difference is that start time has been replaced by two attributes start hr
and start min, and similarly end time has been replaced by attributes end hr and
end min. These attributes also have constraints that ensure that only numbers
representing valid time values appear in those attributes. This version of the
1276 AppendixA DetailedUniversitySchema
schemafortime slotworksonalldatabases,includingOracle.Notethatalthough
Oracle supports thedatetime datatype,datetime includes a speci?c day, month,
and year as well as a time, and is not appropriate here since we want only a
time. There are two alternatives to splitting the time attributes into an hour and
a minute component, but neither is desirable. The ?rst alternative is to use a
varchar type, but that makes it hard to enforce validity constraints on the string
as well as to perform comparison on time. The second alternative is to encode
timeasanintegerrepresentinganumberofminutes(orseconds)frommidnight,
butthisalternativerequiresextracodewitheachquerytocovertvaluesbetween
the standard time representation and the integer encoding. We therefore chose
the two-partsolution.
A.3 Sample Data
In this section we provide sample data for each of the relations de?ned in the
previoussection.
building room number capacity
Packard 101 500
Painter 514 10
Taylor 3128 70
Watson 100 30
Watson 120 50
FigureA.3 The classroom relation.
dept name building budget
Biology Watson 90000
Comp.Sci. Taylor 100000
Elec.Eng. Taylor 85000
Finance Painter 120000
History Painter 50000
Music Packard 80000
Physics Watson 70000
FigureA.4 The department relation.
A.3 SampleData 1277
course id title dept name credits
BIO-101 Intro.toBiology Biology 4
BIO-301 Genetics Biology 4
BIO-399 ComputationalBiology Biology 3
CS-101 Intro.toComputerScience Comp.Sci. 4
CS-190 GameDesign Comp.Sci. 4
CS-315 Robotics Comp.Sci. 3
CS-319 ImageProcessing Comp.Sci. 3
CS-347 DatabaseSystemConcepts Comp.Sci. 3
EE-181 Intro.toDigitalSystems Elec.Eng. 3
FIN-201 InvestmentBanking Finance 3
HIS-351 WorldHistory History 3
MU-199 MusicVideoProduction Music 3
PHY-101 Physical Principles Physics 4
FigureA.5 The course relation.
ID name dept name salary
10101 Srinivasan Comp.Sci. 65000
12121 Wu Finance 90000
15151 Mozart Music 40000
22222 Einstein Physics 95000
32343 ElSaid History 60000
33456 Gold Physics 87000
45565 Katz Comp.Sci. 75000
58583 Cali?eri History 62000
76543 Singh Finance 80000
76766 Crick Biology 72000
83821 Brandt Comp.Sci. 92000
98345 Kim Elec.Eng. 80000
FigureA.6 The instructor relation.
1278 AppendixA DetailedUniversitySchema
course id sec id semester year building room number time slot id
BIO-101 1 Summer 2009 Painter 514 B
BIO-301 1 Summer 2010 Painter 514 A
CS-101 1 Fall 2009 Packard 101 H
CS-101 1 Spring 2010 Packard 101 F
CS-190 1 Spring 2009 Taylor 3128 E
CS-190 2 Spring 2009 Taylor 3128 A
CS-315 1 Spring 2010 Watson 120 D
CS-319 1 Spring 2010 Watson 100 B
CS-319 2 Spring 2010 Taylor 3128 C
CS-347 1 Fall 2009 Taylor 3128 A
EE-181 1 Spring 2009 Taylor 3128 C
FIN-201 1 Spring 2010 Packard 101 B
HIS-351 1 Spring 2010 Painter 514 C
MU-199 1 Spring 2010 Packard 101 D
PHY-101 1 Fall 2009 Watson 100 A
FigureA.7 The section relation.
ID course id sec id semester year
10101 CS-101 1 Fall 2009
10101 CS-315 1 Spring 2010
10101 CS-347 1 Fall 2009
12121 FIN-201 1 Spring 2010
15151 MU-199 1 Spring 2010
22222 PHY-101 1 Fall 2009
32343 HIS-351 1 Spring 2010
45565 CS-101 1 Spring 2010
45565 CS-319 1 Spring 2010
76766 BIO-101 1 Summer 2009
76766 BIO-301 1 Summer 2010
83821 CS-190 1 Spring 2009
83821 CS-190 2 Spring 2009
83821 CS-319 2 Spring 2010
98345 EE-181 1 Spring 2009
FigureA.8 The teaches relation.
A.3 SampleData 1279
ID name dept name tot cred
00128 Zhang Comp.Sci. 102
12345 Shankar Comp.Sci. 32
19991 Brandt History 80
23121 Chavez Finance 110
44553 Peltier Physics 56
45678 Levy Physics 46
54321 Williams Comp.Sci. 54
55739 Sanchez Music 38
70557 Snow Physics 0
76543 Brown Comp.Sci. 58
76653 Aoi Elec.Eng. 60
98765 Bourikas Elec.Eng. 98
98988 Tanaka Biology 120
FigureA.9 The student relation.
ID course id sec id semester year grade
00128 CS-101 1 Fall 2009 A
00128 CS-347 1 Fall 2009 A-
12345 CS-101 1 Fall 2009 C
12345 CS-190 2 Spring 2009 A
12345 CS-315 1 Spring 2010 A
12345 CS-347 1 Fall 2009 A
19991 HIS-351 1 Spring 2010 B
23121 FIN-201 1 Spring 2010 C+
44553 PHY-101 1 Fall 2009 B-
45678 CS-101 1 Fall 2009 F
45678 CS-101 1 Spring 2010 B+
45678 CS-319 1 Spring 2010 B
54321 CS-101 1 Fall 2009 A-
54321 CS-190 2 Spring 2009 B+
55739 MU-199 1 Spring 2010 A-
76543 CS-101 1 Fall 2009 A
76543 CS-319 2 Spring 2010 A
76653 EE-181 1 Spring 2009 C
98765 CS-101 1 Fall 2009 C-
98765 CS-315 1 Spring 2010 B
98988 BIO-101 1 Summer 2009 A
98988 BIO-301 1 Summer 2010 null
Figure A.10 The takes relation.
1280 AppendixA DetailedUniversitySchema
s id i id
00128 45565
12345 10101
23121 76543
44553 22222
45678 22222
76543 45565
76653 98345
98765 98345
98988 76766
Figure A.11 The advisor relation.
time slot id day start time end time
A M 8:00 8:50
A W 8:00 8:50
A F 8:00 8:50
B M 9:00 9:50
B W 9:00 9:50
B F 9:00 9:50
C M 11:00 11:50
C W 11:00 11:50
C F 11:00 11:50
D M 13:00 13:50
D W 13:00 13:50
D F 13:00 13:50
E T 10:30 11:45
E R 10:30 11:45
F T 14:30 15:45
F R 14:30 15:45
G M 16:00 16:50
G W 16:00 16:50
G F 16:00 16:50
H W 10:00 12:30
FigureA.12 The time slot relation.
A.3 SampleData 1281
course id prereq id
BIO-301 BIO-101
BIO-399 BIO-101
CS-190 CS-101
CS-315 CS-101
CS-319 CS-101
CS-347 CS-101
EE-181 PHY-101
Figure A.13 The prereq relation.
time slot id day start hr start min end hr end min
A M 8 0 8 50
A W 8 0 8 50
A F 8 0 8 50
B M 9 0 9 50
B W 9 0 9 50
B F 9 0 9 50
C M 11 0 11 50
C W 11 0 11 50
C F 11 0 11 50
D M 13 0 13 50
D W 13 0 13 50
D F 13 0 13 50
E T 10 30 11 45
E R 10 30 11 45
F T 14 30 15 45
F R 14 30 15 45
G M 16 0 16 50
G W 16 0 16 50
G F 16 0 16 50
H W 10 0 12 30
FigureA.14 The time slot relation with start and end time separated into hour and minute.
This page intentionally left blank 
Bibliography
[Abadi2009] D.Abadi, “DataManagementintheCloud:LimitationsandOppor-
tunities”,DataEngineeringBulletin,Volume32,Number1(2009),pages3–12.
[Abadietal.2008] D. J. Abadi, S. Madden, and N. Hachem, “Column-stores vs.
row-stores: how different are they really?”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(2008),pages967–980.
[Abitebouletal.1995] S.Abiteboul,R.Hull,andV.Vianu,FoundationsofDatabases,
AddisonWesley(1995).
[Abitebouletal.2003] S.Abiteboul,R.Agrawal,P.A.Bernstein,M.J.Carey,etal.
“TheLowellDatabaseResearchSelfAssessment”(2003).
[Achesonetal.2004] A. Acheson, M. Bendixen, J. A. Blakeley, I. P. Carlin, E. Er-
san, J. Fang, X. Jiang, C. Kleinerman, B. Rathakrishnan, G. Schaller, B. Sezgin,
R.Venkatesh,andH.Zhang,“Hostingthe.NETRuntimeinMicrosoftSQLServer”,
InProc.oftheACMSIGMODConf.onManagementofData(2004),pages860–865.
[Adalietal.1996] S. Adali, K. S. Candan, Y. Papakonstantinou, and V. S. Subrah-
manian, “Query Caching and Optimization in Distributed Mediator Systems”,In
Proc.oftheACMSIGMODConf.onManagementofData(1996),pages137–148.
[Adyaetal.2007] A.Adya,J.A.Blakeley,S.Melnik,andS.Muralidhar, “Anatomy
of the ADO.NET entity framework”,InProc. of the ACM SIGMOD Conf. on Man-
agementofData(2007),pages877–888.
[Agarwaletal.1996] S. Agarwal, R. Agrawal, P. M. Deshpande, A. Gupta, J. F.
Naughton, R. Ramakrishnan, and S. Sarawagi, “On the Computation of Multi-
dimensional Attributes”,InProc. of the International Conf. on Very Large Databases
(1996),pages506–521.
[AgrawalandSrikant1994] R.AgrawalandR.Srikant,“FastAlgorithmsforMin-
ingAssociationRulesinLargeDatabases”,InProc.oftheInternationalConf.onVery
LargeDatabases(1994),pages487–499.
1283
1284 Bibliography
[Agrawaletal.1992] R. Agrawal, S. P. Ghosh, T. Imielinski, B. R. Iyer, and A. N.
Swami, “An Interval Classi?er for Database Mining Applications”,InProc. of the
InternationalConf.onVeryLargeDatabases(1992),pages560–573.
[Agrawaletal.1993a] R.Agrawal,T.Imielinski,andA.Swami, “MiningAssocia-
tionRulesbetweenSetsofItemsinLargeDatabases”,InProc.oftheACMSIGMOD
Conf.onManagementofData(1993).
[Agrawaletal.1993b] R.Agrawal,T.Imielinski,andA.N.Swami,“DatabaseMin-
ing:APerformancePerspective”,IEEETransactionsonKnowledgeandDataEngineer-
ing,Volume5,Number6(1993),pages914–925.
[Agrawaletal.2000] S.Agrawal,S.Chaudhuri,andV.R.Narasayya, “Automated
Selection of Materialized Views and Indexes in SQL Databases”,InProc. of the
InternationalConf.onVeryLargeDatabases(2000),pages496–505.
[Agrawaletal.2002] S.Agrawal,S.Chaudhuri,andG.Das,“DBXplorer:ASystem
for Keyword-Based Search overRelational Databases”,InProc. of the International
Conf.onDataEngineering(2002).
[Agrawaletal.2004] S. Agrawal, S. Chaudhuri, L. Kollar, A. Marathe,
V.Narasayya,andM.Syamala,“DatabaseTuningAdvisorforMicrosoftSQLServer
2005”,InProc.oftheInternationalConf.onVeryLargeDatabases(2004).
[Agrawaletal.2009] R.Agrawal,A.Ailamaki, P.A.Bernstein,E.A.Brewer,M.J.
Carey, S. Chaudhuri, A. Doan, D. Florescu, M. J. Franklin, H. Garcia-Molina,
J.Gehrke,L.Gruenwald,L.M.Haas,A.Y .Halevy ,J.M.Hellerstein,Y .E.Ioan-
nidis, H. F. Korth, D. Kossmann, S. Madden, R. Magoulas, B. C. Ooi, T. O^Reilly,
R. Ramakrishnan,S. Sarawagi, andG. W. Michael Stonebraker,AlexanderS. Sza-
lay, “The Claremont Report on Database Research”, Communications of the ACM,
Volume52,Number6(2009),pages56–65.
[Ahmedetal.2006] R.Ahmed,A.Lee,A.Witkowski, D.Das,H.Su, M.Za¨ ?t,and
T. Cruanes, “Cost-Based Query Transformation in Oracle”,InProc. of the Interna-
tionalConf.onVeryLargeDatabases(2006),pages1026–1036.
[Ahoetal.1979a] A. V. Aho, C. Beeri, and J. D. Ullman, “The Theory of Joins in
RelationalDatabases”,ACMTransactionsonDatabaseSystems,Volume4,Number3
(1979),pages297–314.
[Ahoetal.1979b] A. V. Aho, Y. Sagiv, and J. D. Ullman, “Equivalences among
Relational Expressions”, SIAM Journal of Computing, Volume 8, Number 2 (1979),
pages218–246.
[Ailamakietal.2001] A. Ailamaki, D. J. DeWitt, M. D. Hill, and M. Skounakis,
“Weaving Relations for Cache Performance”,InProc. of the International Conf. on
VeryLargeDatabases(2001),pages169–180.
[AlonsoandKorth1993] R. Alonso and H. F. Korth, “Database System Issues in
Nomadic Computing”,InProc. of the ACM SIGMOD Conf. on Management of Data
(1993),pages388–392.
Bibliography 1285
[Amer-Yahiaetal.2004] S. Amer-Yahia, C. Botev, and J. Shanmugasundaram,
“TeXQuery: A Full-Text Search Extension to XQuery”,InProc. of the International
WorldWideWebConf.(2004).
[Andersonetal.1992] D.P.Anderson,Y.Osawa,andR.Govindan,“AFileSystem
forContinuousMedia”,ACMTransactionsonDatabaseSystems,Volume10,Number
4(1992),pages311–337.
[Andersonetal.1998] T.Anderson,Y.Breitbart,H.F.Korth,andA.Wool, “Repli-
cation, Consistency and Practicality: Are These Mutually Exclusive?”,InProc. of
theACMSIGMODConf.onManagementofData(1998).
[ANSI1986] AmericanNational StandardforInformation Systems:DatabaseLanguage
SQL. AmericanNationalStandardsInstitute(1986).
[ANSI1989] DatabaseLanguageSQLwithIntegrityEnhancement,ANSIX3,135–1989.
AmericanNationalStandardsInstitute,NewYork(1989).
[ANSI1992] DatabaseLanguageSQL,ANSIX3,135–1992. AmericanNationalStan-
dardsInstitute,NewYork(1992).
[Antoshenkov1995] G.Antoshenkov, “Byte-alignedBitmapCompression(poster
abstract)”,InIEEEDataCompressionConf.(1995).
[AppeltandIsrael1999] D.E.AppeltandD.J.Israel,“IntroductiontoInformation
Extraction Technology”,InProc. of the International Joint Conferences on Arti?cial
Intelligence(1999).
[AptandPugin1987] K. R. Apt and J. M. Pugin, “Maintenance of Strati?ed
Database Viewed as a Belief Revision System”,InProc. of the ACM Symposium
onPrinciplesofDatabaseSystems(1987),pages136–145.
[Armstrong1974] W. W. Armstrong, “Dependency Structures of Data Base Rela-
tionships”,InProc.ofthe1974IFIPCongress(1974),pages580–583.
[Astrahanetal.1976] M.M.Astrahan,M.W .Blasgen,D.D.Chamberlin,K.P .
Eswaran,J.N.Gray,P.P.Grif?ths,W.F.King,R.A.Lorie,P.R.McJones,J.W.Mehl,
G.R.Putzolu,I.L.Traiger,B.W.Wade,andV.Watson,“SystemR,ARelationalAp-
proachtoDataBaseManagement”,ACMTransactionsonDatabaseSystems,Volume
1,Number2(1976),pages97–137.
[Atreyaetal.2002] M.Atreya,B.Hammond,S.Paine,P.Starrett,andS.Wu,Digital
Signatures,RSAPress(2002).
[AtzeniandAntonellis1993] P. Atzeni and V. D. Antonellis, Relational Database
Theory,BenjaminCummings(1993).
[Baeza-YatesandRibeiro-Neto1999] R.Baeza-Yatesand B.Ribeiro-Neto, Modern
InformationRetrieval,AddisonWesley(1999).
[Bancilhonetal.1989] F.Bancilhon,S.Cluet,andC.Delobel, “AQueryLanguage
for the O
2
Object-Oriented Database”,InProc. of the Second Workshop on Database
ProgrammingLanguages(1989).
1286 Bibliography
[Barasetal.2005] A.Baras,D.Churin,I.Cseri,T.Grabs,E.Kogan,S.Pal,M.Rys,
andO.Seeliger. “ImplementingXQueryinaRelationalDatabaseSystem”(2005).
[Baruetal.1995] C.Baruetal.,“DB2ParallelEdition”,IBMSystemsJournal,Volume
34,Number2(1995),pages292–322.
[Bassiouni1988] M. Bassiouni, “Single-site and Distributed Optimistic Protocols
forConcurrencyControl”,IEEETransactionsonSoftwareEngineering,VolumeSE-14,
Number8(1988),pages1071–1080.
[Batinietal.1992] C. Batini, S. Ceri, and S. Navathe, Database Design: An Entity-
RelationshipApproach,BenjaminCummings(1992).
[Bayer1972] R.Bayer,“SymmetricBinaryB-trees:DataStructureandMaintenance
Algorithms”,ActaInformatica,Volume1,Number4(1972),pages290–306.
[BayerandMcCreight1972] R. Bayer and E. M. McCreight, “Organization and
Maintenance of Large Ordered Indices”, Acta Informatica,V olume1,Number3
(1972),pages173–189.
[BayerandSchkolnick1977] R.BayerandM.Schkolnick, “ConcurrencyofOper-
atingonB-trees”,ActaInformatica,Volume9,Number1(1977),pages1–21.
[BayerandUnterauer1977] R. Bayer and K. Unterauer, “Pre?x B-trees”, ACM
TransactionsonDatabaseSystems,Volume2,Number1(1977),pages11–26.
[Bayeretal.1978] R. Bayer, R. M. Graham, and G. Seegmuller, editors, Operating
Systems:AnAdvancedCourse,SpringerVerlag(1978).
[Beckmannetal.1990] N. Beckmann, H. P. Kriegel, R. Schneider, and B. Seeger,
“TheR
?
-tree:An Ef?cient andRobust Access Methodfor Points andRectangles”,
InProc.oftheACMSIGMODConf.onManagementofData(1990),pages322–331.
[Beerietal.1977] C.Beeri,R.Fagin,andJ.H.Howard, “ACompleteAxiomatiza-
tionforFunctionalandMultivaluedDependencies”,InProc.oftheACMSIGMOD
Conf.onManagementofData(1977),pages47–61.
[Bentley1975] J.L.Bentley,“MultidimensionalBinarySearchTreesUsedforAsso-
ciativeSearching”,CommunicationsoftheACM,Volume18,Number9(1975),pages
509–517.
[Berensonetal.1995] H. Berenson, P. Bernstein, J. Gray, J. Melton, E. O’Neil, and
P.O’Neil, “ACritiqueofANSISQLIsolationLevels”,InProc.oftheACMSIGMOD
Conf.onManagementofData(1995),pages1–10.
[BernsteinandGoodman1981] P. A. Bernstein and N. Goodman, “Concurrency
Control in Distributed Database Systems”, ACM Computing Survey, Volume 13,
Number2(1981),pages185–221.
[BernsteinandNewcomer1997] P. A. Bernstein and E. Newcomer, Principles of
TransactionProcessing,MorganKaufmann(1997).
Bibliography 1287
[Bernsteinetal.1998] P. Bernstein, M. Brodie, S. Ceri, D. DeWitt, M. Franklin,
H.Garcia-Molina,J.Gray,J.Held,J.Hellerstein,H.V.Jagadish,M.Lesk,D.Maier,
J.Naughton,H.Pirahesh,M.Stonebraker,andJ.Ullman,“TheAsilomarReporton
DatabaseResearch”,ACMSIGMODRecord,Volume27,Number4(1998).
[Bersonetal.1995] S. Berson,L. Golubchik, andR. R.Muntz, “Fault TolerantDe-
signofMultimediaServers”,InProc. of the ACMSIGMOD Conf. on Management of
Data(1995),pages364–375.
[Bhalotiaetal.2002] G.Bhalotia,A.Hulgeri,C.Nakhe,S.Chakrabarti,andS.Su-
darshan, “KeywordSearchingandBrowsinginDatabasesusingBANKS”,InProc.
oftheInternationalConf.onDataEngineering(2002).
[BharatandHenzinger1998] K. Bharat and M. R. Henzinger, “Improved Algo-
rithms for Topic Distillation in a Hyperlinked Environment”,InProc. of the ACM
SIGIR Conf. on Researchand Development in Information Retrieval(1998),pages 104–
111.
[Bhattacharjeeetal.2003] B.Bhattacharjee,S.Padmanabhan,T.Malkemus,T.Lai,
L. Cranston, and M. Huras, “Ef?cient Query Processing for Multi-Dimensionally
Clustered Tables in DB2”,InProc. of the International Conf. on Very Large Databases
(2003),pages963–974.
[Biskupetal.1979] J. Biskup, U. Dayal, and P. A. Bernstein, “Synthesizing Inde-
pendentDatabaseSchemas”,InProc.oftheACMSIGMOD Conf.onManagementof
Data(1979),pages143–152.
[Bittonetal.1983] D. Bitton, D. J. DeWitt, and C. Turby?ll, “Benchmarking
Database Systems: A Systematic Approach”,InProc. of the International Conf. on
VeryLargeDatabases(1983).
[Blakeley1996] J. A. Blakeley, “Data Access for the Masses through OLE DB”,In
Proc.oftheACMSIGMODConf.onManagementofData(1996),pages161–172.
[BlakeleyandPizzo2001] J. A. Blakeley and M. Pizzo, “Enabling Component
Databases with OLE DB”,InK.R.DittrichandA.Geppert,editors,Component
DatabaseSystems,MorganKaufmannPublishers(2001),pages139–173.
[Blakeleyetal.1986] J. A. Blakeley, P. Larson, and F. W. Tompa, “Ef?ciently Up-
dating Materialized Views”,InProc. of the ACM SIGMOD Conf. on Management of
Data(1986),pages61–71.
[Blakeleyetal.2005] J. A. Blakeley, C. Cunningham, N. Ellis, B. Rathakrishnan,
and M.-C. Wu, “Distributed/Heterogeneous Query Processing in Microsoft SQL
Server”,InProc.oftheInternationalConf.onDataEngineering(2005).
[Blakeleyetal.2006] J.A.Blakeley,D.Campbell,S.Muralidhar,andA.Nori,“The
ADO.NETentityframework:makingtheconceptuallevelreal”, SIGMOD Record,
Volume35,Number4(2006),pages32–39.
[Blakeleyetal.2008] J.A.Blakeley ,V .Rao,I.Kunen,A.Prout,M.Henaire,and
C. Kleinerman, “.NET database programmability and extensibility in Microsoft
1288 Bibliography
SQL server”,InProc. of the ACM SIGMOD Conf. on Management of Data (2008),
pages1087–1098.
[BlasgenandEswaran1976] M.W.BlasgenandK.P.Eswaran,“OntheEvaluation
of Queries in a Relational Database System”, IBM Systems Journal, Volume 16,
(1976),pages363–377.
[Boyceetal.1975] R.Boyce,D.D.Chamberlin,W.F.King,andM.Hammer,“Spec-
ifyingQueriesasRelationalExpressions”,CommunicationsoftheACM,Volume18,
Number11(1975),pages621–628.
[Brantneretal.2008] M. Brantner, D. Florescu, D. Graf, D. Kossmann, and
T. Kraska, “Building a Database on S3”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(2008),pages251–263.
[Breeseetal.1998] J.Breese,D.Heckerman,andC.Kadie, “EmpiricalAnalysisof
PredictiveAlgorithmsforCollaborativeFiltering”,InProcs.Conf.onUncertaintyin
Arti?cialIntelligence,MorganKaufmann(1998).
[Breitbartetal.1999a] Y. Breitbart, R. Komondoor, R. Rastogi, S. Seshadri, and
A.Silberschatz,“UpdatePropagationProtocolsForReplicatedDatabases”,InProc.
oftheACMSIGMODConf.onManagementofData(1999),pages97–108.
[Breitbartetal.1999b] Y. Breitbart, H. Korth, A. Silberschatz, and S. Sudarshan,
“Distributed Databases”,InEncyclopedia of Electrical and Electronics Engineering,
JohnWileyandSons(1999).
[Brewer2000] E. A. Brewer, “Towards robust distributed systems (abstract)”,In
Proc.oftheACMSymposiumonPrinciplesofDistributedComputing(2000),page7.
[BrinandPage1998] S. Brin and L. Page, “The Anatomy of a Large-Scale Hyper-
textualWebSearchEngine”,InProc.oftheInternationalWorldWideWebConf.(1998).
[Brinkhoffetal.1993] T.Brinkhoff,H.-P.Kriegel,andB.Seeger,“Ef?cientProcess-
ingofSpatialJoinsUsingR-trees”,InProc.oftheACMSIGMODConf.onManagement
ofData(1993),pages237–246.
[Brunoetal.2002] N. Bruno, S. Chaudhuri, and L. Gravano, “Top-k Selection
Queries Over Relational Databases: Mapping Strategies and Performance Eval-
uation”,ACMTransactionsonDatabaseSystems,Volume27,Number2(2002),pages
153–187.
[BuckleyandSilberschatz1983] G. Buckley and A. Silberschatz, “Obtaining Pro-
gressiveProtocolsforaSimpleMultiversionDatabaseModel”,InProc.oftheInter-
nationalConf.onVeryLargeDatabases(1983),pages74–81.
[BuckleyandSilberschatz1984] G. Buckley and A. Silberschatz, “Concurrency
Control in Graph Protocols by Using Edge Locks”,InProc. of the ACM SIGMOD
Conf.onManagementofData(1984),pages45–50.
[BuckleyandSilberschatz1985] G. Buckley and A. Silberschatz, “Beyond Two-
PhaseLocking”,JournaloftheACM,Volume32,Number2(1985),pages314–326.
Bibliography 1289
[Bulmer1979] M.G.Bulmer,PrinciplesofStatistics,DoverPublications(1979).
[Burkhard1976] W. A. Burkhard, “HashingandTrieAlgorithmsforPartialMatch
Retrieval”, ACM Transactions on Database Systems, Volume 1, Number 2 (1976),
pages175–187.
[Burkhard1979] W. A. Burkhard, “Partial-matchHashCoding:Bene?tsofRedun-
dancy”, ACM Transactions on Database Systems,Volume4,Number2(1979), pages
228–239.
[CannanandOtten1993] S. Cannan and G. Otten, SQL—The Standard Handbook,
McGrawHill(1993).
[Carey1983] M. J. Carey, “Granularity Hierarchies in Concurrency Control”,In
Proc.oftheACMSIGMODConf.onManagementofData(1983),pages156–165.
[CareyandKossmann1998] M.J.CareyandD.Kossmann,“ReducingtheBraking
Distance of an SQL Query Engine”,InProc. of the International Conf. on Very Large
Databases(1998),pages158–169.
[Careyetal.1991] M.Carey,M.Franklin,M.Livny,andE.Shekita, “DataCaching
TradeoffsinClient-ServerDBMSArchitectures”,InProc.oftheACMSIGMODConf.
onManagementofData(1991).
[Careyetal.1993] M.J.Carey,D.DeWitt,andJ.Naughton,“TheOO7Benchmark”,
InProc.oftheACMSIGMODConf.onManagementofData(1993).
[Careyetal.1999] M.J.Carey,D.D.Chamberlin,S.Narayanan,B.Vance,D.Doole,
S.Rielau,R.Swagerman,andN.Mattos,“O-O, What Have They Done to DB2?”,
InProc.oftheInternationalConf.onVeryLargeDatabases(1999),pages542–553.
[Cattell2000] R. Cattell, editor, The Object Database Standard: ODMG 3.0,Morgan
Kaufmann(2000).
[CattellandSkeen1992] R.CattellandJ.Skeen, “ObjectOperationsBenchmark”,
ACMTransactionsonDatabaseSystems,Volume17,Number1(1992).
[Chakrabarti1999] S. Chakrabarti, “Recent Results in Automatic Web Resource
Discovery”,ACMComputingSurveys,Volume31,Number4(1999).
[Chakrabarti2000] S. Chakrabarti, “Data Mining for Hypertext: A Tutorial Sur-
vey”,SIGKDDExplorations,Volume1,Number2(2000),pages1–11.
[Chakrabarti2002] S.Chakrabarti,MiningtheWeb:DiscoveringKnowledgefromHy-
perTextData,MorganKaufmann(2002).
[Chakrabartietal.1998] S. Chakrabarti, S. Sarawagi, and B. Dom, “Mining Sur-
prising Patterns Using Temporal Description Length”,InProc. of the International
Conf.onVeryLargeDatabases(1998),pages606–617.
[Chakrabartietal.1999] S. Chakrabarti, M. van den Berg, and B. Dom, “Focused
Crawling:ANewApproachtoTopicSpeci?cWebResourceDiscovery”,InProc.of
theInternationalWorldWideWebConf.(1999).
1290 Bibliography
[Chamberlin1996] D. Chamberlin, Using the New DB2: IBM’s Object-Relational
DatabaseSystem,MorganKaufmann(1996).
[Chamberlin1998] D.D.Chamberlin,ACompleteGuidetoDB2UniversalDatabase,
MorganKaufmann(1998).
[ChamberlinandBoyce1974] D. D. Chamberlin and R. F. Boyce, “SEQUEL: A
StructuredEnglishQueryLanguage”,InACMSIGMODWorkshoponDataDescrip-
tion,Access,andControl(1974),pages249–264.
[Chamberlinetal.1976] D. D. Chamberlin, M. M. Astrahan, K. P. Eswaran, P. P.
Grif?ths,R.A.Lorie,J.W.Mehl,P.Reisner,andB.W.Wade,“SEQUEL2:AUni?ed
ApproachtoDataDe?nition,Manipulation,andControl”,IBM Journal of Research
andDevelopment,Volume20,Number6(1976),pages560–575.
[Chamberlinetal.1981] D. D. Chamberlin, M. M. Astrahan, M. W. Blasgen, J. N.
Gray,W.F.King,B.G.Lindsay,R.A.Lorie,J.W.Mehl,T.G.Price,P.G.Selinger,
M.Schkolnick,D.R.Slutz,I.L.Traiger,B.W.Wade,andR.A.Yost,“AHistory
andEvaluationofSystemR”, Communications of the ACM,Volume24,Number10
(1981),pages632–646.
[Chamberlinetal.2000] D. D. Chamberlin, J. Robie, and D. Florescu, “Quilt: An
XMLQueryLanguageforHeterogeneousDataSources”,InProc.oftheInternational
WorkshopontheWebandDatabases(WebDB)(2000),pages53–62.
[ChanandIoannidis1998] C.-Y.Chan and Y. E. Ioannidis, “Bitmap Index Design
andEvaluation”,InProc.oftheACMSIGMODConf.onManagementofData(1998).
[ChanandIoannidis1999] C.-Y. Chan and Y. E. Ioannidis, “An Ef?cient Bitmap
Encoding Scheme for Selection Queries”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(1999).
[ChandraandHarel1982] A.K.ChandraandD.Harel,“StructureandComplexity
ofRelationalQueries”,JournalofComputerandSystemSciences,Volume15,Number
10(1982),pages99–128.
[Chandrasekaranetal.2003] S. Chandrasekaran, O. Cooper,A. Deshpande, M. J.
Franklin, J. M. Hellerstein, W. Hong, S. Krishnamurthy, S. Madden, V. Raman,
F. Reiss, and M. Shah, “TelegraphCQ: Continuous Data?ow Processing for an
Uncertain World”,InFirst Biennial Conference on Innovative Data Systems Research
(2003).
[Changetal.2008] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach,
M.Burrows,T.Chandra,A.Fikes,andR.E.Gruber,“Bigtable:ADistributedStorage
System for Structured Data”, ACM Trans. Comput. Syst., Volume 26, Number 2
(2008).
[ChatziantoniouandRoss1997] D. Chatziantoniou and K. A. Ross, “Groupwise
Processing of Relational Queries”,InProc. of the International Conf. on Very Large
Databases(1997),pages476–485.
Bibliography 1291
[ChaudhuriandNarasayya1997] S. Chaudhuri and V. Narasayya, “An Ef?cient
Cost-DrivenIndexSelectionToolforMicrosoftSQLServer”,InProc.oftheInterna-
tionalConf.onVeryLargeDatabases(1997).
[ChaudhuriandShim1994] S. Chaudhuri and K. Shim, “Including Group-By in
QueryOptimization”,InProc.oftheInternationalConf.onVeryLargeDatabases(1994).
[Chaudhurietal.1995] S. Chaudhuri, R. Krishnamurthy, S. Potamianos, and
K. Shim, “Optimizing Queries with Materialized Views”,InProc. of the Interna-
tionalConf.onDataEngineering(1995).
[Chaudhurietal.1998] S. Chaudhuri, R. Motwani, and V. Narasayya, “Random
sampling for histogram construction:how much is enough?”,InProc. of the ACM
SIGMODConf.onManagementofData(1998),pages436–447.
[Chaudhurietal.1999] S. Chaudhuri, E. Christensen, G. Graefe, V. Narasayya,
and M. Zwilling, “Self Tuning Technology in Microsoft SQL Server”, IEEE Data
EngineeringBulletin,Volume22,Number2(1999).
[Chaudhurietal.2003] S.Chaudhuri,K.Ganjam,V.Ganti,andR.Motwani, “Ro-
bust and Ef?cient Fuzzy Match for Online Data Cleaning”,InProc. of the ACM
SIGMODConf.onManagementofData(2003).
[Chen1976] P. P. Chen, “The Entity-Relationship Model: Toward a Uni?ed View
ofData”,ACMTransactionsonDatabaseSystems,Volume1,Number1(1976),pages
9–36.
[Chenetal.1994] P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Pat-
terson, “RAID: High-Performance, Reliable Secondary Storage”, ACM Computing
Survey,Volume26,Number2(1994).
[Chenetal.2007] S.Chen,A.Ailamaki,P.B.Gibbons,andT.C.Mowry,“Improving
hashjoinperformancethroughprefetching”,ACMTransactionsonDatabaseSystems,
Volume32,Number3(2007).
[Chomicki1995] J. Chomicki, “Ef?cient Checking of Temporal Integrity Con-
straintsUsingBoundedHistoryEncoding”,ACMTransactionsonDatabaseSystems,
Volume20,Number2(1995),pages149–186.
[ChouandDewitt1985] H. T. Chou and D. J. Dewitt, “An Evaluation of Buffer
Management Strategies for Relational Database Systems”,InProc. of the Interna-
tionalConf.onVeryLargeDatabases(1985),pages127–141.
[Cieslewiczetal.2009] J.Cieslewicz,W.Mee,andK.A.Ross,“Cache-Conscious
Buffering for Database Operatorswith State”,InProc. Fifth International Workshop
onDataManagementonNewHardware(DaMoN2009)(2009).
[Cochraneetal.1996] R. Cochrane, H. Pirahesh, and N. M. Mattos, “Integrating
Triggers and Declarative Constraints in SQL Database Sytems”,InProc. of the
InternationalConf.onVeryLargeDatabases(1996),pages567–578.
1292 Bibliography
[Codd1970] E.F.Codd, “ARelationalModelforLargeSharedDataBanks”, Com-
municationsoftheACM,Volume13,Number6(1970),pages377–387.
[Codd1972] E. F. Codd. “Further Normalization of the Data Base Relational
Model”,InRustin[1972],pages33–64(1972).
[Codd1979] E. F. Codd, “Extending the Database Relational Model to Capture
MoreMeaning”,ACMTransactionsonDatabaseSystems,Volume4,Number4(1979),
pages397–434.
[Codd1982] E. F. Codd, “The 1981 ACM Turing Award Lecture: Relational
Database: A Practical Foundation for Productivity”, Communications of the ACM,
Volume25,Number2(1982),pages109–117.
[Codd1990] E. F. Codd, The Relational Model for Database Management: Version 2,
AddisonWesley(1990).
[Comer1979] D.Comer,“TheUbiquitousB-tree”,ACMComputingSurvey,Volume
11,Number2(1979),pages121–137.
[Comer2009] D. E. Comer, Computer Networks and Internets, 5th edition, Prentice
Hall(2009).
[Cook1996] M.A.Cook,Building EnterpriseInformation Architecture:Reengineering
InformationSystems,PrenticeHall(1996).
[Cooperetal.2008] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein,
P.Bohannon,H.-A.Jacobsen,N.Puz,D.Weaver,andR.Yerneni,“PNUTS:Yahoo!’s
hosted data serving platform”, Proceedings of the VLDB Endowment,V olume1,
Number2(2008),pages1277–1288.
[Cormenetal.1990] T. Cormen, C. Leiserson, and R. Rivest, Introduction to Algo-
rithms,MITPress(1990).
[CortesandVapnik1995] C. Cortes and V. Vapnik, Machine Learning, Volume 20,
Number3(1995),pages273–297.
[CristianiniandShawe-Taylor2000] N.CristianiniandJ.Shawe-Taylor,AnIntro-
duction to Support Vector Machines and other Kernel-Based Learning Methods,Cam-
bridgeUniversityPress(2000).
[DagevilleandZa¨ ?t2002] B. Dageville and M. Za¨ ?t, “SQL Memory Management
in Oracle9i”,InProc. of the International Conf. on Very Large Databases(2002), pages
962–973.
[Dagevilleetal.2004] B.Dageville,D.Das,K.Dias,K.Yagoub,M.Za¨ ?t,andM.Zi-
auddin, “AutomaticSQLTuninginOracle10g”,InProc.oftheInternationalConf.on
VeryLargeDatabases(2004),pages1098–1109.
[Dalvietal.2009] N. Dalvi, R. Kumar, B. Pang, R. Ramakrishnan, A. Tomkins,
P.Bohannon,S.Keerthi,andS.Merugu, “AWebofConcepts”,InProc.oftheACM
SymposiumonPrinciplesofDatabaseSystems(2009).
Bibliography 1293
[Danielsetal.1982] D.Daniels,P.G.Selinger,L.M.Haas,B.G.Lindsay,C.Mohan,
A.Walker,andP.F.Wilms. “AnIntroductiontoDistributedQueryCompilationin
R*”,InSchneider[1982](1982).
[Dashtietal.2003] A.Dashti,S.H.Kim,C.Shahabi,andR.Zimmermann,Stream-
ingMediaServerDesign,PrenticeHall(2003).
[Date1983] C. J. Date, “The Outer Join”,InProc. of the International Conference on
Databases,JohnWileyandSons(1983),pages76–106.
[Date1989] C.Date,AGuidetoDB2,AddisonWesley(1989).
[Date1993] C. J. Date, “How SQL Missed the Boat”, Database Programming and
Design,Volume6,Number9(1993).
[Date2003] C. J. Date, An Introduction to Database Systems, 8th edition, Addison
Wesley(2003).
[DateandDarwen1997] C. J. Date and G. Darwen, A Guide to the SQL Standard,
4thedition,AddisonWesley(1997).
[Davisetal.1983] C. Davis, S. Jajodia, P. A. Ng, and R. Yeh, editors, Entity-
RelationshipApproachtoSoftwareEngineering,NorthHolland(1983).
[DavisonandGraefe1994] D. L. Davison and G. Graefe, “Memory-Contention
Responsive Hash Joins”,InProc. of the International Conf. on Very Large Databases
(1994).
[Dayal1987] U. Dayal, “Of Nests and Trees: A Uni?ed Approach to Processing
Queriesthat Contain Nested Subqueries, Aggregates and Quanti?ers”,InProc. of
theInternationalConf.onVeryLargeDatabases(1987),pages197–208.
[Deutschetal.1999] A.Deutsch,M.Fernandez,D.Florescu,A.Levy,andD.Suciu,
“A Query Language for XML”,InProc. of the International World Wide Web Conf.
(1999).
[DeWitt1990] D. DeWitt, “The Gamma Database Machine Project”, IEEE Transac-
tionsonKnowledgeandDataEngineering,Volume2,Number1(1990).
[DeWittandGray1992] D. DeWitt and J. Gray, “Parallel Database Systems: The
FutureofHigh PerformanceDatabaseSystems”, Communications of the ACM,Vol-
ume35,Number6(1992),pages85–98.
[DeWittetal.1992] D.DeWitt,J.Naughton,D.Schneider,andS.Seshadri,“Practi-
calSkewHandlinginParallelJoins”,InProc.oftheInternationalConf.onVeryLarge
Databases(1992).
[Diasetal.1989] D.Dias,B.Iyer,J.Robinson,andP.Yu, “IntegratedConcurrency-
CoherencyControlsforMultisystem DataSharing”, Software Engineering,Volume
15,Number4(1989),pages437–448.
[DonahooandSpeegle2005] M.J.DonahooandG.D.Speegle,SQL:PracticalGuide
forDevelopers,MorganKaufmann(2005).
1294 Bibliography
[DouglasandDouglas2003] K. Douglas and S. Douglas, PostgreSQL,Sam’sPub-
lishing(2003).
[DuboisandThakkar1992] M. Dubois and S. Thakkar, editors, Scalable Shared
MemoryMultiprocessors,KluwerAcademicPublishers(1992).
[Duncan1990] R. Duncan, “A Survey of Parallel Computer Architectures”, IEEE
Computer,Volume23,Number2(1990),pages5–16.
[EisenbergandMelton1999] A. Eisenberg and J. Melton, “SQL:1999, formerly
knownasSQL3”,ACMSIGMODRecord,Volume28,Number1(1999).
[EisenbergandMelton2004a] A. Eisenberg and J. Melton, “Advancements in
SQL/XML”,ACMSIGMODRecord,Volume33,Number3(2004),pages79–86.
[EisenbergandMelton2004b] A. Eisenberg and J. Melton, “An Early Look at
XQueryAPIforJava(XQJ)”, ACMSIGMOD Record,Volume33,Number2(2004),
pages105–111.
[Eisenbergetal.2004] A. Eisenberg, J. Melton, K. G. Kulkarni, J.-E. Michels, and
F. Zemke, “SQL:2003 Has Been Published”, ACM SIGMOD Record, Volume 33,
Number1(2004),pages119–126.
[Elhemalietal.2007] M.Elhemali, C.A.Galindo-Legaria, T.Grabs,andM.Joshi,
“Execution strategies for SQL subqueries”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(2007),pages993–1004.
[Ellis1987] C. S. Ellis, “Concurrency in Linear Hashing”, ACM Transactions on
DatabaseSystems,Volume12,Number2(1987),pages195–217.
[ElmasriandNavathe2006] R.ElmasriandS.B.Navathe,FundamentalsofDatabase
Systems,5thedition,AddisonWesley(2006).
[Epsteinetal.1978] R. Epstein, M. R. Stonebraker, and E. Wong, “Distributed
QueryProcessingina RelationalDatabaseSystem”,InProc. of the ACM SIGMOD
Conf.onManagementofData(1978),pages169–180.
[Escobar-Molanoetal.1993] M. Escobar-Molano, R. Hull, and D. Jacobs, “Safety
and Translation of Calculus Queries with Scalar Functions”,InProc. of the ACM
SIGMODConf.onManagementofData(1993),pages253–264.
[Eswaranetal.1976] K. P. Eswaran, J. N. Gray,R. A. Lorie,and I. L. Traiger, “The
NotionsofConsistencyandPredicateLocksinaDatabaseSystem”,Communications
oftheACM,Volume19,Number11(1976),pages624–633.
[Fagin1977] R. Fagin, “Multivalued Dependencies and a New Normal Form for
RelationalDatabases”,ACMTransactionsonDatabaseSystems,Volume2,Number3
(1977),pages262–278.
[Fagin1979] R.Fagin,“NormalFormsandRelationalDatabaseOperators”,InProc.
oftheACMSIGMODConf.onManagementofData(1979),pages153–160.
Bibliography 1295
[Fagin1981] R.Fagin, “ANormalFormforRelationalDatabasesThatIsBasedon
Domains and Keys”, ACM Transactions on Database Systems,Volume6,Number3
(1981),pages387–415.
[Faginetal.1979] R. Fagin, J. Nievergelt, N. Pippenger, and H. R. Strong, “Ex-
tendible Hashing — A Fast Access MethodforDynamic Files”, ACM Transactions
onDatabaseSystems,Volume4,Number3(1979),pages315–344.
[FaloutsosandLin1995] C. Faloutsos and K.-I. Lin, “Fast Map: A Fast Algo-
rithmforIndexing,Data-MiningandVisualizationofTraditionalandMultimedia
Datasets”,InProc. of theACM SIGMOD Conf. on Managementof Data(1995),pages
163–174.
[Fayyadetal.1995] U.Fayyad,G.Piatetsky-Shapiro,P.Smyth,andR.Uthurusamy,
AdvancesinKnowledgeDiscoveryandDataMining,MITPress(1995).
[Feketeetal.2005] A. Fekete, D. Liarokapis, E. O’Neil, P. O’Neil, and D. Shasha,
“Making Snapshot Isolation Serializable”, ACM Transactions on Database Systems,
Volume30,Number2(2005).
[FinkelandBentley1974] R.A.FinkelandJ.L.Bentley,“QuadTrees:ADataStruc-
ture for Retrieval on Composite Keys”, Acta Informatica, Volume 4, (1974), pages
1–9.
[Fischer2006] L.Fischer,editor,Work?owHandbook2001,FutureStrategies(2006).
[FlorescuandKossmann1999] D.FlorescuandD.Kossmann,“StoringandQuery-
ing XML Data Using an RDBMS”, IEEE Data Engineering Bulletin (Special Issue on
XML)(1999),pages27–35.
[Florescuetal.2000] D. Florescu, D. Kossmann, and I. Monalescu, “Integrating
Keyword Search into XML Query Processing”,InProc. of the International World
Wide Web Conf.(2000),pages119–135. AlsoappearsinComputerNetworks,Vol.33,
pages119-135.
[Fredkin1960] E. Fredkin, “Trie Memory”, Communications of the ACM,Volume4,
Number2(1960),pages490–499.
[FreedmanandDeWitt1995] C. S. Freedman and D. J. DeWitt, “The SPIFFI Scal-
ableVideo-on-DemandServer”,InProc.oftheACMSIGMODConf.onManagement
ofData(1995),pages352–363.
[Funderburketal.2002a] J. E. Funderburk, G. Kiernan, J. Shanmugasundaram,
E. Shekita, and C. Wei, “XTABLES: Bridging Relational Technology and XML”,
IBMSystemsJournal,Volume41,Number4(2002),pages616–641.
[Funderburketal.2002b] J. E. Funderburk, S. Malaika, and B. Reinwald, “XML
Programming with SQL/XML and XQuery”, IBM Systems Journal, Volume 41,
Number4(2002),pages642–665.
[Galindo-Legaria1994] C.Galindo-Legaria, “OuterjoinsasDisjunctions”,InProc.
oftheACMSIGMODConf.onManagementofData(1994).
1296 Bibliography
[Galindo-LegariaandJoshi2001] C.A.Galindo-LegariaandM.M.Joshi,“Orthog-
onal Optimizationof SubqueriesandAggregation”,InProc. of the ACM SIGMOD
Conf.onManagementofData(2001).
[Galindo-LegariaandRosenthal1992] C. Galindo-Legaria and A. Rosenthal,
“HowtoExtendaConventionalOptimizertoHandleOne-andTwo-SidedOuter-
join”,InProc.oftheInternationalConf.onDataEngineering(1992),pages402–409.
[Galindo-Legariaetal.2004] C. Galindo-Legaria, S. Stefani, and F. Waas, “Query
ProcessingforSQLUpdates”,InProc.oftheACMSIGMODConf.onManagementof
Data(2004),pages844–849.
[Ganguly1998] S.Ganguly, “DesignandAnalysisofParametricQueryOptimiza-
tionAlgorithms”,InProc.oftheInternationalConf.onVeryLargeDatabases(1998).
[Gangulyetal.1992] S. Ganguly,W. Hasan, andR. Krishnamurthy, “QueryOpti-
mizationforParallelExecution”,InProc.oftheACMSIGMODConf.onManagement
ofData(1992).
[Gangulyetal.1996] S. Ganguly, P. Gibbons, Y. Matias, and A. Silberschatz, “A
SamplingAlgorithmforEstimating JoinSize”,InProc. of the ACM SIGMOD Conf.
onManagementofData(1996).
[GanskiandWong1987] R.A.GanskiandH.K.T.Wong,“OptimizationofNested
SQLQueriesRevisited”,InProc.oftheACMSIGMODConf.onManagementofData
(1987).
[GarciaandKorth2005] P .GarciaandH.F.Korth,“Multithreaded Architectures
and the Sort Benchmark”,InProc. of the First International Workshop on Data Man-
agementonModernHardward(DaMoN)(2005).
[Garcia-Molina1982] H.Garcia-Molina,“ElectionsinDistributedComputingSys-
tems”,IEEETransactionsonComputers,VolumeC-31,Number1(1982),pages48–59.
[Garcia-MolinaandSalem1987] H.Garcia-MolinaandK.Salem,“Sagas”,InProc.
oftheACMSIGMODConf.onManagementofData(1987),pages249–259.
[Garcia-MolinaandSalem1992] H.Garcia-MolinaandK.Salem, “MainMemory
Database Systems: An Overview”, IEEE Transactions on Knowledge and Data Engi-
neering,Volume4,Number6(1992),pages509–516.
[Garcia-Molinaetal.2008] H. Garcia-Molina, J. D. Ullman, and J. D. Widom,
DatabaseSystems:TheCompleteBook,2ndedition,PrenticeHall(2008).
[Georgakopoulosetal.1994] D. Georgakopoulos, M. Rusinkiewicz, and A. Seth,
“Using Tickets to Enforcethe Serializability of Multidatabase Transactions”, IEEE
TransactionsonKnowledgeandDataEngineering,Volume6,Number1(1994),pages
166–180.
[GilbertandLynch2002] S. Gilbert and N. Lynch, “Brewer’s conjecture and the
feasibilityofconsistent,available,partition-tolerantwebservices”,SIGACTNews,
Volume33,Number2(2002),pages51–59.
Bibliography 1297
[Graefe1990] G.Graefe, “Encapsulation of Parallelism in theVolcanoQueryPro-
cessingSystem”,InProc.oftheACMSIGMODConf. onManagementofData(1990),
pages102–111.
[Graefe1995] G.Graefe,“TheCascadesFrameworkforQueryOptimization”,Data
EngineeringBulletin,Volume18,Number3(1995),pages19–29.
[Graefe2008] G. Graefe, “The Five-Minute Rule 20 Years Later: and How Flash
Memory Changes the Rules”, ACM Queue, Volume 6, Number 4 (2008), pages
40–52.
[GraefeandMcKenna1993a] G.GraefeandW.McKenna,“TheVolcanoOptimizer
Generator”,InProc.oftheInternationalConf.onDataEngineering(1993),pages209–
218.
[GraefeandMcKenna1993b] G. Graefe and W. J. McKenna, “Extensibility and
SearchEf?ciencyintheVolcanoOptimizerGenerator”,InProc. of the International
Conf.onDataEngineering(1993).
[Graefeetal.1998] G. Graefe, R. Bunker, and S. Cooper, “Hash Joins and Hash
Teams in Microsoft SQL Server”,InProc. of the International Conf. on Very Large
Databases(1998),pages86–97.
[Gray1978] J.Gray. “NotesonDataBaseOperatingSystem”,InBayeretal.[1978],
pages393–481(1978).
[Gray1981] J. Gray, “The Transaction Concept: Virtues and Limitations”,InProc.
oftheInternationalConf.onVeryLargeDatabases(1981),pages144–154.
[Gray1991] J.Gray,TheBenchmarkHandbookforDatabaseandTransactionProcessing
Systems,2ndedition,MorganKaufmann(1991).
[GrayandGraefe1997] J. Gray and G. Graefe, “The Five-Minute Rule Ten Years
Later,andOtherComputerStorageRulesofThumb”,SIGMODRecord,Volume26,
Number4(1997),pages63–68.
[GrayandReuter1993] J.GrayandA.Reuter,Transaction Processing:Concepts and
Techniques,MorganKaufmann(1993).
[Grayetal.1975] J.Gray,R.A.Lorie,andG.R.Putzolu,“GranularityofLocksand
DegreesofConsistencyinaSharedDataBase”,InProc.oftheInternationalConf.on
VeryLargeDatabases(1975),pages428–451.
[Grayetal.1976] J.Gray,R.A.Lorie,G.R.Putzolu,andI.L.Traiger,Granularityof
LocksandDegreesofConsistencyinaSharedDataBase,Nijssen(1976).
[Grayetal.1981] J.Gray,P.R.McJones,andM.Blasgen,“TheRecoveryManager
oftheSystemRDatabaseManager”,ACMComputingSurvey,Volume13,Number
2(1981),pages223–242.
[Grayetal.1995] J.Gray,A. Bosworth,A.Layman,andH. Pirahesh, “DataCube:
A Relational Aggregation Operator Generalizing Group-By, Cross-Tab and Sub-
Totals”,Technicalreport,MicrosoftResearch(1995).
1298 Bibliography
[Grayetal.1996] J. Gray, P. Helland, and P. O’Neil, “The Dangers of Replication
anda Solution”,InProc. of the ACM SIGMOD Conf. on Management of Data (1996),
pages173–182.
[Grayetal.1997] J.Gray ,S.Chaudhuri,A.Bosworth,A.Layman,D.Reichart,
M. Venkatrao, F. Pellow, and H. Pirahesh, “Data Cube: A Relational Aggregation
Operator Generalizing Group-by, Cross-Tab, and Sub Totals”, Data Mining and
KnowledgeDiscovery,Volume1,Number1(1997),pages29–53.
[GregersenandJensen1999] H. Gregersen and C. S. Jensen, “Temporal Entity-
RelationshipModels-ASurvey”,IEEETransactionsonKnowledgeandDataEngineer-
ing,Volume11,Number3(1999),pages464–497.
[GrossmanandFrieder2004] D.A.GrossmanandO.Frieder,InformationRetrieval:
AlgorithmsandHeuristics,2ndedition,SpringerVerlag(2004).
[Gunning2008] P.K.Gunning,DB29forDevelopers,MCPress(2008).
[Guoetal.2003] L. Guo, F. Shao, C. Botev,and J. Shanmugasundaram, “XRANK:
Ranked Keyword Search over XML Documents”,InProc. of the ACM SIGMOD
Conf.onManagementofData(2003).
[Guttman1984] A. Guttman, “R-Trees: A Dynamic Index Structure for Spatial
Searching”,InProc.oftheACMSIGMODConf.onManagementofData(1984),pages
47–57.
[Haasetal.1989] L.M.Haas,J.C.Freytag,G.M.Lohman,andH.Pirahesh,“Ex-
tensible Query Processing in Starburst”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(1989),pages377–388.
[Haasetal.1990] L.M.Haas,W.Chang,G.M.Lohman,J.McPherson,P.F.Wilms,
G.Lapis,B.G.Lindsay,H.Pirahesh,M.J.Carey,andE.J.Shekita, “StarburstMid-
Flight: As the Dust Clears”, IEEE Transactions on Knowledge and Data Engineering,
Volume2,Number1(1990),pages143–160.
[HaerderandReuter1983] T. Haerder and A. Reuter, “Principles of Transaction-
Oriented Database Recovery”, ACM Computing Survey, Volume 15, Number 4
(1983),pages287–318.
[HaerderandRothermel1987] T.HaerderandK.Rothermel,“ConceptsforTrans-
action Recovery in Nested Transactions”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(1987),pages239–248.
[Halsall2006] F. Halsall, Computer Networking and the Internet : With Internet and
MultiamediaApplications,AddisonWesley(2006).
[HanandKamber2000] J. Han and M. Kamber, Data Mining: Concepts and Tech-
niques,MorganKaufmann(2000).
[Harinarayanetal.1996] V.Harinarayan,J.D.Ullman,andA.Rajaraman,“Imple-
mentingDataCubesEf?ciently”,InProc.oftheACMSIGMODConf.onManagement
ofData(1996).
Bibliography 1299
[Haritsaetal.1990] J.Haritsa,M.Carey,andM.Livny,“OnBeingOptimisticabout
Real-TimeConstraints”,InProc.oftheACMSIGMOD Conf.onManagementofData
(1990).
[HarizopoulosandAilamaki2004] S. Harizopoulos andA. Ailamaki, “STEPS to-
wardsCache-residentTransactionProcessing”,InProc.of the International Conf. on
VeryLargeDatabases(2004),pages660–671.
[HellersteinandStonebraker2005] J.M.HellersteinandM.Stonebraker,editors,
ReadingsinDatabaseSystems,4thedition,MorganKaufmann(2005).
[Hellersteinetal.1995] J.M.Hellerstein,J.F.Naughton,andA.Pfeffer,“General-
ized Search Trees for Database Systems”,InProc. of the International Conf. on Very
LargeDatabases(1995),pages562–573.
[Hennessyetal.2006] J.L.Hennessy,D.A.Patterson,andD.Goldberg,Computer
Architecture:AQuantitativeApproach,4thedition,MorganKaufmann(2006).
[HevnerandYao1979] A. R. Hevner and S. B. Yao, “Query Processing in Dis-
tributed Database Systems”, IEEE Transactions on Software Engineering,V olume
SE-5,Number3(1979),pages177–187.
[Heywoodetal.2002] I. Heywood, S. Cornelius,and S. Carver, An Introduction to
GeographicalInformationSystems,2ndedition,PrenticeHall(2002).
[Hongetal.1993] D.Hong,T.Johnson,andS.Chakravarthy, “Real-TimeTransac-
tionScheduling:ACostConsciousApproach”,InProc.oftheACMSIGMODConf.
onManagementofData(1993).
[Howesetal.1999] T.A.Howes,M.C.Smith,andG.S.Good,Understanding and
DeployingLDAPDirectoryServices,MacmillanPublishing(1999).
[HristidisandPapakonstantinou2002] V. Hristidis and Y. Papakonstantinou,
“DISCOVER:KeywordSearchinRelationalDatabases”,InProc.oftheInternational
Conf.onVeryLargeDatabases(2002).
[HuangandGarcia-Molina2001] Y. Huang and H. Garcia-Molina, “Exactly-once
Semanticsin a Replicated Messaging System”,InProc. of the International Conf. on
DataEngineering(2001),pages3–12.
[HulgeriandSudarshan2003] A. Hulgeri and S. Sudarshan, “AniPQO: Almost
Non-IntrusiveParametricQueryOptimizationforNon-LinearCostFunctions”,In
Proc.oftheInternationalConf.onVeryLargeDatabases(2003).
[IBM1987] IBM, “Systems Application Architecture: Common Programming In-
terface,DatabaseReference”,Technicalreport,IBMCorporation,IBMFormNum-
berSC26–4348–0(1987).
[Ilyasetal.2008] I.Ilyas,G.Beskales,andM.A.Soliman,“ASurveyoftop-kquery
processing techniques in relational database systems”, ACM Computing Surveys,
Volume40,Number4(2008).
1300 Bibliography
[ImielinskiandBadrinath1994] T.Imielinski andB. R.Badrinath, “Mobile Com-
puting—SolutionsandChallenges”,CommunicationsoftheACM,Volume37,Num-
ber10(1994).
[ImielinskiandKorth1996] T.ImielinskiandH.F.Korth,editors,MobileComput-
ing,KluwerAcademicPublishers(1996).
[IoannidisandChristodoulakis1993] Y.IoannidisandS.Christodoulakis, “Opti-
malHistogramsforLimitingWorst-CaseErrorPropagationintheSizeofJoinRe-
sults”, ACM Transactions on Database Systems, Volume 18, Number4 (1993), pages
709–748.
[IoannidisandPoosala1995] Y.E.IoannidisandV.Poosala,“BalancingHistogram
OptimalityandPracticalityforQueryResultSizeEstimation”,InProc.ofthe ACM
SIGMODConf.onManagementofData(1995),pages233–244.
[Ioannidisetal.1992] Y.E.Ioannidis,R.T.Ng,K.Shim,andT.K.Sellis,“Parametric
QueryOptimization”,InProc.oftheInternationalConf.onVeryLargeDatabases(1992),
pages103–114.
[JacksonandMoulinier2002] P. Jackson and I. Moulinier, Natural Language Pro-
cessing for Online Applications: Text Retrieval, Extraction, and Categorization,John
Benjamin(2002).
[Jagadishetal.1993] H. V. Jagadish, A. Silberschatz, and S. Sudarshan, “Recov-
ering from Main-Memory Lapses”,InProc. of the International Conf. on Very Large
Databases(1993).
[Jagadishetal.1994] H. Jagadish, D. Lieuwen, R. Rastogi, A. Silberschatz, and
S. Sudarshan, “Dali: A High Performance Main Memory Storage Manager”,In
Proc.oftheInternationalConf.onVeryLargeDatabases(1994).
[JainandDubes1988] A.K.JainandR.C.Dubes,Algorithms for Clustering Data,
PrenticeHall(1988).
[Jensenetal.1994] C.S.Jensenetal.,“AConsensusGlossaryofTemporalDatabase
Concepts”,ACMSIGMODRecord,Volume23,Number1(1994),pages52–64.
[Jensenetal.1996] C.S.Jensen,R.T.Snodgrass,andM.Soo,“ExtendingExisting
Dependency Theory to Temporal Databases”, IEEE Transactions on Knowledge and
DataEngineering,Volume8,Number4(1996),pages563–582.
[Johnson1999] T. Johnson, “Performance Measurements of Compressed Bitmap
Indices”,InProc.oftheInternationalConf.onVeryLargeDatabases(1999).
[JohnsonandShasha1993] T. Johnson and D. Shasha, “The Performanceof Con-
current B-Tree Algorithms”, ACM Transactions on Database Systems, Volume 18,
Number1(1993).
[JonesandWillet1997] K. S. Jones and P. Willet, editors, Readings in Information
Retrieval,MorganKaufmann(1997).
Bibliography 1301
[JordanandRussell2003] D. Jordan and C. Russell, Java Data Objects, O’Reilly
(2003).
[Jorwekaretal.2007] S.Jorwekar,A.Fekete,K.Ramamritham,andS.Sudarshan,
“AutomatingtheDetectionofSnapshotIsolationAnomalies”,InProc. of the Inter-
nationalConf.onVeryLargeDatabases(2007),pages1263–1274.
[Joshi1991] A. Joshi, “Adaptive Locking Strategies in a Multi-Node Shared Data
ModelEnvironment”,InProc.oftheInternationalConf.onVeryLargeDatabases(1991).
[KanneandMoerkotte2000] C.-C.KanneandG.Moerkotte, “Ef?cientStorageof
XMLData”,InProc.oftheInternationalConf.onDataEngineering(2000),page198.
[Katzetal.2004] H.Katz,D.Chamberlin,D.Draper,M.Fernandez,M.Kay,J.Ro-
bie,M.Rys,J.Simeon,J.Tivy,andP.Wadler,XQueryfromtheExperts:AGuidetothe
W3CXMLQueryLanguage,AddisonWesley(2004).
[Kaushiketal.2004] R. Kaushik, R. Krishnamurthy, J. F. Naughton, and R. Ra-
makrishnan, “OntheIntegrationofStructureIndexesandInvertedLists”,InProc.
oftheACMSIGMODConf.onManagementofData(2004).
[KedemandSilberschatz1979] Z. M. Kedem and A. Silberschatz, “Controlling
Concurrency Using Locking Protocols”,InProc. of the Annual IEEE Symposium on
FoundationsofComputerScience(1979),pages275–285.
[KedemandSilberschatz1983] Z. M. Kedem and A. Silberschatz, “Locking Pro-
tocols:FromExclusivetoSharedLocks”,JournaloftheACM,Volume30,Number4
(1983),pages787–804.
[Kiferetal.2005] M.Kifer,A.Bernstein,andP.Lewis, DatabaseSystems:AnAppli-
cationOrientedApproach,CompleteVersion,2ndedition,AddisonWesley(2005).
[Kim1982] W. Kim, “OnOptimizinganSQL-likeNestedQuery”,ACMTransactions
onDatabaseSystems,Volume3,Number3(1982),pages443–469.
[Kim1995] W. Kim, editor, ModernDatabaseSystems,ACMPress(1995).
[Kingetal.1991] R. P. King, N. Halim, H. Garcia-Molina, and C. Polyzois, “Man-
agement of a Remote Backup Copy for Disaster Recovery”, ACM Transactions on
DatabaseSystems,Volume16,Number2(1991),pages338–368.
[KitsuregawaandOgawa1990] M.KitsuregawaandY.Ogawa,“BucketSpreading
Parallel Hash: A New, Robust, Parallel Hash Join Method for Skew in the Super
DatabaseComputer”,InProc.oftheInternationalConf.onVeryLargeDatabases(1990),
pages210–221.
[Kleinberg1999] J. M. Kleinberg, “Authoritative Sources in a Hyperlinked Envi-
ronment”,JournaloftheACM,Volume46,Number5(1999),pages604–632.
[Kleinrock1975] L.Kleinrock,QueueingSystems,Wiley-Interscience(1975).
[Klug1982] A. Klug, “Equivalence of Relational Algebra and Relational Calculus
Query Languages Having Aggregate Functions”, Journal of the ACM, Volume 29,
Number3(1982),pages699–717.
1302 Bibliography
[Knapp1987] E. Knapp, “Deadlock Detection in Distributed Databases”, ACM
ComputingSurvey,Volume19,Number4(1987).
[Knuth1973] D. E. Knuth, The Art of Computer Programming, Volume 3, Addison
Wesley,SortingandSearching(1973).
[KohaviandProvost2001] R. Kohavi and F. Provost, editors, Applications of Data
MiningtoElectronicCommerce,KluwerAcademicPublishers(2001).
[Konstanetal.1997] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R.
Gordon, and J. Riedl, “GroupLens: Applying Collaborative Filtering to Usenet
News”,CommunicationsoftheACM,Volume40,Number3(1997),pages77–87.
[Korth1982] H.F.Korth,“DeadlockFreedomUsingEdgeLocks”,ACMTransactions
onDatabaseSystems,Volume7,Number4(1982),pages632–652.
[Korth1983] H.F.Korth, “LockingPrimitivesinaDatabaseSystem”,Journalofthe
ACM,Volume30,Number1(1983),pages55–79.
[KorthandSpeegle1990] H.F.KorthandG.Speegle,“LongDurationTransactions
in Software Design Projects”,InProc. of the International Conf. on Data Engineering
(1990),pages568–575.
[KorthandSpeegle1994] H.F.KorthandG.Speegle,“FormalAspectsofConcur-
rency Control in Long Duration Transaction Systems Using the NT/PV Model”,
ACMTransactionsonDatabaseSystems,Volume19,Number3(1994),pages492–535.
[Krishnaprasadetal.2004] M.Krishnaprasad,Z.Liu,A.Manikutty,J.W.Warner,
V.Arora,andS.Kotsovolos, “QueryRewriteforXMLinOracleXMLDB”,InProc.
oftheInternationalConf.onVeryLargeDatabases(2004),pages1122–1133.
[KungandLehman1980] H.T.KungandP.L.Lehman,“ConcurrentManipulation
ofBinarySearchTrees”,ACMTransactionsonDatabaseSystems,Volume5,Number
3(1980),pages339–353.
[KungandRobinson1981] H. T. Kung and J. T. Robinson, “Optimistic Concur-
rencyControl”,ACMTransactionsonDatabaseSystems,Volume6,Number2(1981),
pages312–326.
[KuroseandRoss2005] J.KuroseandK.Ross,ComputerNetworking—ATop-Down
ApproachFeaturingtheInternet,3rdedition,AddisonWesley(2005).
[Lahirietal.2001] T.Lahiri,A.Ganesh,R.Weiss,andA.Joshi,“Fast-Start: Quick
Fault Recovery in Oracle”,InProc. of the ACM SIGMOD Conf. on Management of
Data(2001).
[LamandKuo2001] K.-Y.LamandT.-W.Kuo,editors,Real-TimeDatabaseSystems,
KluwerAcademicPublishers(2001).
[Lambetal.1991] C.Lamb,G.Landis, J.Orenstein,andD.Weinreb, “TheObject-
StoreDatabaseSystem”,CommunicationsoftheACM,Volume34,Number10(1991),
pages51–63.
Bibliography 1303
[Lamport1978] L. Lamport, “Time, Clocks, and the Ordering of Events in a Dis-
tributedSystem”,CommunicationsoftheACM,Volume21,Number7(1978),pages
558–565.
[LampsonandSturgis1976] B. Lampson and H. Sturgis, “Crash Recovery in a
DistributedDataStorageSystem”,Technicalreport,ComputerScienceLaboratory,
XeroxPaloAltoResearchCenter,PaloAlto(1976).
[Lecluseetal.1988] C. Lecluse, P. Richard, and F. Velez, “O2: An Object-Oriented
DataModel”,InProc.oftheInternationalConf.onVeryLargeDatabases(1988),pages
424–433.
[LehmanandYao1981] P. L. Lehman and S. B. Yao, “Ef?cient Locking for Con-
current Operations on B-trees”, ACM Transactions on Database Systems,Volume6,
Number4(1981),pages650–670.
[Lehneretal.2000] W. Lehner, R. Sidle, H. Pirahesh, and R. Cochrane, “Main-
tenance of Automatic Summary Tables”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(2000),pages512–513.
[Lindsayetal.1980] B.G.Lindsay,P.G.Selinger,C.Galtieri,J.N.Gray,R.A.Lorie,
T. G. Price, G. R. Putzolu, I. L. Traiger, and B. W. Wade. “Notes on Distributed
Databases”, In Draffen and Poole, editors, Distributed Data Bases, pages 247–284.
CambridgeUniversityPress(1980).
[Litwin1978] W. Litwin, “VirtualHashing:ADynamicallyChangingHashing”,In
Proc.oftheInternationalConf.onVeryLargeDatabases(1978),pages517–523.
[Litwin1980] W. Litwin, “LinearHashing:ANewToolforFileandTableAddress-
ing”,InProc.oftheInternationalConf.onVeryLargeDatabases(1980),pages212–223.
[Litwin1981] W. Litwin, “Trie Hashing”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(1981),pages19–29.
[LoandRavishankar1996] M.-L.LoandC.V.Ravishankar, “SpatialHash-Joins”,
InProc.oftheACMSIGMODConf.onManagementofData(1996).
[Loeb1998] L. Loeb, Secure Electronic Transactions: Introduction and Technical Refer-
ence,ArtechHouse(1998).
[Lomet1981] D.G.Lomet,“DigitalB-trees”,InProc.oftheInternationalConf.onVery
LargeDatabases(1981),pages333–344.
[Lometetal.2009] D.Lomet,A.Fekete,G.Weikum,andM.Zwilling,“Unbundling
TransactionServicesintheCloud”,InProc.4thBiennialConferenceonInnovativeData
SystemsResearch(2009).
[Luetal.1991] H.Lu,M.Shan,andK.T an,“Optimization of Multi-Way Join
Queries for Parallel Execution”,InProc. of the International Conf. on Very Large
Databases(1991),pages549–560.
[LynchandMerritt1986] N.A.LynchandM.Merritt, “IntroductiontotheTheory
ofNestedTransactions”,InProc.oftheInternationalConf.onDatabaseTheory(1986).
1304 Bibliography
[Lynchetal.1988] N.A.Lynch,M.Merritt,W.Weihl,andA.Fekete, “ATheoryof
Atomic Transactions”,InProc. of the International Conf. on Database Theory (1988),
pages41–71.
[Maier1983] D. Maier, The Theory of Relational Databases, Computer Science Press
(1983).
[Manningetal.2008] C.D.Manning,P.Raghavan,andH.Sch¨ utze,Introductionto
InformationRetrieval,CambridgeUniversityPress(2008).
[Martinetal.1989] J.Martin,K.K.Chapman,andJ.Leben,DB2,Concepts,Design,
andProgramming,PrenticeHall(1989).
[Mattison1996] R. Mattison, Data Warehousing: Strategies, Technologies, and Tech-
niques,McGrawHill(1996).
[McHughandWidom1999] J. McHugh and J. Widom, “Query Optimization for
XML”,InProc.oftheInternationalConf.onVeryLargeDatabases(1999),pages315–326.
[Mehrotraetal.1991] S. Mehrotra, R. Rastogi, H. F. Korth, and A. Silberschatz,
“Non-SerializableExecutionsinHeterogeneousDistributedDatabaseSystems”,In
Proc.oftheInternationalConf.onParallelandDistributedInformationSystems(1991).
[Mehrotraetal.2001] S.Mehrotra,R.Rastogi,Y.Breitbart,H.F.Korth,andA.Sil-
berschatz, “Overcoming Heterogeneity and Autonomy in Multidatabase Sys-
tems.”,Inf.Comput.,Volume167,Number2(2001),pages137–172.
[Melniketal.2007] S. Melnik, A. Adya, and P. A. Bernstein, “Compiling map-
pingstobridgeapplicationsanddatabases”,InProc.oftheACMSIGMODConf.on
ManagementofData(2007),pages461–472.
[Melton2002] J. Melton, Advanced SQL:1999 – Understanding Object-Relational and
OtherAdvancedFeatures,MorganKaufmann(2002).
[MeltonandEisenberg2000] J. Melton and A. Eisenberg, Understanding SQL and
Java Together : A Guide to SQLJ, JDBC, and Related Technologies, Morgan Kaufmann
(2000).
[MeltonandSimon1993] J.MeltonandA.R.Simon,UnderstandingTheNew SQL:
ACompleteGuide,MorganKaufmann(1993).
[MeltonandSimon2001] J.MeltonandA.R.Simon,SQL:1999,UnderstandingRe-
lationalLanguageComponents,MorganKaufmann(2001).
[Microsoft1997] Microsoft, Microsoft ODBC 3.0 Software Development Kit and Pro-
grammer’sReference,MicrosoftPress(1997).
[Mistryetal.2001] H.Mistry,P.Roy,S.Sudarshan,andK.Ramamritham,“Materi-
alizedViewSelectionandMaintenanceUsingMulti-QueryOptimization”,InProc.
oftheACMSIGMODConf.onManagementofData(2001).
[Mitchell1997] T.M.Mitchell,MachineLearning,McGrawHill(1997).
Bibliography 1305
[Mohan1990a] C. Mohan, “ARIES/KVL: A Key-Value Locking Method for Con-
currency Control of Multiaction Transactions Operations on B-Tree indexes”,In
Proc.oftheInternationalConf.onVeryLargeDatabases(1990),pages392–405.
[Mohan1990b] C. Mohan, “Commit-LSN: A Novel and Simple Method for Re-
ducing Locking and Latching in Transaction Processing Systems”,InProc. of the
InternationalConf.onVeryLargeDatabases(1990),pages406–418.
[Mohan1993] C.Mohan, “IBM’sRelationalDatabaseProducts:FeaturesandTech-
nologies”,InProc.oftheACMSIGMODConf.onManagementofData(1993).
[MohanandLevine1992] C. Mohan and F. Levine, “ARIES/IM:An Ef?cient and
High-Concurrency Index Management Method Using Write-Ahead Logging”,In
Proc.oftheACMSIGMODConf.onManagementofData(1992).
[MohanandLindsay1983] C.MohanandB.Lindsay,“Ef?cientCommitProtocols
for the Tree of Processes Model of Distributed Transactions”,InProc. of the ACM
SymposiumonPrinciplesofDistributedComputing(1983).
[MohanandNarang1992] C. Mohan and I. Narang, “Ef?cient Locking and
Caching of Data in the Multisystem Shared Disks Transaction Environment”,In
Proc.oftheInternationalConf.onExtendingDatabaseTechnology(1992).
[MohanandNarang1994] C.MohanandI.Narang, “ARIES/CSA:A Methodfor
Database Recovery in Client-Server Architectures”,InProc. of the ACM SIGMOD
Conf.onManagementofData(1994),pages55–66.
[Mohanetal.1986] C. Mohan, B. Lindsay, and R. Obermarck, “Transaction Man-
agement in the R* Distributed Database Management System”, ACM Transactions
onDatabaseSystems,Volume11,Number4(1986),pages378–396.
[Mohanetal.1992] C.Mohan,D.Haderle,B.Lindsay,H.Pirahesh,andP.Schwarz,
“ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking
andPartialRollbacksUsingWrite-AheadLogging”,ACMTransactionsonDatabase
Systems,Volume17,Number1(1992).
[Moss1985] J. E. B. Moss, Nested Transactions: An Approach to Reliable Distributed
Computing,MITPress(1985).
[Moss1987] J.E.B.Moss, “Log-BasedRecoveryforNestedTransactions”,InProc.
oftheInternationalConf.onVeryLargeDatabases(1987),pages427–432.
[MurthyandBanerjee2003] R.MurthyandS.Banerjee, “XMLSchemasinOracle
XML DB”,InProc. of the International Conf. on Very Large Databases (2003), pages
1009–1018.
[Nakayamaetal.1984] T.Nakayama,M.Hirakawa,andT.Ichikawa,“Architecture
andAlgorithmforParallelExecutionofaJoinOperation”,InProc.oftheInternational
Conf.onDataEngineering(1984).
[NgandHan1994] R.T.NgandJ.Han,“Ef?cientandEffectiveClusteringMethods
for Spatial Data Mining”,InProc. of the International Conf. on Very Large Databases
(1994).
1306 Bibliography
[NIST1993] NIST, “Integration De?nition for Information Modeling (IDEF1X)”,
Technical Report Federal Information Processing Standards Publication
184, National Institute of Standards and Technology (NIST), Available at
www.idef.com/Downloads/pdf/Idef1x.pdf(1993).
[Nybergetal.1995] C.Nyberg,T.Barclay,Z.Cvetanovic,J.Gray,andD.B.Lomet,
“AlphaSort: A Cache-Sensitive Parallel External Sort”, VLDB Journal,V olume4,
Number4(1995),pages603–627.
[O’NeilandO’Neil2000] P. O’Neil and E. O’Neil, Database: Principles, Program-
ming,Performance,2ndedition,MorganKaufmann(2000).
[O’NeilandQuass1997] P. O’Neil and D. Quass, “ImprovedQuery Performance
with Variant Indexes”,InProc. of the ACM SIGMOD Conf. on Management of Data
(1997).
[O’Neiletal.2004] P.E.O’Neil,E.J.O’Neil,S.Pal,I.Cseri,G.Schaller,andN.West-
bury, “ORDPATHs: Insert-Friendly XML Node Labels”,InProc. of the ACM SIG-
MODConf.onManagementofData(2004),pages903–908.
[OoiandS.Parthasarathy2009] B. C. Ooi and e. S. Parthasarathy, “Special Issue
onDataManagementonCloudComputingPlatforms”, Data Engineering Bulletin,
Volume32,Number1(2009).
[Orenstein1982] J. A. Orenstein, “Multidimensional Tries Used for Associative
Searching”,InformationProcessingLetters,Volume14,Number4(1982),pages150–
157.
[Ozcanetal.1997] F.Ozcan,S.Nural,P.Koksal,C.Evrendilek,andA.Dogac,“Dy-
namic Query Optimization in Multidatabases”, Data Engineering Bulletin,Volume
20,Number3(1997),pages38–45.
[Ozdenetal.1994] B. Ozden, A. Biliris, R. Rastogi, and A. Silberschatz, “ALow-
costStorageServerforaMovieonDemandDatabase”,InProc. of the International
Conf.onVeryLargeDatabases(1994).
[Ozdenetal.1996a] B. Ozden, R. Rastogi, P. Shenoy, and A. Silberschatz, “Fault-
TolerantArchitecturesforContinuousMediaServers”,InProc.oftheACMSIGMOD
Conf.onManagementofData(1996).
[Ozdenetal.1996b] B.Ozden,R.Rastogi,andA.Silberschatz,“OntheDesignofa
Low-CostVideo-on-DemandStorageSystem”,MultimediaSystemsJournal,Volume
4,Number1(1996),pages40–54.
[OzsoyogluandSnodgrass1995] G.OzsoyogluandR.Snodgrass, “Temporaland
Real-TimeDatabases:ASurvey”,IEEETransactionsonKnowledgeandDataEngineer-
ing,Volume7,Number4(1995),pages513–532.
[OzsuandValduriez1999] T. Ozsu and P. Valduriez, Principles of Distributed
DatabaseSystems,2ndedition,PrenticeHall(1999).
Bibliography 1307
[Padmanabhanetal.2003] S. Padmanabhan, B. Bhattacharjee, T. Malkemus,
L. Cranston, and M. Huras, “Multi-Dimensional Clustering: A New Data Lay-
out Scheme in DB2”,InProc. of the ACM SIGMOD Conf. on Management of Data
(2003),pages637–641.
[Paletal.2004] S.Pal,I.Cseri,G.Schaller,O.Seeliger,L.Giakoumakis,andV.Zolo-
tov,“IndexingXMLDataStoredinaRelationalDatabase”,InProc.oftheInternational
Conf.onVeryLargeDatabases(2004),pages1134–1145.
[Pangetal.1995] H.-H.Pang,M.J.Carey,andM.Livny,“MulticlassSchedulingin
Real-TimeDatabaseSystems”,IEEETransactionsonKnowledgeandDataEngineering,
Volume2,Number4(1995),pages533–551.
[Papakonstantinouetal.1996] Y .P a p a k o n s t a n t i n o u ,A .G u p t a ,a n dL .H a a s ,
“Capabilities-Based Query Rewriting in Mediator Systems”,InProc. of the Inter-
nationalConf.onParallelandDistributedInformationSystems(1996).
[Parkeretal.1983] D.S.Parker,G.J.Popek,G.Rudisin,A.Stoughton,B.J.Walker,
E. Walton, J. M. Chow, D. Edwards, S. Kiser, and C. Kline, “Detection of Mutual
Inconsistency in Distributed Systems”, IEEE Transactions on Software Engineering,
Volume9,Number3(1983),pages240–246.
[PatelandDeWitt1996] J. Patel and D. J. DeWitt, “Partition Based Spatial-Merge
Join”,InProc.oftheACMSIGMODConf.onManagementofData(1996).
[Patterson2004] D.P.Patterson, “LatencyLagsBandwidth”,Communicationsofthe
ACM,Volume47,Number10(2004),pages71–75.
[Pattersonetal.1988] D.A.Patterson,G.Gibson,andR.H.Katz,“ACasefor
Redundant Arrays of Inexpensive Disks (RAID)”,InProc. of the ACM SIGMOD
Conf.onManagementofData(1988),pages109–116.
[Pellenkoftetal.1997] A.Pellenkoft,C.A.Galindo-Legaria,andM.Kersten,“The
ComplexityofTransformation-BasedJoinEnumeration”,InProc.oftheInternational
Conf.onVeryLargeDatabases(1997),pages306–315.
[PetersonandDavie2007] L. L. Peterson and B. S. Davie, Computer Networks: a
SystemsApproach,MorganKaufmannPublishersInc.(2007).
[Pless1998] V.Pless,IntroductiontotheTheoryofError-CorrectingCodes,3rdedition,
JohnWileyandSons(1998).
[Poe1995] V. Poe, Building a Data Warehouse for Decision Support,PrenticeHall
(1995).
[PolyzoisandGarcia-Molina1994] C. Polyzois and H. Garcia-Molina, “Evalua-
tion of Remote Backup Algorithms for Transaction-Processing Systems”, ACM
TransactionsonDatabaseSystems,Volume19,Number3(1994),pages423–449.
[Poosalaetal.1996] V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. J. Shekita, “Im-
proved Histograms for Selectivity Estimation of Range Predicates”,InProc. of the
ACMSIGMODConf.onManagementofData(1996),pages294–305.
1308 Bibliography
[Popeketal.1981] G. J. Popek, B. J. Walker, J. M. Chow, D. Edwards, C. Kline,
G. Rudisin, andG. Thiel, “LOCUS:A Network Transparent,High Reliability Dis-
tributed System”,InProc. of the Eighth Symposium on Operating System Principles
(1981),pages169–177.
[P¨ ossandPotapov2003] M. P¨ oss and D. Potapov, “Data Compressionin Oracle”,
InProc.oftheInternationalConf.onVeryLargeDatabases(2003),pages937–947.
[Rahm1993] E. Rahm, “Empirical Performance Evaluation of Concurrency and
CoherencyControlProtocolsforDatabaseSharingSystems”,ACMTransactionson
DatabaseSystems,Volume8,Number2(1993).
[RamakrishnaandLarson1989] M. V. Ramakrishna and P. Larson, “File Organi-
zation Using Composite Perfect Hashing”, ACM Transactions on Database Systems,
Volume14,Number2(1989),pages231–263.
[RamakrishnanandGehrke2002] R.RamakrishnanandJ.Gehrke,DatabaseMan-
agementSystems,3rdedition,McGrawHill(2002).
[RamakrishnanandUllman1995] R.RamakrishnanandJ.D.Ullman, “ASurvey
ofDeductiveDatabaseSystems”,JournalofLogicProgramming,Volume23,Number
2(1995),pages125–149.
[Ramakrishnanetal.1992] R. Ramakrishnan, D. Srivastava, and S. Sudarshan,
ControllingtheSearchinBottom-upEvaluation(1992).
[Rameshetal.1989] R.Ramesh,A.J.G.Babu,andJ.P.Kincaid, “IndexOptimiza-
tion: Theory and Experimental Results”, ACM Transactions on Database Systems,
Volume14,Number1(1989),pages41–74.
[Ranganetal.1992] P. V. Rangan, H. M. Vin, and S. Ramanathan, “Designing an
On-DemandMultimediaService”,CommunicationsMagazine,Volume1,Number1
(1992),pages56–64.
[RaoandRoss2000] J.RaoandK.A.Ross, “MakingB+-TreesCacheConsciousin
Main Memory”,InProc. of the ACM SIGMOD Conf. on Management of Data (2000),
pages475–486.
[Rathietal.1990] A.Rathi,H.Lu,andG.E.Hedrick,“PerformanceComparisonof
ExtendableHashing andLinearHashing Techniques”,InProc. ACM SIGSmall/PC
SymposiumonSmallSystems(1990),pages178–185.
[Reed1983] D. Reed, “Implementing Atomic Actions on Decentralized Data”,
TransactionsonComputerSystems,Volume1,Number1(1983),pages3–23.
[Revesz2002] P. Revesz, IntroductiontoConstraintDatabases,SpringerVerlag(2002).
[Richardsonetal.1987] J. Richardson, H. Lu, and K. Mikkilineni, “Design and
Evaluation of Parallel Pipelined Join Algorithms”,InProc. of the ACM SIGMOD
Conf.onManagementofData(1987).
[Rivest1976] R. L. Rivest, “Partial Match Retrieval Via the Method of Superim-
posed Codes”, SIAM Journal of Computing, Volume 5, Number 1 (1976), pages
19–50.
Bibliography 1309
[Robinson1981] J. Robinson, “The k-d-B Tree: A Search Structure for Large Mul-
tidimensionalIndexes”,InProc.of theACMSIGMOD Conf. on ManagementofData
(1981),pages10–18.
[Roos2002] R.M.Roos,JavaDataObjects,PearsonEducation(2002).
[Rosch2003] W. L. Rosch, TheWinnL.RoschHardwareBible,6thedition,Que(2003).
[RosenthalandReiner1984] A.RosenthalandD.Reiner,“ExtendingtheAlgebraic
FrameworkofQueryProcessingtoHandleOuterjoins”,InProc.oftheInternational
Conf.onVeryLargeDatabases(1984),pages334–343.
[Ross1990] K. A. Ross, “Modular Strati?cation and Magic Sets for DATALOG
Programs with Negation”,InProc. of the ACM SIGMOD Conf. on Management of
Data(1990).
[Ross1999] S. M. Ross, Introduction to Probability and Statistics for Engineers and
Scientists,Harcourt/AcademicPress(1999).
[RossandSrivastava1997] K. A. Ross and D. Srivastava, “Fast Computation of
SparseDatacubes”,InProc.ofthe International Conf.on VeryLargeDatabases(1997),
pages116–125.
[Rossetal.1996] K. Ross, D. Srivastava, and S. Sudarshan, “Materialized View
MaintenanceandIntegrityConstraintChecking:TradingSpaceforTime”,InProc.
oftheACMSIGMODConf.onManagementofData(1996).
[RothermelandMohan1989] K.RothermelandC.Mohan,“ARIES/NT:ARecov-
ery Method Based on Write-Ahead Logging for Nested Transactions”,InProc. of
theInternationalConf.onVeryLargeDatabases(1989),pages337–346.
[Royetal.2000] P.Roy,S.Seshadri,S.Sudarshan,andS.Bhobhe,“Ef?cientandEx-
tensibleAlgorithmsforMulti-QueryOptimization”,InProc.ofthe ACM SIGMOD
Conf.onManagementofData(2000).
[RusinkiewiczandSheth1995] M.RusinkiewiczandA.Sheth. “Speci?cationand
ExecutionofTransactionalWork?ows”,InKim[1995],pages592–620(1995).
[Rustin1972] R.Rustin,DataBaseSystems,PrenticeHall(1972).
[Rys2001] M.Rys,“BringingtheInternettoYourDatabase:UsingSQLServer2000
and XML to Build Loosely-Coupled Systems”,InProc. of the International Conf. on
DataEngineering(2001),pages465–472.
[Rys2003] M.Rys. “XQueryandRelationalDatabaseSystems”,InH.Katz,editor,
XQueryFromtheExperts,pages353–391.AddisonWesley(2003).
[Rys2004] M. Rys. “What’s New in FOR XML in Microsoft SQL Server 2005”.
http://msdn.microsoft.com/en-us/library/ms345137(SQL.90).aspx(2004).
[SagivandYannakakis1981] Y. Sagiv and M. Yannakakis, “Equivalence among
RelationalExpressionswiththeUnionandDifferenceOperators”,Proc.oftheACM
SIGMODConf.onManagementofData(1981).
1310 Bibliography
[Salton1989] G.Salton,AutomaticTextProcessing,AddisonWesley(1989).
[Samet1990] H.Samet,TheDesignandAnalysisofSpatial DataStructures,Addison
Wesley(1990).
[Samet1995a] H. Samet, “General Research Issues in Multimedia Database Sys-
tems”,ACMComputingSurvey,Volume27,Number4(1995),pages630–632.
[Samet1995b] H. Samet. “Spatial Data Structures”,InKim [1995], pages 361–385
(1995).
[Samet2006] H. Samet, Foundations of Multidimenional and Metric Data Structures,
MorganKaufmann(2006).
[SametandAref1995] H. Samet and W. Aref. “Spatial Data Models and Query
Processing”,InKim[1995],pages338–360(1995).
[Sanders1998] R.E.Sanders,ODBC3.5Developer’sGuide,McGrawHill(1998).
[Sarawagi2000] S. Sarawagi, “User-Adaptive Exploration of Multidimensional
Data”,InProc.oftheInternationalConf.onVeryLargeDatabases(2000),pages307–316.
[Sarawagietal.2002] S. Sarawagi, A. Bhamidipaty, A. Kirpal, and C. Mouli,
“ALIAS: An Active Learning Led Interactive Deduplication System”,InProc. of
theInternationalConf.onVeryLargeDatabases(2002),pages1103–1106.
[Schlageter1981] G. Schlageter, “Optimistic Methods for ConcurrencyControl in
Distributed Database Systems”,InProc. of the International Conf. on Very Large
Databases(1981),pages125–130.
[Schneider1982] H. J. Schneider, “Distributed Data Bases”,InProc. of the Interna-
tionalSymposiumonDistributedDatabases(1982).
[Selingeretal.1979] P.G.Selinger,M.M.Astrahan,D.D.Chamberlin,R.A.Lorie,
andT.G.Price,“Access Path Selection in a Relational Database System”,InProc.
oftheACMSIGMODConf.onManagementofData(1979),pages23–34.
[Sellis1988] T. K. Sellis, “Multiple Query Optimization”, ACM Transactions on
DatabaseSystems,Volume13,Number1(1988),pages23–52.
[Sellisetal.1987] T.K.Sellis,N.Roussopoulos,andC.Faloutsos, “TheR
+
-Tree:A
DynamicIndexforMulti-DimensionalObjects”,InProc.oftheInternationalConf.on
VeryLargeDatabases(1987),pages507–518.
[Seshadrietal.1996] P.Seshadri,H.Pirahesh,andT.Y.C.Leung,“ComplexQuery
Decorrelation”,InProc. of the International Conf. on Data Engineering (1996), pages
450–458.
[Shaferetal.1996] J. C. Shafer, R. Agrawal, and M. Mehta, “SPRINT: A Scalable
Parallel Classi?er forDataMining”,InProc. of the International Conf. on Very Large
Databases(1996),pages544–555.
[Shanmugasundarametal.1999] J. Shanmugasundaram, G. He, K. Tufte,
C. Zhang, D. DeWitt, and J. Naughton, “Relational Databases for Querying XML
Bibliography 1311
Documents: Limitations and Opportunities”,InProc. of the International Conf. on
VeryLargeDatabases(1999).
[Shapiro1986] L. D. Shapiro, “Join Processing in Database Systems with Large
Main Memories”, ACM Transactions on Database Systems, Volume 11, Number 3
(1986),pages239–264.
[ShashaandBonnet2002] D. Shasha and P. Bonnet, Database Tuning: Principles,
Experiments,andTroubleshootingTechniques,MorganKaufmann(2002).
[Silberschatz1982] A. Silberschatz, “A Multi-Version Concurrency Control
Scheme With No Rollbacks”,InProc. of the ACM Symposium on Principles of Dis-
tributedComputing(1982),pages216–223.
[SilberschatzandKedem1980] A. Silberschatz and Z. Kedem, “Consistency in
HierarchicalDatabaseSystems”, JournaloftheACM,Volume27,Number1(1980),
pages72–80.
[Silberschatzetal.1990] A.Silberschatz,M.R.Stonebraker ,andJ.D.Ullman,
“DatabaseSystems:AchievementsandOpportunities”,ACMSIGMODRecord,Vol-
ume19,Number4(1990).
[Silberschatzetal.1996] A.Silberschatz,M.Stonebraker,andJ.Ullman,“Database
Research: Achievements and Opportunities into the 21st Century”, Technical Re-
portCS-TR-96-1563, DepartmentofComputerScience,StanfordUniversity, Stan-
ford(1996).
[Silberschatzetal.2008] A.Silberschatz,P .B.Galvin,andG.Gagne,Operating
SystemConcepts,8thedition,JohnWileyandSons(2008).
[Simmenetal.1996] D. Simmen, E. Shekita, and T. Malkemus, “Fundamental
Techniques for Order Optimization”,InProc. of the ACM SIGMOD Conf. on Man-
agementofData(1996),pages57–67.
[Skeen1981] D. Skeen, “Non-blocking Commit Protocols”,InProc. of the ACM
SIGMODConf.onManagementofData(1981),pages133–142.
[Soderland1999] S. Soderland, “Learning Information Extraction Rules for Semi-
structuredandFreeText”,MachineLearning,Volume34,Number1–3(1999),pages
233–272.
[Soo1991] M.Soo, “BibliographyonTemporalDatabases”,ACMSIGMODRecord,
Volume20,Number1(1991),pages14–23.
[SQL/XML2004] SQL/XML. “ISO/IEC 9075-14:2003, Information Technology:
Databaselanguages:SQL.Part14:XML-RelatedSpeci?cations(SQL/XML)”(2004).
[SrikantandAgrawal1996a] R.SrikantandR.Agrawal,“MiningQuantitativeAs-
sociation Rules in Large Relational Tables”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(1996).
1312 Bibliography
[SrikantandAgrawal1996b] R.SrikantandR.Agrawal, “MiningSequentialPat-
terns:GeneralizationsandPerformanceImprovements”,InProc.oftheInternational
Conf.onExtendingDatabaseTechnology(1996),pages3–17.
[StamandSnodgrass1988] R. Stam and R. Snodgrass, “A Bibliography on Tem-
poral Databases”, IEEE Transactions on Knowledge and Data Engineering,Volume7,
Number4(1988),pages231–239.
[Stinson2002] B.Stinson,PostgreSQLEssentialReference,NewRiders(2002).
[Stonebraker1986] M. Stonebraker, “I n c l u s i o no fN e wT y p e si nR e l a t i o n a l
Database Systems”,InProc. of the International Conf. on Data Engineering (1986),
pages262–269.
[StonebrakerandRowe1986] M.StonebrakerandL.Rowe,“TheDesignofPOST-
GRES”,InProc.oftheACMSIGMODConf.onManagementofData(1986).
[Stonebrakeretal.1989] M. Stonebraker, P. Aoki, and M. Seltzer, “Parallelism in
XPRS”,InProc.oftheACMSIGMODConf.onManagementofData(1989).
[Stonebrakeretal.1990] M. Stonebraker, A. Jhingran, J. Goh, and S. Potamianos,
“On Rules, Procedure, Caching and Views in Database Systems”,InProc. of the
ACMSIGMODConf.onManagementofData(1990),pages281–290.
[Stuartetal.1984] D.G.Stuart,G.Buckley ,andA.Silberschatz,“A Centralized
Deadlock Detection Algorithm”, Technical report, Department of Computer Sci-
ences,UniversityofTexas,Austin(1984).
[Tanenbaum2002] A.S.Tanenbaum,ComputerNetworks,4thedition,PrenticeHall
(2002).
[Tanseletal.1993] A.Tansel,J.Clifford,S.Gadia,S.Jajodia,A.Segev,andR.Snod-
grass, Temporal Databases: Theory, Design and Implementation, BenjaminCummings
(1993).
[Teoreyetal.1986] T. J. Teorey,D. Yang,and J. P. Fry, “A Logical Design Method-
ology for Relational Databases Using the Extended Entity-Relationship Model”,
ACMComputingSurvey,Volume18,Number2(1986),pages197–222.
[Thalheim2000] B.Thalheim,Entity-RelationshipModeling:FoundationsofDatabase
Technology,SpringerVerlag(2000).
[Thomas1996] S. A. Thomas, IPng and the TCP/IP Protocols: Implementing the Next
GenerationInternet,JohnWileyandSons(1996).
[Traigeretal.1982] I.L.Traiger,J.N.Gray,C.A.Galtieri,andB.G.Lindsay,“Trans-
actions and Consistency in Distributed Database Management Systems”, ACM
TransactionsonDatabaseSystems,Volume7,Number3(1982),pages323–342.
[Tyagietal.2003] S.Tyagi,M.Vorburger,K.McCammon,andH.Bobzin,CoreJava
DataObjects,prenticehall(2003).
Bibliography 1313
[Umar1997] A.Umar,Application(Re)Engineering:BuildingWeb-BasedApplications
andDealingWithLegacies,PrenticeHall(1997).
[UniSQL1991] UniSQL/X Database Management System User’s Manual: Release 1.2.
UniSQL,Inc.(1991).
[Verhofstad1978] J. S. M. Verhofstad, “Recovery Techniques for Database Sys-
tems”,ACMComputingSurvey,Volume10,Number2(1978),pages167–195.
[Vista1998] D. Vista, “Integration of Incremental View Maintenance into Query
Optimizers”,InProc. of the International Conf. on Extending Database Technology
(1998).
[Vitter2001] J.S.Vitter, “ExternalMemoryAlgorithmsandDataStructures:Deal-
ingwithMassiveData”,ACMComputingSurveys,Volume33,(2001),pages209–271.
[Walshetal.2007] N. Walsh et al. “XQuery 1.0 and XPath 2.0 Data Model”.
http://www.w3.org/TR/xpath-datamodel. currently a W3C Recommendation
(2007).
[Waltonetal.1991] C. Walton, A. Dale, and R. Jenevein, “A Taxonomy and Per-
formanceModelofDataSkewEffectsinParallelJoins”,InProc.oftheInternational
Conf.onVeryLargeDatabases(1991).
[Weikum1991] G. Weikum, “Principles and Realization Strategies of Multilevel
Transaction Management”, ACM Transactions on Database Systems, Volume 16,
Number1(1991).
[Weikumetal.1990] G.Weikum,C.Hasse,P.Broessler,andP.Muth,“Multi-Level
Recovery”,InProc.oftheACMSIGMODConf.onManagementofData(1990),pages
109–123.
[Wilschutetal.1995] A.N.Wilschut,J.Flokstra,andP.M.Apers, “ParallelEvalu-
ation of Multi-Join Queues”,InProc. of the ACM SIGMOD Conf. on Management of
Data(1995),pages115–126.
[WittenandFrank1999] I.H.WittenandE.Frank, Data Mining: Practical Machine
LearningToolsandTechniqueswithJavaImplementations,MorganKaufmann(1999).
[Wittenetal.1999] I. H. Witten, A. Moffat, and T. C. Bell, Managing Gigabytes:
Compressing and Indexing Documents and images, 2nd edition, Morgan Kaufmann
(1999).
[Wolf1991] J. Wolf, “An Effective Algorithm for Parallelizing Hash Joins in the
PresenceofDataSkew”,InProc.oftheInternationalConf.onDataEngineering(1991).
[WuandBuchmann1998] M. Wu and A. Buchmann, “Encoded Bitmap Indexing
forDataWarehouses”,InProc.oftheInternationalConf.onDataEngineering(1998).
[Wuetal.2003] Y.Wu,J.M.Patel,andH.V .Jagadish,“Structural Join Order Se-
lection for XML Query Optimization”,InProc. of the International Conf. on Data
Engineering(2003).
1314 Bibliography
[X/Open1991] X/Open Snapshot: X/Open DTP: XA Interface. X/Open Company,
Ltd.(1991).
[YanandLarson1995] W. P. Yan and P. A. Larson, “Eager Aggregation and Lazy
Aggregation”,InProc.oftheInternationalConf.onVeryLargeDatabases(1995).
[Yannakakisetal.1979] M. Yannakakis, C. H. Papadimitriou, and H. T. Kung,
“LockingProtocols:SafetyandFreedomfromDeadlock”,InProc.oftheIEEESym-
posiumontheFoundationsofComputerScience(1979),pages286–297.
[Zaharioudakisetal.2000] M.Zaharioudakis,R.Cochrane,G.Lapis,H.Pirahesh,
andM.Urata, “AnsweringComplexSQLQueriesusing AutomaticSummaryTa-
bles”,InProc. of the ACM SIGMOD Conf. on Management of Data (2000), pages
105–116.
[ZellerandGray1990] H. Zeller and J. Gray, “An Adaptive Hash Join Algorithm
for Multiuser Environments”,InProc. of the International Conf. on Very Large
Databases(1990),pages186–197.
[Zhangetal.1996] T.Zhang,R.Ramakrishnan,andM.Livny,“BIRCH:AnEf?cient
Data Clustering Methodfor VeryLarge Databases”,InProc. of the ACM SIGMOD
Conf.onManagementofData(1996),pages103–114.
[ZhouandRoss2004] J.ZhouandK.A.Ross, “BufferingDatabaseOperationsfor
Enhanced Instruction Cache Performance”,InProc. of the ACM SIGMOD Conf. on
ManagementofData(2004),pages191–202.
[Zhugeetal.1995] Y. Zhuge, H. Garcia-Molina, J. Hammer, and J. Widom, “View
maintenance in a warehousing environment”,InProc. of the ACM SIGMOD Conf.
onManagementofData(1995),pages316–327.
[Ziauddinetal.2008] M. Ziauddin, D. Das, H. Su, Y. Zhu, and K. Yagoub, “Op-
timizer plan change management: improved stability and performance in Oracle
11g”,ProceedingsoftheVLDBEndowment,Volume1,Number2(2008),pages1346–
1355.
[Zikopoulosetal.2004] P. Zikopoulos, G. Baklarz, D. deRoos, and R. B. Melnyk,
DB2Version8:TheOf?cialGuide,IBMPress(2004).
[Zikopoulosetal.2007] P .Zikopoulos,G.Baklarz,L.Katsnelson,andC.Eaton,
IBMDB29NewFeatures,McGrawHill(2007).
[Zikopoulosetal.2009] P .Zikopoulos,B.T assi,G.Baklarz,andC.Eaton,Break
FreewithDB29.7,McGrawHill(2009).
[Zilioetal.2004] D. C. Zilio, J. Rao, S. Lightstone, G. M. Lohman, A. Storm,
C. Garcia-Arellano, and S. Fadden, “DB2 Design Advisor: Integrated Automatic
PhysicalDatabaseDesign”,InProc.oftheInternationalConf.onVeryLargeDatabases
(2004),pages1087–1097.
[Zloof1977] M. M. Zloof, “Query-by-Example:A Data Base Language”, IBM Sys-
temsJournal,Volume16,Number4(1977),pages324–343.
Index
2PC.Seetwo-phasecommit
3NF.Seethirdnormalform
3PC.Seethree-phasecommit
abstractdatatypes,1127
accesspaths,542
ACIDproperties.See
atomicity;consistency;
durability;isolation
ActiveServerPages(ASP),397
ADO.NET,169,395,1249,1253
AdvancedEncryptionStandard
(AES),412-413
agglomerativeclustering,
907-908
aggregatefunctions
basic,85-86
Booleanvaluesand,89-90
SQL,84
fusion,960
withgrouping,86-88
havingclause,88-89
nullvaluesand,89-90
aggregation
advancedfeaturesof,192-197
alternativenotationsfor,
304-310
entity-relationship(E-R)
modeland,301-302,304
IBMDB2and,1209-1210
intraoperationparallelism
and,811
.NETCommonLanguage
Runtime(CLR)and,
1257-1258
OLAPand,197-209
PostgreSQLand,1153
queryoptimizationand,597
queryprocessingand,566-567
rankingand,192-195
relationalalgebraand,
235-239
representationof,304
viewmaintenanceand,
610-611
windowing,195-197
Ajax,390-391,398,867
aliases,75,355,829,872-873,
1229
altertable,63,129
altertrigger,185
altertype,140
AmericanNationalStandards
Institute(ANSI),57,1051
analysispass,753
analyticworkspaces,1161
andoperation,66,83-84,1174
anykeyword,92n8
Apache,386,399,426,980
AppleMacintoshOSX,1124
applicationdesign,418
applicationarchitecturesand,
391-396
authenticationand,405-407
business-logiclayerand,
391-392
client-serverarchitectureand,
376-377
commongatewayinterface
(CGI),380-381
cookiesand,382-385
dataaccesslayerand,391,
393,395
disconnectedoperationand,
395-396
encryptionand,411-417
HyperTextMarkupLanguage
(HTML)and,378-380
HyperTextTransferProtocol
(HTTP)and,377-381,383,
395,404-406,417
JavaServerPages(JSP)and,
377,383-391
performanceand,400-402
rapidapplication
developmentand,
396-400
securityand,402-417
servletsand,383-391
three-layerarchitectureand,
318
TP-monitorsand,1095-1096
uniformresourcelocators
(URLs),377-378
userinterfacesand,375-377
WorldWideWeband,377-382
applicationdevelopment
performancebenchmarks
and,1045-1048
performancetuningand,
1029-1045
setorientationand,1030-1031
standardizationand,
1051-1056
testingapplicationsand,
1048-1051
updatesand,1030-1033
applicationmigration,
1050-1051
1315
1316 Index
applicationprogram interface
(API)
ADO.NET, 169,1054
application design and,
383-386,395
C++,1054
customizedmapsand,1068,
1070
DOM, 1020
IBM DB2,1196
Java,158-166,213,383-386,
1018,1030
LDAP,874
Microsoft SQL Server,1229,
1245,1248-1250,
1253-1255,1265,1267
ODBC, 166-169,1053
PostgreSQL, 1125
SAX, 1020
standards for,1051-1056
system architecturesand,772
XML, 985,1008-1009
applyoperator,1230-1231
architectures,767
business logicand,25,1158,
1221-1222,1228,1232,
1253,1263-1267
centralized,769-771
client-server,771-772
cloud-based,777
dataserver,773,775-777
datawarehouse,889-891
distributed systems,784-788
hierarchical, 781,784
hypercube,781
mesh,780-781
networktypesand,788-791
paralleldatabases,797-820
parallelsystems,777-784
processmonitor,774
server system,772-777
shared-disk, 781,783,789
sharedmemory,781-783
shared-nothing,781,783-784,
803
sharedserver,1185
single-user system,770-771
source-driven, 890
storage-areanetwork(SAN),
789
threadpoolingand,1246
three-tier,25
TP-monitor,1092-1095
transaction-server,773-775
two-phasecommitprotocol
(2PC),786-788
two-tier,24-25
wide-areanetworks(WANs),
788,790-791
ARIES, 1146
analysis pass,753
compensationlogrecords
(CLRs),751-752,754
datastructure,751-753
dirtypagetable,750-755
?ne-grainedlocking, 756
fuzzycheckpoints,750-752
log sequencenumber(LSN),
750-755
nestedtopactions,755-756
optimizationand,756
physiological redo and,750
recovery and,751,753,756
redo pass,754
savepointsand,756
transactionrollback and,
754-755
undo pass,754-755
Arpanet,790
arraytypes,956-961
ascexpression,77-78
asclause,75-76
ASP.NET, 387
assertions,11,135-136
assignment operation,176,217,
232,1052
associations,17,43
entitysetsand,290(seealso
entitysets)
relationship setsand,264-267,
308-309,314
rulesfor,904-907
associativeproperty, 584-585
AsterData,816
asymmetric-keyencryption,
412
atomicdomains,42
?rstnormalformand,327-329
object-baseddatabasesand,
947
relationaldatabasedesign
and,327-329
atomicity,4-5,22-23,104,625
cascadeless schedules and,
647-648
commit protocolsand,
832-838
de?ned,628
distributedtransactionsand,
830-832
isolationand,646-648
logrecordsand,726-728,
730-734
recoverableschedules and,
647
recoverysystemsand,726-735
storagestructureand,632-633
work?owsand,1099-1100
attributeinheritance,298-299
attributes
atomicdomainsand,327-329
classi?ers and,896-897
closure ofattributesetsand,
340-342
complex,277-278,284-285
composite,267
continuousvalued, 898
decompositionand,329-338,
348-360
derived,268
design issuesand,290-291
domain,267
entity-relationshipdiagrams
and,277-278
entity-relationship(E-R)
modeland,263,267-269
entitysetsand,283-286,
290-291
multiple-keyaccess and,
506-509
multivalued, 267-268,327-329
namingof,362-363
nesting,958-961
nullvaluesand,268-269
partitioned,896-897
placementof,294-295
searchkeyand,476
simple,267,283-284
single-valued, 267-268
Uni?ed Modeling Language
(UML) and,308-310
uniqui?ersand,498-499
unnesting,958-961
valuesetof,267
xmlattributesand,1015
XMLtypesand,990-998
attribute-valueskew, 800-801
audittrails,409-410
augmentationrule,339
authentication
challenge-responsesystem
and,415
Index 1317
digital certi?catesand,
416-417
digital signaturesand,
416-417
encryptionand,415-417
security and,405-407(seealso
security)
single sign-onsystem and,
406-407
smart cardsand,415-416
two-factor,405-407
authorization,11,21,58
administratorsand,143
application-level, 407-409
checkclause,148
databasedesign and,312
end-user informationand,
407-408
granting/revoking privileges,
29,143-145,149-150
lack of?ne-grained,408-409
rolesand,145-146
schema and,147-148
Security AssertionMarkup
Language(SAML) and,
407
sqlsecurityinvoker,147
transferofprivileges, 148-149
updateand,147,148
viewsand,146-147
autonomy,858
availability
CAPtheoremand,852-853
consistency and,852-853
coordinatorselectionand,
850-852
majority-basedapproachand,
848-849
read one,writeall approach,
849-850
remotebackupand,850
robustnessand,847
site reintegrationand,850
averageresponse time,636
averageseektime,435,540n2
avgexpression,204
aggregatefunctionsand,
84-88
queryprocessing and,566-567
relationalalgebraand,236
backup, 186-187
application design and,415
distributed databasesand,
839,877
IBM DB2and,1215,1218
Microsoft SQL Serverand,
1223,1227-1228,1245,
1262
Oracle and,1165,1169,
1181-1183,1187-1190
paralleldatabasesand,816
recoverysystemsand,726-738
(see also recovery
systems)
remotesystemsfor,756-759,
850,1095-1096
system architecturesand,770
transactionsand,632,1115
backup coordinator,851
balancedrange-partitioning,
801
balancedtree,486
BASEproperties,853
batchscaleup,779
batchupdate,1030-1031
Bayesianclassi?ers,900-901
Bayes’theorem,900
BCNF.SeeBoyce-Coddnormal
form
beginatomic...end,128,176,
181,183-185
best splits,897-899
biasedprotocol,841
big-bangapproach,1050-1051
Bigtable,862-867
binary splits,898
bit-interleavedparity
organization,445
bit-levelstriping,442-444
bitmapindices,507,509,531,
536
B+-treesand,528
ef?cientoperationsof,527-528
existence,526-527
Oracle and,1166-1167
scans, 1153
sequentialrecordsand,
524-525
structure of,525-527
blindwrites,687
blobs, 138,166,457,502,1013,
1198-1199,1259
block-interleavedparity
organization,445-446
block-levelstriping,443-4
blocknested-loopjoin,551-552
Booleanoperations,83,89-90,
94,161,176,873,1256.
See alsospeci?c
operation
bottlenecks
applicationdesign and,402,
1029,1033-1035,1039
?lestructure and,468
systemarchitecturesand,
782-783,800,816-819,
829,839-840
transactionsand,1106-1107,
1116
Boyce-Coddnormalform
(BCNF)
decompositionalgorithms
and,349-356
dependencypreservation
and,334-336
relationaldatabasedesign
and,333-336
broadcast data,1082-1083
B-trees
applicationdevelopmentand,
1039
IBMDB2and,1205
indicesand,504-506,530,1039
Oracle and,1159,1164-1169,
1173
PostgreSQLand,1135,
1148-1150
spatialdataand,1064,
1071-1072,1076,1086
B+-trees,1075
balancedtreesand,486
bitmapindices and,528
bulkloading ofindices and,
503-504
deletiontimeand,491,
495-497,499-501
extensionsand,500-506
fanoutand,487
?leorganizationand,500-502
?ashmemoryand,506
indexing stringsand,502-503
insertiontimeand,491-495,
499-501
onmultiple keys, 508
nonleafnodesof,487
nonuniquesearch keysand,
497-499
performanceand,485-486
pointersand,486
querieson,488-491,538,544
recordrelocation and,502
secondaryindices and,502
structureof,486-488
1318 Index
updatesand,491-500
buffermanager,21,464-466
bufferpools, 1201-1202,1220
buffers
databasebufferingand,
739-741
?le structure and,437-438
force/no-forcepolicy and,
739-740
forceoutputand,725-726
IBM DB2and,1200-1203
log-record buffering and,
738-739
main-memorydatabasesand,
1105-1108
managementof,738-743
multiple pool,1220
operatingsystemrole in,
741-742
recoverysystemsand,738-743
replacement policies and,
465-468
steal/no-stealpolicy and,740
transactionserversand,774
write-aheadlogging (WAL)
rule and,739-741
bugs, 174n4
application design and,404,
1048-1050
recoverysystemsand,721-722
system architecturesand,
787-788
transactionsand,1093,1102
work?owsand,1101
buildinput,558
bulkexport,1032
bulkinsert,1032
bulkloads,503-504,1031-1033
bullyalgorithm,851,852
business logic,25,173
application design and,376,
383,391-393,396,410,418
IBM DB2and,1221-1222
Microsoft SQL Serverand,
1228,1232,1253,
1263-1267
Oracle and,1158
business-logic layer,39,391
bussystem,780
BYNET,806
byte-code,389
C,7,14,
advancedSQL and,157,169,
173,180
application design and,387,
397-398
Microsoft SQL Serverand,
1228,1253
ODBC and,167-168
Uni?ed Modeling Language
(UML) and,308
C++,7n1,14
advancedSQL and,169,173
Microsoft SQL Serverand,
1253
object-baseddatabasesand,
945
persistentsystemsand,
968-971
standards for,1054
Uni?ed Modeling Language
(UML) and,308
caching, 429
application design and,
400-401
coherency and,776
dataserversand,776
locks and,776
multithreading and,817-818
Oracle and,1184
queryplansand,605,775
shared-memoryarchitecture
and,783
call,175
callablestatements,164
callback,776
CallLevelInterface (CLI)
standards, 1053
candidatekeys, 45-46
canonical cover,342-345
CAPtheorem,852-853
Cartesianproducts, 68
equivalence and,584
join expressionsand,229-232
queriesand,573,584,589,
595-596,606,616
relationalalgebraand,50-51,
217,222-232
SQL and,68-75,120,209
cascade,133,150,186
cascadelessschedules, 647-648
cascading stylesheet(CSS)
standard, 380
case construct, 102-103
CASEtools,1194-1195
cast,136,139-140
catalogs
application developmentand,
1053
distributeddatabasesand,
830,848
e-catalogsand,1103-1104
IBMDB2and,1195,1220
indicesand,476(seealso
indices)
informationretrievaland,
915,935
Microsoft SQL Serverand,
1235-1236,1250,1256,
1266
PostgreSQLand,1151,1154
queryoptimizationand,
590-596,1151
SQLand,142-143,165,
168-169
system,462,468,801,1132
transactionprocessing and,
1116
XMLand,1017
categories,935-937
centralizedarchitectures,
769-771
centralizeddeadlockdetection,
845-846
challenge-response system,415
change isolationlevel,649
checkclause,130,134
assertionand,135-136
authorizationand,148
dataconstraintsand,310
dependencypreservation
and,334-336
user-de?nedtypesand,140
checkconstraints, 134,148,310,
317,334,628,1130
checkpoint logrecord,752
checkpoints
fuzzy,750-752
Microsoft SQL Serverand,
1246
Oracleand,1185
recoverysystems and,
734-735,742-743
transactionserversand,774
checksums, 434
classi?cationhierarchy, 935-937
classi?ers
Bayesian,900-901,1191,1266
bestsplits and,897-899
decision-tree,895-900
entropymeasureand,897
Ginimeasureand,897
informationgainand,897-898
kernelfunctionsand,901-902
Index 1319
neural-net,900
partitionsand,896-897
predictionand,894-904
purity and,897
regression and,902-903
SupportVectorMachine,
900-902
training instancesand,895
validating, 903-904
client-serversystems,23,32,
204
application design and,
376-377,1031,1053,1056
Microsoft SQL Serverand,
1228
recovery systemsand,756
system architecture and,
769-772,777,788,791
transactionprocessing and,
1092-1096
client-sidescripting,389-391
clobs, 138,166,457,502,
1010-1013,1196-1199
cloudcomputing, 777
challengeswith,868-870
datarepresentationand,
863-865
partitionsand,865-866
replicationand,866-868
retrievaland,865-866
storageand,862-863
traditionaldatabasesand,868
transactionsand,866-868
clusters,781
agglomerative,907-908
cloud computingand,867
datamining and,894,907-908
divisive, 907-908
hierarchical, 907-908
IBM DB2and,1203-1207
multidimensional, 1203-1207
Oracle and,1173,1186
RealApplicationClusters
(RAC)and,1186
coalescence,491,706
code breaking.Seeencryption
ColdFusionMarkup Language
(CFML),387
collectfunction, 959
collectionvolumes,957,957-958
column-orientedstorage,
892-893
combinatorics, 639
commitprotocols, 832-838
committime,758
commitwork, 127
commongatewayinterface
(CGI) standard,380-381
CommonLanguage Runtime
(CLR),180
CommonObject Request
Broker Architecture
(CORBA),1054-1055
commonsubexpression
elimination,614
commutativeproperty,584-585
compatibilityfunction,662
compensationlogrecords
(CLRs),751-752,754
complexdatatypes,28,31,138,
1061
entity-relationship (E-R)
modeland,946-947
keywordsand,947-949
normalformsand,947
object-baseddatabasesand,
945-949,963,970-974
system architecture and,864
component object model
(COM),1253
compression, 1077-1078
application developmentand,
1041
datawarehousesand,893
IBM DB2and,1194
Microsoft SQL Serverand,
1236
Oracle and,1165-1167
pre?x,503,1166
computer-aideddesign(CAD),
312,1061,1064-1068
conceptual design,15-16,260
concurrency control,631,636,
639,709
access anomaliesand, 5
blind writesand,687
consistency and,695,701-704,
710
deadlock handling and,
674-679
deleteoperationsand,697-701
distributed databasesand,
835-836,839-847
DMLcommandsand,
1138-1139
false cyclesand,846-847
IBM DB2and,1200-1203,
1217-1218
index structuresand,704-708
insert operationsand,697-701
locksand,661-686,839-842
logicalundoand,749-750
long-durationtransactions
and,1111-1112
Microsoft SQL Server and,
1241-1246
multiple granularityand,
679-682
multiversionschemesand,
689-692,1137-1146
Oracle and,1180-1183
PostgreSQLand,1137-1145
predicatereadsand,697-701
recoverysystemsand,729-730
rollbacksand,667,670,
674-679,685,689,691,709
serializability and,650,662,
666-667,671,673,681-690,
693-697,701-704,708
snapshotisolationand,
692-697,729-730
timestamp-basedprotocol
and,682-686
updatesand,867-868
userinteractionsand,702-704
validationand,686-689,
729-730
Webcrawlersand,930-931
condition-de?ned entitysets,
299
con?dence, 893,905
conformance levels,168-169
conjunction, 545-546,594
connect to,170
consistency, 11,22,104
availability and,852-853
CAPtheoremand,852-853
concurrency controland,695,
701-704,710
cursorstability and, 702
deadlocksand,665-666
degree-two,701-702
distributedtransactionsand,
830-832
logicaloperationsand,746
mobilenetworksand,
1083-1085
replicationwithweak,
843-844
requirementof,630
transactionsand,627-631,
635-636,640,648-650,655
userinteractionsand,702-704
weaklevelsof,701-704
1320 Index
constraints
condition-de?ned, 299
decomposition and,354-355
dependencypreservation
and,334-336
disjoint, 300
entity-relationship (E-R)
modeland,269-272,
299-301
IBM DB2and,1199
integrity, 4(seealso integrity
constraints)
keys,271-272
mapping cardinalities and,
269-270,276-277
overlapping,300
partial, 300
participation,270
PostgreSQL and,1130-1131,
1153-1154
total,300
transactionsand,628,
1108-1109
UML and,309-310
user-de?ned,299
contains,93
contextswitch, 1092
conversationgroup, 1262
cookies,382-385,403-405
cores,770
correlatedsubqueries, 93
correlations,906
correlationvariables,605-607
count, 84-86,89,566-567
count-distinct, 236
count value,204
crashes, 434,467-468
actionsafter,736-738
algorithmsfor,735-738
ARIES and,750-756
checkpointsand,734-735
failure classi?cation and,
721-722
recoverysystemsand,736-738
(see also recovery
systems)
transactionsand,628
createassertion,135
createdistincttype,141,
1194-1195,1197
createdomain,140-141
createfunction, 175,177,189
createindex,528-529,1150-1151
createorreplace,174n4
createprocedure, 175,179
createrecursive view,192
createrole,146
createschema,143
createsnapshot, 843-844
createtable,60-63,139,161
withdata,141-142
extensionsfor,141-142
integrity constraintsand,129
object-baseddatabasesand,
950,961-962
createtable...as,142
createtype,139-141
createunique index,529
createview,121-123,125,142,
147,1130-1131
cross-site request forgery
(XSRF),403-404
cross-site scripting (XSS),
403-405
cross-tabulation,199-203,205,
210
CRUDWebinterfaces,399
CrystalReports,399-400
cube by,1221-1222
cube construct, 206-210
current date,137
cursor stability,702
curve ?tting,902-903
Cycproject,925,927
dataabstraction,6-8
dataaccesslayer,391,393,395
dataanalysis
datamining and,893-910
decision-support systems
and,887-891
informationretrievaland,
915-938
warehousingand,887-891
databasedesign,257,313-314
adaptingformodern
architecturesand,
818-819
alternativesand,261-262,
304-310
application,375-418
authorizationrequirements
and,312
automatedtuning and,
1040-1041
bottom-up,297
buffersand,464-468
client-server architecture and,
32,204,376-377,756,
766-767,769-772,777,
788,791
complexityof,260
conceptual-designphaseand,
15-16,260
dataconstraintsand,310-311
direct design process and,
259-260
encryptionand,411-417
entity-relationship(E-R)
modeland,17-18,249-313
IBMDB2and,1194-1195
incompletenessand,262
logical-design phaseand,16,
260-261
Microsoft SQL Serverand,
1223-1228
normalizationand,18-20
Oracleand,1157-1158
overviewofprocess,259-262
parallelsystemsand,815-817
phasesof,259-261
physical-design phase and,
16,261
redundancy and,261-262
relational,323-368
speci?cationoffunctional
requirementsand,16
top-down,297
universitiesand,16-17
userneedsand,260
userrequirementsand,
311-312
work?owand,312-313
databasegraph,671-674
databaseinstance,42
databases
abstractionand,6-8,10
architectureand,28-29,767
(seealso architectures)
bufferingand,739-741(see
also buffers)
concurrency controland,
661-710(seealso
concurrency control)
distributed,825-878,1188
dumping and,743-744
?le-processing systemand,
3-6
forceoutputand,725-726
historyof,29-31
indexing and,475-531(see
also indices)
informationretrievaland,
915-937
Index 1321
locks and,661-679(seealso
locks)
main-memory,1105-1108
mobile,1079-1085
modi?cation and,98-103,
728-729
multimedia, 1076-1079
normalizationand,18-20
parallel,797-820
personal,1079-1085
recovery systemsand,631,
633,721-761(seealso
recovery systems)
storageand,20-22,427(see
also storage)
time in,1062-1064
databasesadministrator(DBA),
28-29,149,1152,
1214-1215,1243
databaseschemas.Seeschemas
DatabaseTaskGroup, 1052
databasewriterprocess, 773
datacleansing,890
datacube,200,206-210
data-de?nitionlanguage
(DDL), 9-12,14,32
authorizationand,58
basictypesand,59-60
concurrency controland,
1144-145
dumping and,743-744
IBM DB2and,1194-1197,1204
indices and, 58
integrity and, 58
Microsoft SQL Serverand,
1225,1228-1233,1245,
1253,1256,1261
Oracle and,1162,1181
PostgreSQL and,1144-1145,
1150
querying and,21-22
schema de?nition and,28,58,
60-63
security and,58
setofrelationsin,58-61
SQL basicsand,57-63,104
storageand,58
datadictionary,12,21,462-464
DataEncryption Standard
(DES), 413
DataGridcontrol,398
dataguard,1183
datainconsistency. See
consistency
dataisolation,Seeisolation
DATAllegro, 816
Datalog,37
data-manipulationlanguage
(DML), 12-14
authorizationand,143
compiler and,21-22
concurrency controland,
1138-1139,1145
declarative, 10
de?ned,10,32
hostlanguage and,15
Microsoft SQL Serverand,
1231-1233,1245,1261
Oracle and,1161-1162,1165,
1181
PostgreSQL and,1137-1138
precompiler and,15
procedural/nonprocedural,
10
querying and,21-22
snapshotisolationand,1137
(seealso snapshot
isolation)
storagemanagerand,20-21
triggersand,1161-1162
datamediation,1018-1019
datamining,25-26,33,771-772,
887-889
association rulesand,904-907
bestsplits and,897-899
classi?cation and,894-904
clusters and,894,907-908
data-visualization, 909
de?ned,893
descriptive patternsand,894
entropymeasureand,897
Gini measureand,897
informationgainand,897-898
Microsoft SQL Serverand,
1266-1267
Oracle and,1191
predictionand,894-904
purity and,897
rulesfor,893-894
text,908
datamodels.Seespeci?cmodel
dataparallelism,805
dataserversystems, 773,
775-777,782
datastorageandde?nition
language,11
datastriping,444
data-transferrate,435-436
datatypes.Seetypes
datawarehouses, 888
column-orientedstorageand,
892-893
componentsof,889-891
deduplicationand,890-891
de?ned,889
facttablesand,891-892
householdingand,891
IBMDB2and,1194,1221-1222
materializedviewsand,
1171-1172
merger-purgeoperationand,
890-891
Microsoft SQL Server and,
1264
Oracle and,1158,1162,
1169-1172,1178,1189
transformationsand,891
updatesand,891
DataEncryption Standard
(DES), 413
datetime,201
DB-Lib,1249
deadlines,1108-1109
deadlocks
consistency and,665-666
distributeddatabasesand,
839,841,844-847
handlingof,674-679
IBMDB2and,1217-1220
long-durationtransactions
and,1110-1111
Microsoft SQL Server and,
1243-1244,1246
PostgreSQLand,1143-1145
preventionof,675-676
rollbackand,678-679
starvationand,679
victim selection and, 678
wait-forgraphand,676-677,
676-678
decisionsupport, 1047
decision-support queries,797
decision-support systems,
887-891
decision-treeclassi?ers,895-900
declare,175-178
decode,208
decomposition
algorithmsfor,348-355
Boyce-Coddnormalform
and,333-336,349-355
dependencypreservation
and,334-336
1322 Index
fourthnormalformand,
358-360
functionaldependenciesand,
329-338,355-360
higher normalformsand,
337-338
keysand,330-333
lossless, 345-346
lossless-join, 345-346
lossy, 345-346
multivalued dependencies
and,355-360
relationaldatabasedesign
and,329-338,348-360
third normalformand,
336-337,352-355
decompositionrule,339
DEC Rdb, 30
deduplication,890-891
DeepWebcrawlers, 931
defaultvalues,133,137,140,
144,425,899,952,
991-992,996,1128
deferredintegrityconstraints,
134
degree-twoconsistency,701-702
deletion,61,63,98-100,102,161
concurrency controland,
697-701
EXEC SQL and,171
hashingand,510,513,516,523
integrity constraintsand,133
PostgreSQL and,1130-1131
privileges and,143-145
transactionsand,629,653
triggersand,183
viewsand,125
deltarelation,186
demand-drivenpipelining,
569-570
denormalization,363-364
dependency preservation,
334-336,346-348
desc,77-78
descriptive attributes.See
attributes
descriptive patterns,894
deviation,215,906-907
dicing,201
dictionary attacks,414
digitalcerti?cates,416-417
digitalsignatures,416
direct-accessstorage,431
directories,935-937
directory informationtree
(DIT),872-875
directorysystems,870-875
dirtyblocks, 741
dirtypage table,750-756,
1244-1245
dirtyread,1137,1181
dirtywrites,649
disabletrigger,185
disconnected operation,
395-396
disjoint entitysets,300
disjoint specialization,296-297
disjunction, 545-546,594
disk-arm-scheduling, 437
diskcontroller,434
distinct types,84-86,138-141
distinguishedname(DN), 872
distributeddatabases,876-878
availability and,847-853
cloud-based,861-870
commit protocolsand,
832-838
concurrency controland,
835-836,839-847
deadlock handling and,
844-847
directory systemsand,
870-875
failure and,831-835
fragmentationand,826-829
heterogeneous,825-826,
857-861
homogeneous,825-826
joinsand,855-857
locks and,839-847
partitionsand,835
persistentmessaging and,
836-838
queryprocessing and,854-860
recovery and,835-836
replicationand,826,829,
843-844
storageand,826-830
timestampsand,842-843
transparency and,829-830
uni?ed viewofdataand,
858-859
distributed-lockmanager,840
distributedsystems
autonomyand,785
availability and, 785
exampleof,786
globaltransactionsand,784
greaterpotentialforbugsin,
787-788
implementationand,786-788
increased processing
overheadof,788
localtransactionsand,784
nodesand,784
readystateand,787
replicationand,785
sharing dataand,785
sitesand,784
software-developmentcost
and,787
two-phasecommitprotocol
(2PC)and,786-788
distributor,1252
divisiveclustering, 907-908
Document Object Model
(DOM), 390
document typede?nition
(DTD),990-994
domain,42
domainconstraints, 11
domain-keynormal form
(DKNF), 360
domainrelationalcalculus,249
examplequeries,246-247
expressive powerof
languages,248
formalde?nition, 245
safetyofexpressions,247-248
double-pipelinedhash-join,
571-572
drilldown, 201
Driver-Managerclass,160
dropindex,529
droptable,63,164
droptrigger,185
droptype,140
dumping, 743-744
duplicateelimination,563-564
durability,22-23,104,625,
630-631
de?ned,628
distributedtransactionsand,
830-832
one-safe,758
remotebackupsystemsand,
758
storagestructureand,632-633
two-safe,758
two-very-safe,758
dynamicSQL, 58,158,175
e-catalogs,1103
Index 1323
Eclipse,386
E-commerce,1102-1105
ef?ciency, 6-8
electionalgorithms,851-852
embeddedSQL,58,158,169-173
emptyrelations,93-94
encryption
Advanced Encryption
Standard(AES),412-413
applicationsof,411-417
asymmetric-key,412
authenticationand,415-417
challenge-response system
and,415
databasesupportand,414-415
dictionary attacksand,414
digital certi?catesand,
416-417
digital signaturesand,416
nonrepudiationand,416
Oracle and,1165-1166
prime numbersand,413
public-key, 412-414
Rijndaelalgorithm and,
412-413
techniquesof,412-414
end-userinformation,407-408
enterpriseinformation,1-2
EntityDataModel,395
entity-relationship(E-R)
diagram,17-18
alternativenotationsfor,
304-310
basicstructure of,274-275
complexattributes,277-278
entitysetsand,279-281
generalizationand,298
identifying relationship and,
280
mapping cardinality, 276
nonbinaryrelationship sets,
278-279
relationship sets,278-279
roles, 278
university enterprise
example,282-283
weakentitysets,279-281
entity-relationship(E-R)
model,9,17-18,259,
313-314,963
aggregationand,301-302,304
alternativemodelingdata
notationsand,304-310
atomicdomainsand,327-329
attributesand,263,267-269,
290-291,294-295,298-299,
327-329
complex datatypesand,
946-947
constraintsand,269-272
design issuesand,290-295
enterpriseschemaand,262
entitysetsand,262-267,
272-274,279-286,290-291,
296-298
extendedfeatures,295-304
generalizationand,297-304
normalizationand,361-362
object-orienteddatamodel
and,27
reductiontorelational
schemasand,283-290
redundancy and,272-274
relationship setsand,264-267,
286-290,291-295,296-297
specializationand,295-297
Uni?ed Modeling Language
(UML) and,308-310
entitysets
alternativenotationsfor,
304-310
attributesand,263,284-285,
290-291
condition-de?ned, 299
de?ned,262-263
design issuesand,290-292
disjoint, 299
extensionof,263
generalizationand,297-304
identifying relationship and,
280
overlapping,299
propertiesof,262-264
relationship setsand,264-267,
291-292
removing redundancy in,
272-274
role of,264-265
simple attributesand,283-284
strong,283-285
subclass, 298
superclass, 298
superclass-subclass
relationship and,296-297
Uni?ed Modeling Language
(UML) and,308-310
user-de?ned, 299
weak,279-281,285-286
EntitySQL, 395
enterpriseresource planning
(ERP) systems,1101
entropymeasure,897-898
equi-joins,549-559,563,566,
571,807,819
equivalence
costanalysisand,601-602
joinordering and,588-589
relationalalgebraand,
582-590
transformationexamplesfor,
586-588
error-correcting code (ECC)
organization,444-445
ERWin, 1194
escape,77
evaluationprimitive,539
everyclause,90
exceptclause,82-83,93,188
exchangesystem,1104
exclusive-modelocks, 661
EXEC SQL,169-173
execute,147
existencebitmap,526-527
existsclause,93
extensibilitycontracts,
1256-1258
externallanguageroutines,
179-180
externalsort-mergealgorithm,
548-549
Facebook,31,862
factorials,639
facttables,891-892
fail-stopassumption, 722
falsecycles,846-847
falsedrops, 929
falsenegatives,903,929-930
falsepositives,903,929-930
falsevalue,90,208
fanout,487
fetching,21,138,906,1078,1097
advancedSQL and,161,
166-173,176,180,194
applicationdesign and,
389-397,1030,1038
IBMDB2and,1199,1202,
1209,1211,1219
informationretrievaland,
921,929,936(seealso
informationretrieval)
Microsoft SQL Server and,
1241,1251
1324 Index
object-baseddatabasesand,
965,969-972
PostgreSQL and,1137,1146,
1151,1153
storageand,437,439,444(see
also storage)
Web crawlersand,930-931
FibreChannel interface,434,
436
?fthnormalforms,360
?leheader,454
?lemanager,21
?leorganization,3-4.Seealso
storage
B+-treesand,500-502
blobs,138,166,457,502,1013,
1198-1199,1259
block-accesstime and,438
clobs, 138,166,457,502,
1010-1013,1196-1199
?xed-length recordsand,
452-454
hashing, 457
heap?le,457
indexing and,475(seealso
indices)
journaling systemsand,439
multitableclusteringand,458,
460-462
pointersand,454
security and,5-6(seealso
security)
sequential,457-459
structured, 451-468
system structureand,451-452
variable-lengthrecords and,
454-457
?lescan, 541-544,550,552,570
?nal/not ?nalexpressions, 949,
953
?ne-granularityparallelism,
771
FireWire interface,434
?rst committerwins,692-693
?rst updater wins,693
?ashstorage,403
B+-treesand,506
cost of,439
erasespeedand,440
hybrid disk drives and,
440-441
NAND,430,439-440
NOR, 430,439
?ashtranslationlayer,440
?oppy disks, 430
?ow-distinct, 1240-1241
FLWOR (for,let,where, order
by, return)expressions,
1002-1003
forcedoutput,465,725-726
force policy,739-740
foreachrowclause,181-184
foreachstatementclause,68,
183
foreignkeys,46,61-62,131-133
fourthnormalforms, 356,
358-360
fragmentation,827-829
freespace control record
(FSCR),1202
from statement
aggregatefunctionsand,
84-90
basicSQL queriesand,63-74
onmultiple relations,66-71
naturaljoinand,71-74
null valuesand,83-84
renameoperationand,74-75
setoperationsand,79-83
onsingle relation,63-66
stringoperationsand,76-79
subqueriesand,95-96
fullouterjoins,117-120,
233-234,565-566
functional dependencies,18,
129
attributesetclosure and,
340-342
augmentationrule and,339
BCNF algorithm and,349-352
Boyce-Coddnormalform
and,333-336
canonicalcover and,342-345
closure ofaset,338-340
decomposition rule,339
dependencypreservation
and,334-336,346-348
extraneous,342
higher normalformsand,
337-338
keysand,330-333
lossless decomposition and,
345-346
multivalued, 355-360
pseudotransitivity rule,339
re?exivity ruleand,339
theoryof,338-348
third normalformand,
336-337,352-355
transitivity ruleand,339
unionrule,339
functionallydetermined
attributes,340-342
function-based indices,
1167-1168
functions. Seealso speci?c
function
declaring,174-175
externallanguageroutines
and,179-180
IBMDB2and,1197-1198
languageconstructsfor,
176-179
polymorphic,1128-1129
PostgreSQL,1133-1135
statetransition,1134
syntaxand,173-174,178
writingin SQL, 173-180
XMLand,1006-1007
fuzzycheckpoints, 742-743,
750-752
generalization
aggregationand,301-302
attributeinheritanceand,
298-299
bottom-updesign and,297
condition-de?ned, 299
constraintson,299-301
disjoint,300
entity-relationship(E-R)
modeland,297-304
overlapping,300
partial,300
representationof,302-304
subclass setand, 298
superclasssetand,298
top-downdesign and,297
total,300
user-de?ned,299
GeneralizedInvertedIndex
(GIN), 1149
generalized-projection,235
GeneralizedSearchTree
(GiST),1148-1149
geographicdata,1061
applicationsof,1068
informationsystemsand,
1065
rasterdataand,1069
representationsof,1065-1066,
1069-1070
spatialqueriesand,1070-1071
vectordataand,1069
Index 1325
getColumnCount method,
164-165
getConnectionmethod,160
GET method,405
getStringmethod,161
Gini measure,897
Glass?sh, 386
globalclassidenti?er,1055
globalcompanyidenti?er,1055
GlobalPositioning System
(GPS), 1068
globalproduct identi?er,1055
globalwait-forgraph, 845-847
Google,31
application design and,
378-382,396,407
distributed databasesand,
862,866-867
informationretrievaland,933
(seealsoinformation
retrieval)
PageRankand,922-925
grant,144-150
grantedby current role,150
graph-basedprotocols, 671-674
Greenplum,816
groupby, 86-89,96,194,203,
206-209
growingphase,667-669
hackers. Seesecurity
handoff, 1081
harddisks, 29-30
hardware RAID,448
hardware tuning,1035-1038
harmonicmean,1046
hashcluster access,1173
hashfunctions, 457-458,476,
530-531
closed, 513
datastructure and,515-516
deletionand,510,513,516,
523-524
dynamic, 515-523
extendable,515
indices and,514-515,523-524
insertionand,513,516-524
insuf?cient bucketsand,512
joinsand,809-810
lookupand,516-518,522,524
open,513
Oracle and,1170
over?owsand,512-513
partitioning and,807
PostgreSQL and,1148
queriesand,516-522
skew and,512
static, 509-515,522-523
updatesand,516-522
hashjoin,602
basics of,558-559
build inputand, 558
cost of,561-562
double-pipelined,571-572
hybrid, 562
over?owsand,560
queryprocessing and,557-562
recursive partitioning and,
539-540
skewedpartitioning and,560
hash-tableover?ow, 560
having,88-89,96
heap?le,457,523,1147-1149,
1153
heuristics, 1075-1076
dataanalysis and,899,910
distributeddatabasesand,859
greedy, 910
IBM DB2and,1211
informationretrievaland,934
Microsoft SQL Serverand,
1240
Oracle and,1176
paralleldatabasesand,815
queryoptimizationand,
598-605,615-616
Hibernatesystem,393-395
hierarchicalarchitecture,781,
784
hierarchicalclassi?cation,
935-937
hierarchicalclustering, 907-908
hierarchicaldatamodel,9
highavailability,756
HIPAA, 1248
histograms,195,591-596,616,
801,901,1152,1175,1211,
1239
HITSalgorithm,925
homeprocessor, 803
homonyms, 925-927
horizontalfragmentation,
827-828
horizontalpartitioning,798
hot-spare con?guration, 758
hot swapping, 449
householding, 891
HP-UX,1193
hubs, 924
hybriddiskdrives,440-441
hybridhashjoin,562
hybridmergejoin,557
hybrid OLAP (HOLAP), 204
hypercube architecture,781
hyperlinks
PageRankand,922-923,925
popularityrankingand,
920-922
searchenginespamming and,
924-925
HyperTextMarkup Language
(HTML), 378-380
client-side scripting and,
388-391
DataGridand,398
embedded,397
informationretrievaland,915
(seealso Information
retrieval)
JavaServerPages(JSP) and,
387-391
rapidapplication
development(RAD)and,
397
security and,402-417
server-side scripting and,
386-388
stylesheetsand,380
Webapplicationframeworks
and,398-400
websessionsand,380-382
XMLand,981
HyperTextTransferProtocol
(HTTP)
applicationdesign and,
377-381,383,395,
404-406,417
asconnectionless,381
digitalcerti?catesand,417
man-in-the-middleattacks
and,406
RepresentationStateTransfer
(REST)and,395
Simple ObjectAccess Protocol
(SOAP) and,1017-1018,
1249-1250
IBMAIX,1193
IBMDB2,30,96,141,160n3,
172,180,184-185,216,
1121
administrative tools,
1215-1216
autonomicfeatures,1214-1215
bufferpoolsand,1201-1202
1326 Index
business intelligence features,
1221-1222
concurrency controland,
1200-1203,1217-1218
constraintsand,1199
ControlCenter,1195,1215
database-designtoolsand,
1194-1195
datatypesupportand,
1196-1197
DataWarehouseEdition, 1221
developmentof,1193-1193
distribution, 1220-1221
externaldataand,1220-1221
indexing and,1199-1205
isolationand,1217-1218
joinsand,1209-1210
large objectsand,1198-1199
locks and,1217-1220
massively parallel processors
(MPP) and,1193
materializedviewsand,
1212-1214
multidimensional clustering
and,1203-1207
queryprocessing and,593,
604,612,1207-1216
recovery and,1200-1203
replication, 1220-1221
rollback and,1218
setoperationsand,1209-1210
SQLvariationsand,1195-1200
storageand,1200-1203
systemarchitecture,1219-1220
System Rand,1193
Universal DatabaseServer,
1193-1194
user-de?nedfunctionsand,
1197-1198
utilities, 1215-1216
Web services, 1199-1200
XMLand,1195-1196
IBMMVS,1193
IBMOS/400,1193-1194
IBMVM,1193-1194
IBMz/OS,1194
identi?ers,546
global,1055
OrdPath, 1260-1261
standards and,1055-1056
tagsand,982-985
identifyingrelationship,280
identitydeclaration,1043
ifclause,184
IllustraInformation
Technologies, 1123-1124
immediate-modi?cation
technique, 729
incompleteness,262
inconsistent state,630.Seealso
consistency
inconstruct, 91,92n8
incrementalviewmaintenance,
608-611
independent parallelism,814
indexednested-loopjoin,
552-553
indexentry,477
indexingstrings, 502-503
index-organizedtables(IOTs),
1164-1165
indexrecord, 477
indices,21,137-138,530-531
access timeand,476,479,523
access typesand,476
bitmap,507,509,524-528,531,
536,1166-1167
block,1205
bulkloadingof,503-504
clustering, 476-477,483-485,
542
comparisonsand,544-545
composite,545-546
concurrency controland,
704-708
constructionof,1150-1151
covering, 509
de?nitioninSQLand,528-529
deletiontimeand,476,483,
491,495-501,523-524
dense,477-483
domain,1168-1169
onexpressions, 1149
function-based,1167-1168
GeneralizedInvertedIndex
(GIN) and,1149
hashed,476(seealso hash
functions)
IBM DB2and,1199-1205
identi?ers and,546
informationretrievaland,
927-929
insertion timeand,476,
482-483,491-495,499-501,
523-524
inverted,927-929
join,1168(seealso joins)
linearsearchand,541-542
logicalrow-idsand,1164-1165
materializedviewsand,612
Microsoft SQL Serverand,
1231-1236
multicolumn,1149
multilevel, 480-482
multiple-keyaccessand,485,
506-509
nonclustering,477
operatorclasses and,1150
Oracleand,1162-1173
ordered,475-485,523-524
partial,1150
partitionsand,1169-1171
performancetuning and,
1039-1041
persistentprogramming
languagesand,964-972
pointersand,546
PostgreSQLand,1135-1136,
1146-1151
primary,476-477,542,544
queryprocessing and,541-544
recordrelocation and,502
searchkeyand,476
secondary,477,483-485,502,
542,544-545
selectionoperationand,
541-544
sequential,485-486
sortingand,547-549
spaceoverheadand,476,479,
486,522
sparse,477-480,482-483
spatialdataand,1071-1076
supportroutinesand,
1135-1136
treesand,1148-1149(seealso
trees)
unique,1149
updatesand,482-483
XML,1160
information-extraction
systems, 932-933
informationgain,897-898
informationretrieval,25-26,
885,938
adjacencytestand,922-923
applicationsof,915-917,
931-935
categoriesand,935-937
de?ned,915
developmentof?eld,915
directoriesand,935-937
falsenegativesand,929-930
falsepositivesand,929-930
Index 1327
homonymsand,925-927
indexing ofdocumentsand,
927-929
informationextractionand,
932-933
keywordsand,916-927
measuring effectivenessof,
929-930
ontologiesand,925-927
PageRankand,922-923,925
precision and,929-930
queryresultdiversityand,932
questionanswering and,
933-934
recall and,929-930
relevance ranking using
terms,917-920
relevance using hyperlinks,
920-925
result diversity and, 932
search enginespamming and,
924-925
similarity-based, 919-920
stopwordsand,918
structured dataqueriesand,
934-935
synonymsand,925-927
TF-IDFapproachand,917-925
Web crawlersand,930-931
Ingres,30
inheritance,298-299
overriding method,952
SQL and,949-956
structuredtypesand,949-952
tablesand,954-956
typesand,952-953
initiallydeferredintegrity
constraints, 134
innerjoins,117-120,601
innerrelation,550
insertion,61,100-102
concurrency controland,
697-701
EXEC SQL and,171
hashingand,513,516-523
lookupand,705
phantomphenomenonand,
698-701
PostgreSQL and,1130-1131
preparedstatementsand,
162-164
privileges and,143-145
transactionsand,629,653
viewsand,124-125
instances,8,904-905
insteadoftriggers,1161-1162
integrateddevelopment
environment (IDE),111,
307,386,397,426,434,932
integrityconstraints,4,58
add, 129
altertable,129
assertion,135-136
checkclause, 130,134-136
createtable,129,130
deferred,134
examplesof,128
foreignkey,131-133
functionaldependenciesand,
129
hashing and,809-810
notnull, 129-130,133
primarykey,130-131
referential,11,46-47,131-136,
151,181-182,628
schema diagramsand,46-47
onsingle relation,129
unique,130-131
user-de?nedtypesand,140
violation during transaction,
133-134
XMLand,1003-1004
integritymanager,21
intention-exclusive(IX)mode,
680
intention-shard(IS)mode,680
interconnectionnetworks,
780-781
interestingsort order,601
InterfaceDescriptionLanguage
(IDL), 1054-1055
interference,780
internalnodes,487
InternationalOrganizationfor
Standardization(ISO),
57,871,1051
Internet,31
direct useraccess and, 2
wireless, 1081-1082
interoperationparallelism,804,
813-814
interqueryparallelism,802-803
intersect,81-82,585
intersectall,81-82
intersection,50
intersectionset,960
intervals,1063-1064
intraoperationparallelism
aggregationand,811
degree ofparallelism and,804
duplicateeliminationand,811
operationevaluationcosts
and,812
parallelexternalsort-merge
and,806
paralleljoinand,806-811
parallelsortand,805-806
projectionand,811
range-partitioningsortand,
805
selectionand, 811
intraqueryparallelism,803-804
invalidationreports,1083
inverse document frequency
(IDF),918
I/Oparallelism
hashingand,799-800
partitioningtechniquesand,
798-800
rangeschemeand,800
round-robin scheme and, 799
skewhandling and,800-802
isnotnull,83
is notunknown, 84
isnull,83
isolation,4,1094
atomicityand,646-648
cascadeless schedules and,
647-648
concurrency controland,631,
636-637,639,650,
1137-1138
de?ned,628
dirty readand,1137
distributedtransactionsand,
830-832
factorialsand,639
improvedthroughputand,
635-636
inconsistentstateand,631
levelsof,648-653
locking and, 651
multiple versions and,
652-653,1137-1138
nonrepeatablereadand,1137
Oracle and,1181-1182
phantomreadand,1137-1138
PostgreSQLand,1137-1138,
1142
readcommitted,649,1042
readuncommitted,648,649
recoverableschedules and,
647
repeatedread,649
1328 Index
resource allocation and,
635-636
rowversioning and,1244
serializability and,640,
648-653
snapshot,652-653,692-697,
704,729-730,1042,1242,
1244
timestampsand,651-652
transactionsand,628,635-640,
646-653
utilizationand,636
waittimeand,636
isunknown, 84
itemshipping, 776
iteration,176,188-190
J++,1228,1253
JakartaProject,386
.jar?les,160
Java,14,157,169,173,387,945
DOMAPI, 1008-1009
JDBCand,158-166
metadataand,164-166
persistentsystemsand,
971-972
SQLJ and,172
Uni?ed Modeling Language
(UML) and,308
Java2Enterprise Edition
(J2EE),386,1157-1158
JavaDatabaseObjects (JDO),
971
JavaScript
application design and,
389-391,398
RepresentationStateTransfer
(REST)and,395
security and,402-411
JavaScript ObjectNotation
(JSON), 395,863-864
JavaServerFaces(JSF)
framework, 397
JavaServerPages(JSP)
application design and,377,
383-391
client-side scripting and,
389-391
security and,405
server-side scripting and,
386-388
servletsand,383-391
Web applicationframeworks
and,399
JBoss,386,399
JDBC (JavaDatabase
Connectivity),380,1052,
1154
advancedSQL and,158-159
blobcolumn,166
caching and,400-401
callable statementsand,164
clob column, 166
connectingtodatabase,
159-161
E-R modeland,269,275
informationprotocolof,
160-161
metadatafeaturesand,
164-166
preparedstatementsand,
162-164
queryresult retrievaland,
161-162
shipping SQLstatementsto,
161
updatableresult setsand,166
joindependencies, 360
joins
complex,563
conditions and,114-115
cost analysisand,555-557,
599-601
distributed processing and,
855-857
equi-joins,549-559,563,566,
571,807,819
?ltering of,1187
fragment-and-replicate,
808-809
full outer,117-120,233-234,
565-566
hashjoin,539-540,557-562,
571-572,602
hybridmerge,557
IBM DB2and,1209-1210
inner,117-120,601
innerrelationand,550
leftouter,116-120,233-235,
565-566
lossless decomposition and,
345-346
merge-join,553-555
minimizationand,613
natural,71-74,87,113(see
also naturaljoins)
nested-loop,550-553(seealso
nested-loopjoin)
Oracle and,1168,1187
ordering and,588-589
outer,115-120,232-235,
565-566,597
outerrelation,550
parallel,806-811,857
partitioned,539-540,807-810
PostgreSQLand,1153
prediction,1267
queryprocessing and,
549-566,855-857
relationalalgebraand,
229-232,239
rightouter,117-120,233-235,
565-566
semijoinstrategyand,856-857
sizeestimationand,595-596
sort-merge-join,553
theta,584-585
typesand,115-120
viewmaintenanceand,609
joinusing, 74,113-114
journaling ?lesystems,439
JPEG (JointPicture Experts
Group), 1077
jukebox systems,431
k-dtrees,1071-1072
kernelfunctions, 901-902
keys,45-46
constraintsand,271-272
decompositionand,354-355
encryptionand,412-418
entity-relationship(E-R)
modeland,271-272
equalityon,542
functionaldependenciesand,
330-333
hashingand,509-519,524
indexing and,476-508,
476-509,524,529
multiple accessand,506-509
nonunique,497-499
smartcardsand,415-416
storageand,457-459
USB,430
uniqui?ersand,498-499
keywords
complexdatatypesand,
947-949
homonymsand,925-927
indicesand,927-929
ontologiesand,925-927
PostgreSQLand,1130-1131
querysimpli?cation and,
1237-1238
rankingand,915-925
Index 1329
search enginespamming and,
924-925
stopwordsand,918
synonymsand,925-927
languageconstructs, 176-179
Language IntegratedQuerying
(LINQ), 1055,1249
large-objecttypes,138
latentfailure,448
lazypropagation,844,868
lazywriter,1246
LDAP DataInterchange
Format(LDIF), 872
leastrecentlyused(LRU)
scheme,465-467
leftouterjoin,116-120,233-235,
565-566
legacysystems,1050-1051
lightweightdirectory access
protocol (LDAP), 406,
871-875
like,76-77
linearregression,902
linearsearch,541-542
linearspeedup, 778-780
Linux,1124,1193-1194,1212
local-areanetworks (LANs),
788-789,1081
localtimestamp,137
localwait-forgraph, 845
location-dependentqueries,
1080
locking protocols, 666
biased,841
distributed lockmanager,840
graph-based,671-674
majority,840-841
primarycopy,840
quorumconsensus,841-842
single lock-manager,839
timestamping,842-843
two-phase,667-669
lockmanager,670-671,773
locks
adaptive granularityand,776
caching and,776
call backand,776
compatibility function and,
662
concurrency controland,
661-674
deadlock and,665-666,
674-679,839,841,844-847,
1217-1220,1243-1246
distributed databasesand,
839-847
dynamic, 1243
exclusive, 651,661-662,
668-669,672-673,679,691,
698-702,706-710,729-730,
740-741,803,839,841
false cyclesand,846-847
?ne-grained,756
grantingof,666-667
growing phaseand,667-669
IBM DB2and,1217-1220
implementationof,670-671
intentionmodesand,680
logical undooperationsand,
744-750
long-duration transactions
and,1110-1111
lower/higherlevel, 745
Microsoft SQL Serverand,
1242-1244,1246
multiple granularity and,
679-682
multiversion schemesand,
691-692
PostgreSQL and,1143-1145
recoverysystemsand,744-750
requestoperationand,
662-671,675-680,709
shared,661,841
shrinking phaseand,667-669
starvation and,679
timestampsand,682-686
transactionserversand,
773-775
truematrixvalue and,662
wait-forgraphand,676-678,
845-847
logdisk, 438-439
logicalclock, 843
logicalcounter,682
logical-designphase,16,
260-261
logicalerror,721
logicallogging,745-746,1115
logicaloperations
consistency and,746
earlylockreleaseand,744-750
rollback and,746-749
undo logrecords, 745-750
logicalrow-ids, 1164-1165
logicalundooperation,745-750
logrecords
ARIES and,750-756
bufferingand,738-739
compensationlogrecords
(CLRs) and,751-752,754
identi?ersand,727
old/newvaluesand,727-728
physical, 745
recoverysystems and,
726-728,730-734
redoand,729-734
steal/no-stealpolicyand,740
undo,729-734,745-746
write-aheadlogging (WAL)
rule and,739-741
logsequence number (LSN),
750-755
logwriterprocess, 773-774
long-durationtransactions
compensationtransactions
and,1113-1114
concurrency controland,
1111-1112
graph-basedprotocolsand,
1110
implementationissues,
1114-1115
multilevel, 1111-1112
nestingand,1111-1112
nonserializable executions
and,1110-1111
operationlogging and,1115
performanceand,1110
recoverabilityand,1110
subtasksand,1109
timestampsand,1110
two-phaselocking and,1110
uncommitteddataand,1109
lookup, 600,1086
concurrency controland,700,
704-708
distributeddatabasesand,
865,867,870
fuzzy,890,1266
indicesand,482,485-500,
505-513,516-518,522,524
Microsoft SQL Server and,
1238,1241,1266
PostgreSQLand,1148
queryprocessing and,544,
552-553
lossless-joindecomposition,
345-346
lossy decomposition, 345-346
lostupdate,692
machinelearning,25-26
1330 Index
magneticdisks, 430
blocksand,436-439
bufferingand,437-438
checksums and,434
crashesand,434
data-transferrateand,435-436
disk controller and,434
failure classi?cation and,722
hybrid, 440-441
logdisk and,438-439
meantimetofailure and,436
optimizationofdisk-block
accessand,436-439
parallelsystemsand,781-782
performancemeasuresof,
435-436
physical characteristics of,
432-435
read-aheadand,437
read-writeheadsand,432-435
recording density and,
433-434
redundant arraysof
independentdisks
(RAID)and,441-449
scheduling and,437
scrubbing and,448
sectorsand,432-434
seek-timeand,435-436
sizesof,433
main-memorydatabase
systems, 724n1
majorityprotocol,840-841,
848-849
man-in-the-middleattacks,406
manyserver,many-router
model,1094
many-server,single-router
model,1093
many-to-manymapping,270,
276-277
many-to-onemapping,270,276
mappingcardinalities,269-270,
276-277
markuplanguages.Seealso
speci?c language
?le processing and,981-982
structure of,981-990
tagsand,982-985
transactionsand,983-985
master-slavereplication,
843-844
mastertable,1032-1033
materialization,567-568
materializedquerytables
(MQTs), 1212-1214,1221
materializedviews,123-124,
607
aggregationand,610-611
IBM DB2and,1212-1214
indexselectionand,612
joinoperationand,609
maintenanceand,608-611
Oracle and,1171-1172,1174,
1188
performancetuning and,
1039-1040
projectionand,609-610
queryoptimizationand,
611-612
replicationand,1251-1253
selection and,609-610
max,84,86,96,236,566-567
meantimetofailure(MTTF),
436
MediaAccess Control(MAC),
1129
mediators,859-860,1018-1019
memory.Seealsostorage
buffersand,1184(seealso
buffers)
bulk loading ofindices and,
503-504
cache,429,817-818(seealso
caching)
dataaccessand,724-726
?ash,403,430,439-441,506
forceoutputand,725-726
magnetic-disk, 430,432-439
main,429-430
main-memorydatabasesand,
1105-1108
Microsoft SQL Serverand,
1246-1247
multitasking and,1092-1095
.NET CommonLanguage
Runtime(CLR)and,
1255-1256
nonvolatile random-access,
438
optical,430
Oracle structures and,
1183-1184
over?owsand,560
persistentprogramming
languagesand,964-972
querycostsand,544
recoverysystemsand,724-726
(seealso Recovery
systems)
redologbufferand,1184
sharedpool,1184
merge-join,553
merge-purgeoperation,890-891
merging
complex,1173-1174
duplicate elimination and,
563-564
Oracleand,1173-1174
parallelexternalsort-merge
and,806
performancetuning and,1033
queryprocessing and,
547-549,553-555,557
meshsystem,780-781
messagedeliveryprocess, 838
metadata,12,164-166
Microsoft, 3,31,141
advancedSQL and,160n3,
169,173,180,184,197,205
applicationdesign and,387,
395-401,406-407
distributeddatabasesand,863
paralleldatabasesand,816
queryoptimizationand,612
storageand,438
Microsoft ActiveServerPages
(ASP), 397
Microsoft DatabaseTuning
Assistant, 1040
Microsoft Distributed
TransactionCoordinator
(MSDTC),1242
Microsoft Of?ce, 55,399,1016
Microsoft SQLServer,1042,
1121
business intelligence and,
1263-1267
compilationand,1236-1237
compressionand,1236
concurrency controland,
1241-1246
dataaccessand,1248-1250
databasemirroring and,
1245-1246
datamining and,1266-1267
datatypesand,1229-1230
design toolsand,1223-1228
developmentof,1223
?legroupsand,1233-1234
indexing and,1231-1236
locksand,1242-1244
Index 1331
managementtoolsand,
1223-1228
memorymanagementand,
1246-1247
pageunitsand,1233-1234
partitionsand,1235
Query Editor,1224-1225
queryprocessing and,
1223-1231,1236-1241,
1250-1251
read-aheadand,1235-1236
recovery and,1241-1246
reorderingand,1238-1239
replicationand,1251-1253
routinesand,1231
security and,1247-1248
server programmingin .NET,
1253-1258
snapshotisolationand,1242,
1244
SQL Pro?ler and,1225-1227
SQL ServerBroker and,
1261-1263
SQL ServerManagement
Studio and,1223-1224,
1227-1228
SQLvariationsand,1228-1233
storageand,1233-1236
systemarchitecture,1246-1248
tablesand,1234
threadpoolingand,1246
triggersand,1232-1233
tuning and,1224,1227
typesand,1257-1258
updatesand,1232-1233,1239
WindowsMobile and,1223
XMLsupportand,1258-1261
Microsoft TransactionServer,
1091
Microsoft Windows, 195,426,
1078
IBM DB2and,1193-1194,1212
PostgreSQL and,1124,1155
SQL Serverand,1223-1224,
1228,1242,1246-1248
storageand,438
min,84,86,236,566-567
minpctused, 1203
minus,82n7
mirroring,441-442,444,
1245-1246
mobility,1062,1086
broadcast dataand,1082-1083
consistency and,1083-1085
disconnectivity and,
1083-1085
handoffand,1081
invalidation reportsand,1083
mobile computermodeland,
1080-1082
queriesand,1082
recoverability and,1083
routing and,1082
updatesand,1083-1084
version-numberingschemes
and,1083-1084
wireless communications
and,1080-1082
Model-View-Controldesign,
1157-1158
mostrecently used(MRU)
scheme,467
most-speci?ctype,953
MPEG(MovingPictureExperts
Group), 1077-1078
multicoreprocessors, 817-819
multidatabasesystem,857-861
multidimensionaldata,199
multimasterreplication,844
multimediadata,1062
multimediadatabases,
1076-1079
multiplegranularity
concurrency controland,
679-682
hierarchyde?nitionfor,679
intention-exclusive (IX)mode
and,680
intention-shared(IS) mode
and,680
locking protocoland,681-682
sharedand
intention-exclusive (SIX)
modeand,680
treearchitecture and,679-682
multiple-keyaccess,506-509
multiqueryoptimization,614
multisetrelationalalgebra,238
multisettypes,956-961
multisystemapplications,1096
multitablecluster?le
organization,458,
460-462
multitasking,771,1092-1095
multithreading,817-818,1093
multivaluedattributes,267-268,
327-329
multivalueddependencies,
355-360
multiversionconcurrency
control (MVCC)
DDLcommandsand,
1144-1145
DMLcommandsand,
1138-1139
implementationof,1139-1143
implications ofusing,
1143-1144
indicesand,1145
isolationlevelsand,1137-1138
locksand,1145
recoveryand,1145-1146
schemafor,689-692
multiwaysplits,898
MySQL, 31,76,111,160n3,
1123,1155
Na¨ ?veBayesianclassi?ers,901,
1191,1266
na¨ ?veusers, 27-28
nameservers,829
NAND ?ashmemory,430,
440-441
naturaljoins,49-50,87,113
conditionsand,114-115
fullouter,117-120,233-234,
565-566
inner,117-120,601
leftouter,116-120,233-235,
565
onconditionand,114-115
outer,115-120
SQLqueriesand,71-74
relationalalgebraand,
229-232
rightouter,117-120,233-234,
565-566
typesand,115-120
nearest-neighbor query,
1070-1071
negation,595
nested-loopjoin,1071
IBMDB2and,1209-1210
Oracle and,1173
parallel,807,810-811
PostgreSQLand,1152-1153
queryoptimizationand,
600n2,602,604
queryprocessing and,
550-555,558-560,565,
571,573
nestedsubqueries
applicationdevelopmentand,
1031,1047
1332 Index
duplicate tuplesand,94-95
emptyrelationsand,93-94
fromclause and,95-96
optimizationof,605-607
scalar, 97-98
setoperationsand,90-93
with clause and,97
nesting
ARIES and,755-756
concurrency controland,679,
709
granularities and,679
IBM DB2and,1197,
1209-1210,1218
long-duration transactions
and,1112-1113
object-baseddatabasesand,
945,948,958-961
Oracle and,1159,1164,1182
queriesand,601-607,
1004-1007,1013-1014,
1017
transactionsand,1091,
1112-1113,1116,1218
XMLand,27,943,984-998,
1001,1004-1007,1010
.NET, 169
NetBeans,386,397
.NET CommonLanguage
Runtime(CLR)
aggregatesand,1257-1258
basicconceptsof,1254
extensibility contractsand,
1256-1258
Microsoft SQL Serverand,
1253-1258
routinesand,1256-1257
SQL hosting and,1254-1256
tablefunctionsand,1256-1257
typesand,1257-1258
Netezza,816
networks
datamodeland,9,1080-1082
localarea,788-789,1081
mobility and,1079-1085
wide-areatypesand,788,
790-791
nextvalfor,1043
nodes.See alsostorage
B+-treesand,485-506
coalescing, 491,706
distributed systemsand,784
IBM DB2and,1200-1201
mesh architectureand,
780-781
multiple granularity and,
679-682
nonleaf,487
over?tting and,899-900
splitting of,491,706
updatesand,491-500
XMLand,998
no-force policy,739-740
nonacceptable termination
states,1099
nonclustering, 477
nondeclarativeactions,158
nonleafnodes, 487
nonprocedural languages,
47-48
nonrepeatableread,1137
nonrepudiation, 416
nonuniquesearchkeys,497-499
nonvolatilerandom-access
memory(NVRAM), 438
nonvolatilestorage,432,632,
722,724-726,743-744
nonvolatilewritebuffers, 438
NOR ?ashmemory,430,439
normalforms, 18
atomicdomainsand,327-329
Boyce-Codd,333-336,349-352,
354-356
complexdatatypesand,947
domain-key,360
?fth,360
?rst, 327-329
fourth,356,358-360
higher,337-338
join dependenciesand,360
project-join,360
second,361
third, 336-337
normalization,16,18-20
denormalizationand,363-364
entity-relationship (E-R)
modeland,361-362
performanceand,363-364
relationaldatabasedesign
and,361-362
no-stealpolicy,740
not connective,66
not exists,93,192
not in,90-91,92n8
notnull,61,83,129-130,133,140
not unique,95
null bitmap,456
null values,19,83-84
aggregationwith,89-90
attributesand,268-269
decodeand,208
decorrelationand,1174
?leorganizationand,451-468
integrityconstraints and,
128-130,133-134
leftouterjoin,233
OLAP and,202
querysimpli?cation and,
1237-1238
rightouterjoin,234-235
temporaldataand,364-367
user-de?nedtypesand,140
numeric,59,62
nvarchar,60
N-waymerge,547
object-baseddatabases,975
arraytypesand,956-961
collectionvolumes and,
957-958
complexdatatypesand,
946-949
correspondenceand,955
featureimplementationand,
963-964
inheritanceand,949-956
mappingand,973
multisettypesand,956-961
nestingand,945,948,958-961
object-identitytypesand,
961-963
object-orientedvs.
object-relational
approachesand,973-974
persistentprogramming
languagesand,964-972,
974
referencetypesand,961-963
relationaldatamodeland,945
structuredtypesand,949-953
unnestingand,958-961
Object DatabaseManagement
Group (ODMG),
1054-1055
object-orienteddatabases,393
object-orienteddatamodel,27
object-relationaldatamodel,27
object-relationalmapping,393,
946,973
observable externalwrites,
634-635
ODBC (Open Database
Connectivity),380,1052
advancedSQL and,166-169
APIde?nition and,166-167
Index 1333
caching and,401
conformancelevelsand,
168-169
Microsoft SQL Serverand,
1249
PostgreSQL and,1154
standards for,1053-1054
typede?nition and,168
OLAP (onlineanalytical
processing), 1046
all attributeand,201-203,205
applicationsfor,197-201
cross-tabulation and,199-203,
205,210
datacubeand,200,206-210
decode functionand,208
dicing and,201
drill downand,201
implementationof,204
Microsoft SQL Serverand,
1223,1266
multidimensional dataand,
199
null value and,202
Oracle and,1161
order byclause and,205
pivot clause and,205,210
relationaltablesand,202-203,
205
rollup and,201,206-210
slicing and, 201
in SQL, 205-209
OLE-DB, 1249
OLTP(onlinetransaction
processing), 1046-1047,
1165,1186,1264
oncondition,114-115
ondeletecascade,133,185
one-to-manymapping,269,276
one-to-onemapping,269,276
ontologies,925-927
OOXML (Of?ce Open XML),
1016
Open Document Format
(ODF), 1016
openstatement,170-171
operationlogging,1115
operatortree,803-804
opticalstorage,430-431,449-450
optimisticconcurrency control
without readvalidation,
704
Oracle,3,30,216,1121
access pathselection and,
1174
analyticworkspacesand,1161
archiver and,1185
caching and,1179-1180,1184
checkpoint and,1185
clusters and,1173,1186
compression and,1165
concurrency controland,
1180-1183
databaseadministrationtools
and,1189-1191
databasedesignand,355,386,
396,408n5,409,1157-1158
databasewriter,1185
dataguard, 1183
datamining and,1191
datawarehousing and,1158
dedicatedserversand,
1183-1185
dimensional modeling and,
1160,1171
distribution and,1188-1189
encryptionand,1165-1166
Exadataand,1187-1188
externaldataand,1188-1189
hashing and,1170
indices and,1162-1173
isolationlevelsand,1181-1182
joinsand,1168,1187
logicalrow-idsand,1164-1165
logwriterand,1185
materializedviewsand,
1171-1172,1174,1188
memorystructuresand,
1183-1184
optimizerof,1174-1176
parallelexecutionand,1178
partitionsand,1169-1172,
1176
processmonitorand,1185
processstructuresand,
1184-1185
projectionand,1187
queryoptimizationand,582,
593,603-604,612,
1173-1178
queryprocessing and,
1157-1158,1162-1172
RealApplication Clusters
(RAC)and,1186
recoveryand,1180-1183,1185
replicationand,1188-1189
result caching and,1179-1180
security and,1165-1166
segmentsand,1163
serializability and,1181-1182
sharedserver and,1185
asSoftwareDevelopment
Laboratories,1157
SQLbasicsand,55,75n4,
82n7,96,141,160-161,
172-174,178,180,
184-185,197,205
SQLLoaderand,1189
SQLPlan Managementand,
1177-1178
SQL TuningAdvisor,
1176-1177
SQLvariationsand,1158-1162
subquery?atteningand,1174
systemarchitecture,795,803,
843,1183-1188
systemmonitor,1185
tablesand,1163-1166,
1172-1173,1187,1189
transactionsand,649,653,
692-693,697,710,718
transformationsand,
1173-1174
treesand,1191
triggersand,1161-1162
updatesand,1179-1180
virtualprivatedatabaseand,
1166
XMLDBand,1159-1160
Oracle Application
Development
Framework(ADF),
1157-1158
Oracle AutomaticStorage
Manager,1186-1187
Oracle AutomaticWorkload
Repository(AWR),1190
Oracle BusinessIntelligence
Suite (OBI),1158
Oracle DatabaseResource
Management,1190-1191
Oracle Designer,1158
Oracle EnterpriseManager
(OEM), 1190
Oracle JDeveloper,1158
Oracle Tuxedo,1091
orconnective,66
orderby,77-78,193
organizeby dimensions,1204
oroperation,83-84
outer-join,115-120,232-235,
565-566,597
outerrelation,550
over?tting,899-900
over?owavoidance,560
1334 Index
over?owbuckets, 512-514
over?owresolution,560
overlappingentitysets,300
overlapping specialization,
296-297
overloading,968
P+Qredundancy schema,446
PageLSN, 751,753,754
PageRank,922-925,928
pageshipping, 776
paralleldatabases
cache memoryand,817-818
cost of,797
decision-support queriesand,
797
failure ratesand,816
increased use of,797
interoperationparallelism
and,813-814
interqueryparallelism and,
802-803
intraoperationparallelism
and,804-812
intraqueryparallelism and,
803-804
I/Oparallelism and,798-802
massively parallel processors
(MPP) and,1193
multicore processors and,
817-819
multithreading and,817-818
operatortreeand,803-804
Oracle and,1178-1179
partitioning techniquesand,
798-799
pipelinesand,814-815
queryoptimizationand,
814-817
rawspeedand,817
skewand,800-801,805-808,
812,814,819
success of,797
system design and,815-817
parallelexternalsort-merge,
806
parallelism,442-444
paralleljoins,806,857
fragment-and-replicate,
808-809
hash,809-810
nested-loop,810-811
partitioned,807-810
parallelprocessing, 401-402
parallelsystems
coarse-grain,777
?ne-grain,777
hierarchical, 781
interconnection networks
and,780-781
interferenceand,780
massively parallel,777-778
scaleup and,778-780
shareddisk, 781
sharedmemory,781-783
sharednothing,781
skewand,780
speedupand,778-780
start-upcostsand,780
throughputand,778
parameterstyle general,179
parametricqueryoptimization,
615
paritybits,444-446
parsing
application design and,388
bulkloadsand,1031-1033
queryprocessing and,
537-539,572,1236-1237
participationconstraints, 270
partitioningvector,798-799
partitions
attributesand,896-897
availability and,847-853
balancedrange,801
classi?ers and,896-897
cloudcomputingand,865-866
composite,1170-1171
condition and,896-897
distributed databasesand,
832,835
hash,798-799,807,1170
joinsand,807-810
list, 1170
Microsoft SQL Serverand,
1235
Oracle and,1169-1172,1176
pointqueriesand,799
pruning and,1176
queryoptimizationand,
814-815
range,798-800,805,1170
reference,1171
round-robin,798-801
scanning arelationand,799
PartnerInterface Processes
(PIPs), 1055
passwords. Seealsosecurity
applicationdesign and,376,
382,385,393,405-407,415
dictionary attacksand,414
distributeddatabasesand,871
leakageof,405
man-in-the-middleattacks
and,406
one-time,406
single sign-onsystem and,
406-407
SQLand,142,160,168,170
storageand,463-464
PATA(parallelATA), 434
pctfree,1203
performance
accesstimeand,431-439,447,
450-451,476,479,523,
540-541,817
applicationdesign and,
400-402
B+-treesand,485-486
caching and,400-401
data-transferrateand,435-436
denormalizationand,363-364
magneticdisk storageand,
435-436
parallelprocessing and,
401-402
responsetime and,400(see
also responsetime)
seektimesand,433,435-439,
450-451,540,555
sequentialindicesand,
485-486
transactiontimeand,365n8,
1062
webapplicationsand,377-382
performance benchmarks
database-applicationclasses
and,1046
suitesoftasks,1045-1046
TransactionProcessing
PerformanceCouncil
(TPC),1046-1048
performance tuning
bottlenecklocationsand,
1033-1035
bulkloadsand,1031-1033
concurrenttransactionsand,
1041-1044
hardwareand,1035-1038
indicesand,1039-1041
materializedviewsand,
1039-1041
Index 1335
parameteradjustmentand,
1029-1030,1035
physical design and,
1040-1041
RAID choice and,1037-1038
ofschema,1038-1039
setorientationand,1030-1031
simulation and,1044-1045
updatesand,1030-1033
Perl,180,387,1154
persistent messaging,836-837
persistent programming
languages,974
approachesfor,966-967
bytecodeenhancementand,
971
C++,968-971
class extentsand,969,972
databasemappingand,971
de?ned,965
iteratorinterfaceand,970
Java,971-972
object-baseddatabasesand,
964-972,974
objectidentityand,967
objectpersistenceand,
966-968
overloading and,968
persistentobjectsand,969
pointersand,967,969,972
reachability and, 971
relationships and,969
singlereferencetypesand,972
transactionsand,970
updatesand,970
person-in-the-middleattacks,
1105
phantom phenomenon,698-701
phantomread,1137-1138,1142,
1217-1218
PHP, 387-388
physical dataindependence,6
physical-designphase,16,261
physiological redo,750
pinnedblocks, 465
pipelining,539,568
demand-driven, 569-570
double-pipelined hash-join
and,571-572
paralleldatabasesand,
813-815
producer-driven, 569-571
pulling dataand,570-571
pivotclause,205,210,1230
plancaching, 605
PL/SQL, 173,178
pointers.Seealso indices
application design and,409
child nodesand,1074
concurrency controland,
706-707
IBM DB2and,1199,1202-1203
informationretrievaland,936
main-memorydatabasesand,
1107
multimedia databasesand,
1077
Oracle and,1165
persistentprogramming
languagesand,967,969,
972
PostgreSQL and,1134,
1147-1148
queryoptimizationand,612
queryprocessing and,
544-546,554
recovery systemsand,727,
754
SQL basicsand,166,179-180
storageand,439,452-462
point queries,799
polymorphictypes,1128-1129
popularityranking,920-925
PostgreSQL, 31,1121
access methodsand,1153
aggregationand,1153
command-line editing and,
1124
concurrency controland,692,
697,701,1137-1145
constraintsand,1130-1131,
1153-1154
DMLcommandsand,
1138-1139
extensibility, 1132
functions, 1133-1135
GeneralizedInvertedIndex
(GIN) and,1149
GeneralizedSearch Tree
(GiST) and,1148-1149
hashing and,1148
indices and,1135-1136,
1146-1151
isolation levels and,
1137-1138,1142
joinsand,1153
locks and,1143-1145
majorreleasesof,1123-1124
multiversion concurrency
control(MVCC)and,
1137-1146
operatorclasses and,1150
operatorstatementsand,1136
paralleldatabasesand,
816-817
performancetuning and,1042
pointersand,1134,1147-1148
procedurallanguagesand,
1136
queryoptimizationand,582,
593
queryprocessing and,
1151-1154
recoveryand,718
rollbacksand,1142-1144
rulesand,1130-1131
serializability and,1142-1143
serverprogramming
interface,1136
sortand,1153
SQLbasicsand,140,160,173,
180,184
statetransitionand,1134
storageand,1146-1151
systemarchitecture,1154-1155
systemcatalogsand,1132
transactionmanagementin,
649,653,1137-1146
treesand,1148-1149
triggersand,1153-1154
trusted/untrustedlanguages
and,1136
tupleIDand,1147-1148
tuplevisibility and,1139
types,1126-1129,1132-1133
updatesand,1130,1141-1144,
1147-1148
userinterfaces,1124-1126
vacuum,1143
precedence graph,644
precision, 903
predicatereads,697-701
prediction
classi?ers and,894-904
datamining and,894-904
joinsand,1267
preparedstatements,162-164
presentationfacilities,
1094-1095
presentationlayer,391
prestigeranking, 920-925,
930-931
primarycopy, 840
1336 Index
primarykeys, 45-46,60-62
decomposition and,354-355
entity-relationship (E-R)
modeland,271-272
functionaldependenciesand,
330-333
integrity constraints and,
130-131
primarysite,756
privacy,402,410-411,418,828,
869-870,1104
privileges
all, 143-144
executeand,147
grantingof,143-145
public, 144
revokingof,143-145,149-150
transferof,148-149
procedural DMLs, 10
procedural languages,20
advancedSQL and,157-158,
173,178
IBM DB2and,1194
Oracle and,1160,1191
PostgreSQL and,1130,1133,
1136
relationalmodeland,47-48
procedures
declaring, 174-175
externallanguageroutines
and,179-180
language constructsfor,
176-179
syntaxand,173-174,178
writing in SQL, 173-180
producer-driven pipeline,
569-570
program globalarea(PGA),
1183
programming languages.See
also speci?clanguage
accessing SQL from,157-173
mismatch and,158
variable operationsof,158
projection
intraoperationparallelism
and,811
Oracle and,1187
queriesand,564,597
view maintenanceand,
609-610
project-joinnormalform
(PJNF), 360
projectoperation,219
PR quadtrees,1073
pseudotransitivityrule,339
public-key encryption, 412-414
publishing, 1013,1251-1253
pullingdata,570-571
purity,897
Python, 180,377,387,1123,
1125,1136
QBE,37,245,770
quadraticsplit,1075-1076
quadtrees,1069,1072-1073
queries,10
ADO.NET and,169
availability and,826-827
B+-treesand,488-491
basicstructure ofSQL, 63-71
caching and,400-401
Cartesianproduct and,50-51,
68-69,71-75,120,209,
217,222-229,232,573,
584,589,595-596,606,616
complex datatypesand,
946-949
correlatedsubqueriesand,93
data-de?nition language
(DDL)and,21-22
data-manipulationlanguage
(DML) and,21-22
decision-support, 797
deleteand,98-100
distributed databasesand,
825-878(seealso
distributeddatabases)
hashingand,475,516-522(see
also hashfunctions)
indices and,475(seealso
indices)
informationretrievaland,
915-938
insert and,100-101
intermediateSQL and,
113-151
JDBC and,158-166
location-dependent,1080
metadataand,164-166
multiple-key access and,
506-509
onmultiple relations,66-71
naturaljoinsand,71-74,87,
113-120(seealso joins)
nearest-neighbor,1070-1071
nestedsubqueries,90-98
null valuesand,83-84
object-baseddatabasesand,
945-975
ODBCand,166-169
OLAPand,197-209
Oracleand,1171-1172
PageRankand,922-923
paralleldatabasesand,
797-820
persistentprogramming
languagesand,964-972
point,799
programminglanguage
accessand,157-173
range,799
readonly,804
recursive, 187-192
result diversity and, 932
ResultSetobjectand,159,161,
164-166,393,397-398,490
retrieving results,161-162
scalar subqueriesand,97-98
security and,402-417
servletsand,383-391
setoperationsand,79-83,
90-93
onsingle relation,63-66
spatialdataand,1070-1071
stringoperationsand,76-79
transactionserversand,775
universalTuringmachine
and,14
userrequirementsand,
311-312
viewsand,120-128
XMLand,998-1008
query cost
Microsoft SQL Serverand,
1237-1239
optimizationand,580-581,
590-602
processing and,540-541,544,
548,555-557,561
queryevaluationengine,22
query-evaluationplans,
537-539
choiceof,598-607
expressionsand,567-572
materializationand,567-568
optimizationand,579-616
pipeliningand,568-572
responsetime and,541
setoperationsand,564
viewing,582
query-executionengine,539
query-executionplan,539
Index 1337
query languages,249.Seealso
speci?c language
accessing froma
programming language,
157-173
centralized systemsand,
770-771
domain relationalcalculus
and,245-248
expressive powerof
languages,244,248
formalrelational,217-248
nonprocedural,239-244
procedural,217-239
relationalalgebraand,
217-239
relationalmodeland,47-48,
50
temporal,1064
tuple relationalcalculus and,
239-244
queryoptimization,22,537,
539,552-553,562,616
access pathselection and,
1174-1176
aggregationand,597
cost analysisand,580-581,
590-602
distributed databasesand,
854-855
equivalence and,582-588
estimating statistics of
expression results,
590-598
heuristics in,602-605
IBM DB2and,1211-1212
joinminimization,613
materializedviewsand,
607-612
Microsoft SQL Serverand,
1236-1241
multiquery, 614
nestedsubqueriesand,
605-607
Oracle and,1173-1178
paralleldatabasesand,
814-817
parametric,615
parallelexecutionand,
1178-1179
partialsearchand,1240
partitionsand,1174-1176
planchoicefor,598-607
PostgreSQL and,1151-1154
processstructure and,1179
relationalalgebraand,
579-590
result caching and,1179-1180
set operationsand,597
sharedscansand,614
simpli?cation and,1237-1238
SQL Plan Managementand,
1177-1178
SQL TuningAdvisor and,
1176-1177
top-K,613
transformationsand,582-590,
1173-1174
updatesand,613-614
queryprocessing, 21-22,30,32
aggregation,566-567
basicstepsof,537
binding and,1236-1237
comparisonsand,544-545
compilationand,1236-1237
cost analysisof,540-541,544,
548,555-557,561
CPUspeedsand,540
distributed databasesand,
854-857,859-860
distributed heterogeneous,
1250-1251
duplicate elimination and,
563-564
evaluationofexpressions,
567-572
executormoduleand,
1152-1153
?le scan and,541-544,550,
552,570
hashing and,557-562
IBM DB2and,1207-1216
identi?ers and,546
informationretrievaland,
915-937
joinoperationand,549-566
LINQ and,1249
materializationand,567-568,
1212-1214
Microsoft SQL Serverand,
1223-1231,1236-1241,
1250-1251
mobile,1082
operationevaluationand,
538-539
Oracle and,1157-1158,
1172-1180
parsingand,537-539,572-573,
1236-1237
pipelining and,568-572
PostgreSQLand,1151-1154
projectionand,563-564
recursive partitioning and,
539-540
relationalalgebraand,
537-539
reorderingand,1238-1239
selectionoperationand,
541-546
setoperationsand,564-565
sortingand,546-549
SQLand,537-538
standardplannerand,1152
syntaxand,537
transformationand,854-855
triggersand,1153-1154
XMLand,1259-1260
questionanswering, 933-934
queueingsystems,1034-1035
quorum consensus protocol,
841-842
random access,437
random samples,593
random walkmodel,922
range-partitioningsort,805
range-partitioningvector,801
rangequeries,799
ranking,192-195
rapidapplicationdevelopment
(RAD)
functionslibrary and,396
reportgeneratorsand,399-400
userinterface building tools
and,396-398
Webapplicationframeworks
and,398-399
rasterdata,1069
RationalRose,1194
read-ahead,437
readcommitted
applicationdevelopmentand,
1042
Microsoft SQL Server and,
1242
Oracle and,1181
PostgreSQLand,1138,
1141-1142
transactionmanagementand,
649,658,685,701-702
readone,writeallavailable
protocol, 849-850
readone,writeallprotocol,849
readonlyqueries,804
readquorum, 841-842
1338 Index
readuncommitted,648
read-writecontention,
1041-1042
read/writeoperations,653-654
real,double precision,59
real-timetransactionsystems,
1108-1109
recall,903
recoveryinterval,1244-1245
recoverymanager,22-23
recoverysystems,186,631,
760-761,1083
actionsaftercrash,736-738
algorithmfor,735-738
ARIES, 750-756
atomicity and,726-735
buffermanagementand,
738-743
checkpointsand,734-735,
742-743
concurrency controland,
729-730
dataaccessand,724-726
databasemirroring and,
1245-1246
databasemodi?cation and,
728-729
disk failure and,722
distributed databasesand,
835-836
earlylockreleaseand,744-750
fail-stop assumptionand,722
failure and,721-723,743-744
force/no-forcepolicy and,
739-740
IBM DB2and,1200-1203,
1217-1218
logical undooperationsand,
744-750
log recordsand,726-728,
730-734,738-739
log sequencenumber(LSN)
and,750
long-duration transactions
and,1110
Microsoft SQL Serverand,
1241-1246
Oracle and,1180-1183
partitionsand,1169-1172
PostgreSQL and,1145-1146
redo and,729-738
remotebackup,723,756-759,
850,1095-1096
rollback and,729-734,736
shadow-copyschemeand,
727
snapshotisolationand,
729-730
steal/no-stealpolicy and,740
storageand,722-726,734-735,
743-744
successful completionand,
723
undo and,729-738
work?owsand,1101
write-aheadlogging (WAL)
rule and,739-741,
1145-1146
recoverytime,758
recursive partitioning,539-540
recursive queries,187
iterationand,188-190
SQL and,190-192
transitiveclosureand,188-190
recursive relationshipsets,265
redo
actionsaftercrash,736-738
pass,754
phase,736-738
recoverysystemsand,729-738
redundancy, 4,261-262,272-274
redundant arraysof
independent disks
(RAID),435,759,1147
bit-levelstriping, 442-444
error-correcting-code (ECC)
organizationand,444-445
hardwareissues, 448-449
hotswappingand,449
levels, 444-448
mirroring and,441-442,444
parallelism and,442-444
paritybitsand,444-446
performancereliability and,
442-444
performancetuning and,
1037-1038
recovery systemsand,723
reliability improvementand,
441-442
scrubbing and,448
softwareRAIDand,448
striping dataand,442-444
references,131-133,148
referencingnewrowas,181-182
referencingnewtableas,183
referencing oldrowas,182
referencingoldtableas,183
referencingrelation,46
referentialintegrity,11,46-47,
131-136,151,181-182,628
referrals,875
re?exivityrule,339
regionquadtrees,1073
regression,902-903,1048-1049
relationalalgebra,51-52,
248-249,427
aggregatefunctions,235-239
assignment,232
avg,236
Cartesian-product,222-226
compositionofrelational
operationsand,219-220
count-distinct,236
equivalenceand,582-588,
601-602
expressiontransformation
and,582-590
expressive powerof
languages,244
formalde?nitions of,228
fundamentaloperations,
217-228
generalized-projection,235
joinexpressions, 239
max,236
min,236
multiset, 238
natural-join,229-232
outer-join,232-235
projectoperation,219
queryoptimizationand,
579-590
queryprocessing and,537-539
rename,226-228
selectoperation,217-219
semijoinstrategyand,856-857
set-difference,221-222
set-intersection,229
SQLand,219,239
sum,235-236
unionoperation,220-221
relationaldatabasedesign,368
atomicdomainsand,327-329
attributenaming,362-363
decompositionand,329-338,
348-360
design processand,361-364
featuresofgood,323-327
?rstnormalformand,327-329
fourthnormalformand,356,
358-360
functionaldependenciesand,
329-348
Index 1339
larger schemasand,324-325
multivalued dependencies
and,355-360
normalizationand,361-362
relationship naming,362-363
secondnormalformand,
336n5,361
smaller schemasand,325-327
temporaldatamodelingand,
364-367
third normalformand,
336-337
relationaldatabases
access fromapplication
programsand,14-15
data-de?nition languageand,
14
data-manipulationlanguage
(DML) and,13-14
storageand,1010-1014
tablesand,12-13
relationalmodel,9
disadvantages of,30
domain and,42
keysand,45-46
naturaljoinsand,49-50
operationsand,48-52
querylanguagesand,47-48,
50
referencingrelationand,46
schema for,42-47,302-304,
1012
structure of,39-42
tablesfor,39-44,49-51,
202-205
tuplesand,40-42,49-50
relationinstance,42-45,264
relationshipsets
alternativenotationsfor,
304-310
atomicdomainsand,327-329
attributeplacementand,
294-295
binary vs.n-ary,292-294
descriptive attributes,267
design issuesand,291-295
entity-relationship diagrams
and,278-279
entity-relationship (E-R)
modeland,264-267,
286-290,296-297
entitysetsand,291-292
namingof,362-363
nonbinary,278-279
recursive, 265
redundancy and,288
representationof,286-290
schema combinationand,
288-290
superclass-subclass, 296-297
Uni?ed Modeling Language
(UML) and,308-310
relativedistinguished names
(RDNs), 872
relevance
adjacency testand,922-923
hubsand,924
PageRankand,922-923,925
popularity rankingand,
920-922
ranking using TF-IDF,
917-920,925
search enginespamming and,
924-925
similarity-based retrievaland,
919-920
TF-IDFapproachand,
917-925,928-929
using hyperlinksand,3421
Web crawlersand,930-931
relevancefeedback,919-920
remotebackupsystems, 723,
756-759,850,1095-1096
remote-procedure-call(RPC)
mechanism,1096
renameoperation,75-76,
226-228
repeat,176
repeatableread,649
repeatloop,188,341,343,490
replication
cloudcomputingand,866-868
distributed databasesand,
843-844
Microsoft SQL Serverand,
1251-1253
system architecturesand,785,
826,829
report generators,399-400
RepresentationStateTransfer
(REST),395
request forgery,403-405
request operation
deadlock handling and,
675-679
locks and,662-671,675-680,
709
lookup and,706
multiple granularity and,
679-680
multiversionschemesand,
691
snapshotisolationand,693
timestampsand,682
resource managers,1095
response time
applicationdesign and,400,
1037,1046
concurrency controland,688
E-Rmodeland,311
Microsoft SQL Server and,
1261
Oracle and,1176-1177,1190
queryevaluationplansand,
541
queryprocessing and,541
storageand,444,1106,
1109-1110
transactionsand,636
systemarchitecture and,778,
798,800,802
restriction,149-150,347
ResultSetobject,159,161,
164-166,393,397-398,490
revoke,145,149
rightouterjoin,117-120,
233-235,565-566
Rijndaelalgorithm,412-413
robustness, 847
roles,264-265
authorizationand,145-146
entity-relationshipdiagrams,
278
Uni?ed Modeling Language
(UML) and,308-310
rollback,173
ARIESand,754-755
cascading, 667
concurrency controland,667,
670,674-679,685,689,
691,709
IBMDB2and,1218
logicaloperationsand,
746-749
PostgreSQLand,1142-1144
recoverysystems and,
729-734,736
remotebackupsystemsand,
758-759
transactionsand,736
timestampsand,685-686
undoand,729-734
rollbackwork, 127
rollup,201,206-210,1221-1222
RosettaNet,1055
1340 Index
rowtriggers,1161-1162
R-trees,1073-1076
RubyonRails,387,399
runstats,593
SAS(SerialattachedSCSI),434
Sarbanes-OxleyAct, 1248
SATA(serialATA),434,436
savepoints,756
scalarsubqueries, 97-98
scaleup,778-780
scheduling
Microsoft SQL Serverand,
1254-1255
PostgreSQL and,1127
queryoptimizationand,
814-815
storageand,437
transactionsand,641,
1099-1100,1108
schema de?nition,28
schema diagrams,46-47
schemas,8
alternativenotationsfor
modelingdata,304-310
authorizationon,147-148
basicSQL querystructures
and,63-74
canonicalcover and,342-345
catalogsand,142-143
combinationof,288-290
concurrency controland,
661-710(seealso
concurrency control)
data-de?nition language
(DDL)and,58,60-63
datamining, 893-910
datawarehouses,889-893
entity-relationship (E-R)
modeland,262-313
functionaldependenciesand,
329-348
generalizationand,297-304
larger,324-325
locks and,661-686
performancetuning of,
1038-1039
physical-organization
modi?cation and,28
recoverysystemsand,721-761
reductiontorelational,
283-290
redundancy of,288
relationalalgebraand,
217-239
relationaldatabasedesign
and,323-368
relationalmodeland,42-47
relationship setsand,286-288
shadow-copy,727
smaller, 325-327
strongentitysetsand,283-285
timestampsand,682-686
tuple relationalcalculus,
239-244
version-numbering,1083-1084
weakentitysetsand,285-286
XMLdocuments,990-998
scripting languages,389
scrubbing, 448
searchenginespamming,
924-925
searchkeys
hashingand,509-519,524
indexing and,476-509,524,
529
nonunique,497-499
storageand,457-459
uniqui?ers and,498-499
secondary site,756
second normalform,361
Secure ElectronicTransaction
(SET) protocol,1105
security,5,147
abstractionand,6-8,10
application design and,
402-417
audit trailsand,409-410
authenticationand,405-407
authorizationand,11,21,
407-409
concurrency controland,
661-710(seealso
concurrency control)
cross sitescripting and,
403-405
dictionary attacksand,414
encryptionand,411-417,
1165-1166
end-user informationand,
407-408
GET methodand,405
integrity managerand,21
isolationand,628,635-640,
646-653
keysand,45-46
locks and,661-686(seealso
locks)
long-duration transactions
and,1109-1115
man-in-the-middleattacks
and,406
Microsoft SQL Serverand,
1247-1248
observableexternalwrites
and,634-635
Oracleand,1165-1166
passwordsand,142,160,168,
170,376,382,385,
393,405-407,415,463-464,871
person-in-the-middleattacks
and,1105
physicaldataindependence
and,6
privacyand,402,410-411,418,
828,869-870,1104
remotebackupsystemsand,
756-759
requestforgeryand,403-405
single sign-onsystem and,
406-407
SQLinjection and,402-403
uniqueidenti?cation and,
410-411
virtualprivatedatabaseand,
1166
Security AssertionMarkup
Language (SAML), 407
seektimes,433,435-439,
450-451,540,555
select,363
aggregatefunctionsand,
84-90
attributespeci?cation,77
basicSQL queriesand,63-74
onmultiple relations,66-71
naturaljoinand,71-74
nullvaluesand,83-84
privilegesand,143-145,148
rankingand,194
renameoperationand,74-75
setmembershipand,90-91
setoperationsand,79-83
onsingle relation,63-65,63-66
stringoperationsand,76-79
selectall,65
selectdistinct,64-65,84-85,91,
125
select-from-where
deleteand,98-100
function/procedure writing
and,174-180
inheritanceand,949-956
insertand,100-101
Index 1341
join expressionsand,71-74,
87,113-120
naturaljoinsand,71-74,87,
113-120
nestedsubqueriesand,90-98
transactionsand,651-654
typeshandling and,949-963
updateand,101-103
viewsand,120-128
selection
comparisonsand,544-545
complex,545-546
conjunctive, 545-546
disjunctive, 545-546
equivalence and,582-588
?le scansand,541-544,550,
552,570
identi?ers and,546
indices and,541-544
intraoperationparallelism
and,811
linearsearchand,541-542
relationalalgebraand,
217-219
SQL and,
view maintenanceand,
609-610
SemanticWeb,927
semistructureddatamodels,9,
27
sensitivity,903
Sequel,57
sequence associations,906-907
sequence counters, 1043
sequential-accessstorage,431,
436
sequential?les,459
sequentialscans, 1153
serializability
blind writesand,687
concurrency controland,662,
666-667,671,673,681-690,
693-697,701-704,708
con?ict,641-643
distributed databasesand,
860-861
isolationand,648-653
Oracle and,1181-1182
orderof,644-646
performancetuning and,1042
PostgreSQL and,1142-1143
precedencegraphand,644
predicate readsand,701
intherealworld,650
snapshotisolationand,
693-697
topologicalsortingand,
644-646
transactionsand,640-646,648,
650-653
view, 687
serializableschedules, 640
serverprogramminginterface
(SPI),1136
server-side scripting,386-388
serversystems
categorizationof,772-773
client-server, 771-772
cloud-based,777
dataservers,773,775-777
transaction-server,773-775
servlets
client-side scripting and,
389-391
exampleof,383-384
life cycle and,385-386
server-side scripting and,
386-388
sessions and,384-385
supportand,385-386
setclause,103
setdefault,133
setdifference, 50,221-222,585
set-intersection,2229
setnull,133
setoperations,79,83
IBM DB2and,1209-1210
intersect,50,81-82,585,960
nestedsubqueriesand,90-93
queryoptimizationand,597
queryprocessing and,564-565
set comparisonand,91-93
union,80-81,220-221,339,585
setrole,150
settransactionsisolationlevel
serializable,649
shadow-copy scheme,727
shadowing, 441-442
shadow-paging, 727
sharedandintention-exclusive
(SIX)mode,680
shared-diskarchitecture,781,
783,789
shared-memoryarchitecture,
781-783
shared-mode locks,661
shared-nothing architecture,
781,783-784
sharedscans, 614
Sherpa/PNUTS, 866-867
shredding, 1013,1258-1259
similarity-basedretrieval,
919-920,1079
SimpleAPI for XML(SAX),
1009
SimpleObject Access Protocol
(SOAP), 1017-1018,1056,
1249-1250
singlelock-manager,839-840
single-servermodel,1092-1093
single-valuedattributes,
267-268
sitereintegration,850
skew, 512
attribute-value,800-801
paralleldatabasesand,
800-801,805-808,812,
814,819
parallelsystemsand,780
partitioningand,560,800-801
slicing,201
small-computer-system
interconnect (SCSI),434
snapshot isolation,652-653,
704,1042
Microsoft SQL Server and,
1244
recoverysystemsand,729-730
serializability and,693-697
validationand,692-693
snapshot replication,1252-1253
snapshots
DMLcommandsand,
1138-1139
Microsoft SQL Server and,
1242
multiversion concurrency
control(MVCC)and,
1137-1146
PostgreSQLand,1137-1146
readcommitted,1242
software RAID,448
Solaris,1193
sold-statedrives,430
some,90,92,92n8
sorting,546
costanalysisof,548-549
duplicate elimination and,
563-564
externalsort-mergealgorithm
and,547-549
parallelexternalsort-merge
and,806
PostgreSQLand,1153
1342 Index
range-partitioning,805
topological,644-646
XMLand,1106
sort-merge-join,553
space overhead,476,479,486,
522
spatialdata
computer-aided-designdata
and,1061,1064-1068
geographicdataand,1061,
1064-1066
indexing of,1071-1076
queriesand,1070-1071
representationofgeometric
informationand,
1065-1066
topographicalinformation
and,1070
triangulation and,1065
vector dataand,1069
specialization
entity-relationship (E-R)
modeland,295-296
partial, 300
single entitysetand,298
total,300
specialtydatabases,943
object-baseddatabasesand,
945-975
XMLand,981-1020
speci?cationof functional
requirements,16,260
speci?city, 903
speedup, 778-780
spidertraps,930
SQL(Structured Query
Language),10,13-14,57,
151,210,582
accessing froma
programming language,
157-163
advanced, 157-210
aggregatefunctions,84-90,
192-197
application-level
authorizationand,
407-409
applicationprogramsand,
14-15
arraytypesand,956-961
authorizationand,58,143-150
basictypesand,59-60
blobsand,138,166,457,502,
1013,1198-1199,1259
bulkloadsand,1031-1033
catalogs,142-143
clobsand,138,166,457,502,
1010-1013,1196-1199
CLR hostingand,1254-1256
createtable,60-63,141-142
databasemodi?cation and,
98-103
data-de?nition language
(DDL)and,57-63,104
data-manipulationlanguage
(DML) and,57-58,104
datamining and,26
date/timetypesin, 136-137
decision-support systems
and,887-889
default valuesand,137
deleteand,98-100
dumping and,743-744
dynamic, 58,158
embedded,58,158,169-173,
773
Entity, 395
environments,43
functionwritingand,173-180
IBM DB2and,1195-1200,1210
indexcreationand,137-138,
528-529
inheritance and,949-956
injectionand,402-403
insert and,100-101
integrity constraintsand,58,
128-136
intermediate,113-151
isolationlevels and,648-653
JDBC and,158-166
join expressionsand,71-120
(see also joins)
lack of?ne-grained
authorizationand,
408-409
large-typeobjects,138
ManagementofExternalData
(MED) and,1077
Microsoft SQL Serverand,
1223-1267
multiset typesand,956-961
MySQL and,31,76,111,
160n3,1123,1155
nestedsubqueriesand,90-98
nonstandardsyntaxand,178
null valuesand,83-84
object-baseddatabasesand,
945-975
ODBC and,166-169
OLAP and,197-209
Oraclevariationsand,
1158-1162
overviewof,57-58
persistentprogramming
languagesand,964-972
PostgreSQLand,31(seealso
PostgreSQL)
preparedstatementsand,
162-164
procedurewriting and,
173-180
queryprocessing and,537-538
(seealso query
processing)
rapidapplication
development(RAD)and,
397
relationalalgebraand,219,
239
renameoperationand,74-80
reportgeneratorsand,399-400
ResultSetobjectand,159,161,
164-166,393,397-398,490
revokingofprivileges and,
149-150
rolesand,145-146
schemasand,47,58-63,
141-143,147-148
security and,402-403
selectclause and, 77
setoperationsand,79-83
asstandardrelational
databaselanguage,57
standardsfor,1052-1053
stringoperationsand,76-77
SystemRand,30,57
timespeci?cation in,
1063-1064
transactionsand,58,127-128,
773(seealsotransactions)
transferofprivileges and,
148-149
triggersand,180-187
tuplesand,77-78(seealso
tuples)
underprivilege and,956
updateand,101-103
user-de?nedtypes,138-141
viewsand,58,120-128,
146-147
whereclausepredicates,78-79
SQLLoader, 1032,1189
SQL AccessGroup, 1053
SQL/DS, 30
SQL environment,143
Index 1343
SQLJ, 172
SQLPlanManagement,
1177-1178
SQLPro?ler,1225-1227
SQLSecurity Invoker, 147
SQLServerAnalysisServices
(SSAS), 1264,1266-1267
SQLServerBroker,1261-1263
SQLServerIntegration
Services(SSIS),
1263-1266
SQLServerManagement
Studio, 1223-1224,
1227-1228
SQLServerReportingServices
(SSRS),1264,1267
sqlstate,179
SQLTransparent Data
Encryption, 1248
SQLTuning Advisor,1176-1177
SQL/XML standard,1014-1015
StandardGeneralizedMarkup
Language (SGML), 981
standards
ANSI, 57,1051
anticipatory,1051
CallLevelInterface(CLI),
1053
databaseconnectivity,
1053-1054
datapumpexport/import
and,1189
DBTGCODASYL, 1052
ISO, 57,871,1051
ODBC, 1053-1055
reactionary,1051
SQL, 1052-1053
Wi-Max, 1081
XML, 1055-1056
X/OpenXA,1053-1054
Starburst, 1193
start-upcosts, 780
starvation,679
Statementobject,161-164
statementtriggers,1161-1162
statetransition,1134
statevalue,1134
statistics
cataloginformationand,
590-592
computing,593
join sizeestimationand,
595-596
maintaining, 593
numberofdistinct values
and,597-598
queryoptimizationand,
590-598
randomsamplesand,593
selection size estimationand,
592-595
stealpolicy,740
steps,1096
stopwords, 918
storage,427
archival, 431
atomicity and,632-633
authorizationand,21
AutomaticStorageManager
and,1186-1187
backup,431,723,756-759,850,
1095-1096
bit-levelstriping, 442-444
buffermanagerand,21(see
also buffers)
byteamountand,20
checkpointsand,734-735,
742-743
clob valuesand,1010-1011
cloud-based,777,862-863
column-oriented,892-893
contentdumpand,743
cost perbit,431
crashesand,467-468(seealso
crashes)
dataaccessand,724-726
data-dictionary, 462-464
datamining and,25-26,
893-910
data-transferrateand,435-436
datawarehousesand,888
decision-storage systems and,
887-889
direct-access, 431
distributed databasesand,
826-830
distributed systemsand,
784-788
dumping and,743-744
durability and,632-633
error-correcting-code (ECC)
organizationand,444-445
Exadataand,1187-1188
?le managerand,21
?le organizationand,451-462
?ash,403,430,439-441,506
?at?lesand,1009-1010
forceoutputand,725-726
fragmentationand,826-829
harddisks and,29-30
IBMDB2and,1200-1203
indicesand,21(seealso
indices)
informationretrievaland,
915-937
integritymanagerand,21
jukebox,431
magneticdisk, 430,432-439
mainmemoryand,429-430
Microsoft SQL Server and,
1233-1236
mirroringand,441-442,
1245-1246
native,1013-1014
nonrelationaldata,1009-1010
nonvolatile,432,632,722,
724-726,743-744
optical,430-431,449-450
Oracle and,1162-1172,
1186-1188
parallelsystemsand,777-784
persistentprogramming
languagesand,967-968
physicalmedia for,429-432
PostgreSQLand,1146-1151
publishing/shredding data
and,1013,1258-1259
punchedcardsand,29
queryprocessor and,21-22
recoverysystemsand,722-726
(see also recovery
systems)
redundantarraysof
independentdisks
(RAID),435,441-449
relationaldatabasesand,
1010-1014
remotebackupsystemsand,
723,756-759,850,
1095-1096
replicationand,826,829
scrubbingand,448
seektimesand,433,435-439,
450-451,540,555
segmentsand,1163
sequential-access,431,436
solid-statedrives and,430
stable,632,722-724
striping dataand,442-444
tape,431,450-451
tertiary,431,449-451
transactionmanagerand,21
(seealso transactions)
transparencyand,829-830
1344 Index
volatile, 431,632,722
walletsand,415
XMLand,1009-1016
storageareanetwork(SAN),
434-435,789
storagemanager,20-21
string operations
aggregate,84
attributespeci?cation,77
escape,77
JDBCand,158-166
like, 76-77
lower,76
queryresult retrievaland,
161-162
similar to,77
trim,76
tuple displayorder,77-78
upperfunction,76
wherepredicates,78-79
stripingdata,442-444
structuredtypes,138-141,
949-952
stylesheets,380
sublinearspeedup,778-780
submultiset,960
suf?x, 874
sum,84,123,207,235-236,
566-567,1134
superclass-subclass
relationship,296-297
superkeys, 45-46,271-272,
330-333
superuser, 143
Support Vector Machine
(SVM), 900-901,1191
swapspace,742
Swing, 399
Sybase,1223
symmetricmultiprocessors
(SMPs), 1193
synonyms, 925-927
sysaux,1172-1173
systemarchitecture.See
architectures
systemcatalogs,462-464,1132
systemchange number (SCN),
1180-1181
systemerror,721
SystemR,30,57,1193
tableinheritance,954-956
tables,12-13
?ltering and,1187
IBM DB2and,1200-1203
materialized,1212-1214
Microsoft SQL Serverand,
1230,1234
.NET CommonLanguage
Runtime(CLR)and,
1257-1258
Oracle and,1163-1166,1187,
1189
partitionsand,1169-1172
relationalmodeland,39-44,
49-51
SQL Server Brokerand,1262
tablespaces,1146,1172-1173
taglibrary,388
tag
application design and,
378-379,388,404
informationretrievaland,916
XMLand,982-986,989,994,
999,1004,1019
tapestorage,431,450-451
Tapestry,399
task ?ow. Seework?ows
Tcl,180,1123-1125,1136
temporaldata,1061
intervals and,1063-1064
querylanguagesand,1064
relationaldatabasesand,
364-367
time indatabasesand,
1062-1064
timestampsand,1063-1064
transactiontime and,1062
temporalrelation,1062-1063
TeradataPurpose-Built
PlatformFamily,806
termfrequency(TF),918
terminationstates,1099
tertiarystorage,431,449-451
TF-IDFapproach,928-929
thetajoin,584-585
thirdnormalform(3NF)
decomposition algorithms
and,352-355
relationaldatabasesand,
336-337,352-355
Thomas’writerule,685-686
threadpooling,1246
three-phasecommit(3PC)
protocol, 826
three-tierarchitecture,25
throughput
application developmentand,
1037,1045-1046
de?ned,311
harmonicmeanof,1046
improved,635-636,655
logrecordsand,1106
mainmemoriesand,1116
Microsoft SQL Serverand,
1255
Oracleand,1159,1184
parallelsystemsand,778
performanceand,1110
rangepartitioningand,800
storageand,444,468
systemarchitecturesand,771,
778,800,802,819
transactionsand,635-636,655
timestamps,136-167
concurrency controland,
682-686,703
distributeddatabasesand,
842-843
logicalcounterand,682
long-durationtransactions
and,1110
multiversionschemesand,
690-691
orderingschemeand,682-685
rollbackand,685-686
temporaldataand,1063-1064
Thomas’writerule and,
685-686
transactionsand,651-652
withtimezone,1063
timetocompletion,1045
timewithtimezone,1063
timezone,136-137,1063
Tomcat,386
top-down design,297
top-Koptimization,613
topographicinformation,1070
topologicalsorting,644-646
traininginstances, 895
transactionalreplication,
1252-1253
transactioncontrol,58
transactioncoordinator,
830-831,834-835,850-852
transactionmanager,21,23,
830-831
transaction-processing
monitors,1091
applicationcoordination
using, 1095-1096
architecturesof,1092-1095
durablequeueand,1094
many-server,many-router
modeland,1094
Index 1345
many-server,single-router
modeland,1093
multitasking and,1092-1095
presentationfacilitiesand,
1094-1095
single-server modeland,
1092-1093
switching and,1092
TransactionProcessing
Performance Council
(TPC),1046-1048
transactions,32,625,655-656,
1116
aborted,633-634,647
actionsaftercrash,736-738
active,633
advancedprocessing of,
1091-1116
association rulesand,904-907
atomicityand,22-23,628,
633-635,646-648(seealso
atomicity)
availability and,847-853
begin/endoperationsand,
627
cascadeless schedules and,
647-648
check constraintsand,628
cloudcomputingand,866-868
commit protocolsand,
832-838
committed,127,633-635,639,
647,692-693,730,758,
832-838,1107,1218
compensating,633,1113-1114
conceptof,627-629
concurrency controland,
661-710,1241-1246(see
also concurrency control)
consistency and,22,627-631,
635-636,640,648-650,655
(seealsoconsistency)
crashesand,628
datamining and,893-910
decision-storage systems and,
887-889
de?ned,22,627
distributed databasesand,
830-832
durability and,22-23,628,
633-635(seealso
durability)
E-commerce and,1102-1105
failure of,633,721-722
force/no-forcepolicy and,
739-740
global,784,830,860-861
integrity constraintviolation
and,133-134
isolationand,628,635-640,
646-653(seealso
isolation)
killed, 634
local, 784,830,860-861
locks and,661-669,661-686
(see also locks)
log recordsand,726-728,
730-734
long-duration,1109-1115
main-memorydatabasesand,
1105-1108
multidatabasesand,860-861
multilevel, 1112-1113
multitasking and,1092-1095
multiversion concurrency
control(MVCC)and,
1137-1146
multiversion schemesand,
689-692
object-baseddatabasesand,
945-975
observableexternalwrites
and,634-635
paralleldatabasesand,
797-820
performancetuning and,
1041-1044
persistentmessaging and,
836-837
persistentprogramming
languagesand,970
person-in-the-middleattacks
and,1105
PostgreSQL and,1137-1146
read/writeoperationsand,
653-654
real-timesystemsand,
1108-1109
recoverable schedules and,
647
recovery managerand,22-23
recovery systemsand,631,
633(seealsorecovery
systems)
remotebackupsystemsand,
756-759
restartof,634
rollback and,127,736,
746-749,754-755
serializability and,640-653
shadow-copyschemeand,
727
simple modelfor,629-631
SQL Server Brokerand,
1261-1263
asSQL statements,653-654
starved,666
statesof,633-635
steal/no-stealpolicyand,740
storagestructureand,632-633
timestampsand,682-686
two-phasecommitprotocol
(2PC)and,786-788
uncommitted,648
asunit ofprogram,627
validationand,686-689
wait-forgraphand,676-678
work?owsand,836-838,
1096-1102
write-aheadlogging (WAL)
ruleand,739-740,739-741
transactionscaleup,779
transactions-consistent
snapshot, 843-844
transaction-server systems,
773-775
transactionsper second(TPS),
1046-1047
transactiontime,365n8,1062
TransactSQL, 173
transferofcontrol,757
transferofprestige,921-922
transformations
equivalencerulesand,
583-586
examplesof,586-588
joinordering and,588-589
queryoptimizationand,
582-590
relationalalgebraand,
582-590
XMLand,998-1008
transitiontables,183-184
transitionvariable,181
transitiveclosure,188-190
transitivityrule,339-340
transparency, 829-830,854-855
trees,1086
B,504-506,530,1039,1064,
1071-1072,1076,1086,
1135,1148-1150,1159,
1164-1169,1173,1205
B+,12-34-1235(seealso
B+-trees)
1346 Index
decision-tree classi?ers and,
895-900
directoryinformation(DIT),
872-875
distributed directory,874-875
GeneralizedSearch Tree
(GiST) and,1148-1149
index-organizedtables(IOTs)
and,1164-1165
k-d,1071-1072
multiple granularity and,
679-682
Oracle and,1164-1165,1191
over?tting and,899-900
PostgreSQL and,1148-1149
quadraticsplit and,1075-1076
quadtrees,1069,1072-1073
queryoptimizationand,
814-815(seealso query
optimization)
R,1073-1076
scheduling and,814-815
spatialdatasupportand,
1064-1076
XML, 998,1011
triggers
alter,185
disable, 185
drop,185
IBM DB2and,1210
Microsoft SQL Serverand,
1232-1233
needfor,180-181
nonstandardsyntaxand,184
Oracle and,1161-1162
PostgreSQL and,1153-1154
recovery and,186
in SQL, 181-187
transitiontablesand,183-184
whennottouse,186-187
truenegatives,903
truepredicate,67
truerelation,90,93
tupleID, 1147-1148
tuplerelationalcalculus, 239,
249
examplequeries,240-242
expressive powerof
languages,244
formalde?nition, 243
safetyofexpressions,244
tuples,40-42
aggregatefunctionsand,
84-90
Cartesianproduct and,50
deleteand,98-100
domain relationalcalculus
and,245-248
duplicate, 94-95
eagergenerationof,569-570
insert and,100-101
joinsand,550-553(seealso
joins)
lazygenerationof,570-571
ordering display of,77-78
paralleldatabasesand,
797-820
pipelining and,568-572
PostgreSQL and,1137-1146
querystructuresand,68
queryoptimizationand,
579-616
queryprocessing and,537-573
rankingand,192-195
relationalalgebraand,
217-239,582-590
setoperationsand,79-83
updateand,101-103
viewsand,120-128
windowingand,195-197
tuplevisibility,1139
two-factorauthentication,
405-407
two-phase commit(2PC)
protocol, 786-788,
832-836
two-tierarchitecture,24-25
types,1017,1159
abstractdata,1127
array,956-961
base,1127
blob,138,166,457,502,1013,
1198-1199,1259
clob,138,166,457,502,
1010-1013,1196-1199
complexdata,946-949(see
also complexdatatypes)
composite,1127
documenttypede?nition
(DTD),990-994
enumerated,1128
IBM DB2and,1196-1197
inheritance and,949-956
Microsoft SQL Serverand,
1229-1230,1257-1258
most-speci?c, 953
multiset, 956-961
.NET CommonLanguage
Runtime(CLR)and,
1257-1258
nonstandard,1129-1130
object-baseddatabasesand,
949-963
object-identity,961-963
Oracleand,1158-1160
performancetuning and,1043
polymorphic,1128-1129
PostgreSQL,1126-1129,
1132-1133
pseudotypes,1128
reference,961-963
user-de?ned,138-141
single reference, 972
wide-area,788-791
XML,990-998,1006-1007
UDF. Seeuser-de?ned
functions
Ultra320SCSIinterface,436
Ultriumformat,451
underprivilege,956
undo
concurrency controland,
749-750
logicaloperationsand,
745-750
Oracleand,1163
recoverysystemsand,729-738
transactionrollback and,
746-749
undo pass, 754
undo phase, 737
Uni?edModeling Language
(UML), 17-18
associationsand,308-309
cardinality constraintsand,
309-310
componentsof,308
relationshipsetsand,308-309
uniform resource locators
(URLs),377-378
union, 80-81,585,220-221
unionall,80
unionrule,339
unique, 94-95
decompositionand,354-355
integrityconstraints and,
130-131
uniqui?er,498-499
UnitedStates,17,45,263,267n3,
411,788,858,869,922
Universal CoordinatedTime
(UTC),1063
Index 1347
UniversalDescription,
Discovery, and
Integration(UDDI),1018
UniversalSerialBus(USB)
slots,430
universal Turingmachine,14
universities,2
application design and,375,
392,407-409
concurrency controland,698
databasedesign and,16-17
databasesfor,3-8,11-12,
15-19,27,30
E-R modeland,261-274,280,
282,292,294-299
indexing and,477,510,529
queryoptimizationand,586,
589,605
queryprocessing and,566
recovery systemand,724
relationaldatabasedesign
and,323-330,334,355,
364-365
relationalmodeland,41,
43-48
SQL and,61-63,70-72,75,99,
125-134,145-150,153,
170,173,187,192-193,
197,226-227
storageand,452,458,460
system architecture and,785,
828,872
transactionsand,653
UniversityofCalifornia,
Berkeley,30,1123
Unix,77,438,713,727,1124,
1154,1193-1194,1212,
1223
unknown, 83,90
unnesting, 958-961
updatableresult sets,166
update-anywhere replication,
844
updates,101-103
authorizationand,147,148
B+-treesand,491-497,499-500
batch,1030-1031
complexityof,499-500
concurrency controland,
867-868
datawarehousesand,891
deletiontimeand,491,
495-500
distributed databasesand,
826-827
EXEC SQL and,171
hashing and,516-522
indices and,482-483
insertiontimeand,491-495,
499-500
log recordsand,726-734
lost,692
Microsoft SQL Serverand,
1232-1233,1239
mobile,1083-1084
Oracle and,1179-1180
performancetuning and,
1030-1033,1043-1044
persistentprogramming
languagesand,970
PostgreSQL and,1130,
1141-1144,1147-1148
privileges and,143-145
queryoptimizationand,
613-614
shipping SQLstatementsto
database,161
snapshotisolationand,
692-697
triggersand,182,184
viewsand,124-128
XMLand,1259-1260
user-de?ned entitysets,299
user-de?ned functions(UDFs),
1197-1198
user-de?ned types,138-141
userinterfaces,27-28
application architecturesand,
391-396
applicationprogramsand,
375-377
asback-endcomponent,376
business-logic layer and,
391-392
client-server architecture and,
32,204,376-377,756-772,
777,788,791
client-side scripting and,
389-391
cloudcomputingand,861-870
commongatewayinterface
(CGI),380-381
cookiesand,382-385,403-405
CRUD,399
dataaccesslayer and,391,
393,395
disconnected operationand,
395-396
HyperTextTransferProtocol
(HTTP)and,377-383,395,
404-406,417
IBMDB2,1195
mobile,1079-1085
persistentprogramming
languagesand,970
PostgreSQLand,1124-1126
presentationlayerand,391
reportgeneratorsand,399-400
security and,402-417
storageand,434(seealso
storage)
toolsforbuilding, 396-398
Webservicesand,395
WorldWide Weband,377-382
userrequirements,15-16,27-28
E-Rmodeland,260,298
performanceand,311-312
responsetime and,311
throughputand,311
using, 114
utilization,636
vacuum,1143
validation,703-704
classi?ers and,903-904
concurrency controland,
686-689
?rstcommitterwins and,
692-693
?rstupdaterwinsand,693
long-durationtransactions
and,1111
phasesof,688
recoverysystemsand,729-730
snapshotisolationand,
692-693
view serializability and, 687
validtime,365
varchar,59-60,62
VBScript,387
vectordata,1069
vectorspacemodel,919-920
version-numbering schemas,
1083-1084
version-vectorscheme,1084
verticalfragmentation,828
videoservers, 1078
viewde?nition, 58
viewmaintenance,608-611
views,120
authorizationon,146-147
withcheckoption,126
1348 Index
complex merging and,
1173-1174
createview and,121-125
cube,1221-1222
deferredmaintenanceand,
1039-1040
de?nition, 121-122
delete,125
immediate maintenanceand,
1039-1040
insert into,124-125
maintenance,124
materialized,123-124(seealso
materializedviews)
performancetuning and,
1039-1041
SQL queriesand,122-123
updateof,124-128
viewserializability,687
virtualmachines,777
virtualprocessor, 801
VirtualRealityMarkup
Language (VRML),
390-391
VisualBasic,169,180,397-398,
1228
VisualWeb,397
volatilestorage,431,632,722
wait-forgraph,676-678,845-847
Webcrawlers,930-931
Weblogic,386
WebObjects, 399
Webservers,380-382
Webservices,395,1199-1200
WebServicesDescription
Language (WSDL), 1018
WebSphere, 386
whenclause,181,184
where clause,311
aggregatefunctionsand,
84-90
basicSQL queriesand,63-74
between,78
onmultiple relations,66-71
naturaljoinand,71-74
notbetween,78
null valuesand,83-84
queryoptimizationand,
605-607
renameoperationand,74-75
security and,409
setoperationsand,79-83
onsingle relation,63-65,63-66
string operationsand,76-79
transactionsand,651-654
whileloop,168,171,176
wide-areanetworks(WANs),
788,790-791
Wi-Max,1081
windowing, 195-197
Windows Mobile,1223
Wirelessapplicationprotocol
(WAP),1081-1082
wirelesscommunications,
1079-1085
withcheckoption,126
withclause,97,190
withdata,141-142
withgrantoption,148
withrecursive clause,190
withtimezone,136
WordNet, 927
work?ows, 312-313,836-838,
1017
acceptableterminationstates
and,1099
bugsand,1101
business-logic layer and,
391-392
executionand,1097-1101
externalvariablesand,
1098-1099
failures and,1099-1102
managementsystemsfor,
1101-1102
multisystem applications and,
1096
nonacceptabletermination
statesand,1099
performanceand,1029-1048
recoveryof,1101
speci?cation and,1097-1099
stepsand,1096
tasksand,1096
transactional,1096-1102
workloadcompression, 1041
WorldWide Web,31,885
application design and,
377-382
cookiesand,382-385,403-405
encryptionand,411-417
HyperTextMarkupLanguage
(HTML),378-380
HyperTextTransferProtocol
(HTTP)and,377-381,383,
395,404-406,417
informationretrievaland,915
(seealso information
retrieval)
security and,402-417
services processing and, 395
Simple ObjectAccess Protocol
(SOAP) and,1017-1018
three-layerarchitectureand,
318
uniformresourcelocators
(URLs),377-378
Webapplicationframeworks
and,398-400
Webserversand,380-382
XMLand,1017-1018
WorldWide WebConsortium
(W3C),927,1056
wrapping, 1055-1056
write-aheadlogging(WAL),
739-7841,1145-1146
writeonce,read-many
(WORM) disks,431
writequorum, 841-842
write-writecontention,1042
X.500directoryaccess protocol,
871
XML(ExtensibleMarkup
Language),31,169,386,
1020
applicationprogram
interfaces(APIs) to,
1008-1009
applications,1016-1019
clobvaluesand,1010-1011
dataexchangeformatsand,
1016-1017
datamediation and,
1018-1019
datastructure,986-990
documentschema,990-998
documenttypede?nition
(DTD),990-994
asdominantformat,985
?leprocessing and,981-982
format?exibility of,985
HTMLand,981
IBMDB2and,1195-1196
joinsand,1003-1004
markupconceptand,981-985
Microsoft SQL Serverand,
1258-1261
nestingand,27,943,984-990,
995-998,1001,1004-1007,
1010
OracleXMLDBand,
1159-1160
Index 1349
publishing/shredding data
and,1013
queriesand,998-1008,
1259-1260
relationaldatabasesand,
1010-1014
relationalmapsand,1012
Simple ObjectAccess Protocol
(SOAP) and,1017-1018
sorting and,1006
SQL/XML standard and,
1014-1015
standards for,1055-1056
storageand,1009-1016
tagsand,982-986,989,994,
999,1004,1019
textualcontextand,986
transformationand,998-1008
treemodelof,998
updatesand,1259-1260
webservicesand,1017-1018
wrappingand,1055-1056
xmlagg,1015
xmlattributes,1015
xmlconcat,1015
xmlelement,1015
xmlforest,1015
XMLIndex,1160
XMLSchema,994-998
XMLType,1159
X/OpenXAstandards,
1053-1054
XORoperation,413
XPath,1160
documentschema and,997
queriesand,998-1002
storageand,1009-1015
XQuery,31,998
FLWOR expressions and,
1002-1003
functionsand,1006-1007
joinsand,103-104
Microsoft SQL Server and,
1260-1261
nestedqueriesand,1004-1005
Oracle and,1160
sortingofresultsand,1006
storageand,1009-1015
transformationsand,
1002-1008
typesand,1006-1007
XSLT,1160
Yahoo,390,863
YUIlibrary,390
